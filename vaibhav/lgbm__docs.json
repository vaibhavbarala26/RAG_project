[
{"chunk_id": "chunk_0", "url": "https://lightgbm.readthedocs.io/en/latest/index.html", "title": "Welcome to LightGBM’s documentation!", "page_title": "Welcome to LightGBM’s documentation! — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Welcome to LightGBM’s documentation!", "content": "Welcome to LightGBM’s documentation! LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: - Faster training speed and higher efficiency. - Lower memory usage. - Better accuracy. - Support of parallel, distributed, and GPU learning. - Capable of handling large-scale data. For more details, please refer to Features.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1", "url": "https://lightgbm.readthedocs.io/en/latest/index.html", "title": "Indices and Tables", "page_title": "Welcome to LightGBM’s documentation! — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Indices and Tables", "content": "Indices and Tables - Index", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_2", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Versioning", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Versioning", "content": "Versioning LightGBM releases use a 3-part version number, with this format: {major}.{minor}.{patch} This version follows a scheme called Intended Effort Versioning (“Effver” for short). Changes to a component of the version indicate how much effort it will likely take to update code using a previous version. - major= updating will require significant effort - minor= some effort - patch= no or very little effort This means that new minor versions can contain breaking changes, but these are typically small or limited to less-frequently-used parts of the project. When built from source on an unreleased commit, this version takes the following form: {major}.{minor}.{patch}.99 That .99 is added to ensure that a version built from an unreleased commit is considered “newer” than all previous releases, and “older” than all future releases. You can find such artifacts from the latest successful build on the master branch (nightly builds) here: . For more details on why LightGBM uses EffVer instead of other schemes like semantic versioning, see https://jacobtomlinson.dev/effver/.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_3", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "General Installation Notes", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "General Installation Notes", "content": "General Installation Notes All instructions below are aimed at compiling the 64-bit version of LightGBM. It is worth compiling the 32-bit version only in very rare special cases involving environmental limitations. The 32-bit version is slow and untested, so use it at your own risk and don’t forget to adjust some of the commands below when installing. By default, instructions below will use VS Build Tools or make tool to compile the code. It it possible to use Ninja tool instead of make on all platforms, but VS Build Tools cannot be replaced with Ninja. You can add -G Ninja to CMake flags to use Ninja. By default, instructions below will produce a shared library file and an executable file with command-line interface. You can add -DBUILD_CLI=OFF to CMake flags to disable the executable compilation. If you need to build a static library instead of a shared one, you can add -DBUILD_STATIC_LIB=ON to CMake flags. By default, instructions below will place header files into system-wide folder. You can add -DINSTALL_HEADERS=OFF to CMake flags to disable headers installation. By default, on macOS, CMake is looking into Homebrew standard folders for finding dependencies (e.g. OpenMP). You can add -DUSE_HOMEBREW_FALLBACK=OFF to CMake flags to disable this behaviour. Users who want to perform benchmarking can make LightGBM output time costs for different internal routines by adding -DUSE_TIMETAG=ON to CMake flags. It is possible to build LightGBM in debug mode. In this mode all compiler optimizations are disabled and LightGBM performs more checks internally. To enable debug mode you can add -DUSE_DEBUG=ON to CMake flags or choose Debug_* configuration (e.g. Debug_DLL, Debug_mpi) in Visual Studio depending on how you are building LightGBM. In addition to the debug mode, LightGBM can be built with compiler sanitizers. To enable them add -DUSE_SANITIZER=ON -DENABLED_SANITIZERS=\"address;leak;undefined\" to CMake flags. These", "prev_chunk_id": "chunk_2", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_4", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "General Installation Notes", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "General Installation Notes", "content": "values refer to the following supported sanitizers: - address- AddressSanitizer (ASan); - leak- LeakSanitizer (LSan); - undefined- UndefinedBehaviorSanitizer (UBSan); - thread- ThreadSanitizer (TSan). Please note, that ThreadSanitizer cannot be used together with other sanitizers. For more info and additional sanitizers’ parameters please refer to the following docs. It is very useful to build C++ unit tests with sanitizers.", "prev_chunk_id": "chunk_3", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_5", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, LightGBM can be built using - Visual Studio; - CMakeandVS Build Tools; - CMakeandMinGW.", "prev_chunk_id": "chunk_4", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_6", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "With GUI", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "With GUI", "content": "With GUI - InstallVisual Studio. - Navigate to one of the releases athttps://github.com/microsoft/LightGBM/releases, downloadLightGBM-complete_source_code_zip.zip, and unzip it. - Go toLightGBM-complete_source_code_zip/windowsfolder. - OpenLightGBM.slnfile withVisual Studio, chooseReleaseconfiguration if you need executable file orDLLconfiguration if you need shared library and clickBuild->BuildSolution(Ctrl+Shift+B).If you have errors aboutPlatform Toolset, go toProject->Properties->ConfigurationProperties->Generaland select the toolset installed on your machine. The .exe file will be in LightGBM-complete_source_code_zip/windows/x64/Release folder. The .dll file will be in LightGBM-complete_source_code_zip/windows/x64/DLL folder.", "prev_chunk_id": "chunk_5", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_7", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "From Command Line", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "From Command Line", "content": "From Command Line - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois already installed). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64cmake --build build --target ALL_BUILD --config Release The .exe and .dll files will be in LightGBM/Release folder.", "prev_chunk_id": "chunk_6", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_8", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "MinGW-w64", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MinGW-w64", "content": "MinGW-w64 - InstallGit for Windows,CMakeandMinGW-w64. - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -G \"MinGW Makefiles\"cmake --build build -j4 The .exe and .dll files will be in LightGBM/ folder. Note: You may need to run the cmake -B build -S . -G \"MinGW Makefiles\" one more time or add -DCMAKE_SH=CMAKE_SH-NOTFOUND to CMake flags if you encounter the sh.exe was found in your PATH error. It is recommended that you use Visual Studio since it has better multithreading efficiency in Windows for many-core systems (see Question 4 and Question 8).", "prev_chunk_id": "chunk_7", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_9", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, LightGBM can be built using - CMakeandgcc; - CMakeandClang. After compilation the executable and .so files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_8", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_10", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S. cmake--buildbuild-j4", "prev_chunk_id": "chunk_9", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_11", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,ClangandOpenMP. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S. cmake--buildbuild-j4", "prev_chunk_id": "chunk_10", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_12", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS On macOS, LightGBM can be installed using - Homebrew; - MacPorts; or can be built using - CMakeandApple Clang; - CMakeandgcc.", "prev_chunk_id": "chunk_11", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_13", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Install Using Homebrew", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Install Using Homebrew", "content": "Install Using Homebrew brew install lightgbm Refer to https://formulae.brew.sh/formula/lightgbm for more details.", "prev_chunk_id": "chunk_12", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_14", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Install Using MacPorts", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Install Using MacPorts", "content": "Install Using MacPorts sudo port install LightGBM Refer to https://ports.macports.org/port/LightGBM for more details. Note: Port for LightGBM is not maintained by LightGBM’s maintainers.", "prev_chunk_id": "chunk_13", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_15", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build from GitHub", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build from GitHub", "content": "Build from GitHub After compilation the executable and .dylib files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_14", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_16", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Apple Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apple Clang", "content": "Apple Clang - InstallCMakeandOpenMP:brewinstallcmakelibomp - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S. cmake--buildbuild-j4", "prev_chunk_id": "chunk_15", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_17", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc:brewinstallcmakegcc - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=g++-7CC=gcc-7# replace \"7\" with version of gcc installed on your machinecmake-Bbuild-S. cmake--buildbuild-j4", "prev_chunk_id": "chunk_16", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_18", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Docker", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Docker", "content": "Docker Refer to Docker folder.", "prev_chunk_id": "chunk_17", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_19", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build Threadless Version (not Recommended)", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build Threadless Version (not Recommended)", "content": "Build Threadless Version (not Recommended) The default build version of LightGBM is based on OpenMP. You can build LightGBM without OpenMP support but it is strongly not recommended.", "prev_chunk_id": "chunk_18", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_20", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, a version of LightGBM without OpenMP support can be built using - Visual Studio; - CMakeandVS Build Tools; - CMakeandMinGW.", "prev_chunk_id": "chunk_19", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_21", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "With GUI", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "With GUI", "content": "With GUI - InstallVisual Studio. - Navigate to one of the releases athttps://github.com/microsoft/LightGBM/releases, downloadLightGBM-complete_source_code_zip.zip, and unzip it. - Go toLightGBM-complete_source_code_zip/windowsfolder. - OpenLightGBM.slnfile withVisual Studio, chooseReleaseconfiguration if you need executable file orDLLconfiguration if you need shared library. - Go toProject->Properties->ConfigurationProperties->C/C++->Languageand change theOpenMPSupportproperty toNo(/openmp-). - Get back to the project’s main screen and clickBuild->BuildSolution(Ctrl+Shift+B).If you have errors aboutPlatform Toolset, go toProject->Properties->ConfigurationProperties->Generaland select the toolset installed on your machine. The .exe file will be in LightGBM-complete_source_code_zip/windows/x64/Release folder. The .dll file will be in LightGBM-complete_source_code_zip/windows/x64/DLL folder.", "prev_chunk_id": "chunk_20", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_22", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "From Command Line", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "From Command Line", "content": "From Command Line - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois already installed). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64 -DUSE_OPENMP=OFFcmake --build build --target ALL_BUILD --config Release The .exe and .dll files will be in LightGBM/Release folder.", "prev_chunk_id": "chunk_21", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_23", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "MinGW-w64", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MinGW-w64", "content": "MinGW-w64 - InstallGit for Windows,CMakeandMinGW-w64. - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -G \"MinGW Makefiles\" -DUSE_OPENMP=OFFcmake --build build -j4 The .exe and .dll files will be in LightGBM/ folder. Note: You may need to run the cmake -B build -S . -G \"MinGW Makefiles\" -DUSE_OPENMP=OFF one more time or add -DCMAKE_SH=CMAKE_SH-NOTFOUND to CMake flags if you encounter the sh.exe was found in your PATH error.", "prev_chunk_id": "chunk_22", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_24", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, a version of LightGBM without OpenMP support can be built using - CMakeandgcc; - CMakeandClang. After compilation the executable and .so files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_23", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_25", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_OPENMP=OFF cmake--buildbuild-j4", "prev_chunk_id": "chunk_24", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_26", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMakeandClang. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DUSE_OPENMP=OFF cmake--buildbuild-j4", "prev_chunk_id": "chunk_25", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_27", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS On macOS, a version of LightGBM without OpenMP support can be built using - CMakeandApple Clang; - CMakeandgcc. After compilation the executable and .dylib files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_26", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_28", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Apple Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apple Clang", "content": "Apple Clang - InstallCMake:brewinstallcmake - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_OPENMP=OFF cmake--buildbuild-j4", "prev_chunk_id": "chunk_27", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_29", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc:brewinstallcmakegcc - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=g++-7CC=gcc-7# replace \"7\" with version of gcc installed on your machinecmake-Bbuild-S.-DUSE_OPENMP=OFF cmake--buildbuild-j4", "prev_chunk_id": "chunk_28", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_30", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build MPI Version", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build MPI Version", "content": "Build MPI Version The default build version of LightGBM is based on socket. LightGBM also supports MPI. MPI is a high performance communication approach with RDMA support. If you need to run a distributed learning application with high performance communication, you can build the LightGBM with MPI support.", "prev_chunk_id": "chunk_29", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_31", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, an MPI version of LightGBM can be built using - MS MPIandVisual Studio; - MS MPI,CMakeandVS Build Tools. Note: Building MPI version by MinGW is not supported due to the miss of MPI library in it.", "prev_chunk_id": "chunk_30", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_32", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "With GUI", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "With GUI", "content": "With GUI - You need to installMS MPIfirst. Bothmsmpisdk.msiandmsmpisetup.exeare needed. - InstallVisual Studio. - Navigate to one of the releases athttps://github.com/microsoft/LightGBM/releases, downloadLightGBM-complete_source_code_zip.zip, and unzip it. - Go toLightGBM-complete_source_code_zip/windowsfolder. - OpenLightGBM.slnfile withVisual Studio, chooseRelease_mpiconfiguration and clickBuild->BuildSolution(Ctrl+Shift+B).If you have errors aboutPlatform Toolset, go toProject->Properties->ConfigurationProperties->Generaland select the toolset installed on your machine. The .exe file will be in LightGBM-complete_source_code_zip/windows/x64/Release_mpi folder.", "prev_chunk_id": "chunk_31", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_33", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "From Command Line", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "From Command Line", "content": "From Command Line - You need to installMS MPIfirst. Bothmsmpisdk.msiandmsmpisetup.exeare needed. - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois already installed). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64 -DUSE_MPI=ONcmake --build build --target ALL_BUILD --config Release The .exe and .dll files will be in LightGBM/Release folder.", "prev_chunk_id": "chunk_32", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_34", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, an MPI version of LightGBM can be built using - CMake,gccandOpen MPI; - CMake,ClangandOpen MPI. After compilation the executable and .so files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_33", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_35", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,gccandOpen MPI. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_MPI=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_34", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_36", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,Clang,OpenMPandOpen MPI. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DUSE_MPI=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_35", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_37", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS On macOS, an MPI version of LightGBM can be built using - CMake,Open MPIandApple Clang; - CMake,Open MPIandgcc. After compilation the executable and .dylib files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_36", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_38", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Apple Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apple Clang", "content": "Apple Clang - InstallCMake,OpenMPandOpen MPI:brewinstallcmakelibompopen-mpi - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_MPI=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_37", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_39", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,Open MPIandgcc:brewinstallcmakeopen-mpigcc - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=g++-7CC=gcc-7# replace \"7\" with version of gcc installed on your machinecmake-Bbuild-S.-DUSE_MPI=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_38", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_40", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, a GPU version of LightGBM (device_type=gpu) can be built using - OpenCL,Boost,CMakeandVS Build Tools; - OpenCL,Boost,CMakeandMinGW. If you use MinGW, the build procedure is similar to the build on Linux. Following procedure is for the MSVC (Microsoft Visual C++) build. - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois installed). - InstallOpenCLfor Windows. The installation depends on the brand (NVIDIA, AMD, Intel) of your GPU card.For running on Intel, getIntel SDK for OpenCL.For running on AMD, get AMD APP SDK.For running on NVIDIA, getCUDA Toolkit.Further reading and correspondence table:GPU SDK Correspondence and Device Targeting Table. - InstallBoost Binaries.Note: Match your Visual C++ version:Visual Studio 2015 ->msvc-14.0-64.exe,Visual Studio 2017 ->msvc-14.1-64.exe,Visual Studio 2019 ->msvc-14.2-64.exe,Visual Studio 2022 ->msvc-14.3-64.exe. - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64 -DUSE_GPU=ON -DBOOST_ROOT=C:/local/boost_1_63_0 -DBOOST_LIBRARYDIR=C:/local/boost_1_63_0/lib64-msvc-14.0#ifyouhaveinstalledNVIDIACUDAtoacustomizedlocation,youshouldspecifypathstoOpenCLheadersandlibrarylikethefollowing:#cmake-Bbuild-S.-Ax64-DUSE_GPU=ON-DBOOST_ROOT=C:/local/boost_1_63_0-DBOOST_LIBRARYDIR=C:/local/boost_1_63_0/lib64-msvc-14.0-DOpenCL_LIBRARY=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/OpenCL.lib\"-DOpenCL_INCLUDE_DIR=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\"cmake --build build --target ALL_BUILD --config ReleaseNote:C:/local/boost_1_63_0andC:/local/boost_1_63_0/lib64-msvc-14.0are locations of yourBoostbinaries (assuming you’ve downloaded 1.63.0 version for Visual Studio 2015). The .exe and .dll files will be in LightGBM/Release folder.", "prev_chunk_id": "chunk_39", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_41", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, a GPU version of LightGBM (device_type=gpu) can be built using - CMake,OpenCL,Boostandgcc; - CMake,OpenCL,BoostandClang. OpenCL headers and libraries are usually provided by GPU manufacture. The generic OpenCL ICD packages (for example, Debian packages ocl-icd-libopencl1, ocl-icd-opencl-dev, pocl-opencl-icd) can also be used. Required Boost libraries (Boost.Align, Boost.System, Boost.Filesystem, Boost.Chrono) should be provided by the following Debian packages: libboost-dev, libboost-system-dev, libboost-filesystem-dev, libboost-chrono-dev. After compilation the executable and .so files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_40", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_42", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,gcc,OpenCLandBoost. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_GPU=ON# if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:# cmake -B build -S . -DUSE_GPU=ON -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/cmake--buildbuild-j4", "prev_chunk_id": "chunk_41", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_43", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,Clang,OpenMP,OpenCLandBoost. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DUSE_GPU=ON# if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:# cmake -B build -S . -DUSE_GPU=ON -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/cmake--buildbuild-j4", "prev_chunk_id": "chunk_42", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_44", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS The GPU version is not supported on macOS.", "prev_chunk_id": "chunk_43", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_45", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Docker", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Docker", "content": "Docker Refer to GPU Docker folder.", "prev_chunk_id": "chunk_44", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_46", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build CUDA Version", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build CUDA Version", "content": "Build CUDA Version The original GPU version of LightGBM (device_type=gpu) is based on OpenCL. The CUDA-based version (device_type=cuda) is a separate implementation. Use this version in Linux environments with an NVIDIA GPU with compute capability 6.0 or higher.", "prev_chunk_id": "chunk_45", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_47", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows The CUDA version is not supported on Windows. Use the GPU version (device_type=gpu) for GPU acceleration on Windows.", "prev_chunk_id": "chunk_46", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_48", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, a CUDA version of LightGBM can be built using - CMake,gccandCUDA; - CMake,ClangandCUDA. Please refer to this detailed guide for CUDA libraries installation. After compilation the executable and .so files will be in LightGBM/ folder.", "prev_chunk_id": "chunk_47", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_49", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,gccandCUDA. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_CUDA=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_48", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_50", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,Clang,OpenMPandCUDA. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DUSE_CUDA=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_49", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_51", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS The CUDA version is not supported on macOS.", "prev_chunk_id": "chunk_50", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_52", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build Java Wrapper", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build Java Wrapper", "content": "Build Java Wrapper Using the following instructions you can generate a JAR file containing the LightGBM C API wrapped by SWIG. After compilation the .jar file will be in LightGBM/build folder.", "prev_chunk_id": "chunk_51", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_53", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, a Java wrapper of LightGBM can be built using - Java,SWIG,CMakeandVS Build Tools; - Java,SWIG,CMakeandMinGW.", "prev_chunk_id": "chunk_52", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_54", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "VS Build Tools", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "VS Build Tools", "content": "VS Build Tools - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois already installed). - InstallSWIGandJava(also make sure thatJAVA_HOMEenvironment variable is set properly). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64 -DUSE_SWIG=ONcmake --build build --target ALL_BUILD --config Release", "prev_chunk_id": "chunk_53", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_55", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "MinGW-w64", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MinGW-w64", "content": "MinGW-w64 - InstallGit for Windows,CMakeandMinGW-w64. - InstallSWIGandJava(also make sure thatJAVA_HOMEenvironment variable is set properly). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -G \"MinGW Makefiles\" -DUSE_SWIG=ONcmake --build build -j4 Note: You may need to run the cmake -B build -S . -G \"MinGW Makefiles\" -DUSE_SWIG=ON one more time or add -DCMAKE_SH=CMAKE_SH-NOTFOUND to CMake flags if you encounter the sh.exe was found in your PATH error. It is recommended to use VS Build Tools (Visual Studio) since it has better multithreading efficiency in Windows for many-core systems (see Question 4 and Question 8).", "prev_chunk_id": "chunk_54", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_56", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, a Java wrapper of LightGBM can be built using - CMake,gcc,JavaandSWIG; - CMake,Clang,JavaandSWIG.", "prev_chunk_id": "chunk_55", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_57", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,gcc,SWIGandJava(also make sure thatJAVA_HOMEenvironment variable is set properly). - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_SWIG=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_56", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_58", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,Clang,OpenMP,SWIGandJava(also make sure thatJAVA_HOMEenvironment variable is set properly). - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DUSE_SWIG=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_57", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_59", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS On macOS, a Java wrapper of LightGBM can be built using - CMake,Java,SWIGandApple Clang; - CMake,Java,SWIGandgcc.", "prev_chunk_id": "chunk_58", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_60", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Apple Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apple Clang", "content": "Apple Clang - InstallCMake,Java(also make sure thatJAVA_HOMEenvironment variable is set properly),SWIGandOpenMP:brewinstallcmakeopenjdkswiglibompexportJAVA_HOME=\"$(brew--prefixopenjdk)/libexec/openjdk.jdk/Contents/Home/\" - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DUSE_SWIG=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_59", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_61", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMake,Java(also make sure thatJAVA_HOMEenvironment variable is set properly),SWIGandgcc:brewinstallcmakeopenjdkswiggccexportJAVA_HOME=\"$(brew--prefixopenjdk)/libexec/openjdk.jdk/Contents/Home/\" - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=g++-7CC=gcc-7# replace \"7\" with version of gcc installed on your machinecmake-Bbuild-S.-DUSE_SWIG=ON cmake--buildbuild-j4", "prev_chunk_id": "chunk_60", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_62", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build Python-package", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build Python-package", "content": "Build Python-package Refer to Python-package folder.", "prev_chunk_id": "chunk_61", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_63", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Build R-package", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build R-package", "content": "Build R-package Refer to R-package folder.", "prev_chunk_id": "chunk_62", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_64", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Windows", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Windows", "content": "Windows On Windows, C++ unit tests of LightGBM can be built using - CMakeandVS Build Tools; - CMakeandMinGW.", "prev_chunk_id": "chunk_63", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_65", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "VS Build Tools", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "VS Build Tools", "content": "VS Build Tools - InstallGit for Windows,CMakeandVS Build Tools(VS Build Toolsis not needed ifVisual Studiois already installed). - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -A x64 -DBUILD_CPP_TEST=ONcmake --build build --target testlightgbm --config Debug The .exe file will be in LightGBM/Debug folder.", "prev_chunk_id": "chunk_64", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_66", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "MinGW-w64", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MinGW-w64", "content": "MinGW-w64 - InstallGit for Windows,CMakeandMinGW-w64. - Run the following commands:git clone --recursive https://github.com/microsoft/LightGBMcd LightGBMcmake -B build -S . -G \"MinGW Makefiles\" -DBUILD_CPP_TEST=ONcmake --build build --target testlightgbm -j4 The .exe file will be in LightGBM/ folder. Note: You may need to run the cmake -B build -S . -G \"MinGW Makefiles\" -DBUILD_CPP_TEST=ON one more time or add -DCMAKE_SH=CMAKE_SH-NOTFOUND to CMake flags if you encounter the sh.exe was found in your PATH error.", "prev_chunk_id": "chunk_65", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_67", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Linux", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Linux", "content": "Linux On Linux, a C++ unit tests of LightGBM can be built using - CMakeandgcc; - CMakeandClang. After compilation the executable file will be in LightGBM/ folder.", "prev_chunk_id": "chunk_66", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_68", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DBUILD_CPP_TEST=ON cmake--buildbuild--targettestlightgbm-j4", "prev_chunk_id": "chunk_67", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_69", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Clang", "content": "Clang - InstallCMake,ClangandOpenMP. - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=clang++-14CC=clang-14# replace \"14\" with version of Clang installed on your machinecmake-Bbuild-S.-DBUILD_CPP_TEST=ON cmake--buildbuild--targettestlightgbm-j4", "prev_chunk_id": "chunk_68", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_70", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "macOS", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "macOS", "content": "macOS On macOS, a C++ unit tests of LightGBM can be built using - CMakeandApple Clang; - CMakeandgcc. After compilation the executable file will be in LightGBM/ folder.", "prev_chunk_id": "chunk_69", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_71", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "Apple Clang", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apple Clang", "content": "Apple Clang - InstallCMakeandOpenMP:brewinstallcmakelibomp - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBM cmake-Bbuild-S.-DBUILD_CPP_TEST=ON cmake--buildbuild--targettestlightgbm-j4", "prev_chunk_id": "chunk_70", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_72", "url": "https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html", "title": "gcc", "page_title": "Installation Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "gcc", "content": "gcc - InstallCMakeandgcc:brewinstallcmakegcc - Run the following commands:gitclone--recursivehttps://github.com/microsoft/LightGBMcdLightGBMexportCXX=g++-7CC=gcc-7# replace \"7\" with version of gcc installed on your machinecmake-Bbuild-S.-DBUILD_CPP_TEST=ON cmake--buildbuild--targettestlightgbm-j4", "prev_chunk_id": "chunk_71", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_73", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Quick Start", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Quick Start", "content": "Quick Start This is a quick start guide for LightGBM CLI version. Follow the Installation Guide to install LightGBM first. List of other helpful links - Parameters - Parameters Tuning - Python-package Quick Start - Python API", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_74", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Training Data Format", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Training Data Format", "content": "Training Data Format LightGBM supports input data files with CSV, TSV and LibSVM (zero-based) formats. Files could be both with and without headers. Label column could be specified both by index and by name. Some columns could be ignored.", "prev_chunk_id": "chunk_73", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_75", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Categorical Feature Support", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Categorical Feature Support", "content": "Categorical Feature Support LightGBM can use categorical features directly (without one-hot encoding). The experiment on Expo data shows about 8x speed-up compared with one-hot encoding. For the setting details, please refer to the categorical_feature parameter.", "prev_chunk_id": "chunk_74", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_76", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Weight and Query/Group Data", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Weight and Query/Group Data", "content": "Weight and Query/Group Data LightGBM also supports weighted training, it needs an additional weight data. And it needs an additional query data for ranking task. Also, weight and query data could be specified as columns in training data in the same manner as label.", "prev_chunk_id": "chunk_75", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_77", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Parameters Quick Look", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Parameters Quick Look", "content": "Parameters Quick Look The parameters format is key1=value1 key2=value2 .... Parameters can be set both in config file and command line. If one parameter appears in both command line and config file, LightGBM will use the parameter from the command line. The most important parameters which new users should take a look at are located into Core Parameters and the top of Learning Control Parameters sections of the full detailed list of LightGBM’s parameters.", "prev_chunk_id": "chunk_76", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_78", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Run LightGBM", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Run LightGBM", "content": "Run LightGBM lightgbm config=your_config_file other_args ... Parameters can be set both in the config file and command line, and the parameters in command line have higher priority than in the config file. For example, the following command line will keep num_trees=10 and ignore the same parameter in the config file. lightgbm config=train.conf num_trees=10", "prev_chunk_id": "chunk_77", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_79", "url": "https://lightgbm.readthedocs.io/en/latest/Quick-Start.html", "title": "Examples", "page_title": "Quick Start — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Examples", "content": "Examples - Binary Classification - Regression - Lambdarank - Distributed Learning", "prev_chunk_id": "chunk_78", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_80", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Python-package Introduction", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Python-package Introduction", "content": "Python-package Introduction This document gives a basic walk-through of LightGBM Python-package. List of other helpful links - Python Examples - Python API - Parameters Tuning", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_81", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Install", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Install", "content": "Install The preferred way to install LightGBM is via pip: pip install lightgbm Refer to Python-package folder for the detailed installation guide. To verify your installation, try to import lightgbm in Python: import lightgbm as lgb", "prev_chunk_id": "chunk_80", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_82", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Data Interface", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data Interface", "content": "Data Interface The LightGBM Python module can load data from: - LibSVM (zero-based) / TSV / CSV format text file - NumPy 2D array(s), pandas DataFrame, pyarrow Table, SciPy sparse matrix - LightGBM binary file - LightGBMSequenceobject(s) The data is stored in a Dataset object. Many of the examples in this page use functionality from numpy. To run the examples, be sure to import numpy in your session. import numpy as np To load a LibSVM (zero-based) text file or a LightGBM binary file into Dataset: train_data = lgb.Dataset('train.svm.bin') To load a numpy array into Dataset: rng = np.random.default_rng() data = rng.uniform(size=(500, 10)) # 500 entities, each contains 10 features label = rng.integers(low=0, high=2, size=(500, )) # binary target train_data = lgb.Dataset(data, label=label) To load a scipy.sparse.csr_matrix array into Dataset: import scipy csr = scipy.sparse.csr_matrix((dat, (row, col))) train_data = lgb.Dataset(csr) Load from Sequence objects: We can implement Sequence interface to read binary files. The following example shows reading HDF5 file with h5py. import h5py class HDFSequence(lgb.Sequence): def __init__(self, hdf_dataset, batch_size): self.data = hdf_dataset self.batch_size = batch_size def __getitem__(self, idx): return self.data[idx] def __len__(self): return len(self.data) f = h5py.File('train.hdf5', 'r') train_data = lgb.Dataset(HDFSequence(f['X'], 8192), label=f['Y'][:]) Features of using Sequence interface: - Data sampling uses random access, thus does not go through the whole dataset - Reading data in batch, thus saves memory when constructingDatasetobject - Supports creatingDatasetfrom multiple data files Please refer to Sequence API doc. dataset_from_multi_hdf5.py is a detailed example. Saving Dataset into a LightGBM binary file will make loading faster: train_data = lgb.Dataset('train.svm.txt') train_data.save_binary('train.bin') Create validation data: validation_data = train_data.create_valid('validation.svm') or validation_data = lgb.Dataset('validation.svm', reference=train_data) In LightGBM, the validation data should be aligned with training data. Specific feature names and categorical features: train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3']) LightGBM can use categorical features as input directly. It", "prev_chunk_id": "chunk_81", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_83", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Data Interface", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data Interface", "content": "doesn’t need to convert to one-hot encoding, and is much faster than one-hot encoding (about 8x speed-up). Note: You should convert your categorical features to int type before you construct Dataset. Weights can be set when needed: rng = np.random.default_rng() w = rng.uniform(size=(500, )) train_data = lgb.Dataset(data, label=label, weight=w) or train_data = lgb.Dataset(data, label=label) rng = np.random.default_rng() w = rng.uniform(size=(500, )) train_data.set_weight(w) And you can use Dataset.set_init_score() to set initial score, and Dataset.set_group() to set group/query data for ranking tasks. Memory efficient usage: The Dataset object in LightGBM is very memory-efficient, it only needs to save discrete bins. However, Numpy/Array/Pandas object is memory expensive. If you are concerned about your memory consumption, you can save memory by: - Setfree_raw_data=True(default isTrue) when constructing theDataset - Explicitly setraw_data=Noneafter theDatasethas been constructed - Callgc", "prev_chunk_id": "chunk_82", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_84", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Setting Parameters", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Setting Parameters", "content": "Setting Parameters LightGBM can use a dictionary to set Parameters. For instance: - Booster parameters:param={'num_leaves':31,'objective':'binary'}param['metric']='auc' - You can also specify multiple eval metrics:param['metric']=['auc','binary_logloss']", "prev_chunk_id": "chunk_83", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_85", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Training", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Training", "content": "Training Training a model requires a parameter list and data set: num_round = 10 bst = lgb.train(param, train_data, num_round, valid_sets=[validation_data]) After training, the model can be saved: bst.save_model('model.txt') The trained model can also be dumped to JSON format: json_model = bst.dump_model() A saved model can be loaded: bst = lgb.Booster(model_file='model.txt') # init model", "prev_chunk_id": "chunk_84", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_86", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "CV", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "CV", "content": "CV Training with 5-fold CV: lgb.cv(param, train_data, num_round, nfold=5)", "prev_chunk_id": "chunk_85", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_87", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Early Stopping", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Early Stopping", "content": "Early Stopping If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. Early stopping requires at least one set in valid_sets. If there is more than one, it will use all of them except the training data: bst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, callbacks=[lgb.early_stopping(stopping_rounds=5)]) bst.save_model('model.txt', num_iteration=bst.best_iteration) The model will train until the validation score stops improving. Validation score needs to improve at least every stopping_rounds to continue training. The index of iteration that has the best performance will be saved in the best_iteration field if early stopping logic is enabled by setting early_stopping callback. Note that train() will return a model from the best iteration. This works with both metrics to minimize (L2, log loss, etc.) and to maximize (NDCG, AUC, etc.). Note that if you specify more than one evaluation metric, all of them will be used for early stopping. However, you can change this behavior and make LightGBM check only the first metric for early stopping by passing first_metric_only=True in early_stopping callback constructor.", "prev_chunk_id": "chunk_86", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_88", "url": "https://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "title": "Prediction", "page_title": "Python-package Introduction — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Prediction", "content": "Prediction A model that has been trained or loaded can perform predictions on datasets: # 7 entities, each contains 10 features rng = np.random.default_rng() data = rng.uniform(size=(7, 10)) ypred = bst.predict(data) If early stopping is enabled during training, you can get predictions from the best iteration with bst.best_iteration: ypred = bst.predict(data, num_iteration=bst.best_iteration)", "prev_chunk_id": "chunk_87", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_89", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Features", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Features", "content": "Features This is a conceptual overview of how LightGBM works[1]. We assume familiarity with decision tree boosting algorithms to focus instead on aspects of LightGBM that may differ from other boosting packages. For detailed algorithms, please refer to the citations or source code.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_90", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Optimization in Speed and Memory Usage", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Optimization in Speed and Memory Usage", "content": "Optimization in Speed and Memory Usage Many boosting tools use pre-sort-based algorithms[2, 3] (e.g. default algorithm in xgboost) for decision tree learning. It is a simple solution, but not easy to optimize. LightGBM uses histogram-based algorithms[4, 5, 6], which bucket continuous feature (attribute) values into discrete bins. This speeds up training and reduces memory usage. Advantages of histogram-based algorithms include the following: - Reduced cost of calculating the gain for each splitPre-sort-based algorithms have time complexityO(#data)Computing the histogram has time complexityO(#data), but this involves only a fast sum-up operation. Once the histogram is constructed, a histogram-based algorithm has time complexityO(#bins), and#binsis far smaller than#data. - Use histogram subtraction for further speedupTo get one leaf’s histograms in a binary tree, use the histogram subtraction of its parent and its neighborSo it needs to construct histograms for only one leaf (with smaller#datathan its neighbor). It then can get histograms of its neighbor by histogram subtraction with small cost (O(#bins)) - Reduce memory usageReplaces continuous values with discrete bins. If#binsis small, can use small data type, e.g. uint8_t, to store training dataNo need to store additional information for pre-sorting feature values - Reduce communication cost for distributed learning", "prev_chunk_id": "chunk_89", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_91", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Sparse Optimization", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Sparse Optimization", "content": "Sparse Optimization - Need onlyO(2*#non_zero_data)to construct histogram for sparse features", "prev_chunk_id": "chunk_90", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_92", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Leaf-wise (Best-first) Tree Growth", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Leaf-wise (Best-first) Tree Growth", "content": "Leaf-wise (Best-first) Tree Growth Most decision tree learning algorithms grow trees by level (depth)-wise, like the following image: LightGBM grows trees leaf-wise (best-first)[7]. It will choose the leaf with max delta loss to grow. Holding #leaf fixed, leaf-wise algorithms tend to achieve lower loss than level-wise algorithms. Leaf-wise may cause over-fitting when #data is small, so LightGBM includes the max_depth parameter to limit tree depth. However, trees still grow leaf-wise even when max_depth is specified.", "prev_chunk_id": "chunk_91", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_93", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Optimal Split for Categorical Features", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Optimal Split for Categorical Features", "content": "Optimal Split for Categorical Features It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy. Instead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. If the feature has k categories, there are 2^(k-1) - 1 possible partitions. But there is an efficient solution for regression trees[8]. It needs about O(k * log(k)) to find the optimal partition. The basic idea is to sort the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (sum_gradient / sum_hessian) and then finds the best split on the sorted histogram.", "prev_chunk_id": "chunk_92", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_94", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Optimization in Network Communication", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Optimization in Network Communication", "content": "Optimization in Network Communication It only needs to use some collective communication algorithms, like “All reduce”, “All gather” and “Reduce scatter”, in distributed learning of LightGBM. LightGBM implements state-of-the-art algorithms[9]. These collective communication algorithms can provide much better performance than point-to-point communication.", "prev_chunk_id": "chunk_93", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_95", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Optimization in Distributed Learning", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Optimization in Distributed Learning", "content": "Optimization in Distributed Learning LightGBM provides the following distributed learning algorithms.", "prev_chunk_id": "chunk_94", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_96", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Traditional Algorithm", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Traditional Algorithm", "content": "Traditional Algorithm Feature parallel aims to parallelize the “Find Best Split” in the decision tree. The procedure of traditional feature parallel is: - Partition data vertically (different machines have different feature set). - Workers find local best split point {feature, threshold} on local feature set. - Communicate local best splits with each other and get the best one. - Worker with best split to perform split, then send the split result of data to other workers. - Other workers split data according to received data. The shortcomings of traditional feature parallel: - Has computation overhead, since it cannot speed up “split”, whose time complexity isO(#data). Thus, feature parallel cannot speed up well when#datais large. - Need communication of split result, which costs aboutO(#data/8)(one bit for one data).", "prev_chunk_id": "chunk_95", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_97", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Feature Parallel in LightGBM", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Feature Parallel in LightGBM", "content": "Feature Parallel in LightGBM Since feature parallel cannot speed up well when #data is large, we make a little change: instead of partitioning data vertically, every worker holds the full data. Thus, LightGBM doesn’t need to communicate for split result of data since every worker knows how to split data. And #data won’t be larger, so it is reasonable to hold the full data in every machine. The procedure of feature parallel in LightGBM: - Workers find local best split point {feature, threshold} on local feature set. - Communicate local best splits with each other and get the best one. - Perform best split. However, this feature parallel algorithm still suffers from computation overhead for “split” when #data is large. So it will be better to use data parallel when #data is large.", "prev_chunk_id": "chunk_96", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_98", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Traditional Algorithm", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Traditional Algorithm", "content": "Traditional Algorithm Data parallel aims to parallelize the whole decision learning. The procedure of data parallel is: - Partition data horizontally. - Workers use local data to construct local histograms. - Merge global histograms from all local histograms. - Find best split from merged global histograms, then perform splits. The shortcomings of traditional data parallel: - High communication cost. If using point-to-point communication algorithm, communication cost for one machine is aboutO(#machine*#feature*#bin). If using collective communication algorithm (e.g. “All Reduce”), communication cost is aboutO(2*#feature*#bin)(check cost of “All Reduce” in chapter 4.5 at[9]).", "prev_chunk_id": "chunk_97", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_99", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Data Parallel in LightGBM", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data Parallel in LightGBM", "content": "Data Parallel in LightGBM We reduce communication cost of data parallel in LightGBM: - Instead of “Merge global histograms from all local histograms”, LightGBM uses “Reduce Scatter” to merge histograms of different (non-overlapping) features for different workers. Then workers find the local best split on local merged histograms and sync up the global best split. - As aforementioned, LightGBM uses histogram subtraction to speed up training. Based on this, we can communicate histograms only for one leaf, and get its neighbor’s histograms by subtraction as well. All things considered, data parallel in LightGBM has time complexity O(0.5 * #feature * #bin).", "prev_chunk_id": "chunk_98", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_100", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Voting Parallel", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Voting Parallel", "content": "Voting Parallel Voting parallel further reduces the communication cost in Data Parallel to constant cost. It uses two-stage voting to reduce the communication cost of feature histograms[10].", "prev_chunk_id": "chunk_99", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_101", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "GPU Support", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Support", "content": "GPU Support Thanks @huanzhang12 for contributing this feature. Please read [11] to get more details. - GPU Installation - GPU Tutorial", "prev_chunk_id": "chunk_100", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_102", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Applications and Metrics", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Applications and Metrics", "content": "Applications and Metrics LightGBM supports the following applications: - regression, the objective function is L2 loss - binary classification, the objective function is logloss - multi classification - cross-entropy, the objective function is logloss and supports training on non-binary labels - LambdaRank, the objective function is LambdaRank with NDCG LightGBM supports the following metrics: - L1 loss - L2 loss - Log loss - Classification error rate - AUC - NDCG - MAP - Multi-class log loss - Multi-class error rate - AUC-mu(newinv3.0.0) - Average precision(newinv3.1.0) - Fair - Huber - Poisson - Quantile - MAPE - Kullback-Leibler - Gamma - Tweedie For more details, please refer to Parameters.", "prev_chunk_id": "chunk_101", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_103", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "Other Features", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Other Features", "content": "Other Features - Limitmax_depthof tree while grows tree leaf-wise - DART - L1/L2 regularization - Bagging - Column (feature) sub-sample - Continued train with input GBDT model - Continued train with the input score file - Weighted training - Validation metric output during training - Multiple validation data - Multiple metrics - Early stopping (both training and prediction) - Prediction for leaf index For more details, please refer to Parameters.", "prev_chunk_id": "chunk_102", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_104", "url": "https://lightgbm.readthedocs.io/en/latest/Features.html", "title": "References", "page_title": "Features — LightGBM 4.6.0.99 documentation", "breadcrumbs": "References", "content": "References [1] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree.” Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157. [2] Mehta, Manish, Rakesh Agrawal, and Jorma Rissanen. “SLIQ: A fast scalable classifier for data mining.” International Conference on Extending Database Technology. Springer Berlin Heidelberg, 1996. [3] Shafer, John, Rakesh Agrawal, and Manish Mehta. “SPRINT: A scalable parallel classifier for data mining.” Proc. 1996 Int. Conf. Very Large Data Bases. 1996. [4] Ranka, Sanjay, and V. Singh. “CLOUDS: A decision tree classifier for large datasets.” Proceedings of the 4th Knowledge Discovery and Data Mining Conference. 1998. [5] Machado, F. P. “Communication and memory efficient parallel decision tree construction.” (2003). [6] Li, Ping, Qiang Wu, and Christopher J. Burges. “Mcrank: Learning to rank using multiple classification and gradient boosting.” Advances in Neural Information Processing Systems 20 (NIPS 2007). [7] Shi, Haijian. “Best-first decision tree learning.” Diss. The University of Waikato, 2007. [8] Walter D. Fisher. “On Grouping for Maximum Homogeneity.” Journal of the American Statistical Association. Vol. 53, No. 284 (Dec., 1958), pp. 789-798. [9] Thakur, Rajeev, Rolf Rabenseifner, and William Gropp. “Optimization of collective communication operations in MPICH.” International Journal of High Performance Computing Applications 19.1 (2005), pp. 49-66. [10] Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. “A Communication-Efficient Parallel Algorithm for Decision Tree.” Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287. [11] Huan Zhang, Si Si and Cho-Jui Hsieh. “GPU Acceleration for Large-scale Tree Boosting.” SysML Conference, 2018.", "prev_chunk_id": "chunk_103", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_105", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Comparison Experiment", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Comparison Experiment", "content": "Comparison Experiment For the detailed experiment scripts and output logs, please refer to this repo.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_106", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "History", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "History", "content": "History 08 Mar, 2020: update according to the latest master branch (1b97eaf for XGBoost, bcad692 for LightGBM). (xgboost_exact is not updated for it is too slow.) 27 Feb, 2017: first version.", "prev_chunk_id": "chunk_105", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_107", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Data", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data", "content": "Data We used 5 datasets to conduct our comparison experiments. Details of data are listed in the following table: Data | Task | Link | #Train_Set | #Feature | Comments Higgs | Binary classification | link | 10,500,000 | 28 | last 500,000 samples were used as test set Yahoo LTR | Learning to rank | link | 473,134 | 700 | set1.train as train, set1.test as test MS LTR | Learning to rank | link | 2,270,296 | 137 | {S1,S2,S3} as train set, {S5} as test set Expo | Binary classification | link | 11,000,000 | 700 | last 1,000,000 samples were used as test set Allstate | Binary classification | link | 13,184,290 | 4228 | last 1,000,000 samples were used as test set", "prev_chunk_id": "chunk_106", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_108", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Environment", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Environment", "content": "Environment We ran all experiments on a single Linux server (Azure ND24s) with the following specifications: OS | CPU | Memory Ubuntu 16.04 LTS | 2 * E5-2690 v4 | 448GB", "prev_chunk_id": "chunk_107", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_109", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Baseline", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Baseline", "content": "Baseline We used xgboost as a baseline. Both xgboost and LightGBM were built with OpenMP support.", "prev_chunk_id": "chunk_108", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_110", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Settings", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Settings", "content": "Settings We set up total 3 settings for experiments. The parameters of these settings are: - xgboost:eta = 0.1 max_depth = 8 num_round = 500 nthread = 16 tree_method = exact min_child_weight = 100 - xgboost_hist (using histogram based algorithm):eta = 0.1 num_round = 500 nthread = 16 min_child_weight = 100 tree_method = hist grow_policy = lossguide max_depth = 0 max_leaves = 255 - LightGBM:learning_rate = 0.1 num_leaves = 255 num_trees = 500 num_threads = 16 min_data_in_leaf = 0 min_sum_hessian_in_leaf = 100 xgboost grows trees depth-wise and controls model complexity by max_depth. LightGBM uses a leaf-wise algorithm instead and controls model complexity by num_leaves. So we cannot compare them in the exact same model setting. For the tradeoff, we use xgboost with max_depth=8, which will have max number leaves to 255, to compare with LightGBM with num_leaves=255. Other parameters are default values.", "prev_chunk_id": "chunk_109", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_111", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Speed", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Speed", "content": "Speed We compared speed using only the training task without any test or metric output. We didn’t count the time for IO. For the ranking tasks, since XGBoost and LightGBM implement different ranking objective functions, we used regression objective for speed benchmark, for the fair comparison. The following table is the comparison of time cost: Data | xgboost | xgboost_hist | LightGBM Higgs | 3794.34 s | 165.575 s | 130.094 s Yahoo LTR | 674.322 s | 131.462 s | 76.229 s MS LTR | 1251.27 s | 98.386 s | 70.417 s Expo | 1607.35 s | 137.65 s | 62.607 s Allstate | 2867.22 s | 315.256 s | 148.231 s LightGBM ran faster than xgboost on all experiment data sets.", "prev_chunk_id": "chunk_110", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_112", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Accuracy", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Accuracy", "content": "Accuracy We computed all accuracy metrics only on the test data set. Data | Metric | xgboost | xgboost_hist | LightGBM Higgs | AUC | 0.839593 | 0.845314 | 0.845724 Yahoo LTR | NDCG1 | 0.719748 | 0.720049 | 0.732981 NDCG3 | 0.717813 | 0.722573 | 0.735689 NDCG5 | 0.737849 | 0.740899 | 0.75352 NDCG10 | 0.78089 | 0.782957 | 0.793498 MS LTR | NDCG1 | 0.483956 | 0.485115 | 0.517767 NDCG3 | 0.467951 | 0.47313 | 0.501063 NDCG5 | 0.472476 | 0.476375 | 0.504648 NDCG10 | 0.492429 | 0.496553 | 0.524252 Expo | AUC | 0.756713 | 0.776224 | 0.776935 Allstate | AUC | 0.607201 | 0.609465 | 0.609072", "prev_chunk_id": "chunk_111", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_113", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Memory Consumption", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Memory Consumption", "content": "Memory Consumption We monitored RES while running training task. And we set two_round=true (this will increase data-loading time and reduce peak memory usage but not affect training speed or accuracy) in LightGBM to reduce peak memory usage. Data | xgboost | xgboost_hist | LightGBM (col-wise) | LightGBM (row-wise) Higgs | 4.853GB | 7.335GB | 0.897GB | 1.401GB Yahoo LTR | 1.907GB | 4.023GB | 1.741GB | 2.161GB MS LTR | 5.469GB | 7.491GB | 0.940GB | 1.296GB Expo | 1.553GB | 2.606GB | 0.555GB | 0.711GB Allstate | 6.237GB | 12.090GB | 1.116GB | 1.755GB", "prev_chunk_id": "chunk_112", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_114", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "History", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "History", "content": "History 27 Feb, 2017: first version.", "prev_chunk_id": "chunk_113", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_115", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Data", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data", "content": "Data We used a terabyte click log dataset to conduct parallel experiments. Details are listed in following table: Data | Task | Link | #Data | #Feature Criteo | Binary classification | link | 1,700,000,000 | 67 This data contains 13 integer features and 26 categorical features for 24 days of click logs. We statisticized the click-through rate (CTR) and count for these 26 categorical features from the first ten days. Then we used next ten days’ data, after replacing the categorical features by the corresponding CTR and count, as training data. The processed training data have a total of 1.7 billions records and 67 features.", "prev_chunk_id": "chunk_114", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_116", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Environment", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Environment", "content": "Environment We ran our experiments on 16 Windows servers with the following specifications: OS | CPU | Memory | Network Adapter Windows Server 2012 | 2 * E5-2670 v2 | DDR3 1600Mhz, 256GB | Mellanox ConnectX-3, 54Gbps, RDMA support", "prev_chunk_id": "chunk_115", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_117", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Settings", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Settings", "content": "Settings learning_rate = 0.1 num_leaves = 255 num_trees = 100 num_thread = 16 tree_learner = data We used data parallel here because this data is large in #data but small in #feature. Other parameters were default values.", "prev_chunk_id": "chunk_116", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_118", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "Results", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Results", "content": "Results #Machine | Time per Tree | Memory Usage(per Machine) 1 | 627.8 s | 176GB 2 | 311 s | 87GB 4 | 156 s | 43GB 8 | 80 s | 22GB 16 | 42 s | 11GB The results show that LightGBM achieves a linear speedup with distributed learning.", "prev_chunk_id": "chunk_117", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_119", "url": "https://lightgbm.readthedocs.io/en/latest/Experiments.html", "title": "GPU Experiments", "page_title": "Experiments — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Experiments", "content": "GPU Experiments Refer to GPU Performance.", "prev_chunk_id": "chunk_118", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_120", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Parameters", "content": "Parameters This page contains descriptions of all parameters in LightGBM. List of other helpful links - Python API - Parameters Tuning", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_121", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Parameters Format", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Parameters Format", "content": "Parameters Format Parameters are merged together in the following order (later items overwrite earlier ones): - LightGBM’s default values - special files forweight,init_score,query, andpositions(seeOthers) - (CLI only) configuration in a file passed likeconfig=train.conf - (CLI only) configuration passed via the command line - (Python, R) special keyword arguments to some functions (e.g.num_boost_roundintrain()) - (Python, R)paramsfunction argument (including**kwargsin Python and...in R) - (C API)parametersorparamsfunction argument Many parameters have “aliases”, alternative names which refer to the same configuration. Where a mix of the primary parameter name and aliases are given, the primary parameter name is always preferred to any aliases. For example, in Python: # use learning rate of 0.07, because 'learning_rate' # is the primary parameter name lgb.train( params={ \"learning_rate\": 0.07, \"shrinkage_rate\": 0.12 }, train_set=dtrain ) Where multiple aliases are given, and the primary parameter name is not, the first alias appearing in the lists returned by Config::parameter2aliases() in the C++ library is used. Those lists are hard-coded in a fairly arbitrary way… wherever possible, avoid relying on this behavior. For example, in Python: # use learning rate of 0.12, LightGBM has a hard-coded preference for 'shrinkage_rate' # over any other aliases, and 'learning_rate' is not provided lgb.train( params={ \"eta\": 0.19, \"shrinkage_rate\": 0.12 }, train_set=dtrain ) CLI The parameters format is key1=value1 key2=value2 .... Parameters can be set both in config file and command line. By using command line, parameters should not have spaces before and after =. By using config files, one line can only contain one parameter. You can use # to comment. Python Any parameters that accept multiple values should be passed as a Python list. params = { \"monotone_constraints\": [-1, 0, 1] } R Any parameters that accept multiple values should be passed as an R list. params <- list( monotone_constraints = c(-1, 0, 1) )", "prev_chunk_id": "chunk_120", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_122", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Core Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Core Parameters", "content": "Core Parameters - config🔗︎, default =\"\", type = string, aliases:config_filepath of config fileNote: can be used only in CLI version - task🔗︎, default =train, type = enum, options:train,predict,convert_model,refit, aliases:task_typetrain, for training, aliases:trainingpredict, for prediction, aliases:prediction,testconvert_model, for converting model file into if-else format, see more information inConvert Parametersrefit, for refitting existing models with new data, aliases:refit_treesave_binary, load train (and validation) data then save dataset to binary file. Typical usage:save_binaryfirst, then run multipletraintasks in parallel using the saved binary fileNote: can be used only in CLI version; for language-specific packages you can use the correspondent functions - objective🔗︎, default =regression, type = enum, options:regression,regression_l1,huber,fair,poisson,quantile,mape,gamma,tweedie,binary,multiclass,multiclassova,cross_entropy,cross_entropy_lambda,lambdarank,rank_xendcg, aliases:objective_type,app,application,lossregression applicationregression, L2 loss, aliases:regression_l2,l2,mean_squared_error,mse,l2_root,root_mean_squared_error,rmseregression_l1, L1 loss, aliases:l1,mean_absolute_error,maehuber,Huber lossfair,Fair losspoisson,Poisson regressionquantile,Quantile regressionmape,MAPE loss, aliases:mean_absolute_percentage_errorgamma, Gamma regression with log-link. It might be useful, e.g., for modeling insurance claims severity, or for any target that might begamma-distributedtweedie, Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any target that might betweedie-distributedbinary classification applicationbinary, binarylog lossclassification (or logistic regression)requires labels in {0, 1}; seecross-entropyapplication for general probability labels in [0, 1]multi-class classification applicationmulticlass,softmaxobjective function, aliases:softmaxmulticlassova,One-vs-Allbinary objective function, aliases:multiclass_ova,ova,ovrnum_classshould be set as wellcross-entropy applicationcross_entropy, objective function for cross-entropy (with optional linear weights), aliases:xentropycross_entropy_lambda, alternative parameterization of cross-entropy, aliases:xentlambdalabel is anything in interval [0, 1]ranking applicationlambdarank,lambdarankobjective.label_gaincan be used to set the gain (weight) ofintlabel and all values inlabelmust be smaller than number of elements inlabel_gainrank_xendcg,XE_NDCG_MARTranking objective function, aliases:xendcg,xe_ndcg,xe_ndcg_mart,xendcg_martrank_xendcgis faster than and achieves the similar performance aslambdaranklabel should beinttype, and larger number represents the higher relevance (e.g. 0:bad, 1:fair, 2:good, 3:perfect)custom objective function (gradients and hessians not computed directly by LightGBM)custommust be passed through parameters explicitly in the C APINote: cannot be used in CLI version - boosting🔗︎, default =gbdt, type = enum, options:gbdt,rf,dart, aliases:boosting_type,boostgbdt, traditional Gradient Boosting Decision Tree, aliases:gbrtrf, Random Forest, aliases:random_forestdart,Dropouts meet Multiple", "prev_chunk_id": "chunk_121", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_123", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Core Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Core Parameters", "content": "Additive Regression TreesNote: internally, LightGBM usesgbdtmode for the first1/learning_rateiterations - data_sample_strategy🔗︎, default =bagging, type = enum, options:bagging,gossbagging, Randomly Bagging SamplingNote:baggingis only effective whenbagging_freq>0andbagging_fraction<1.0goss, Gradient-based One-Side SamplingNew in version 4.0.0 - data🔗︎, default =\"\", type = string, aliases:train,train_data,train_data_file,data_filenamepath of training data, LightGBM will train from this dataNote: can be used only in CLI version - valid🔗︎, default =\"\", type = string, aliases:test,valid_data,valid_data_file,test_data,test_data_file,valid_filenamespath(s) of validation/test data, LightGBM will output metrics for these datasupport multiple validation data, separated by,Note: can be used only in CLI version - num_iterations🔗︎, default =100, type = int, aliases:num_iteration,n_iter,num_tree,num_trees,num_round,num_rounds,nrounds,num_boost_round,n_estimators,max_iter, constraints:num_iterations>=0number of boosting iterationsNote: internally, LightGBM constructsnum_class*num_iterationstrees for multi-class classification problems - learning_rate🔗︎, default =0.1, type = double, aliases:shrinkage_rate,eta, constraints:learning_rate>0.0shrinkage rateindart, it also affects on normalization weights of dropped trees - num_leaves🔗︎, default =31, type = int, aliases:num_leaf,max_leaves,max_leaf,max_leaf_nodes, constraints:1<num_leaves<=131072max number of leaves in one tree - tree_learner🔗︎, default =serial, type = enum, options:serial,feature,data,voting, aliases:tree,tree_type,tree_learner_typeserial, single machine tree learnerfeature, feature parallel tree learner, aliases:feature_paralleldata, data parallel tree learner, aliases:data_parallelvoting, voting parallel tree learner, aliases:voting_parallelrefer toDistributed Learning Guideto get more details - num_threads🔗︎, default =0, type = int, aliases:num_thread,nthread,nthreads,n_jobsused only intrain,predictionandrefittasks or in correspondent functions of language-specific packagesnumber of threads for LightGBM0means default number of threads in OpenMPfor the best speed, set this to the number ofreal CPU cores, not the number of threads (most CPUs usehyper-threadingto generate 2 threads per CPU core)do not set it too large if your dataset is small (for instance, do not use 64 threads for a dataset with 10,000 rows)be aware a task manager or any similar CPU monitoring tool might report that cores not being fully utilized.This is normalfor distributed learning, do not use all CPU cores because this will cause poor performance for the network communicationNote: pleasedon’tchange this during training, especially when running multiple jobs simultaneously by external packages, otherwise it may cause", "prev_chunk_id": "chunk_122", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_124", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Core Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Core Parameters", "content": "undesirable errors - device_type🔗︎, default =cpu, type = enum, options:cpu,gpu,cuda, aliases:devicedevice for the tree learningcpusupports all LightGBM functionality and is portable across the widest range of operating systems and hardwarecudaoffers faster training thangpuorcpu, but only works on GPUs supporting CUDAgpucan be faster thancpuand works on a wider range of GPUs than CUDANote: it is recommended to use the smallermax_bin(e.g. 63) to get the better speed upNote: for the faster speed, GPU uses 32-bit float point to sum up by default, so this may affect the accuracy for some tasks. You can setgpu_use_dp=trueto enable 64-bit float point, but it will slow down the trainingNote: refer toInstallation Guideto build LightGBM with GPU or CUDA support - seed🔗︎, default =None, type = int, aliases:random_seed,random_statethis seed is used to generate other seeds, e.g.data_random_seed,feature_fraction_seed, etc.by default, this seed is unused in favor of default values of other seedsthis seed has lower priority in comparison with other seeds, which means that it will be overridden, if you set other seeds explicitly - deterministic🔗︎, default =false, type = boolused only withcpudevice typesetting this totrueshould ensure the stable results when using the same data and the same parameters (and differentnum_threads)when you use the different seeds, different LightGBM versions, the binaries compiled by different compilers, or in different systems, the results are expected to be differentyou canraise issuesin LightGBM GitHub repo when you meet the unstable resultsNote: setting this totruemay slow down the trainingNote: to avoid potential instability due to numerical issues, please setforce_col_wise=trueorforce_row_wise=truewhen settingdeterministic=true", "prev_chunk_id": "chunk_123", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_125", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "Learning Control Parameters - force_col_wise🔗︎, default =false, type = boolused only withcpudevice typeset this totrueto force col-wise histogram buildingenabling this is recommended when:the number of columns is large, or the total number of bins is largenum_threadsis large, e.g.>20you want to reduce memory costNote: when bothforce_col_wiseandforce_row_wisearefalse, LightGBM will firstly try them both, and then use the faster one. To remove the overhead of testing set the faster one totruemanuallyNote: this parameter cannot be used at the same time withforce_row_wise, choose only one of them - force_row_wise🔗︎, default =false, type = boolused only withcpudevice typeset this totrueto force row-wise histogram buildingenabling this is recommended when:the number of data points is large, and the total number of bins is relatively smallnum_threadsis relatively small, e.g.<=16you want to use smallbagging_fractionorgosssample strategy to speed upNote: setting this totruewill double the memory cost for Dataset object. If you have not enough memory, you can try settingforce_col_wise=trueNote: when bothforce_col_wiseandforce_row_wisearefalse, LightGBM will firstly try them both, and then use the faster one. To remove the overhead of testing set the faster one totruemanuallyNote: this parameter cannot be used at the same time withforce_col_wise, choose only one of them - histogram_pool_size🔗︎, default =-1.0, type = double, aliases:hist_pool_sizemax cache size in MB for historical histogram<0means no limit - max_depth🔗︎, default =-1, type = intlimit the max depth for tree model. This is used to deal with over-fitting when#datais small. Tree still grows leaf-wise<=0means no limit - min_data_in_leaf🔗︎, default =20, type = int, aliases:min_data_per_leaf,min_data,min_child_samples,min_samples_leaf, constraints:min_data_in_leaf>=0minimal number of data in one leaf. Can be used to deal with over-fittingNote: this is an approximation based on the Hessian, so occasionally you may observe splits which produce leaf nodes that have less than this many observations - min_sum_hessian_in_leaf🔗︎, default =1e-3, type = double, aliases:min_sum_hessian_per_leaf,min_sum_hessian,min_hessian,min_child_weight, constraints:min_sum_hessian_in_leaf>=0.0minimal sum hessian in one leaf. Likemin_data_in_leaf, it can be used", "prev_chunk_id": "chunk_124", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_126", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "to deal with over-fitting - bagging_fraction🔗︎, default =1.0, type = double, aliases:sub_row,subsample,bagging, constraints:0.0<bagging_fraction<=1.0likefeature_fraction, but this will randomly select part of data without resamplingcan be used to speed up trainingcan be used to deal with over-fittingNote: to enable bagging,bagging_freqshould be set to a non zero value as well - pos_bagging_fraction🔗︎, default =1.0, type = double, aliases:pos_sub_row,pos_subsample,pos_bagging, constraints:0.0<pos_bagging_fraction<=1.0used only inbinaryapplicationused for imbalanced binary classification problem, will randomly sample#pos_samples*pos_bagging_fractionpositive samples in baggingshould be used together withneg_bagging_fractionset this to1.0to disableNote: to enable this, you need to setbagging_freqandneg_bagging_fractionas wellNote: if bothpos_bagging_fractionandneg_bagging_fractionare set to1.0, balanced bagging is disabledNote: if balanced bagging is enabled,bagging_fractionwill be ignored - neg_bagging_fraction🔗︎, default =1.0, type = double, aliases:neg_sub_row,neg_subsample,neg_bagging, constraints:0.0<neg_bagging_fraction<=1.0used only inbinaryapplicationused for imbalanced binary classification problem, will randomly sample#neg_samples*neg_bagging_fractionnegative samples in baggingshould be used together withpos_bagging_fractionset this to1.0to disableNote: to enable this, you need to setbagging_freqandpos_bagging_fractionas wellNote: if bothpos_bagging_fractionandneg_bagging_fractionare set to1.0, balanced bagging is disabledNote: if balanced bagging is enabled,bagging_fractionwill be ignored - bagging_freq🔗︎, default =0, type = int, aliases:subsample_freqfrequency for bagging0means disable bagging;kmeans perform bagging at everykiteration. Everyk-th iteration, LightGBM will randomly selectbagging_fraction*100%of the data to use for the nextkiterationsNote: bagging is only effective when0.0<bagging_fraction<1.0 - bagging_seed🔗︎, default =3, type = int, aliases:bagging_fraction_seedrandom seed for bagging - bagging_by_query🔗︎, default =false, type = boolwhether to do bagging sample by queryNew in version 4.6.0 - feature_fraction🔗︎, default =1.0, type = double, aliases:sub_feature,colsample_bytree, constraints:0.0<feature_fraction<=1.0LightGBM will randomly select a subset of features on each iteration (tree) iffeature_fractionis smaller than1.0. For example, if you set it to0.8, LightGBM will select 80% of features before training each treecan be used to speed up trainingcan be used to deal with over-fitting - feature_fraction_bynode🔗︎, default =1.0, type = double, aliases:sub_feature_bynode,colsample_bynode, constraints:0.0<feature_fraction_bynode<=1.0LightGBM will randomly select a subset of features on each tree node iffeature_fraction_bynodeis smaller than1.0. For example, if you set it to0.8, LightGBM will select 80% of features", "prev_chunk_id": "chunk_125", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_127", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "at each tree nodecan be used to deal with over-fittingNote: unlikefeature_fraction, this cannot speed up trainingNote: if bothfeature_fractionandfeature_fraction_bynodeare smaller than1.0, the final fraction of each node isfeature_fraction*feature_fraction_bynode - feature_fraction_seed🔗︎, default =2, type = intrandom seed forfeature_fraction - extra_trees🔗︎, default =false, type = bool, aliases:extra_treeuse extremely randomized treesif set totrue, when evaluating node splits LightGBM will check only one randomly-chosen threshold for each featurecan be used to speed up trainingcan be used to deal with over-fitting - extra_seed🔗︎, default =6, type = intrandom seed for selecting thresholds whenextra_treesis true - early_stopping_round🔗︎, default =0, type = int, aliases:early_stopping_rounds,early_stopping,n_iter_no_changewill stop training if one metric of one validation data doesn’t improve in lastearly_stopping_roundrounds<=0means disablecan be used to speed up training - early_stopping_min_delta🔗︎, default =0.0, type = double, constraints:early_stopping_min_delta>=0.0when early stopping is used (i.e.early_stopping_round>0), require the early stopping metric to improve by at least this delta to be considered an improvementNew in version 4.4.0 - first_metric_only🔗︎, default =false, type = boolLightGBM allows you to provide multiple evaluation metrics. Set this totrue, if you want to use only the first metric for early stopping - max_delta_step🔗︎, default =0.0, type = double, aliases:max_tree_output,max_leaf_outputused to limit the max output of tree leaves<=0means no constraintthe final max output of leaves islearning_rate*max_delta_step - lambda_l1🔗︎, default =0.0, type = double, aliases:reg_alpha,l1_regularization, constraints:lambda_l1>=0.0L1 regularization - lambda_l2🔗︎, default =0.0, type = double, aliases:reg_lambda,lambda,l2_regularization, constraints:lambda_l2>=0.0L2 regularization - linear_lambda🔗︎, default =0.0, type = double, constraints:linear_lambda>=0.0linear tree regularization, corresponds to the parameterlambdain Eq. 3 ofGradient Boosting with Piece-Wise Linear Regression Trees - min_gain_to_split🔗︎, default =0.0, type = double, aliases:min_split_gain, constraints:min_gain_to_split>=0.0the minimal gain to perform splitcan be used to speed up training - drop_rate🔗︎, default =0.1, type = double, aliases:rate_drop, constraints:0.0<=drop_rate<=1.0used only indartdropout rate: a fraction of previous trees to drop during the dropout - max_drop🔗︎, default =50, type = intused only indartmax number of dropped trees", "prev_chunk_id": "chunk_126", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_128", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "during one boosting iteration<=0means no limit - skip_drop🔗︎, default =0.5, type = double, constraints:0.0<=skip_drop<=1.0used only indartprobability of skipping the dropout procedure during a boosting iteration - xgboost_dart_mode🔗︎, default =false, type = boolused only indartset this totrue, if you want to use XGBoost DART mode - uniform_drop🔗︎, default =false, type = boolused only indartset this totrue, if you want to use uniform drop - drop_seed🔗︎, default =4, type = intused only indartrandom seed to choose dropping models - top_rate🔗︎, default =0.2, type = double, constraints:0.0<=top_rate<=1.0used only ingossthe retain ratio of large gradient data - other_rate🔗︎, default =0.1, type = double, constraints:0.0<=other_rate<=1.0used only ingossthe retain ratio of small gradient data - min_data_per_group🔗︎, default =100, type = int, constraints:min_data_per_group>0used for the categorical featuresminimal number of data per categorical group - max_cat_threshold🔗︎, default =32, type = int, constraints:max_cat_threshold>0used for the categorical featureslimit number of split points considered for categorical features. Seethe documentation on how LightGBM finds optimal splits for categorical featuresfor more detailscan be used to speed up training - cat_l2🔗︎, default =10.0, type = double, constraints:cat_l2>=0.0used for the categorical featuresL2 regularization in categorical split - cat_smooth🔗︎, default =10.0, type = double, constraints:cat_smooth>=0.0used for the categorical featuresthis can reduce the effect of noises in categorical features, especially for categories with few data - max_cat_to_onehot🔗︎, default =4, type = int, constraints:max_cat_to_onehot>0used for the categorical featureswhen number of categories of one feature smaller than or equal tomax_cat_to_onehot, one-vs-other split algorithm will be used - top_k🔗︎, default =20, type = int, aliases:topk, constraints:top_k>0used only invotingtree learner, refer toVoting parallelset this to larger value for more accurate result, but it will slow down the training speed - monotone_constraints🔗︎, default =None, type = multi-int, aliases:mc,monotone_constraint,monotonic_cstused for constraints of monotonic features1means increasing,-1means decreasing,0means non-constraintyou need to specify all features in order. For example,mc=-1,0,1means decreasing for the 1st feature, non-constraint for", "prev_chunk_id": "chunk_127", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_129", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "the 2nd feature and increasing for the 3rd feature - monotone_constraints_method🔗︎, default =basic, type = enum, options:basic,intermediate,advanced, aliases:monotone_constraining_method,mc_methodused only ifmonotone_constraintsis setmonotone constraints methodbasic, the most basic monotone constraints method. It does not slow down the training speed at all, but over-constrains the predictionsintermediate, amore advanced method, which may slow down the training speed very slightly. However, this method is much less constraining than the basic method and should significantly improve the resultsadvanced, aneven more advanced method, which may slow down the training speed. However, this method is even less constraining than the intermediate method and should again significantly improve the results - monotone_penalty🔗︎, default =0.0, type = double, aliases:monotone_splits_penalty,ms_penalty,mc_penalty, constraints:monotone_penalty>=0.0used only ifmonotone_constraintsis setmonotone penalty: a penalization parameter X forbids any monotone splits on the first X (rounded down) level(s) of the tree. The penalty applied to monotone splits on a given depth is a continuous, increasing function the penalization parameterif0.0(the default), no penalization is applied - feature_contri🔗︎, default =None, type = multi-double, aliases:feature_contrib,fc,fp,feature_penaltyused to control feature’s split gain, will usegain[i]=max(0,feature_contri[i])*gain[i]to replace the split gain of i-th featureyou need to specify all features in order - forcedsplits_filename🔗︎, default =\"\", type = string, aliases:fs,forced_splits_filename,forced_splits_file,forced_splitspath to a.jsonfile that specifies splits to force at the top of every decision tree before best-first learning commences.jsonfile can be arbitrarily nested, and each split containsfeature,thresholdfields, as well asleftandrightfields representing subsplitscategorical splits are forced in a one-hot fashion, withleftrepresenting the split containing the feature value andrightrepresenting other valuesNote: the forced split logic will be ignored, if the split makes gain worseseethis fileas an example - refit_decay_rate🔗︎, default =0.9, type = double, constraints:0.0<=refit_decay_rate<=1.0decay rate ofrefittask, will useleaf_output=refit_decay_rate*old_leaf_output+(1.0-refit_decay_rate)*new_leaf_outputto refit treesused only inrefittask in CLI version or as argument inrefitfunction in language-specific package - cegb_tradeoff🔗︎, default =1.0, type = double, constraints:cegb_tradeoff>=0.0cost-effective gradient boosting multiplier for all penalties - cegb_penalty_split🔗︎, default =0.0,", "prev_chunk_id": "chunk_128", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_130", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "type = double, constraints:cegb_penalty_split>=0.0cost-effective gradient-boosting penalty for splitting a node - cegb_penalty_feature_lazy🔗︎, default =0,0,...,0, type = multi-doublecost-effective gradient boosting penalty for using a featureapplied per data point - cegb_penalty_feature_coupled🔗︎, default =0,0,...,0, type = multi-doublecost-effective gradient boosting penalty for using a featureapplied once per forest - path_smooth🔗︎, default =0, type = double, constraints:path_smooth>=0.0controls smoothing applied to tree nodeshelps prevent overfitting on leaves with few samplesif0.0(the default), no smoothing is appliedifpath_smooth>0thenmin_data_in_leafmust be at least2larger values give stronger regularizationthe weight of each node isw*(n/path_smooth)/(n/path_smooth+1)+w_p/(n/path_smooth+1), wherenis the number of samples in the node,wis the optimal node weight to minimise the loss (approximately-sum_gradients/sum_hessians), andw_pis the weight of the parent nodenote that the parent outputw_pitself has smoothing applied, unless it is the root node, so that the smoothing effect accumulates with the tree depth - interaction_constraints🔗︎, default =\"\", type = stringcontrols which features can appear in the same branchby default interaction constraints are disabled, to enable them you can specifyfor CLI, lists separated by commas, e.g.[0,1,2],[2,3]for Python-package, list of lists, e.g.[[0,1,2],[2,3]]for R-package, list of character or numeric vectors, e.g.list(c(\"var1\",\"var2\",\"var3\"),c(\"var3\",\"var4\"))orlist(c(1L,2L,3L),c(3L,4L)). Numeric vectors should use 1-based indexing, where1Lis the first feature,2Lis the second feature, etc.any two features can only appear in the same branch only if there exists a constraint containing both features - verbosity🔗︎, default =1, type = int, aliases:verbosecontrols the level of LightGBM’s verbosity<0: Fatal,=0: Error (Warning),=1: Info,>1: Debug - input_model🔗︎, default =\"\", type = string, aliases:model_input,model_infilename of input modelforpredictiontask, this model will be applied to prediction datafortraintask, training will be continued from this modelNote: can be used only in CLI version - output_model🔗︎, default =LightGBM_model.txt, type = string, aliases:model_output,model_outfilename of output model in trainingNote: can be used only in CLI version - saved_feature_importance_type🔗︎, default =0, type = intthe feature importance type in the saved model file0: count-based feature importance (numbers of splits are counted);1: gain-based", "prev_chunk_id": "chunk_129", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_131", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Learning Control Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Learning Control Parameters", "content": "feature importance (values of gain are counted)Note: can be used only in CLI version - snapshot_freq🔗︎, default =-1, type = int, aliases:save_periodfrequency of saving model file snapshotset this to positive value to enable this function. For example, the model file will be snapshotted at each iteration ifsnapshot_freq=1Note: can be used only in CLI version - use_quantized_grad🔗︎, default =false, type = boolwhether to use gradient quantization when trainingenabling this will discretize (quantize) the gradients and hessians into bins ofnum_grad_quant_binswith quantized training, most arithmetics in the training process will be integer operationsgradient quantization can accelerate training, with little accuracy drop in most casesNote: works only withcpuandcudadevice typeNew in version 4.0.0 - num_grad_quant_bins🔗︎, default =4, type = intused only ifuse_quantized_grad=truenumber of bins to quantization gradients and hessianswith more bins, the quantized training will be closer to full precision trainingNote: works only withcpuandcudadevice typeNew in version 4.0.0 - quant_train_renew_leaf🔗︎, default =false, type = boolused only ifuse_quantized_grad=truewhether to renew the leaf values with original gradients when quantized trainingrenewing is very helpful for good quantized training accuracy for ranking objectivesNote: works only withcpuandcudadevice typeNew in version 4.0.0 - stochastic_rounding🔗︎, default =true, type = boolused only ifuse_quantized_grad=truewhether to use stochastic rounding in gradient quantizationNote: works only withcpuandcudadevice typeNew in version 4.0.0", "prev_chunk_id": "chunk_130", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_132", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Dataset Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dataset Parameters", "content": "Dataset Parameters - linear_tree🔗︎, default =false, type = bool, aliases:linear_treesfit piecewise linear gradient boosting treetree splits are chosen in the usual way, but the model at each leaf is linear instead of constantthe linear model at each leaf includes all the numerical features in that leaf’s branchthe first tree has constant leaf valuescategorical features are used for splits as normal but are not used in the linear modelsmissing values should not be encoded as0. Usenp.nanfor Python,NAfor the CLI, andNA,NA_real_, orNA_integer_for Rit is recommended to rescale data before training so that features have similar mean and standard deviationNote: works only withcpu,gpudevice type andserialtree learnerNote:regression_l1objective is not supported with linear tree boostingNote: settinglinear_tree=truesignificantly increases the memory use of LightGBMNote: if you specifymonotone_constraints, constraints will be enforced when choosing the split points, but not when fitting the linear models on leaves - max_bin🔗︎, default =255, type = int, aliases:max_bins, constraints:max_bin>1max number of bins that feature values will be bucketed insmall number of bins may reduce training accuracy but may increase general power (deal with over-fitting)LightGBM will auto compress memory according tomax_bin. For example, LightGBM will useuint8_tfor feature value ifmax_bin=255 - max_bin_by_feature🔗︎, default =None, type = multi-intmax number of bins for each featureif not specified, will usemax_binfor all features - min_data_in_bin🔗︎, default =3, type = int, constraints:min_data_in_bin>0minimal number of data inside one binuse this to avoid one-data-one-bin (potential over-fitting) - bin_construct_sample_cnt🔗︎, default =200000, type = int, aliases:subsample_for_bin, constraints:bin_construct_sample_cnt>0number of data that sampled to construct feature discrete binssetting this to larger value will give better training result, but may increase data loading timeset this to larger value if data is very sparseNote: don’t set this to small values, otherwise, you may encounter unexpected errors and poor accuracy - data_random_seed🔗︎, default =1, type = int, aliases:data_seedrandom seed for sampling data to construct histogram bins - is_enable_sparse🔗︎,", "prev_chunk_id": "chunk_131", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_133", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Dataset Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dataset Parameters", "content": "default =true, type = bool, aliases:is_sparse,enable_sparse,sparseused to enable/disable sparse optimization - enable_bundle🔗︎, default =true, type = bool, aliases:is_enable_bundle,bundleset this tofalseto disable Exclusive Feature Bundling (EFB), which is described inLightGBM: A Highly Efficient Gradient Boosting Decision TreeNote: disabling this may cause the slow training speed for sparse datasets - use_missing🔗︎, default =true, type = boolset this tofalseto disable the special handle of missing value - zero_as_missing🔗︎, default =false, type = boolset this totrueto treat all zero as missing values (including the unshown values in LibSVM / sparse matrices)set this tofalseto usenafor representing missing values - feature_pre_filter🔗︎, default =true, type = boolset this totrue(the default) to tell LightGBM to ignore the features that are unsplittable based onmin_data_in_leafas dataset object is initialized only once and cannot be changed after that, you may need to set this tofalsewhen searching parameters withmin_data_in_leaf, otherwise features are filtered bymin_data_in_leaffirstly if you don’t reconstruct dataset objectNote: setting this tofalsemay slow down the training - pre_partition🔗︎, default =false, type = bool, aliases:is_pre_partitionused for distributed learning (excluding thefeature_parallelmode)trueif training data are pre-partitioned, and different machines use different partitions - two_round🔗︎, default =false, type = bool, aliases:two_round_loading,use_two_round_loadingset this totrueif data file is too big to fit in memoryby default, LightGBM will map data file to memory and load features from memory. This will provide faster data loading speed, but may cause run out of memory error when the data file is very bigNote: works only in case of loading data directly from text file - header🔗︎, default =false, type = bool, aliases:has_headerset this totrueif input data has headerNote: works only in case of loading data directly from text file - label_column🔗︎, default =\"\", type = int or string, aliases:labelused to specify the label columnuse number for index, e.g.label=0means column_0 is the labeladd a prefixname:for column name, e.g.label=name:is_clickif omitted, the first column", "prev_chunk_id": "chunk_132", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_134", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Dataset Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dataset Parameters", "content": "in the training data is used as the labelNote: works only in case of loading data directly from text file - weight_column🔗︎, default =\"\", type = int or string, aliases:weightused to specify the weight columnuse number for index, e.g.weight=0means column_0 is the weightadd a prefixname:for column name, e.g.weight=name:weightNote: works only in case of loading data directly from text fileNote: index starts from0and it doesn’t count the label column when passing type isint, e.g. when label is column_0, and weight is column_1, the correct parameter isweight=0Note: weights should be non-negative - group_column🔗︎, default =\"\", type = int or string, aliases:group,group_id,query_column,query,query_idused to specify the query/group id columnuse number for index, e.g.query=0means column_0 is the query idadd a prefixname:for column name, e.g.query=name:query_idNote: works only in case of loading data directly from text fileNote: data should be grouped by query_id, for more information, seeQuery DataNote: index starts from0and it doesn’t count the label column when passing type isint, e.g. when label is column_0 and query_id is column_1, the correct parameter isquery=0 - ignore_column🔗︎, default =\"\", type = multi-int or string, aliases:ignore_feature,blacklistused to specify some ignoring columns in traininguse number for index, e.g.ignore_column=0,1,2means column_0, column_1 and column_2 will be ignoredadd a prefixname:for column name, e.g.ignore_column=name:c1,c2,c3means c1, c2 and c3 will be ignoredNote: works only in case of loading data directly from text fileNote: index starts from0and it doesn’t count the label column when passing type isintNote: despite the fact that specified columns will be completely ignored during the training, they still should have a valid format allowing LightGBM to load file successfully - categorical_feature🔗︎, default =\"\", type = multi-int or string, aliases:cat_feature,categorical_column,cat_column,categorical_featuresused to specify categorical featuresuse number for index, e.g.categorical_feature=0,1,2means column_0, column_1 and column_2 are categorical featuresadd a prefixname:for column name, e.g.categorical_feature=name:c1,c2,c3means c1, c2 and c3 are categorical featuresNote: all values will be cast toint32(integer", "prev_chunk_id": "chunk_133", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_135", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Dataset Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dataset Parameters", "content": "codes will be extracted from pandas categoricals in the Python-package)Note: index starts from0and it doesn’t count the label column when passing type isintNote: all values should be less thanInt32.MaxValue(2147483647)Note: using large values could be memory consuming. Tree decision rule works best when categorical features are presented by consecutive integers starting from zeroNote: all negative values will be treated asmissing valuesNote: the output cannot be monotonically constrained with respect to a categorical featureNote: floating point numbers in categorical features will be rounded towards 0 - forcedbins_filename🔗︎, default =\"\", type = stringpath to a.jsonfile that specifies bin upper bounds for some or all features.jsonfile should contain an array of objects, each containing the wordfeature(integer feature index) andbin_upper_bound(array of thresholds for binning)seethis fileas an example - save_binary🔗︎, default =false, type = bool, aliases:is_save_binary,is_save_binary_fileiftrue, LightGBM will save the dataset (including validation data) to a binary file. This speed ups the data loading for the next timeNote:init_scoreis not saved in binary fileNote: can be used only in CLI version; for language-specific packages you can use the correspondent function - precise_float_parser🔗︎, default =false, type = booluse precise floating point number parsing for text parser (e.g. CSV, TSV, LibSVM input)Note: setting this totruemay lead to much slower text parsing - parser_config_file🔗︎, default =\"\", type = stringpath to a.jsonfile that specifies customized parser initialized configurationseelightgbm-transformfor usage examplesNote:lightgbm-transformis not maintained by LightGBM’s maintainers. Bug reports or feature requests should go toissues pageNew in version 4.0.0", "prev_chunk_id": "chunk_134", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_136", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Predict Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Predict Parameters", "content": "Predict Parameters - start_iteration_predict🔗︎, default =0, type = intused only inpredictiontaskused to specify from which iteration to start the prediction<=0means from the first iteration - num_iteration_predict🔗︎, default =-1, type = intused only inpredictiontaskused to specify how many trained iterations will be used in prediction<=0means no limit - predict_raw_score🔗︎, default =false, type = bool, aliases:is_predict_raw_score,predict_rawscore,raw_scoreused only inpredictiontaskset this totrueto predict only the raw scoresset this tofalseto predict transformed scores - predict_leaf_index🔗︎, default =false, type = bool, aliases:is_predict_leaf_index,leaf_indexused only inpredictiontaskset this totrueto predict with leaf index of all trees - predict_contrib🔗︎, default =false, type = bool, aliases:is_predict_contrib,contribused only inpredictiontaskset this totrueto estimateSHAP values, which represent how each feature contributes to each predictionproduces#features+1values where the last value is the expected value of the model output over the training dataNote: if you want to get more explanation for your model’s predictions using SHAP values like SHAP interaction values, you can installshap packageNote: unlike the shap package, withpredict_contribwe return a matrix with an extra column, where the last column is the expected valueNote: this feature is not implemented for linear trees - predict_disable_shape_check🔗︎, default =false, type = boolused only inpredictiontaskcontrol whether or not LightGBM raises an error when you try to predict on data with a different number of features than the training dataiffalse(the default), a fatal error will be raised if the number of features in the dataset you predict on differs from the number seen during trainingiftrue, LightGBM will attempt to predict on whatever data you provide. This is dangerous because you might get incorrect predictions, but you could use it in situations where it is difficult or expensive to generate some features and you are very confident that they were never chosen for splits in the modelNote: be very careful setting this parameter totrue - pred_early_stop🔗︎, default =false, type = boolused only", "prev_chunk_id": "chunk_135", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_137", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Predict Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Predict Parameters", "content": "inpredictiontaskused only inclassificationandrankingapplicationsused only for predicting normal or raw scoresiftrue, will use early-stopping to speed up the prediction. May affect the accuracyNote: cannot be used withrfboosting type or custom objective function - pred_early_stop_freq🔗︎, default =10, type = intused only inpredictiontask and ifpred_early_stop=truethe frequency of checking early-stopping prediction - pred_early_stop_margin🔗︎, default =10.0, type = doubleused only inpredictiontask and ifpred_early_stop=truethe threshold of margin in early-stopping prediction - output_result🔗︎, default =LightGBM_predict_result.txt, type = string, aliases:predict_result,prediction_result,predict_name,prediction_name,pred_name,name_predused only inpredictiontaskfilename of prediction resultNote: can be used only in CLI version", "prev_chunk_id": "chunk_136", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_138", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Convert Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Convert Parameters", "content": "Convert Parameters - convert_model_language🔗︎, default =\"\", type = stringused only inconvert_modeltaskonlycppis supported yet; for conversion model to other languages consider usingm2cgenutilityifconvert_model_languageis set andtask=train, the model will be also convertedNote: can be used only in CLI version - convert_model🔗︎, default =gbdt_prediction.cpp, type = string, aliases:convert_model_fileused only inconvert_modeltaskoutput filename of converted modelNote: can be used only in CLI version", "prev_chunk_id": "chunk_137", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_139", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Objective Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Objective Parameters", "content": "Objective Parameters - objective_seed🔗︎, default =5, type = intused only inrank_xendcgobjectiverandom seed for objectives, if random process is needed - num_class🔗︎, default =1, type = int, aliases:num_classes, constraints:num_class>0used only inmulti-classclassification application - is_unbalance🔗︎, default =false, type = bool, aliases:unbalance,unbalanced_setsused only inbinaryandmulticlassovaapplicationsset this totrueif training data are unbalancedNote: while enabling this should increase the overall performance metric of your model, it will also result in poor estimates of the individual class probabilitiesNote: this parameter cannot be used at the same time withscale_pos_weight, choose onlyoneof them - scale_pos_weight🔗︎, default =1.0, type = double, constraints:scale_pos_weight>0.0used only inbinaryandmulticlassovaapplicationsweight of labels with positive classNote: while enabling this should increase the overall performance metric of your model, it will also result in poor estimates of the individual class probabilitiesNote: this parameter cannot be used at the same time withis_unbalance, choose onlyoneof them - sigmoid🔗︎, default =1.0, type = double, constraints:sigmoid>0.0used only inbinaryandmulticlassovaclassification and inlambdarankapplicationsparameter for the sigmoid function - boost_from_average🔗︎, default =true, type = boolused only inregression,binary,multiclassovaandcross-entropyapplicationsadjusts initial score to the mean of labels for faster convergence - reg_sqrt🔗︎, default =false, type = boolused only inregressionapplicationused to fitsqrt(label)instead of original values and prediction result will be also automatically converted toprediction^2might be useful in case of large-range labels - alpha🔗︎, default =0.9, type = double, constraints:alpha>0.0used only inhuberandquantileregressionapplicationsparameter forHuber lossandQuantile regression - fair_c🔗︎, default =1.0, type = double, constraints:fair_c>0.0used only infairregressionapplicationparameter forFair loss - poisson_max_delta_step🔗︎, default =0.7, type = double, constraints:poisson_max_delta_step>0.0used only inpoissonregressionapplicationparameter forPoisson regressionto safeguard optimization - tweedie_variance_power🔗︎, default =1.5, type = double, constraints:1.0<=tweedie_variance_power<2.0used only intweedieregressionapplicationused to control the variance of the tweedie distributionset this closer to2to shift towards aGammadistributionset this closer to1to shift towards aPoissondistribution - lambdarank_truncation_level🔗︎, default =30, type = int, constraints:lambdarank_truncation_level>0used only inlambdarankapplicationcontrols the number of top-results to focus on during training, refer to “truncation level” in the Sec. 3 ofLambdaMART paperthis parameter", "prev_chunk_id": "chunk_138", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_140", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Objective Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Objective Parameters", "content": "is closely related to the desirable cutoffkin the metricNDCG@kthat we aim at optimizing the ranker for. The optimal setting for this parameter is likely to be slightly higher thank(e.g.,k+3) to include more pairs of documents to train on, but perhaps not too high to avoid deviating too much from the desired target metricNDCG@k - lambdarank_norm🔗︎, default =true, type = boolused only inlambdarankapplicationset this totrueto normalize the lambdas for different queries, and improve the performance for unbalanced dataset this tofalseto enforce the original lambdarank algorithm - label_gain🔗︎, default =0,1,3,7,15,31,63,...,2^30-1, type = multi-doubleused only inlambdarankapplicationrelevant gain for labels. For example, the gain of label2is3in case of default label gainsseparate by, - lambdarank_position_bias_regularization🔗︎, default =0.0, type = double, constraints:lambdarank_position_bias_regularization>=0.0used only inlambdarankapplication when positional information is provided and position bias is modeledlarger values reduce the inferred position bias factorsNew in version 4.1.0", "prev_chunk_id": "chunk_139", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_141", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Metric Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Metric Parameters", "content": "Metric Parameters - metric🔗︎, default =\"\", type = multi-enum, aliases:metrics,metric_typesmetric(s) to be evaluated on the evaluation set(s)\"\"(empty string or not specified) means that metric corresponding to specifiedobjectivewill be used (this is possible only for pre-defined objective functions, otherwise no evaluation metric will be added)\"None\"(string,notaNonevalue) means that no metric will be registered, aliases:na,null,customl1, absolute loss, aliases:mean_absolute_error,mae,regression_l1l2, square loss, aliases:mean_squared_error,mse,regression_l2,regressionrmse, root square loss, aliases:root_mean_squared_error,l2_rootquantile,Quantile regressionmape,MAPE loss, aliases:mean_absolute_percentage_errorhuber,Huber lossfair,Fair losspoisson, negative log-likelihood forPoisson regressiongamma, negative log-likelihood forGammaregressiongamma_deviance, residual deviance forGammaregressiontweedie, negative log-likelihood forTweedieregressionndcg,NDCG, aliases:lambdarank,rank_xendcg,xendcg,xe_ndcg,xe_ndcg_mart,xendcg_martmap,MAP, aliases:mean_average_precisionauc,AUCaverage_precision,average precision scorebinary_logloss,log loss, aliases:binarybinary_error, for one sample:0for correct classification,1for error classificationauc_mu,AUC-mumulti_logloss, log loss for multi-class classification, aliases:multiclass,softmax,multiclassova,multiclass_ova,ova,ovrmulti_error, error rate for multi-class classificationcross_entropy, cross-entropy (with optional linear weights), aliases:xentropycross_entropy_lambda, “intensity-weighted” cross-entropy, aliases:xentlambdakullback_leibler,Kullback-Leibler divergence, aliases:kldivsupport multiple metrics, separated by, - metric_freq🔗︎, default =1, type = int, aliases:output_freq, constraints:metric_freq>0frequency for metric outputNote: can be used only in CLI version - is_provide_training_metric🔗︎, default =false, type = bool, aliases:training_metric,is_training_metric,train_metricset this totrueto output metric result over training datasetNote: can be used only in CLI version - eval_at🔗︎, default =1,2,3,4,5, type = multi-int, aliases:ndcg_eval_at,ndcg_at,map_eval_at,map_atused only withndcgandmapmetricsNDCGandMAPevaluation positions, separated by, - multi_error_top_k🔗︎, default =1, type = int, constraints:multi_error_top_k>0used only withmulti_errormetricthreshold for top-k multi-error metricthe error on each sample is0if the true class is among the topmulti_error_top_kpredictions, and1otherwisemore precisely, the error on a sample is0if there are at leastnum_classes-multi_error_top_kpredictions strictly less than the prediction on the true classwhenmulti_error_top_k=1this is equivalent to the usual multi-error metric - auc_mu_weights🔗︎, default =None, type = multi-doubleused only withauc_mumetriclist representing flattened matrix (in row-major order) giving loss weights for classification errorslist should haven*nelements, wherenis the number of classesthe matrix co-ordinate[i,j]should correspond to thei*n+j-th element of the listif not specified, will use equal weights for all classes", "prev_chunk_id": "chunk_140", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_142", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Network Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Network Parameters", "content": "Network Parameters - num_machines🔗︎, default =1, type = int, aliases:num_machine, constraints:num_machines>0the number of machines for distributed learning applicationthis parameter is needed to be set in bothsocketandMPIversions - local_listen_port🔗︎, default =12400(randomforDask-package), type = int, aliases:local_port,port, constraints:local_listen_port>0TCP listen port for local machinesNote: don’t forget to allow this port in firewall settings before training - time_out🔗︎, default =120, type = int, constraints:time_out>0socket time-out in minutes - machine_list_filename🔗︎, default =\"\", type = string, aliases:machine_list_file,machine_list,mlistpath of file that lists machines for this distributed learning applicationeach line contains one IP and one port for one machine. The format isipport(space as a separator)Note: can be used only in CLI version - machines🔗︎, default =\"\", type = string, aliases:workers,nodeslist of machines in the following format:ip1:port1,ip2:port2", "prev_chunk_id": "chunk_141", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_143", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "GPU Parameters", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Parameters", "content": "GPU Parameters - gpu_platform_id🔗︎, default =-1, type = intused only withgpudevice typeOpenCL platform ID. Usually each GPU vendor exposes one OpenCL platform-1means the system-wide default platformNote: refer toGPU Targetsfor more details - gpu_device_id🔗︎, default =-1, type = intOpenCL device ID in the specified platform or CUDA device ID. Each GPU in the selected platform has a unique device ID-1means the default device in the selected platformNote: refer toGPU Targetsfor more details - gpu_use_dp🔗︎, default =false, type = boolset this totrueto use double precision math on GPU (by default single precision is used)Note: can be used only in OpenCL implementation (device_type=\"gpu\"), in CUDA implementation only double precision is currently supported - num_gpu🔗︎, default =1, type = int, constraints:num_gpu>0number of GPUsNote: can be used only in CUDA implementation (device_type=\"cuda\")", "prev_chunk_id": "chunk_142", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_144", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Continued Training with Input Score", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Continued Training with Input Score", "content": "Continued Training with Input Score LightGBM supports continued training with initial scores. It uses an additional file to store these initial scores, like the following: 0.5 -0.1 0.9 ... It means the initial score of the first data row is 0.5, second is -0.1, and so on. The initial score file corresponds with data file line by line, and has per score per line. If the name of data file is train.txt, the initial score file should be named as train.txt.init and placed in the same folder as the data file. In this case, LightGBM will auto load initial score file if it exists. If binary data files exist for raw data file train.txt, for example in the name train.txt.bin, then the initial score file should be named as train.txt.bin.init.", "prev_chunk_id": "chunk_143", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_145", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Weight Data", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Weight Data", "content": "Weight Data LightGBM supports weighted training. It uses an additional file to store weight data, like the following: 1.0 0.5 0.8 ... It means the weight of the first data row is 1.0, second is 0.5, and so on. Weights should be non-negative. The weight file corresponds with data file line by line, and has per weight per line. And if the name of data file is train.txt, the weight file should be named as train.txt.weight and placed in the same folder as the data file. In this case, LightGBM will load the weight file automatically if it exists. Also, you can include weight column in your data file. Please refer to the weight_column parameter in above.", "prev_chunk_id": "chunk_144", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_146", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters.html", "title": "Query Data", "page_title": "Parameters — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Query Data", "content": "Query Data For learning to rank, it needs query information for training data. LightGBM uses an additional file to store query data, like the following: 27 18 67 ... For wrapper libraries like in Python and R, this information can also be provided as an array-like via the Dataset parameter group. [27, 18, 67, ...] For example, if you have a 112-document dataset with group = [27, 18, 67], that means that you have 3 groups, where the first 27 records are in the first group, records 28-45 are in the second group, and records 46-112 are in the third group. Note: data should be ordered by the query. If the name of data file is train.txt, the query file should be named as train.txt.query and placed in the same folder as the data file. In this case, LightGBM will load the query file automatically if it exists. Also, you can include query/group id column in your data file. Please refer to the group_column parameter in above.", "prev_chunk_id": "chunk_145", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_147", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Parameters Tuning", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Parameters Tuning", "content": "Parameters Tuning This page contains parameters tuning guides for different scenarios. List of other helpful links - Parameters - Python API - FLAMLfor automated hyperparameter tuning - Optunafor automated hyperparameter tuning", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_148", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Tune Parameters for the Leaf-wise (Best-first) Tree", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Tune Parameters for the Leaf-wise (Best-first) Tree", "content": "Tune Parameters for the Leaf-wise (Best-first) Tree LightGBM uses the leaf-wise tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can converge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters. To get good results using a leaf-wise tree, these are some important parameters: - num_leaves. This is the main parameter to control the complexity of the tree model. Theoretically, we can setnum_leaves=2^(max_depth)to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. A leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune thenum_leaves, we should let it be smaller than2^(max_depth). For example, when themax_depth=7the depth-wise tree can get good accuracy, but settingnum_leavesto127may cause over-fitting, and setting it to70or80may get better accuracy than depth-wise. - min_data_in_leaf. This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples andnum_leaves. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset. - max_depth. You also can usemax_depthto limit the tree depth explicitly. If you setmax_depth, also explicitly setnum_leavesto some value<=2^max_depth.", "prev_chunk_id": "chunk_147", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_149", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Add More Computational Resources", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Add More Computational Resources", "content": "Add More Computational Resources On systems where it is available, LightGBM uses OpenMP to parallelize many operations. The maximum number of threads used by LightGBM is controlled by the parameter num_threads. By default, this will defer to the default behavior of OpenMP (one thread per real CPU core or the value in environment variable OMP_NUM_THREADS, if it is set). For best performance, set this to the number of real CPU cores available. You might be able to achieve faster training by moving to a machine with more available CPU cores. Using distributed (multi-machine) training might also reduce training time. See the Distributed Learning Guide for details.", "prev_chunk_id": "chunk_148", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_150", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Use a GPU-enabled version of LightGBM", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Use a GPU-enabled version of LightGBM", "content": "Use a GPU-enabled version of LightGBM You might find that training is faster using a GPU-enabled build of LightGBM. See the GPU Tutorial for details.", "prev_chunk_id": "chunk_149", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_151", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Grow Shallower Trees", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Grow Shallower Trees", "content": "Grow Shallower Trees The total training time for LightGBM increases with the total number of tree nodes added. LightGBM comes with several parameters that can be used to control the number of nodes per tree. The suggestions below will speed up training, but might hurt training accuracy.", "prev_chunk_id": "chunk_150", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_152", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease max_depth", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease max_depth", "content": "Decrease max_depth This parameter is an integer that controls the maximum distance between the root node of each tree and a leaf node. Decrease max_depth to reduce training time.", "prev_chunk_id": "chunk_151", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_153", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease num_leaves", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease num_leaves", "content": "Decrease num_leaves LightGBM adds nodes to trees based on the gain from adding that node, regardless of depth. This figure from the feature documentation illustrates the process. Because of this growth strategy, it isn’t straightforward to use max_depth alone to limit the complexity of trees. The num_leaves parameter sets the maximum number of nodes per tree. Decrease num_leaves to reduce training time.", "prev_chunk_id": "chunk_152", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_154", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Increase min_gain_to_split", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Increase min_gain_to_split", "content": "Increase min_gain_to_split When adding a new tree node, LightGBM chooses the split point that has the largest gain. Gain is basically the reduction in training loss that results from adding a split point. By default, LightGBM sets min_gain_to_split to 0.0, which means “there is no improvement that is too small”. However, in practice you might find that very small improvements in the training loss don’t have a meaningful impact on the generalization error of the model. Increase min_gain_to_split to reduce training time.", "prev_chunk_id": "chunk_153", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_155", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Increase min_data_in_leaf and min_sum_hessian_in_leaf", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Increase min_data_in_leaf and min_sum_hessian_in_leaf", "content": "Increase min_data_in_leaf and min_sum_hessian_in_leaf Depending on the size of the training data and the distribution of features, it’s possible for LightGBM to add tree nodes that only describe a small number of observations. In the most extreme case, consider the addition of a tree node that only a single observation from the training data falls into. This is very unlikely to generalize well, and probably is a sign of overfitting. This can be prevented indirectly with parameters like max_depth and num_leaves, but LightGBM also offers parameters to help you directly avoid adding these overly-specific tree nodes. - min_data_in_leaf: Minimum number of observations that must fall into a tree node for it to be added. - min_sum_hessian_in_leaf: Minimum sum of the Hessian (second derivative of the objective function evaluated for each observation) for observations in a leaf. For some regression objectives, this is just the minimum number of records that have to fall into each node. For classification objectives, it represents a sum over a distribution of probabilities. Seethis Stack Overflow answerfor a good description of how to reason about values of this parameter.", "prev_chunk_id": "chunk_154", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_156", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease num_iterations", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease num_iterations", "content": "Decrease num_iterations The num_iterations parameter controls the number of boosting rounds that will be performed. Since LightGBM uses decision trees as the learners, this can also be thought of as “number of trees”. If you try changing num_iterations, change the learning_rate as well. learning_rate will not have any impact on training time, but it will impact the training accuracy. As a general rule, if you reduce num_iterations, you should increase learning_rate. Choosing the right value of num_iterations and learning_rate is highly dependent on the data and objective, so these parameters are often chosen from a set of possible values through hyperparameter tuning. Decrease num_iterations to reduce training time.", "prev_chunk_id": "chunk_155", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_157", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Use Early Stopping", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Use Early Stopping", "content": "Use Early Stopping If early stopping is enabled, after each boosting round the model’s training accuracy is evaluated against a validation set that contains data not available to the training process. That accuracy is then compared to the accuracy as of the previous boosting round. If the model’s accuracy fails to improve for some number of consecutive rounds, LightGBM stops the training process. That “number of consecutive rounds” is controlled by the parameter early_stopping_round. For example, early_stopping_round=1 says “the first time accuracy on the validation set does not improve, stop training”. Set early_stopping_round and provide a validation set to possibly reduce training time.", "prev_chunk_id": "chunk_156", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_158", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Consider Fewer Splits", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Consider Fewer Splits", "content": "Consider Fewer Splits The parameters described in previous sections control how many trees are constructed and how many nodes are constructed per tree. Training time can be further reduced by reducing the amount of time needed to add a tree node to the model. The suggestions below will speed up training, but might hurt training accuracy.", "prev_chunk_id": "chunk_157", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_159", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Enable Feature Pre-Filtering When Creating Dataset", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Enable Feature Pre-Filtering When Creating Dataset", "content": "Enable Feature Pre-Filtering When Creating Dataset By default, when a LightGBM Dataset object is constructed, some features will be filtered out based on the value of min_data_in_leaf. For a simple example, consider a 1000-observation dataset with a feature called feature_1. feature_1 takes on only two values: 25.0 (995 observations) and 50.0 (5 observations). If min_data_in_leaf = 10, there is no split for this feature which will result in a valid split at least one of the leaf nodes will only have 5 observations. Instead of reconsidering this feature and then ignoring it every iteration, LightGBM filters this feature out at before training, when the Dataset is constructed. If this default behavior has been overridden by setting feature_pre_filter=False, set feature_pre_filter=True to reduce training time.", "prev_chunk_id": "chunk_158", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_160", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease max_bin or max_bin_by_feature When Creating Dataset", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease max_bin or max_bin_by_feature When Creating Dataset", "content": "Decrease max_bin or max_bin_by_feature When Creating Dataset LightGBM training buckets continuous features into discrete bins to improve training speed and reduce memory requirements for training. This binning is done one time during Dataset construction. The number of splits considered when adding a node is O(#feature * #bin), so reducing the number of bins per feature can reduce the number of splits that need to be evaluated. max_bin is controls the maximum number of bins that features will bucketed into. It is also possible to set this maximum feature-by-feature, by passing max_bin_by_feature. Reduce max_bin or max_bin_by_feature to reduce training time.", "prev_chunk_id": "chunk_159", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_161", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Increase min_data_in_bin When Creating Dataset", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Increase min_data_in_bin When Creating Dataset", "content": "Increase min_data_in_bin When Creating Dataset Some bins might contain a small number of observations, which might mean that the effort of evaluating that bin’s boundaries as possible split points isn’t likely to change the final model very much. You can control the granularity of the bins by setting min_data_in_bin. Increase min_data_in_bin to reduce training time.", "prev_chunk_id": "chunk_160", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_162", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease feature_fraction", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease feature_fraction", "content": "Decrease feature_fraction By default, LightGBM considers all features in a Dataset during the training process. This behavior can be changed by setting feature_fraction to a value > 0 and <= 1.0. Setting feature_fraction to 0.5, for example, tells LightGBM to randomly select 50% of features at the beginning of constructing each tree. This reduces the total number of splits that have to be evaluated to add each tree node. Decrease feature_fraction to reduce training time.", "prev_chunk_id": "chunk_161", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_163", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Decrease max_cat_threshold", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Decrease max_cat_threshold", "content": "Decrease max_cat_threshold LightGBM uses a custom approach for finding optimal splits for categorical features. In this process, LightGBM explores splits that break a categorical feature into two groups. These are sometimes called “k-vs.-rest” splits. Higher max_cat_threshold values correspond to more split points and larger possible group sizes to search. Decrease max_cat_threshold to reduce training time.", "prev_chunk_id": "chunk_162", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_164", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Use Bagging", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Use Bagging", "content": "Use Bagging By default, LightGBM uses all observations in the training data for each iteration. It is possible to instead tell LightGBM to randomly sample the training data. This process of training over multiple random samples without replacement is called “bagging”. Set bagging_freq to an integer greater than 0 to control how often a new sample is drawn. Set bagging_fraction to a value > 0.0 and < 1.0 to control the size of the sample. For example, {\"bagging_freq\": 5, \"bagging_fraction\": 0.75} tells LightGBM “re-sample without replacement every 5 iterations, and draw samples of 75% of the training data”. Decrease bagging_fraction to reduce training time.", "prev_chunk_id": "chunk_163", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_165", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Save Constructed Datasets with save_binary", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Save Constructed Datasets with save_binary", "content": "Save Constructed Datasets with save_binary This only applies to the LightGBM CLI. If you pass parameter save_binary, the training dataset and all validations sets will be saved in a binary format understood by LightGBM. This can speed up training next time, because binning and other work done when constructing a Dataset does not have to be re-done.", "prev_chunk_id": "chunk_164", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_166", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "For Better Accuracy", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "For Better Accuracy", "content": "For Better Accuracy - Use largemax_bin(may be slower) - Use smalllearning_ratewith largenum_iterations - Use largenum_leaves(may cause over-fitting) - Use bigger training data - Trydart", "prev_chunk_id": "chunk_165", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_167", "url": "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html", "title": "Deal with Over-fitting", "page_title": "Parameters Tuning — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Deal with Over-fitting", "content": "Deal with Over-fitting - Use smallmax_bin - Use smallnum_leaves - Usemin_data_in_leafandmin_sum_hessian_in_leaf - Use bagging by setbagging_fractionandbagging_freq - Use feature sub-sampling by setfeature_fraction - Use bigger training data - Trylambda_l1,lambda_l2andmin_gain_to_splitfor regularization - Trymax_depthto avoid growing deep tree - Tryextra_trees - Try increasingpath_smooth", "prev_chunk_id": "chunk_166", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_168", "url": "https://lightgbm.readthedocs.io/en/latest/C-API.html", "title": "C API", "page_title": "C API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "C API", "content": "C API CopyrightCopyright (c) 2016 Microsoft Corporation. All rights reserved. Licensed under the MIT License. See LICENSE file in the project root for license information.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_169", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Data Structure API", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Data Structure API", "content": "Data Structure API Dataset(data[, label, reference, weight, ...]) | Dataset in LightGBM. Booster([params, train_set, model_file, ...]) | Booster in LightGBM. CVBooster([model_file]) | CVBooster in LightGBM. Sequence() | Generic data access interface.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_170", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Training API", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Training API", "content": "Training API train(params, train_set[, num_boost_round, ...]) | Perform the training with given parameters. cv(params, train_set[, num_boost_round, ...]) | Perform the cross-validation with given parameters.", "prev_chunk_id": "chunk_169", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_171", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Scikit-learn API", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Scikit-learn API", "content": "Scikit-learn API LGBMModel(*[, boosting_type, num_leaves, ...]) | Implementation of the scikit-learn API for LightGBM. LGBMClassifier(*[, boosting_type, ...]) | LightGBM classifier. LGBMRegressor(*[, boosting_type, ...]) | LightGBM regressor. LGBMRanker(*[, boosting_type, num_leaves, ...]) | LightGBM ranker.", "prev_chunk_id": "chunk_170", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_172", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Dask API", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dask API", "content": "Dask API DaskLGBMClassifier(*[, boosting_type, ...]) | Distributed version of lightgbm.LGBMClassifier. DaskLGBMRegressor(*[, boosting_type, ...]) | Distributed version of lightgbm.LGBMRegressor. DaskLGBMRanker(*[, boosting_type, ...]) | Distributed version of lightgbm.LGBMRanker.", "prev_chunk_id": "chunk_171", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_173", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Callbacks", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Callbacks", "content": "Callbacks early_stopping(stopping_rounds[, ...]) | Create a callback that activates early stopping. log_evaluation([period, show_stdv]) | Create a callback that logs the evaluation results. record_evaluation(eval_result) | Create a callback that records the evaluation history into eval_result. reset_parameter(**kwargs) | Create a callback that resets the parameter after the first iteration.", "prev_chunk_id": "chunk_172", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_174", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Plotting", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Plotting", "content": "Plotting plot_importance(booster[, ax, height, xlim, ...]) | Plot model's feature importances. plot_split_value_histogram(booster, feature) | Plot split value histogram for the specified feature of the model. plot_metric(booster[, metric, ...]) | Plot one metric during training. plot_tree(booster[, ax, tree_index, ...]) | Plot specified tree. create_tree_digraph(booster[, tree_index, ...]) | Create a digraph representation of specified tree.", "prev_chunk_id": "chunk_173", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_175", "url": "https://lightgbm.readthedocs.io/en/latest/Python-API.html", "title": "Utilities", "page_title": "Python API — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Utilities", "content": "Utilities register_logger(logger[, info_method_name, ...]) | Register custom logger.", "prev_chunk_id": "chunk_174", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_176", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Distributed Learning Guide", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Distributed Learning Guide", "content": "Distributed Learning Guide This guide describes distributed learning in LightGBM. Distributed learning allows the use of multiple machines to produce a single model. Follow the Quick Start to know how to use LightGBM first.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_177", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "How Distributed LightGBM Works", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "How Distributed LightGBM Works", "content": "How Distributed LightGBM Works This section describes how distributed learning in LightGBM works. To learn how to do this in various programming languages and frameworks, please see Integrations.", "prev_chunk_id": "chunk_176", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_178", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Choose Appropriate Parallel Algorithm", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Choose Appropriate Parallel Algorithm", "content": "Choose Appropriate Parallel Algorithm LightGBM provides 3 distributed learning algorithms now. Parallel Algorithm | How to Use Data parallel | tree_learner=data Feature parallel | tree_learner=feature Voting parallel | tree_learner=voting These algorithms are suited for different scenarios, which is listed in the following table: | #data is small | #data is large #feature is small | Feature Parallel | Data Parallel #feature is large | Feature Parallel | Voting Parallel More details about these parallel algorithms can be found in optimization in distributed learning.", "prev_chunk_id": "chunk_177", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_179", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Integrations", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Integrations", "content": "Integrations This section describes how to run distributed LightGBM training in various programming languages and frameworks. To learn how distributed learning in LightGBM works generally, please see How Distributed LightGBM Works.", "prev_chunk_id": "chunk_178", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_180", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Apache Spark", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Apache Spark", "content": "Apache Spark Apache Spark users can use SynapseML for machine learning workflows with LightGBM. This project is not maintained by LightGBM’s maintainers. See this SynapseML example for additional information on using LightGBM on Spark.", "prev_chunk_id": "chunk_179", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_181", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Dask", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dask", "content": "Dask LightGBM’s Python-package supports distributed learning via Dask. This integration is maintained by LightGBM’s maintainers.", "prev_chunk_id": "chunk_180", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_182", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Dask Examples", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dask Examples", "content": "Dask Examples For sample code using lightgbm.dask, see these Dask examples.", "prev_chunk_id": "chunk_181", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_183", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Training with Dask", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Training with Dask", "content": "Training with Dask This section contains detailed information on performing LightGBM distributed training using Dask.", "prev_chunk_id": "chunk_182", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_184", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Configuring the Dask Cluster", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Configuring the Dask Cluster", "content": "Configuring the Dask Cluster Allocating Threads When setting up a Dask cluster for training, give each Dask worker process at least two threads. If you do not do this, training might be substantially slower because communication work and training work will block each other. If you do not have other significant processes competing with Dask for resources, just accept the default nthreads from your chosen dask.distributed cluster. from distributed import Client, LocalCluster cluster = LocalCluster(n_workers=3) client = Client(cluster) Managing Memory Use the Dask diagnostic dashboard or your preferred monitoring tool to monitor Dask workers’ memory consumption during training. As described in the Dask worker documentation, Dask workers will automatically start spilling data to disk if memory consumption gets too high. This can substantially slow down computations, since disk I/O is usually much slower than reading the same data from memory. To reduce the risk of hitting memory limits, consider restarting each worker process before running any data loading or training code. client.restart()", "prev_chunk_id": "chunk_183", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_185", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Setting Up Training Data", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Setting Up Training Data", "content": "Setting Up Training Data The estimators in lightgbm.dask expect that matrix-like or array-like data are provided in Dask DataFrame, Dask Array, or (in some cases) Dask Series format. See the Dask DataFrame documentation and the Dask Array documentation for more information on how to create such data structures. While setting up for training, lightgbm will concatenate all of the partitions on a worker into a single dataset. Distributed training then proceeds with one LightGBM worker process per Dask worker. When setting up data partitioning for LightGBM training with Dask, try to follow these suggestions: - ensure that each worker in the cluster has some of the training data - try to give each worker roughly the same amount of data, especially if your dataset is small - if you plan to train multiple models (for example, to tune hyperparameters) on the same data, useclient.persist()before training to materialize the data one time", "prev_chunk_id": "chunk_184", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_186", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Using a Specific Dask Client", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Using a Specific Dask Client", "content": "Using a Specific Dask Client In most situations, you should not need to tell lightgbm.dask to use a specific Dask client. By default, the client returned by distributed.default_client() will be used. However, you might want to explicitly control the Dask client used by LightGBM if you have multiple active clients in the same session. This is useful in more complex workflows like running multiple training jobs on different Dask clusters. LightGBM’s Dask estimators support setting an attribute client to control the client that is used. import lightgbm as lgb from distributed import Client, LocalCluster cluster = LocalCluster() client = Client(cluster) # option 1: keyword argument in constructor dask_model = lgb.DaskLGBMClassifier(client=client) # option 2: set_params() after construction dask_model = lgb.DaskLGBMClassifier() dask_model.set_params(client=client)", "prev_chunk_id": "chunk_185", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_187", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Using Specific Ports", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Using Specific Ports", "content": "Using Specific Ports At the beginning of training, lightgbm.dask sets up a LightGBM network where each Dask worker runs one long-running task that acts as a LightGBM worker. During training, LightGBM workers communicate with each other over TCP sockets. By default, random open ports are used when creating these sockets. If the communication between Dask workers in the cluster used for training is restricted by firewall rules, you must tell LightGBM exactly what ports to use. Option 1: provide a specific list of addresses and ports LightGBM supports a parameter machines, a comma-delimited string where each entry refers to one worker (host name or IP) and a port that that worker will accept connections on. If you provide this parameter to the estimators in lightgbm.dask, LightGBM will not search randomly for ports. For example, consider the case where you are running one Dask worker process on each of the following IP addresses: 10.0.1.0 10.0.2.0 10.0.3.0 You could edit your firewall rules to allow traffic on one additional port on each of these hosts, then provide machines directly. import lightgbm as lgb machines = \"10.0.1.0:12401,10.0.2.0:12402,10.0.3.0:15000\" dask_model = lgb.DaskLGBMRegressor(machines=machines) If you are running multiple Dask worker processes on physical host in the cluster, be sure that there are multiple entries for that IP address, with different ports. For example, if you were running a cluster with nprocs=2 (2 Dask worker processes per machine), you might open two additional ports on each of these hosts, then provide machines as follows. import lightgbm as lgb machines = \",\".join([ \"10.0.1.0:16000\", \"10.0.1.0:16001\", \"10.0.2.0:16000\", \"10.0.2.0:16001\", ]) dask_model = lgb.DaskLGBMRegressor(machines=machines) Option 2: specify one port to use on every worker If you are only running one Dask worker process on each host, and if you can reliably identify a port that is open on every host, using machines", "prev_chunk_id": "chunk_186", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_188", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Using Specific Ports", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Using Specific Ports", "content": "is unnecessarily complicated. If local_listen_port is given and machines is not, LightGBM will not search for ports randomly, but it will limit the list of addresses in the LightGBM network to those Dask workers that have a piece of the training data. For example, consider the case where you are running one Dask worker process on each of the following IP addresses: 10.0.1.0 10.0.2.0 10.0.3.0 You could edit your firewall rules to allow communication between any of the workers over one port, then provide that port via parameter local_listen_port. import lightgbm as lgb dask_model = lgb.DaskLGBMRegressor(local_listen_port=12400)", "prev_chunk_id": "chunk_187", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_189", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Using Custom Objective Functions with Dask", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Using Custom Objective Functions with Dask", "content": "Using Custom Objective Functions with Dask It is possible to customize the boosting process by providing a custom objective function written in Python. See the Dask API’s documentation for details on how to implement such functions. Follow the example below to use a custom implementation of the regression_l2 objective. import dask.array as da import lightgbm as lgb import numpy as np from distributed import Client, LocalCluster cluster = LocalCluster(n_workers=2) client = Client(cluster) X = da.random.random((1000, 10), (500, 10)) y = da.random.random((1000,), (500,)) def custom_l2_obj(y_true, y_pred): grad = y_pred - y_true hess = np.ones(len(y_true)) return grad, hess dask_model = lgb.DaskLGBMRegressor( objective=custom_l2_obj ) dask_model.fit(X, y)", "prev_chunk_id": "chunk_188", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_190", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Prediction with Dask", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Prediction with Dask", "content": "Prediction with Dask The estimators from lightgbm.dask can be used to create predictions based on data stored in Dask collections. In that interface, .predict() expects a Dask Array or Dask DataFrame, and returns a Dask Array of predictions. See the Dask prediction example for some sample code that shows how to perform Dask-based prediction. For model evaluation, consider using the metrics functions from dask-ml. Those functions are intended to provide the same API as equivalent functions in sklearn.metrics, but they use distributed computation powered by Dask to compute metrics without all of the input data ever needing to be on a single machine.", "prev_chunk_id": "chunk_189", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_191", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Saving Dask Models", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Saving Dask Models", "content": "Saving Dask Models After training with Dask, you have several options for saving a fitted model. Option 1: pickle the Dask estimator LightGBM’s Dask estimators can be pickled directly with cloudpickle, joblib, or pickle. import dask.array as da import pickle import lightgbm as lgb from distributed import Client, LocalCluster cluster = LocalCluster(n_workers=2) client = Client(cluster) X = da.random.random((1000, 10), (500, 10)) y = da.random.random((1000,), (500,)) dask_model = lgb.DaskLGBMRegressor() dask_model.fit(X, y) with open(\"dask-model.pkl\", \"wb\") as f: pickle.dump(dask_model, f) A model saved this way can then later be loaded with whichever serialization library you used to save it. import pickle with open(\"dask-model.pkl\", \"rb\") as f: dask_model = pickle.load(f) Option 2: pickle the sklearn estimator The estimators available from lightgbm.dask can be converted to an instance of the equivalent class from lightgbm.sklearn. Choosing this option allows you to use Dask for training but avoid depending on any Dask libraries at scoring time. import dask.array as da import joblib import lightgbm as lgb from distributed import Client, LocalCluster cluster = LocalCluster(n_workers=2) client = Client(cluster) X = da.random.random((1000, 10), (500, 10)) y = da.random.random((1000,), (500,)) dask_model = lgb.DaskLGBMRegressor() dask_model.fit(X, y) # convert to sklearn equivalent sklearn_model = dask_model.to_local() print(type(sklearn_model)) #> lightgbm.sklearn.LGBMRegressor joblib.dump(sklearn_model, \"sklearn-model.joblib\") A model saved this way can then later be loaded with whichever serialization library you used to save it. import joblib sklearn_model = joblib.load(\"sklearn-model.joblib\") Option 3: save the LightGBM Booster The lowest-level model object in LightGBM is the lightgbm.Booster. After training, you can extract a Booster from the Dask estimator. import dask.array as da import lightgbm as lgb from distributed import Client, LocalCluster cluster = LocalCluster(n_workers=2) client = Client(cluster) X = da.random.random((1000, 10), (500, 10)) y = da.random.random((1000,), (500,)) dask_model = lgb.DaskLGBMRegressor() dask_model.fit(X, y) # get underlying Booster object bst = dask_model.booster_ From the point forward, you can use any of the", "prev_chunk_id": "chunk_190", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_192", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Saving Dask Models", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Saving Dask Models", "content": "following methods to save the Booster: - serialize withcloudpickle,joblib, orpickle - bst.dump_model(): dump the model to a dictionary which could be written out as JSON - bst.model_to_string(): dump the model to a string in memory - bst.save_model(): write the output ofbst.model_to_string()to a text file", "prev_chunk_id": "chunk_191", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_193", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Kubeflow", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Kubeflow", "content": "Kubeflow Kubeflow users can also use the Kubeflow XGBoost Operator for machine learning workflows with LightGBM. You can see this example for more details. Kubeflow integrations for LightGBM are not maintained by LightGBM’s maintainers.", "prev_chunk_id": "chunk_192", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_194", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Preparation", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Preparation", "content": "Preparation By default, distributed learning with LightGBM uses socket-based communication. If you need to build distributed version with MPI support, please refer to Installation Guide.", "prev_chunk_id": "chunk_193", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_195", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Socket Version", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Socket Version", "content": "Socket Version It needs to collect IP of all machines that want to run distributed learning in and allocate one TCP port (assume 12345 here) for all machines, and change firewall rules to allow income of this port (12345). Then write these IP and ports in one file (assume mlist.txt), like following: machine1_ip 12345 machine2_ip 12345", "prev_chunk_id": "chunk_194", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_196", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "MPI Version", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MPI Version", "content": "MPI Version It needs to collect IP (or hostname) of all machines that want to run distributed learning in. Then write these IP in one file (assume mlist.txt) like following: machine1_ip machine2_ip Note: For Windows users, need to start “smpd” to start MPI service. More details can be found here.", "prev_chunk_id": "chunk_195", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_197", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Socket Version", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Socket Version", "content": "Socket Version - Edit following parameters in config file:tree_learner=your_parallel_algorithm, edityour_parallel_algorithm(e.g. feature/data) here.num_machines=your_num_machines, edityour_num_machines(e.g. 4) here.machine_list_file=mlist.txt,mlist.txtis created inPreparation section.local_listen_port=12345,12345is allocated inPreparation section. - Copy data file, executable file, config file andmlist.txtto all machines. - Run following command on all machines, you need to changeyour_config_fileto real config file.For Windows:lightgbm.execonfig=your_config_fileFor Linux:./lightgbmconfig=your_config_file", "prev_chunk_id": "chunk_196", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_198", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "MPI Version", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "MPI Version", "content": "MPI Version - Edit following parameters in config file:tree_learner=your_parallel_algorithm, edityour_parallel_algorithm(e.g. feature/data) here.num_machines=your_num_machines, edityour_num_machines(e.g. 4) here. - Copy data file, executable file, config file andmlist.txtto all machines.Note: MPI needs to be run in thesame path on all machines. - Run following command on one machine (not need to run on all machines), need to changeyour_config_fileto real config file.For Windows:mpiexec.exe /machinefile mlist.txt lightgbm.exe config=your_config_fileFor Linux:mpiexec --machinefile mlist.txt ./lightgbm config=your_config_file", "prev_chunk_id": "chunk_197", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_199", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Example", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Example", "content": "Example - A simple distributed learning example", "prev_chunk_id": "chunk_198", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_200", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Ray", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Ray", "content": "Ray Ray is a Python-based framework for distributed computing. The lightgbm_ray project, maintained within the official Ray GitHub organization, can be used to perform distributed LightGBM training using ray. See the lightgbm_ray documentation for usage examples.", "prev_chunk_id": "chunk_199", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_201", "url": "https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html", "title": "Mars", "page_title": "Distributed Learning Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Mars", "content": "Mars Mars is a tensor-based framework for large-scale data computation. LightGBM integration, maintained within the Mars GitHub repository, can be used to perform distributed LightGBM training using pymars. See the mars documentation for usage examples.", "prev_chunk_id": "chunk_200", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_202", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "LightGBM GPU Tutorial", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "LightGBM GPU Tutorial", "content": "LightGBM GPU Tutorial The purpose of this document is to give you a quick step-by-step tutorial on GPU training. We will use the GPU instance on Microsoft Azure cloud computing platform for demonstration, but you can use any machine with modern AMD or NVIDIA GPUs.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_203", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "GPU Setup", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Setup", "content": "GPU Setup You need to launch a NV type instance on Azure (available in East US, North Central US, South Central US, West Europe and Southeast Asia zones) and select Ubuntu 16.04 LTS as the operating system. For testing, the smallest NV6 type virtual machine is sufficient, which includes 1/2 M60 GPU, with 8 GB memory, 180 GB/s memory bandwidth and 4,825 GFLOPS peak computation power. Don’t use the NC type instance as the GPUs (K80) are based on an older architecture (Kepler). First we need to install minimal NVIDIA drivers and OpenCL development environment: sudo apt-get update sudo apt-get install --no-install-recommends nvidia-375 sudo apt-get install --no-install-recommends nvidia-opencl-icd-375 nvidia-opencl-dev opencl-headers After installing the drivers you need to restart the server. sudo init 6 After about 30 seconds, the server should be up again. If you are using an AMD GPU, you should download and install the AMDGPU-Pro driver and also install packages ocl-icd-libopencl1 and ocl-icd-opencl-dev.", "prev_chunk_id": "chunk_202", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_204", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Build LightGBM", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build LightGBM", "content": "Build LightGBM Now install necessary building tools and dependencies: sudo apt-get install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev The NV6 GPU instance has a 320 GB ultra-fast SSD mounted at /mnt. Let’s use it as our workspace (skip this if you are using your own machine): sudo mkdir -p /mnt/workspace sudo chown $(whoami):$(whoami) /mnt/workspace cd /mnt/workspace Now we are ready to checkout LightGBM and compile it with GPU support: git clone --recursive https://github.com/microsoft/LightGBM cd LightGBM cmake -B build -S . -DUSE_GPU=1 # if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following: # cmake -B build -S . -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ cmake --build build -j$(nproc) You will see two binaries are generated, lightgbm and lib_lightgbm.so. If you are building on macOS, you probably need to remove macro BOOST_COMPUTE_USE_OFFLINE_CACHE in src/treelearner/gpu_tree_learner.h to avoid a known crash bug in Boost.Compute.", "prev_chunk_id": "chunk_203", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_205", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Install Python Interface (optional)", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Install Python Interface (optional)", "content": "Install Python Interface (optional) If you want to use the Python interface of LightGBM, you can install it now (along with some necessary Python-package dependencies): sudo apt-get -y install python-pip sudo -H pip install setuptools numpy scipy scikit-learn -U sudo sh ./build-python.sh install --precompile You need to set an additional parameter \"device\" : \"gpu\" (along with your other options like learning_rate, num_leaves, etc) to use GPU in Python. You can read our Python-package Examples for more information on how to use the Python interface.", "prev_chunk_id": "chunk_204", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_206", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Dataset Preparation", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Dataset Preparation", "content": "Dataset Preparation Using the following commands to prepare the Higgs dataset: git clone https://github.com/guolinke/boosting_tree_benchmarks.git cd boosting_tree_benchmarks/data wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\" gunzip HIGGS.csv.gz python higgs2libsvm.py cd ../.. ln -s boosting_tree_benchmarks/data/higgs.train ln -s boosting_tree_benchmarks/data/higgs.test Now we create a configuration file for LightGBM by running the following commands (please copy the entire block and run it as a whole): cat > lightgbm_gpu.conf <<EOF max_bin = 63 num_leaves = 255 num_iterations = 50 learning_rate = 0.1 tree_learner = serial task = train is_training_metric = false min_data_in_leaf = 1 min_sum_hessian_in_leaf = 100 ndcg_eval_at = 1,3,5,10 device = gpu gpu_platform_id = 0 gpu_device_id = 0 EOF echo \"num_threads=$(nproc)\" >> lightgbm_gpu.conf GPU is enabled in the configuration file we just created by setting device=gpu. In this configuration we use the first GPU installed on the system (gpu_platform_id=0 and gpu_device_id=0). If gpu_platform_id or gpu_device_id is not set, the default platform and GPU will be selected. You might have multiple platforms (AMD/Intel/NVIDIA) or GPUs. You can use the clinfo utility to identify the GPUs on each platform. On Ubuntu, you can install clinfo by executing sudo apt-get install clinfo. If you have a discrete GPU by AMD/NVIDIA and an integrated GPU by Intel, make sure to select the correct gpu_platform_id to use the discrete GPU.", "prev_chunk_id": "chunk_205", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_207", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Run Your First Learning Task on GPU", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Run Your First Learning Task on GPU", "content": "Run Your First Learning Task on GPU Now we are ready to start GPU training! First we want to verify the GPU works correctly. Run the following command to train on GPU, and take a note of the AUC after 50 iterations: ./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc Now train the same dataset on CPU using the following command. You should observe a similar AUC: ./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc device=cpu Now we can make a speed test on GPU without calculating AUC after each iteration. ./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=binary metric=auc Speed test on CPU: ./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=binary metric=auc device=cpu You should observe over three times speedup on this GPU. The GPU acceleration can be used on other tasks/metrics (regression, multi-class classification, ranking, etc) as well. For example, we can train the Higgs dataset on GPU as a regression task: ./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=regression_l2 metric=l2 Also, you can compare the training speed with CPU: ./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=regression_l2 metric=l2 device=cpu", "prev_chunk_id": "chunk_206", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_208", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Further Reading", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Further Reading", "content": "Further Reading - GPU Tuning Guide and Performance Comparison - GPU SDK Correspondence and Device Targeting Table", "prev_chunk_id": "chunk_207", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_209", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html", "title": "Reference", "page_title": "LightGBM GPU Tutorial — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Reference", "content": "Reference Please kindly cite the following article in your publications if you find the GPU acceleration useful: Huan Zhang, Si Si and Cho-Jui Hsieh. “GPU Acceleration for Large-scale Tree Boosting.” SysML Conference, 2018.", "prev_chunk_id": "chunk_208", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_210", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Missing Value Handle", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Missing Value Handle", "content": "Missing Value Handle - LightGBM enables the missing value handle by default. Disable it by settinguse_missing=false. - LightGBM uses NA (NaN) to represent missing values by default. Change it to use zero by settingzero_as_missing=true. - Whenzero_as_missing=false(default), the unrecorded values in sparse matrices (and LightSVM) are treated as zeros. - Whenzero_as_missing=true, NA and zeros (including unrecorded values in sparse matrices (and LightSVM)) are treated as missing.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_211", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Categorical Feature Support", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Categorical Feature Support", "content": "Categorical Feature Support - LightGBM offers good accuracy with integer-encoded categorical features. LightGBM appliesFisher (1958)to find the optimal split over categories asdescribed here. This often performs better than one-hot encoding. - Usecategorical_featureto specify the categorical features. Refer to the parametercategorical_featureinParameters. - Categorical features will be cast toint32(integer codes will be extracted from pandas categoricals in the Python-package) so they must be encoded as non-negative integers (negative values will be treated as missing) less thanInt32.MaxValue(2147483647). It is best to use a contiguous range of integers started from zero. Floating point numbers in categorical features will be rounded towards 0. - Usemin_data_per_group,cat_smoothto deal with over-fitting (when#datais small or#categoryis large). - For a categorical feature with high cardinality (#categoryis large), it often works best to treat the feature as numeric, either by simply ignoring the categorical interpretation of the integers or by embedding the categories in a low-dimensional numeric space.", "prev_chunk_id": "chunk_210", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_212", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "LambdaRank", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "LambdaRank", "content": "LambdaRank - The label should be of typeint, such that larger numbers correspond to higher relevance (e.g. 0:bad, 1:fair, 2:good, 3:perfect). - Uselabel_gainto set the gain(weight) ofintlabel. - Uselambdarank_truncation_levelto truncate the max DCG.", "prev_chunk_id": "chunk_211", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_213", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Cost Efficient Gradient Boosting", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Cost Efficient Gradient Boosting", "content": "Cost Efficient Gradient Boosting Cost Efficient Gradient Boosting (CEGB) makes it possible to penalise boosting based on the cost of obtaining feature values. CEGB penalises learning in the following ways: - Each time a tree is split, a penalty ofcegb_penalty_splitis applied. - When a feature is used for the first time,cegb_penalty_feature_coupledis applied. This penalty can be different for each feature and should be specified as onedoubleper feature. - When a feature is used for the first time for a data row,cegb_penalty_feature_lazyis applied. Likecegb_penalty_feature_coupled, this penalty is specified as onedoubleper feature. Each of the penalties above is scaled by cegb_tradeoff. Using this parameter, it is possible to change the overall strength of the CEGB penalties by changing only one parameter.", "prev_chunk_id": "chunk_212", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_214", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Parameters Tuning", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Parameters Tuning", "content": "Parameters Tuning - Refer toParameters Tuning.", "prev_chunk_id": "chunk_213", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_215", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Distributed Learning", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Distributed Learning", "content": "Distributed Learning - Refer toDistributed Learning Guide.", "prev_chunk_id": "chunk_214", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_216", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "GPU Support", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Support", "content": "GPU Support - Refer toGPU TutorialandGPU Targets.", "prev_chunk_id": "chunk_215", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_217", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Support for Position Bias Treatment", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Support for Position Bias Treatment", "content": "Support for Position Bias Treatment Often the relevance labels provided in Learning-to-Rank tasks might be derived from implicit user feedback (e.g., clicks) and therefore might be biased due to their position/location on the screen when having been presented to a user. LightGBM can make use of positional data. For example, consider the case where you expect that the first 3 results from a search engine will be visible in users’ browsers without scrolling, and all other results for a query would require scrolling. LightGBM could be told to account for the position bias from results being “above the fold” by providing a positions array encoded as follows: 0 0 0 1 1 0 0 0 1 ... Where 0 = \"above the fold\" and 1 = \"requires scrolling\". The specific values are not important, as long as they are consistent across all observations in the training data. An encoding like 100 = \"above the fold\" and 17 = \"requires scrolling\" would result in exactly the same trained model. In that way, positions in LightGBM’s API are similar to a categorical feature. Just as with non-ordinal categorical features, an integer representation is just used for memory and computational efficiency… LightGBM does not care about the absolute or relative magnitude of the values. Unlike a categorical feature, however, positions are used to adjust the target to reduce the bias in predictions made by the trained model. The position file corresponds with training data file line by line, and has one position per line. And if the name of training data file is train.txt, the position file should be named as train.txt.position and placed in the same folder as the data file. In this case, LightGBM will load the position file automatically if it exists. The positions can also be specified through the", "prev_chunk_id": "chunk_216", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_218", "url": "https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html", "title": "Support for Position Bias Treatment", "page_title": "Advanced Topics — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Support for Position Bias Treatment", "content": "Dataset constructor when using Python API. If the positions are specified in both approaches, the .position file will be ignored. Currently, implemented is an approach to model position bias by using an idea of Generalized Additive Models (GAM) to linearly decompose the document score s into the sum of a relevance component f and a positional component g: s(x, pos) = f(x) + g(pos) where the former component depends on the original query-document features and the latter depends on the position of an item. During the training, the compound scoring function s(x, pos) is fit with a standard ranking algorithm (e.g., LambdaMART) which boils down to jointly learning the relevance component f(x) (it is later returned as an unbiased model) and the position factors g(pos) that help better explain the observed (biased) labels. Similar score decomposition ideas have previously been applied for classification & pointwise ranking tasks with assumptions of binary labels and binary relevance (a.k.a. “two-tower” models, refer to the papers: Towards Disentangling Relevance and Bias in Unbiased Learning to Rank, PAL: a position-bias aware learning framework for CTR prediction in live recommender systems, A General Framework for Debiasing in CTR Prediction). In LightGBM, we adapt this idea to general pairwise Lerarning-to-Rank with arbitrary ordinal relevance labels. Besides, GAMs have been used in the context of explainable ML (Accurate Intelligible Models with Pairwise Interactions) to linearly decompose the contribution of each feature (and possibly their pairwise interactions) to the overall score, for subsequent analysis and interpretation of their effects in the trained models.", "prev_chunk_id": "chunk_217", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_219", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "LightGBM FAQ", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "LightGBM FAQ", "content": "LightGBM FAQ Please post questions, feature requests, and bug reports at https://github.com/microsoft/LightGBM/issues. This project is mostly maintained by volunteers, so please be patient. If your request is time-sensitive or more than a month goes by without a response, please tag the maintainers below for help. - @guolinkeGuolin Ke - @shiyu1994Yu Shi - @jameslambJames Lamb - @jmoralezJosé Morales", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_220", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "1. Where do I find more details about LightGBM parameters?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "1. Where do I find more details about LightGBM parameters?", "content": "1. Where do I find more details about LightGBM parameters? Take a look at Parameters.", "prev_chunk_id": "chunk_219", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_221", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "2. On datasets with millions of features, training does not start (or starts after a very long time).", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "2. On datasets with millions of features, training does not start (or starts after a very long time).", "content": "2. On datasets with millions of features, training does not start (or starts after a very long time). Use a smaller value for bin_construct_sample_cnt and a larger value for min_data.", "prev_chunk_id": "chunk_220", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_222", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "3. When running LightGBM on a large dataset, my computer runs out of RAM.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "3. When running LightGBM on a large dataset, my computer runs out of RAM.", "content": "3. When running LightGBM on a large dataset, my computer runs out of RAM. Multiple Solutions: set the histogram_pool_size parameter to the MB you want to use for LightGBM (histogram_pool_size + dataset size = approximately RAM used), lower num_leaves or lower max_bin (see Microsoft/LightGBM#562).", "prev_chunk_id": "chunk_221", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_223", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "4. I am using Windows. Should I use Visual Studio or MinGW for compiling LightGBM?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "4. I am using Windows. Should I use Visual Studio or MinGW for compiling LightGBM?", "content": "4. I am using Windows. Should I use Visual Studio or MinGW for compiling LightGBM? Visual Studio performs best for LightGBM.", "prev_chunk_id": "chunk_222", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_224", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "5. When using LightGBM GPU, I cannot reproduce results over several runs.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "5. When using LightGBM GPU, I cannot reproduce results over several runs.", "content": "5. When using LightGBM GPU, I cannot reproduce results over several runs. This is normal and expected behaviour, but you may try to use gpu_use_dp = true for reproducibility (see Microsoft/LightGBM#560). You may also use the CPU version.", "prev_chunk_id": "chunk_223", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_225", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "6. Bagging is not reproducible when changing the number of threads.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "6. Bagging is not reproducible when changing the number of threads.", "content": "6. Bagging is not reproducible when changing the number of threads. LightGBM bagging is multithreaded, so its output depends on the number of threads used. There is no workaround currently. Starting from #2804 bagging result doesn’t depend on the number of threads. So this issue should be solved in the latest version.", "prev_chunk_id": "chunk_224", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_226", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "7. I tried to use Random Forest mode, and LightGBM crashes!", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "7. I tried to use Random Forest mode, and LightGBM crashes!", "content": "7. I tried to use Random Forest mode, and LightGBM crashes! This is expected behaviour for arbitrary parameters. To enable Random Forest, you must use bagging_fraction and feature_fraction different from 1, along with a bagging_freq. This thread includes an example.", "prev_chunk_id": "chunk_225", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_227", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "8. CPU usage is low (like 10%) in Windows when using LightGBM on very large datasets with many-core systems.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "8. CPU usage is low (like 10%) in Windows when using LightGBM on very large datasets with many-core systems.", "content": "8. CPU usage is low (like 10%) in Windows when using LightGBM on very large datasets with many-core systems. Please use Visual Studio as it may be 10x faster than MinGW especially for very large trees.", "prev_chunk_id": "chunk_226", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_228", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "9. When I’m trying to specify a categorical column with the categorical_feature parameter, I get the following sequence of warnings, but there are no negative values in the column.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "9. When I’m trying to specify a categorical column with the categorical_feature parameter, I get the following sequence of warnings, but there are no negative values in the column.", "content": "9. When I’m trying to specify a categorical column with the categorical_feature parameter, I get the following sequence of warnings, but there are no negative values in the column. [LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN [LightGBM] [Warning] There are no meaningful features, as all feature values are constant. The column you’re trying to pass via categorical_feature likely contains very large values. Categorical features in LightGBM are limited by int32 range, so you cannot pass values that are greater than Int32.MaxValue (2147483647) as categorical features (see Microsoft/LightGBM#1359). You should convert them to integers ranging from zero to the number of categories first.", "prev_chunk_id": "chunk_227", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_229", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized.", "content": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized. OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized. OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. Possible Cause: This error means that you have multiple OpenMP libraries installed on your machine and they conflict with each other. (File extensions in the error message may differ depending on the operating system). If you are using Python distributed by Conda, then it is highly likely that the error is caused by the numpy package from Conda which includes the mkl package which in turn conflicts with the system-wide library. In this case you can update the numpy package in Conda or replace the Conda’s OpenMP library instance with system-wide one by creating a symlink to it in Conda environment folder $CONDA_PREFIX/lib. Solution: Assuming you are using macOS with Homebrew, the command which overwrites OpenMP library files in the current active Conda environment with symlinks to the system-wide library ones installed by Homebrew: ln -sf `ls -d \"$(brew --cellar libomp)\"/*/lib`/* $CONDA_PREFIX/lib The described above fix worked fine before the release of OpenMP 8.0.0 version. Starting from 8.0.0 version, Homebrew formula for OpenMP includes -DLIBOMP_INSTALL_ALIASES=OFF option which leads to that the fix doesn’t work anymore. However, you can", "prev_chunk_id": "chunk_228", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_230", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized.", "content": "create symlinks to library aliases manually: for LIBOMP_ALIAS in libgomp.dylib libiomp5.dylib libomp.dylib; do sudo ln -sf \"$(brew --cellar libomp)\"/*/lib/libomp.dylib $CONDA_PREFIX/lib/$LIBOMP_ALIAS; done Another workaround would be removing MKL optimizations from Conda’s packages completely: conda install nomkl If this is not your case, then you should find conflicting OpenMP library installations on your own and leave only one of them.", "prev_chunk_id": "chunk_229", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_231", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "11. LightGBM hangs when multithreading (OpenMP) and using forking in Linux at the same time.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "11. LightGBM hangs when multithreading (OpenMP) and using forking in Linux at the same time.", "content": "11. LightGBM hangs when multithreading (OpenMP) and using forking in Linux at the same time. Use nthreads=1 to disable multithreading of LightGBM. There is a bug with OpenMP which hangs forked sessions with multithreading activated. A more expensive solution is to use new processes instead of using fork, however, keep in mind it is creating new processes where you have to copy memory and load libraries (example: if you want to fork 16 times your current process, then you will require to make 16 copies of your dataset in memory) (see Microsoft/LightGBM#1789). An alternative, if multithreading is really necessary inside the forked sessions, would be to compile LightGBM with Intel toolchain. Intel compilers are unaffected by this bug. For C/C++ users, any OpenMP feature cannot be used before the fork happens. If an OpenMP feature is used before the fork happens (example: using OpenMP for forking), OpenMP will hang inside the forked sessions. Use new processes instead and copy memory as required by creating new processes instead of forking (or, use Intel compilers). Cloud platform container services may cause LightGBM to hang, if they use Linux fork to run multiple containers on a single instance. For example, LightGBM hangs in AWS Batch array jobs, which use the ECS agent to manage multiple running jobs. Setting nthreads=1 mitigates the issue.", "prev_chunk_id": "chunk_230", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_232", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "12. Why is early stopping not enabled by default in LightGBM?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "12. Why is early stopping not enabled by default in LightGBM?", "content": "12. Why is early stopping not enabled by default in LightGBM? Early stopping involves choosing a validation set, a special type of holdout which is used to evaluate the current state of the model after each iteration to see if training can stop. In LightGBM, we have decided to require that users specify this set directly. Many options exist for splitting training data into training, test, and validation sets. The appropriate splitting strategy depends on the task and domain of the data, information that a modeler has but which LightGBM as a general-purpose tool does not.", "prev_chunk_id": "chunk_231", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_233", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "13. Does LightGBM support direct loading data from zero-based or one-based LibSVM format file?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "13. Does LightGBM support direct loading data from zero-based or one-based LibSVM format file?", "content": "13. Does LightGBM support direct loading data from zero-based or one-based LibSVM format file? LightGBM supports loading data from zero-based LibSVM format file directly.", "prev_chunk_id": "chunk_232", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_234", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "14. Why CMake cannot find the compiler when compiling LightGBM with MinGW?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "14. Why CMake cannot find the compiler when compiling LightGBM with MinGW?", "content": "14. Why CMake cannot find the compiler when compiling LightGBM with MinGW? CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage This is a known issue of CMake when using MinGW. The easiest solution is to run again your cmake command to bypass the one time stopper from CMake. Or you can upgrade your version of CMake to at least version 3.17.0. See Microsoft/LightGBM#3060 for more details.", "prev_chunk_id": "chunk_233", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_235", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "15. Where can I find LightGBM’s logo to use it in my presentation?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "15. Where can I find LightGBM’s logo to use it in my presentation?", "content": "15. Where can I find LightGBM’s logo to use it in my presentation? You can find LightGBM’s logo in different file formats and resolutions here.", "prev_chunk_id": "chunk_234", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_236", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "16. LightGBM crashes randomly or operating system hangs during or after running LightGBM.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "16. LightGBM crashes randomly or operating system hangs during or after running LightGBM.", "content": "16. LightGBM crashes randomly or operating system hangs during or after running LightGBM. Possible Cause: This behavior may indicate that you have multiple OpenMP libraries installed on your machine and they conflict with each other, similarly to the FAQ #10. If you are using any Python-package that depends on threadpoolctl, you also may see the following warning in your logs in this case: /root/miniconda/envs/test-env/lib/python3.8/site-packages/threadpoolctl.py:546: RuntimeWarning: Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at the same time. Both libraries are known to be incompatible and this can cause random crashes or deadlocks on Linux when loaded in the same Python program. Using threadpoolctl may cause crashes or deadlocks. For more information and possible workarounds, please see https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md Detailed description of conflicts between multiple OpenMP instances is provided in the following document. Solution: Assuming you are using LightGBM Python-package and conda as a package manager, we strongly recommend using conda-forge channel as the only source of all your Python package installations because it contains built-in patches to workaround OpenMP conflicts. Some other workarounds are listed here under the “Workarounds for Intel OpenMP and LLVM OpenMP case” section. If this is not your case, then you should find conflicting OpenMP library installations on your own and leave only one of them.", "prev_chunk_id": "chunk_235", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_237", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "17. Loading LightGBM fails like: cannot allocate memory in static TLS block", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "17. Loading LightGBM fails like: cannot allocate memory in static TLS block", "content": "17. Loading LightGBM fails like: cannot allocate memory in static TLS block When loading LightGBM, you may encounter errors like the following. lib/libgomp.so.1: cannot allocate memory in static TLS block This most commonly happens on aarch64 Linux systems. gcc’s OpenMP library (libgomp.so) tries to allocate a small amount of static thread-local storage (“TLS”) when it’s dynamically loaded. That error can happen when the loader isn’t able to find a large enough block of memory. On aarch64 Linux, processes and loaded libraries share the same pool of static TLS, which makes such failures more likely. See these discussions: - https://bugzilla.redhat.com/show_bug.cgi?id=1722181#c6 - https://gcc.gcc.gnu.narkive.com/vOXMQqLA/failure-to-dlopen-libgomp-due-to-static-tls-data If you are experiencing this issue when using the lightgbm Python-package, try upgrading to at least v4.6.0. For older versions of the Python-package, or for other LightGBM APIs, this issue can often be avoided by loading libgomp.so.1. That can be done directly by setting environment variable LD_PRELOAD, like this: export LD_PRELOAD=/root/miniconda3/envs/test-env/lib/libgomp.so.1 It can also be done indirectly by changing the order that other libraries are loaded into processes, which varies by programming language and application type. For more details, see these discussions: - https://github.com/microsoft/LightGBM/pull/6654#issuecomment-2352014275 - https://github.com/microsoft/LightGBM/issues/6509 - https://maskray.me/blog/2021-02-14-all-about-thread-local-storage - https://bugzilla.redhat.com/show_bug.cgi?id=1722181#c6", "prev_chunk_id": "chunk_236", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_238", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "1. Any training command using LightGBM does not work after an error occurred during the training of a previous LightGBM model.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "1. Any training command using LightGBM does not work after an error occurred during the training of a previous LightGBM model.", "content": "1. Any training command using LightGBM does not work after an error occurred during the training of a previous LightGBM model. In older versions of the R-package (prior to v3.3.0), this could happen occasionally and the solution was to run lgb.unloader(wipe = TRUE) to remove all LightGBM-related objects. Some conversation about this could be found in Microsoft/LightGBM#698. That is no longer necessary as of v3.3.0, and function lgb.unloader() has since been removed from the R-package.", "prev_chunk_id": "chunk_237", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_239", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "2. I used setinfo(), tried to print my lgb.Dataset, and now the R console froze!", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "2. I used setinfo(), tried to print my lgb.Dataset, and now the R console froze!", "content": "2. I used setinfo(), tried to print my lgb.Dataset, and now the R console froze! As of at least LightGBM v3.3.0, this issue has been resolved and printing a Dataset object does not cause the console to freeze. In older versions, avoid printing the Dataset after calling setinfo(). As of LightGBM v4.0.0, setinfo() has been replaced by a new method, set_field().", "prev_chunk_id": "chunk_238", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_240", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "3. error in data.table::data.table()...argument 2 is NULL.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "3. error in data.table::data.table()...argument 2 is NULL.", "content": "3. error in data.table::data.table()...argument 2 is NULL. If you are experiencing this error when running lightgbm, you may be facing the same issue reported in #2715 and later in #2989. We have seen that in some situations, using data.table 1.11.x results in this error. To get around this, you can upgrade your version of data.table to at least version 1.12.0.", "prev_chunk_id": "chunk_239", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_241", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "4. package/dependency ‘Matrix’ is not available ...", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "4. package/dependency ‘Matrix’ is not available ...", "content": "4. package/dependency ‘Matrix’ is not available ... In April 2024, Matrix==1.7-0 was published to CRAN. That version had a floor of R (>=4.4.0). {Matrix} is a hard runtime dependency of {lightgbm}, so on any version of R older than 4.4.0, running install.packages(\"lightgbm\") results in something like the following. package ‘Matrix’ is not available for this version of R To fix that without upgrading to R 4.4.0 or greater, manually install an older version of {Matrix}. install.packages('https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz', repos = NULL)", "prev_chunk_id": "chunk_240", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_242", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "1. Error: setup script specifies an absolute path when installing from GitHub using python setup.py install.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "1. Error: setup script specifies an absolute path when installing from GitHub using python setup.py install.", "content": "1. Error: setup script specifies an absolute path when installing from GitHub using python setup.py install. error: Error: setup script specifies an absolute path: /Users/Microsoft/LightGBM/python-package/lightgbm/../../lib_lightgbm.so setup() arguments must *always* be /-separated paths relative to the setup.py directory, *never* absolute paths. This error should be solved in latest version. If you still meet this error, try to remove lightgbm.egg-info folder in your Python-package and reinstall, or check this thread on stackoverflow.", "prev_chunk_id": "chunk_241", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_243", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "2. Error messages: Cannot ... before construct dataset.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "2. Error messages: Cannot ... before construct dataset.", "content": "2. Error messages: Cannot ... before construct dataset. I see error messages like… Cannot get/set label/weight/init_score/group/num_data/num_feature before construct dataset but I’ve already constructed a dataset by some code like: train = lightgbm.Dataset(X_train, y_train) or error messages like Cannot set predictor/reference/categorical feature after freed raw data, set free_raw_data=False when construct Dataset to avoid this. Solution: Because LightGBM constructs bin mappers to build trees, and train and valid Datasets within one Booster share the same bin mappers, categorical features and feature names etc., the Dataset objects are constructed when constructing a Booster. If you set free_raw_data=True (default), the raw data (with Python data struct) will be freed. So, if you want to: - get label (or weight/init_score/group/data) before constructing a dataset, it’s same as getself.label; - set label (or weight/init_score/group) before constructing a dataset, it’s same asself.label=some_label_array; - get num_data (or num_feature) before constructing a dataset, you can get data withself.data. Then, if your data isnumpy.ndarray, use some code likeself.data.shape. But do not do this after subsetting the Dataset, because you’ll get alwaysNone; - set predictor (or reference/categorical feature) after constructing a dataset, you should setfree_raw_data=Falseor init a Dataset object with the same raw data.", "prev_chunk_id": "chunk_242", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_244", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "3. I encounter segmentation faults (segfaults) randomly after installing LightGBM from PyPI using pip install lightgbm.", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "3. I encounter segmentation faults (segfaults) randomly after installing LightGBM from PyPI using pip install lightgbm.", "content": "3. I encounter segmentation faults (segfaults) randomly after installing LightGBM from PyPI using pip install lightgbm. We are doing our best to provide universal wheels which have high running speed and are compatible with any hardware, OS, compiler, etc. at the same time. However, sometimes it’s just impossible to guarantee the possibility of usage of LightGBM in any specific environment (see Microsoft/LightGBM#1743). Therefore, the first thing you should try in case of segfaults is compiling from the source using pip install --no-binary lightgbm lightgbm. For the OS-specific prerequisites see https://github.com/microsoft/LightGBM/blob/master/python-package/README.rst. Also, feel free to post a new issue in our GitHub repository. We always look at each case individually and try to find a root cause.", "prev_chunk_id": "chunk_243", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_245", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "4. I would like to install LightGBM from conda. What channel should I choose?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "4. I would like to install LightGBM from conda. What channel should I choose?", "content": "4. I would like to install LightGBM from conda. What channel should I choose? We strongly recommend installation from the conda-forge channel and not from the default one. For some specific examples, see this comment. In addition, as of lightgbm==4.4.0, the conda-forge package automatically supports CUDA-based GPU acceleration.", "prev_chunk_id": "chunk_244", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_246", "url": "https://lightgbm.readthedocs.io/en/latest/FAQ.html", "title": "5. How do I subclass scikit-learn estimators?", "page_title": "LightGBM FAQ — LightGBM 4.6.0.99 documentation", "breadcrumbs": "5. How do I subclass scikit-learn estimators?", "content": "5. How do I subclass scikit-learn estimators? For lightgbm <= 4.5.0, copy all of the constructor arguments from the corresponding lightgbm class into the constructor of your custom estimator. For later versions, just ensure that the constructor of your custom estimator calls super().__init__(). Consider the example below, which implements a regressor that allows creation of truncated predictions. This pattern will work with lightgbm > 4.5.0. import numpy as np from lightgbm import LGBMRegressor from sklearn.datasets import make_regression class TruncatedRegressor(LGBMRegressor): def __init__(self, **kwargs): super().__init__(**kwargs) def predict(self, X, max_score: float = np.inf): preds = super().predict(X) np.clip(preds, a_min=None, a_max=max_score, out=preds) return preds X, y = make_regression(n_samples=1_000, n_features=4) reg_trunc = TruncatedRegressor().fit(X, y) preds = reg_trunc.predict(X) print(f\"mean: {preds.mean():.2f}, max: {preds.max():.2f}\") # mean: -6.81, max: 345.10 preds_trunc = reg_trunc.predict(X, max_score=preds.mean()) print(f\"mean: {preds_trunc.mean():.2f}, max: {preds_trunc.max():.2f}\") # mean: -56.50, max: -6.81", "prev_chunk_id": "chunk_245", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_247", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Algorithms", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Algorithms", "content": "Algorithms Refer to Features for understanding of important algorithms used in LightGBM.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_248", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Important Classes", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Important Classes", "content": "Important Classes Class | Description Application | The entrance of application, including training and prediction logic Bin | Data structure used for storing feature discrete values (converted from float values) Boosting | Boosting interface (GBDT, DART, etc.) Config | Stores parameters and configurations Dataset | Stores information of dataset DatasetLoader | Used to construct dataset FeatureGroup | Stores the data of feature, could be multiple features Metric | Evaluation metrics Network | Network interfaces and communication algorithms ObjectiveFunction | Objective functions used to train Tree | Stores information of tree model TreeLearner | Used to learn trees", "prev_chunk_id": "chunk_247", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_249", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Code Structure", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Code Structure", "content": "Code Structure Path | Description ./include | Header files ./include/utils | Some common functions ./src/application | Implementations of training and prediction logic ./src/boosting | Implementations of Boosting ./src/io | Implementations of IO related classes, including Bin, Config, Dataset, DatasetLoader, Feature and Tree ./src/metric | Implementations of metrics ./src/network | Implementations of network functions ./src/objective | Implementations of objective functions ./src/treelearner | Implementations of tree learners", "prev_chunk_id": "chunk_248", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_250", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Documents API", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Documents API", "content": "Documents API Refer to docs README.", "prev_chunk_id": "chunk_249", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_251", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "C API", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "C API", "content": "C API Refer to C API or the comments in c_api.h file, from which the documentation is generated.", "prev_chunk_id": "chunk_250", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_252", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Tests", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Tests", "content": "Tests C++ unit tests are located in the ./tests/cpp_tests folder and written with the help of Google Test framework. To run tests locally first refer to the Installation Guide for how to build tests and then simply run compiled executable file. It is highly recommended to build tests with sanitizers.", "prev_chunk_id": "chunk_251", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_253", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "High Level Language Package", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "High Level Language Package", "content": "High Level Language Package See the implementations at Python-package and R-package.", "prev_chunk_id": "chunk_252", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_254", "url": "https://lightgbm.readthedocs.io/en/latest/Development-Guide.html", "title": "Questions", "page_title": "Development Guide — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Questions", "content": "Questions Refer to FAQ. Also feel free to open issues if you met problems.", "prev_chunk_id": "chunk_253", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_255", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "_", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "_", "content": "_ __init__() (lightgbm.Booster method) (lightgbm.CVBooster method) (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.Dataset method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) (lightgbm.Sequence method)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_256", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "A", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "A", "content": "A add_features_from() (lightgbm.Dataset method) | add_valid() (lightgbm.Booster method)", "prev_chunk_id": "chunk_255", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_257", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "B", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "B", "content": "B batch_size (lightgbm.Sequence attribute) best_iteration (lightgbm.CVBooster attribute) best_iteration_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) best_score_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) | Booster (class in lightgbm) booster_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) BoosterHandle (C type) boosters (lightgbm.CVBooster attribute) ByteBufferHandle (C type)", "prev_chunk_id": "chunk_256", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_258", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "C", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "C", "content": "C C_API_DTYPE_FLOAT32 (C macro) C_API_DTYPE_FLOAT64 (C macro) C_API_DTYPE_INT32 (C macro) C_API_DTYPE_INT64 (C macro) C_API_FEATURE_IMPORTANCE_GAIN (C macro) C_API_FEATURE_IMPORTANCE_SPLIT (C macro) C_API_MATRIX_TYPE_CSC (C macro) C_API_MATRIX_TYPE_CSR (C macro) C_API_PREDICT_CONTRIB (C macro) C_API_PREDICT_LEAF_INDEX (C macro) C_API_PREDICT_NORMAL (C macro) | C_API_PREDICT_RAW_SCORE (C macro) classes_ (lightgbm.DaskLGBMClassifier property) (lightgbm.LGBMClassifier property) client_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) construct() (lightgbm.Dataset method) create_tree_digraph() (in module lightgbm) create_valid() (lightgbm.Dataset method) current_iteration() (lightgbm.Booster method) cv() (in module lightgbm) CVBooster (class in lightgbm)", "prev_chunk_id": "chunk_257", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_259", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "D", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "D", "content": "D DaskLGBMClassifier (class in lightgbm) DaskLGBMRanker (class in lightgbm) DaskLGBMRegressor (class in lightgbm) | Dataset (class in lightgbm) DatasetHandle (C type) dump_model() (lightgbm.Booster method)", "prev_chunk_id": "chunk_258", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_260", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "E", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "E", "content": "E early_stopping() (in module lightgbm) eval() (lightgbm.Booster method) eval_train() (lightgbm.Booster method) eval_valid() (lightgbm.Booster method) evals_result_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property)", "prev_chunk_id": "chunk_259", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_261", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "F", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "F", "content": "F FastConfigHandle (C type) feature_importance() (lightgbm.Booster method) feature_importances_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) feature_name() (lightgbm.Booster method) feature_name_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) | feature_names_in_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) feature_num_bin() (lightgbm.Dataset method) fit() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) free_dataset() (lightgbm.Booster method) free_network() (lightgbm.Booster method)", "prev_chunk_id": "chunk_260", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_262", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "G", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "G", "content": "G get_data() (lightgbm.Dataset method) get_feature_name() (lightgbm.Dataset method) get_field() (lightgbm.Dataset method) get_group() (lightgbm.Dataset method) get_init_score() (lightgbm.Dataset method) get_label() (lightgbm.Dataset method) get_leaf_output() (lightgbm.Booster method) get_metadata_routing() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) | get_params() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.Dataset method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) get_position() (lightgbm.Dataset method) get_ref_chain() (lightgbm.Dataset method) get_split_value_histogram() (lightgbm.Booster method) get_weight() (lightgbm.Dataset method)", "prev_chunk_id": "chunk_261", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_263", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "I", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "I", "content": "I INLINE_FUNCTION (C macro)", "prev_chunk_id": "chunk_262", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_264", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "L", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "L", "content": "L LastErrorMsg (C function) LGBM_BoosterAddValidData (C function) LGBM_BoosterCalcNumPredict (C function) LGBM_BoosterCreate (C function) LGBM_BoosterCreateFromModelfile (C function) LGBM_BoosterDumpModel (C function) LGBM_BoosterFeatureImportance (C function) LGBM_BoosterFree (C function) LGBM_BoosterFreePredictSparse (C function) LGBM_BoosterGetCurrentIteration (C function) LGBM_BoosterGetEval (C function) LGBM_BoosterGetEvalCounts (C function) LGBM_BoosterGetEvalNames (C function) LGBM_BoosterGetFeatureNames (C function) LGBM_BoosterGetLeafValue (C function) LGBM_BoosterGetLinear (C function) LGBM_BoosterGetLoadedParam (C function) LGBM_BoosterGetLowerBoundValue (C function) LGBM_BoosterGetNumClasses (C function) LGBM_BoosterGetNumFeature (C function) LGBM_BoosterGetNumPredict (C function) LGBM_BoosterGetPredict (C function) LGBM_BoosterGetUpperBoundValue (C function) LGBM_BoosterLoadModelFromString (C function) LGBM_BoosterMerge (C function) LGBM_BoosterNumberOfTotalModel (C function) LGBM_BoosterNumModelPerIteration (C function) LGBM_BoosterPredictForArrow (C function) LGBM_BoosterPredictForCSC (C function) LGBM_BoosterPredictForCSR (C function) LGBM_BoosterPredictForCSRSingleRow (C function) LGBM_BoosterPredictForCSRSingleRowFast (C function) LGBM_BoosterPredictForCSRSingleRowFastInit (C function) LGBM_BoosterPredictForFile (C function) LGBM_BoosterPredictForMat (C function) LGBM_BoosterPredictForMats (C function) LGBM_BoosterPredictForMatSingleRow (C function) LGBM_BoosterPredictForMatSingleRowFast (C function) LGBM_BoosterPredictForMatSingleRowFastInit (C function) LGBM_BoosterPredictSparseOutput (C function) LGBM_BoosterRefit (C function) LGBM_BoosterResetParameter (C function) LGBM_BoosterResetTrainingData (C function) LGBM_BoosterRollbackOneIter (C function) LGBM_BoosterSaveModel (C function) LGBM_BoosterSaveModelToString (C function) LGBM_BoosterSetLeafValue (C function) LGBM_BoosterShuffleModels (C function) LGBM_BoosterUpdateOneIter (C function) LGBM_BoosterUpdateOneIterCustom (C function) LGBM_BoosterValidateFeatureNames (C function) | LGBM_ByteBufferFree (C function) LGBM_ByteBufferGetAt (C function) LGBM_DatasetAddFeaturesFrom (C function) LGBM_DatasetCreateByReference (C function) LGBM_DatasetCreateFromArrow (C function) LGBM_DatasetCreateFromCSC (C function) LGBM_DatasetCreateFromCSR (C function) LGBM_DatasetCreateFromCSRFunc (C function) LGBM_DatasetCreateFromFile (C function) LGBM_DatasetCreateFromMat (C function) LGBM_DatasetCreateFromMats (C function) LGBM_DatasetCreateFromSampledColumn (C function) LGBM_DatasetCreateFromSerializedReference (C function) LGBM_DatasetDumpText (C function) LGBM_DatasetFree (C function) LGBM_DatasetGetFeatureNames (C function) LGBM_DatasetGetFeatureNumBin (C function) LGBM_DatasetGetField (C function) LGBM_DatasetGetNumData (C function) LGBM_DatasetGetNumFeature (C function) LGBM_DatasetGetSubset (C function) LGBM_DatasetInitStreaming (C function) LGBM_DatasetMarkFinished (C function) LGBM_DatasetPushRows (C function) LGBM_DatasetPushRowsByCSR (C function) LGBM_DatasetPushRowsByCSRWithMetadata (C function) LGBM_DatasetPushRowsWithMetadata (C function) LGBM_DatasetSaveBinary (C function) LGBM_DatasetSerializeReferenceToBinary (C function) LGBM_DatasetSetFeatureNames (C function) LGBM_DatasetSetField (C function) LGBM_DatasetSetFieldFromArrow (C function) LGBM_DatasetSetWaitForManualFinish (C function) LGBM_DatasetUpdateParamChecking (C function) LGBM_DumpParamAliases (C function) LGBM_FastConfigFree (C function) LGBM_GetLastError (C function) LGBM_GetMaxThreads (C function) LGBM_GetSampleCount (C function) LGBM_NetworkFree (C function) LGBM_NetworkInit (C function) LGBM_NetworkInitWithFunctions (C function) LGBM_RegisterLogCallback (C function) LGBM_SampleIndices (C function) LGBM_SetLastError (C function) LGBM_SetMaxThreads (C function) LGBMClassifier (class in lightgbm) LGBMModel (class in", "prev_chunk_id": "chunk_263", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_265", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "L", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "L", "content": "lightgbm) LGBMRanker (class in lightgbm) LGBMRegressor (class in lightgbm) log_evaluation() (in module lightgbm) lower_bound() (lightgbm.Booster method)", "prev_chunk_id": "chunk_264", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_266", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "M", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "M", "content": "M model_from_string() (lightgbm.Booster method) (lightgbm.CVBooster method) | model_to_string() (lightgbm.Booster method) (lightgbm.CVBooster method)", "prev_chunk_id": "chunk_265", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_267", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "N", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "N", "content": "N n_classes_ (lightgbm.DaskLGBMClassifier property) (lightgbm.LGBMClassifier property) n_estimators_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) n_features_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) n_features_in_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) | n_iter_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property) num_data() (lightgbm.Dataset method) num_feature() (lightgbm.Booster method) (lightgbm.Dataset method) num_model_per_iteration() (lightgbm.Booster method) num_trees() (lightgbm.Booster method)", "prev_chunk_id": "chunk_266", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_268", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "O", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "O", "content": "O objective_ (lightgbm.DaskLGBMClassifier property) (lightgbm.DaskLGBMRanker property) (lightgbm.DaskLGBMRegressor property) (lightgbm.LGBMClassifier property) (lightgbm.LGBMModel property) (lightgbm.LGBMRanker property) (lightgbm.LGBMRegressor property)", "prev_chunk_id": "chunk_267", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_269", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "P", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "P", "content": "P plot_importance() (in module lightgbm) plot_metric() (in module lightgbm) plot_split_value_histogram() (in module lightgbm) plot_tree() (in module lightgbm) predict() (lightgbm.Booster method) (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) | predict_proba() (lightgbm.DaskLGBMClassifier method) (lightgbm.LGBMClassifier method)", "prev_chunk_id": "chunk_268", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_270", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "R", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "R", "content": "R record_evaluation() (in module lightgbm) refit() (lightgbm.Booster method) register_logger() (in module lightgbm) | reset_parameter() (in module lightgbm) (lightgbm.Booster method) rollback_one_iter() (lightgbm.Booster method)", "prev_chunk_id": "chunk_269", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_271", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "S", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "S", "content": "S save_binary() (lightgbm.Dataset method) save_model() (lightgbm.Booster method) (lightgbm.CVBooster method) score() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMRegressor method) Sequence (class in lightgbm) set_categorical_feature() (lightgbm.Dataset method) set_feature_name() (lightgbm.Dataset method) set_field() (lightgbm.Dataset method) set_fit_request() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) set_group() (lightgbm.Dataset method) set_init_score() (lightgbm.Dataset method) set_label() (lightgbm.Dataset method) set_leaf_output() (lightgbm.Booster method) set_network() (lightgbm.Booster method) set_params() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) | set_position() (lightgbm.Dataset method) set_predict_proba_request() (lightgbm.DaskLGBMClassifier method) (lightgbm.LGBMClassifier method) set_predict_request() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMModel method) (lightgbm.LGBMRanker method) (lightgbm.LGBMRegressor method) set_reference() (lightgbm.Dataset method) set_score_request() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRegressor method) (lightgbm.LGBMClassifier method) (lightgbm.LGBMRegressor method) set_train_data_name() (lightgbm.Booster method) set_weight() (lightgbm.Dataset method) shuffle_models() (lightgbm.Booster method) subset() (lightgbm.Dataset method)", "prev_chunk_id": "chunk_270", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_272", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "T", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "T", "content": "T THREAD_LOCAL (C macro) to_local() (lightgbm.DaskLGBMClassifier method) (lightgbm.DaskLGBMRanker method) (lightgbm.DaskLGBMRegressor method) | train() (in module lightgbm) trees_to_dataframe() (lightgbm.Booster method)", "prev_chunk_id": "chunk_271", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_273", "url": "https://lightgbm.readthedocs.io/en/latest/genindex.html", "title": "U", "page_title": "Index — LightGBM 4.6.0.99 documentation", "breadcrumbs": "U", "content": "U update() (lightgbm.Booster method) | upper_bound() (lightgbm.Booster method)", "prev_chunk_id": "chunk_272", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_274", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html", "title": "GPU Targets Table", "page_title": "GPU SDK Correspondence and Device Targeting Table — LightGBM 4.6.0.99 documentation", "breadcrumbs": "GPU Targets Table", "content": "GPU Targets Table OpenCL is a universal massively parallel programming framework that targets multiple backends (GPU, CPU, FPGA, etc). Basically, to use a device from a vendor, you have to install drivers from that specific vendor. Intel’s and AMD’s OpenCL runtime also include x86 CPU target support. NVIDIA’s OpenCL runtime only supports NVIDIA GPU (no CPU support). In general, OpenCL CPU backends are quite slow, and should be used for testing and debugging only. You can find below a table of correspondence: SDK | CPU Intel/AMD | GPU Intel | GPU AMD | GPU NVIDIA Intel SDK for OpenCL | Supported | Supported | Not Supported | Not Supported AMD APP SDK * | Supported | Not Supported | Supported | Not Supported PoCL | Supported | Not Supported | Supported | Not Supported NVIDIA CUDA Toolkit | Not Supported | Not Supported | Not Supported | Supported Legend: * AMD APP SDK is deprecated. On Windows, OpenCL is included in AMD graphics driver. On Linux, newer generation AMD cards are supported by the ROCm driver. You can download an archived copy of AMD APP SDK from our GitHub repo (for Linux and for Windows).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_275", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html", "title": "Query OpenCL Devices in Your System", "page_title": "GPU SDK Correspondence and Device Targeting Table — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Query OpenCL Devices in Your System", "content": "Query OpenCL Devices in Your System Your system might have multiple GPUs from different vendors (“platforms”) installed. Setting up LightGBM GPU device requires two parameters: OpenCL Platform ID (gpu_platform_id) and OpenCL Device ID (gpu_device_id). Generally speaking, each vendor provides an OpenCL platform, and devices from the same vendor have different device IDs under that platform. For example, if your system has an Intel integrated GPU and two discrete GPUs from AMD, you will have two OpenCL platforms (with gpu_platform_id=0 and gpu_platform_id=1). If the platform 0 is Intel, it has one device (gpu_device_id=0) representing the Intel GPU; if the platform 1 is AMD, it has two devices (gpu_device_id=0, gpu_device_id=1) representing the two AMD GPUs. If you have a discrete GPU by AMD/NVIDIA and an integrated GPU by Intel, make sure to select the correct gpu_platform_id to use the discrete GPU as it usually provides better performance. On Windows, OpenCL devices can be queried using GPUCapsViewer, under the OpenCL tab. Note that the platform and device IDs reported by this utility start from 1. So you should minus the reported IDs by 1. On Linux, OpenCL devices can be listed using the clinfo command. On Ubuntu, you can install clinfo by executing sudo apt-get install clinfo.", "prev_chunk_id": "chunk_274", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_276", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html", "title": "Examples", "page_title": "GPU SDK Correspondence and Device Targeting Table — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Examples", "content": "Examples We provide test R code below, but you can use the language of your choice with the examples of your choices: library(lightgbm) data(agaricus.train, package = \"lightgbm\") train <- agaricus.train train$data[, 1] <- 1:6513 dtrain <- lgb.Dataset(train$data, label = train$label) data(agaricus.test, package = \"lightgbm\") test <- agaricus.test dtest <- lgb.Dataset.create.valid(dtrain, test$data, label = test$label) valids <- list(test = dtest) params <- list(objective = \"regression\", metric = \"rmse\", device = \"gpu\", gpu_platform_id = 0, gpu_device_id = 0, nthread = 1, boost_from_average = FALSE, num_tree_per_iteration = 10, max_bin = 32) model <- lgb.train(params, dtrain, 2, valids, min_data = 1, learning_rate = 1, early_stopping_rounds = 10) Make sure you list the OpenCL devices in your system and set gpu_platform_id and gpu_device_id correctly. In the following examples, our system has 1 GPU platform (gpu_platform_id = 0) from AMD APP SDK. The first device gpu_device_id = 0 is a GPU device (AMD Oland), and the second device gpu_device_id = 1 is the x86 CPU backend. Example of using GPU (gpu_platform_id = 0 and gpu_device_id = 0 in our system): > params <- list(objective = \"regression\", + metric = \"rmse\", + device = \"gpu\", + gpu_platform_id = 0, + gpu_device_id = 0, + nthread = 1, + boost_from_average = FALSE, + num_tree_per_iteration = 10, + max_bin = 32) > model <- lgb.train(params, + dtrain, + 2, + valids, + min_data = 1, + learning_rate = 1, + early_stopping_rounds = 10) [LightGBM] [Info] This is the GPU trainer!! [LightGBM] [Info] Total Bins 232 [LightGBM] [Info] Number of data: 6513, number of used features: 116 [LightGBM] [Info] Using GPU Device: Oland, Vendor: Advanced Micro Devices, Inc. [LightGBM] [Info] Compiling OpenCL Kernel with 16 bins... [LightGBM] [Info] GPU programs have been built [LightGBM] [Info] Size of histogram bin entry: 12 [LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred", "prev_chunk_id": "chunk_275", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_277", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html", "title": "Examples", "page_title": "GPU SDK Correspondence and Device Targeting Table — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Examples", "content": "to GPU in 0.004211 secs. 76 sparse feature groups. [LightGBM] [Info] No further splits with positive gain, best gain: -inf [LightGBM] [Info] Trained a tree with leaves=16 and depth=8 [1]: test's rmse:1.10643e-17 [LightGBM] [Info] No further splits with positive gain, best gain: -inf [LightGBM] [Info] Trained a tree with leaves=7 and depth=5 [2]: test's rmse:0 Running on OpenCL CPU backend devices is in generally slow, and we observe crashes on some Windows and macOS systems. Make sure you check the Using GPU Device line in the log and it is not using a CPU. The above log shows that we are using Oland GPU from AMD and not CPU. Example of using CPU (gpu_platform_id = 0, gpu_device_id = 1). The GPU device reported is Intel(R) Core(TM) i7-4600U CPU, so it is using the CPU backend rather than a real GPU. > params <- list(objective = \"regression\", + metric = \"rmse\", + device = \"gpu\", + gpu_platform_id = 0, + gpu_device_id = 1, + nthread = 1, + boost_from_average = FALSE, + num_tree_per_iteration = 10, + max_bin = 32) > model <- lgb.train(params, + dtrain, + 2, + valids, + min_data = 1, + learning_rate = 1, + early_stopping_rounds = 10) [LightGBM] [Info] This is the GPU trainer!! [LightGBM] [Info] Total Bins 232 [LightGBM] [Info] Number of data: 6513, number of used features: 116 [LightGBM] [Info] Using requested OpenCL platform 0 device 1 [LightGBM] [Info] Using GPU Device: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz, Vendor: GenuineIntel [LightGBM] [Info] Compiling OpenCL Kernel with 16 bins... [LightGBM] [Info] GPU programs have been built [LightGBM] [Info] Size of histogram bin entry: 12 [LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred to GPU in 0.004540 secs. 76 sparse feature groups. [LightGBM] [Info] No further splits with positive gain, best gain: -inf [LightGBM] [Info] Trained", "prev_chunk_id": "chunk_276", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_278", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html", "title": "Examples", "page_title": "GPU SDK Correspondence and Device Targeting Table — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Examples", "content": "a tree with leaves=16 and depth=8 [1]: test's rmse:1.10643e-17 [LightGBM] [Info] No further splits with positive gain, best gain: -inf [LightGBM] [Info] Trained a tree with leaves=7 and depth=5 [2]: test's rmse:0 Known issues: - Using a bad combination ofgpu_platform_idandgpu_device_idcan potentially lead to acrashdue to OpenCL driver issues on some machines (you will lose your entire session content). Beware of it. - On some systems, if you have integrated graphics card (Intel HD Graphics) and a dedicated graphics card (AMD, NVIDIA), the dedicated graphics card will automatically override the integrated graphics card. The workaround is to disable your dedicated graphics card to be able to use your integrated graphics card.", "prev_chunk_id": "chunk_277", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_279", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "How It Works?", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "How It Works?", "content": "How It Works? In LightGBM, the main computation cost during training is building the feature histograms. We use an efficient algorithm on GPU to accelerate this process. The implementation is highly modular, and works for all learning tasks (classification, ranking, regression, etc). GPU acceleration also works in distributed learning settings. GPU algorithm implementation is based on OpenCL and can work with a wide range of GPUs.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_280", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Supported Hardware", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Supported Hardware", "content": "Supported Hardware We target AMD Graphics Core Next (GCN) architecture and NVIDIA Maxwell and Pascal architectures. Most AMD GPUs released after 2012 and NVIDIA GPUs released after 2014 should be supported. We have tested the GPU implementation on the following GPUs: - AMD RX 480 with AMDGPU-pro driver 16.60 on Ubuntu 16.10 - AMD R9 280X (aka Radeon HD 7970) with fglrx driver 15.302.2301 on Ubuntu 16.10 - NVIDIA GTX 1080 with driver 375.39 and CUDA 8.0 on Ubuntu 16.10 - NVIDIA Titan X (Pascal) with driver 367.48 and CUDA 8.0 on Ubuntu 16.04 - NVIDIA Tesla M40 with driver 375.39 and CUDA 7.5 on Ubuntu 16.04 Using the following hardware is discouraged: - NVIDIA Kepler (K80, K40, K20, most GeForce GTX 700 series GPUs) or earlier NVIDIA GPUs. They don’t support hardware atomic operations in local memory space and thus histogram construction will be slow. - AMD VLIW4-based GPUs, including Radeon HD 6xxx series and earlier GPUs. These GPUs have been discontinued for years and are rarely seen nowadays.", "prev_chunk_id": "chunk_279", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_281", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "How to Achieve Good Speedup on GPU", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "How to Achieve Good Speedup on GPU", "content": "How to Achieve Good Speedup on GPU - You want to run a few datasets that we have verified with good speedup (including Higgs, epsilon, Bosch, etc) to ensure your setup is correct. If you have multiple GPUs, make sure to setgpu_platform_idandgpu_device_idto use the desired GPU. Also make sure your system is idle (especially when using a shared computer) to get accuracy performance measurements. - GPU works best on large scale and dense datasets. If dataset is too small, computing it on GPU is inefficient as the data transfer overhead can be significant. If you have categorical features, use thecategorical_columnoption and input them into LightGBM directly; do not convert them into one-hot variables. - To get good speedup with GPU, it is suggested to use a smaller number of bins. Settingmax_bin=63is recommended, as it usually does not noticeably affect training accuracy on large datasets, but GPU training can be significantly faster than using the default bin size of 255. For some dataset, even using 15 bins is enough (max_bin=15); using 15 bins will maximize GPU performance. Make sure to check the run log and verify that the desired number of bins is used. - Try to use single precision training (gpu_use_dp=false) when possible, because most GPUs (especially NVIDIA consumer GPUs) have poor double-precision performance.", "prev_chunk_id": "chunk_280", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_282", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Performance Comparison", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Performance Comparison", "content": "Performance Comparison We evaluate the training performance of GPU acceleration on the following datasets: Data | Task | Link | #Examples | #Features | Comments Higgs | Binary classification | link1 | 10,500,000 | 28 | use last 500,000 samples as test set Epsilon | Binary classification | link2 | 400,000 | 2,000 | use the provided test set Bosch | Binary classification | link3 | 1,000,000 | 968 | use the provided test set Yahoo LTR | Learning to rank | link4 | 473,134 | 700 | set1.train as train, set1.test as test MS LTR | Learning to rank | link5 | 2,270,296 | 137 | {S1,S2,S3} as train set, {S5} as test set Expo | Binary classification (Categorical) | link6 | 11,000,000 | 700 | use last 1,000,000 as test set We used the following hardware to evaluate the performance of LightGBM GPU training. Our CPU reference is a high-end dual socket Haswell-EP Xeon server with 28 cores; GPUs include a budget GPU (RX 480) and a mainstream (GTX 1080) GPU installed on the same server. It is worth mentioning that the GPUs used are not the best GPUs in the market; if you are using a better GPU (like AMD RX 580, NVIDIA GTX 1080 Ti, Titan X Pascal, Titan Xp, Tesla P100, etc), you are likely to get a better speedup. Hardware | Peak FLOPS | Peak Memory BW | Cost (MSRP) AMD Radeon RX 480 | 5,161 GFLOPS | 256 GB/s | $199 NVIDIA GTX 1080 | 8,228 GFLOPS | 320 GB/s | $499 2x Xeon E5-2683v3 (28 cores) | 1,792 GFLOPS | 133 GB/s | $3,692 During benchmarking on CPU we used only 28 physical cores of the CPU, and did not use hyper-threading cores, because we found that using too many threads actually", "prev_chunk_id": "chunk_281", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_283", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Performance Comparison", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Performance Comparison", "content": "makes performance worse. The following shows the training configuration we used: max_bin = 63 num_leaves = 255 num_iterations = 500 learning_rate = 0.1 tree_learner = serial task = train is_training_metric = false min_data_in_leaf = 1 min_sum_hessian_in_leaf = 100 ndcg_eval_at = 1,3,5,10 device = gpu gpu_platform_id = 0 gpu_device_id = 0 num_thread = 28 We use the configuration shown above, except for the Bosch dataset, we use a smaller learning_rate=0.015 and set min_sum_hessian_in_leaf=5. For all GPU training we vary the max number of bins (255, 63 and 15). The GPU implementation is from commit 0bb4a82 of LightGBM, when the GPU support was just merged in. The following table lists the accuracy on test set that CPU and GPU learner can achieve after 500 iterations. GPU with the same number of bins can achieve a similar level of accuracy as on the CPU, despite using single precision arithmetic. For most datasets, using 63 bins is sufficient. | CPU 255 bins | CPU 63 bins | CPU 15 bins | GPU 255 bins | GPU 63 bins | GPU 15 bins Higgs AUC | 0.845612 | 0.845239 | 0.841066 | 0.845612 | 0.845209 | 0.840748 Epsilon AUC | 0.950243 | 0.949952 | 0.948365 | 0.950057 | 0.949876 | 0.948365 Yahoo-LTR NDCG1 | 0.730824 | 0.730165 | 0.729647 | 0.730936 | 0.732257 | 0.73114 Yahoo-LTR NDCG3 | 0.738687 | 0.737243 | 0.736445 | 0.73698 | 0.739474 | 0.735868 Yahoo-LTR NDCG5 | 0.756609 | 0.755729 | 0.754607 | 0.756206 | 0.757007 | 0.754203 Yahoo-LTR NDCG10 | 0.79655 | 0.795827 | 0.795273 | 0.795894 | 0.797302 | 0.795584 Expo AUC | 0.776217 | 0.771566 | 0.743329 | 0.776285 | 0.77098 | 0.744078 MS-LTR NDCG1 | 0.521265 | 0.521392 | 0.518653 | 0.521789 | 0.522163 | 0.516388 MS-LTR NDCG3 | 0.503153 | 0.505753 | 0.501697 | 0.503886", "prev_chunk_id": "chunk_282", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_284", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Performance Comparison", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Performance Comparison", "content": "| 0.504089 | 0.501691 MS-LTR NDCG5 | 0.509236 | 0.510391 | 0.507193 | 0.509861 | 0.510095 | 0.50663 MS-LTR NDCG10 | 0.527835 | 0.527304 | 0.524603 | 0.528009 | 0.527059 | 0.524722 Bosch AUC | 0.718115 | 0.721791 | 0.716677 | 0.717184 | 0.724761 | 0.717005 We record the wall clock time after 500 iterations, as shown in the figure below: When using a GPU, it is advisable to use a bin size of 63 rather than 255, because it can speed up training significantly without noticeably affecting accuracy. On CPU, using a smaller bin size only marginally improves performance, sometimes even slows down training, like in Higgs (we can reproduce the same slowdown on two different machines, with different GCC versions). We found that GPU can achieve impressive acceleration on large and dense datasets like Higgs and Epsilon. Even on smaller and sparse datasets, a budget GPU can still compete and be faster than a 28-core Haswell server.", "prev_chunk_id": "chunk_283", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_285", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Memory Usage", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Memory Usage", "content": "Memory Usage The next table shows GPU memory usage reported by nvidia-smi during training with 63 bins. We can see that even the largest dataset just uses about 1 GB of GPU memory, indicating that our GPU implementation can scale to huge datasets over 10x larger than Bosch or Epsilon. Also, we can observe that generally a larger dataset (using more GPU memory, like Epsilon or Bosch) has better speedup, because the overhead of invoking GPU functions becomes significant when the dataset is small. Datasets | Higgs | Epsilon | Bosch | MS-LTR | Expo | Yahoo-LTR GPU Memory Usage (MB) | 611 | 901 | 1067 | 413 | 405 | 291", "prev_chunk_id": "chunk_284", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_286", "url": "https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html", "title": "Further Reading", "page_title": "GPU Tuning Guide and Performance Comparison — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Further Reading", "content": "Further Reading You can find more details about the GPU algorithm and benchmarks in the following article: Huan Zhang, Si Si and Cho-Jui Hsieh. GPU Acceleration for Large-scale Tree Boosting. SysML Conference, 2018.", "prev_chunk_id": "chunk_285", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_287", "url": "https://lightgbm.readthedocs.io/en/latest/README.html", "title": "Documentation", "page_title": "Documentation — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Documentation", "content": "Documentation Documentation for LightGBM is generated using Sphinx and Breathe, which works on top of Doxygen output. List of parameters and their descriptions in Parameters.rst is generated automatically from comments in config file by this script. After each commit on master, documentation is updated and published to Read the Docs.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_288", "url": "https://lightgbm.readthedocs.io/en/latest/README.html", "title": "Build", "page_title": "Documentation — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Build", "content": "Build It is not necessary to re-build this documentation while modifying LightGBM’s source code. The HTML files generated using Sphinx are not checked into source control. However, you may want to build them locally during development to test changes.", "prev_chunk_id": "chunk_287", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_289", "url": "https://lightgbm.readthedocs.io/en/latest/README.html", "title": "Docker", "page_title": "Documentation — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Docker", "content": "Docker The most reliable way to build the documentation locally is with Docker, using the same images Read the Docs uses. Run the following from the root of this repository to pull the relevant image and run a container locally. docker run \\ --rm \\ --user=0 \\ -v $(pwd):/opt/LightGBM \\ --env C_API=true \\ --env CONDA=/opt/miniforge \\ --env READTHEDOCS=true \\ --workdir=/opt/LightGBM/docs \\ --entrypoint=\"\" \\ readthedocs/build:ubuntu-24.04-2024.06.17 \\ /bin/bash build-docs.sh When that code completes, open docs/_build/html/index.html in your browser.", "prev_chunk_id": "chunk_288", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_290", "url": "https://lightgbm.readthedocs.io/en/latest/README.html", "title": "Without Docker", "page_title": "Documentation — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Without Docker", "content": "Without Docker You can build the documentation locally without Docker. Just install Doxygen and run in docs folder pip install breathe sphinx 'sphinx_rtd_theme>=0.5' make html Note that this will not build the R documentation. Consider using common R utilities for documentation generation, if you need it. Or use the Docker-based approach described above to build the R documentation locally. Optionally, you may also install scikit-learn and get richer documentation for the classes in Scikit-learn API. If you faced any problems with Doxygen installation or you simply do not need documentation for C code, it is possible to build the documentation without it: pip install sphinx 'sphinx_rtd_theme>=0.5' export C_API=NO || set C_API=NO make html", "prev_chunk_id": "chunk_289", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_291", "url": "https://lightgbm.readthedocs.io/en/latest/", "title": "Welcome to LightGBM’s documentation!", "page_title": "Welcome to LightGBM’s documentation! — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Welcome to LightGBM’s documentation!", "content": "Welcome to LightGBM’s documentation! LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: - Faster training speed and higher efficiency. - Lower memory usage. - Better accuracy. - Support of parallel, distributed, and GPU learning. - Capable of handling large-scale data. For more details, please refer to Features.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_292", "url": "https://lightgbm.readthedocs.io/en/latest/", "title": "Indices and Tables", "page_title": "Welcome to LightGBM’s documentation! — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Indices and Tables", "content": "Indices and Tables - Index", "prev_chunk_id": "chunk_291", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_293", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "Source code for lightgbm.basic # coding: utf-8 \"\"\"Wrapper for C API of LightGBM.\"\"\" # This import causes lib_lightgbm.{dll,dylib,so} to be loaded. # It's intentionally done here, as early as possible, to avoid issues like # \"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux. # # For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst. from .libpath import _LIB # isort: skip import abc import ctypes import inspect import json import warnings from collections import OrderedDict from copy import deepcopy from enum import Enum from functools import wraps from os import SEEK_END, environ from os.path import getsize from pathlib import Path from tempfile import NamedTemporaryFile from typing import TYPE_CHECKING, Any, Callable, Dict, Iterable, Iterator, List, Optional, Set, Tuple, Union import numpy as np import scipy.sparse from .compat import ( CFFI_INSTALLED, PANDAS_INSTALLED, PYARROW_INSTALLED, arrow_cffi, arrow_is_boolean, arrow_is_floating, arrow_is_integer, concat, pa_Array, pa_chunked_array, pa_ChunkedArray, pa_compute, pa_Table, pd_CategoricalDtype, pd_DataFrame, pd_Series, ) if TYPE_CHECKING: from typing import Literal # typing.TypeGuard was only introduced in Python 3.10 try: from typing import TypeGuard except ImportError: from typing_extensions import TypeGuard __all__ = [ \"Booster\", \"Dataset\", \"LGBMDeprecationWarning\", \"LightGBMError\", \"register_logger\", \"Sequence\", ] _BoosterHandle = ctypes.c_void_p _DatasetHandle = ctypes.c_void_p _ctypes_int_ptr = Union[ \"ctypes._Pointer[ctypes.c_int32]\", \"ctypes._Pointer[ctypes.c_int64]\", ] _ctypes_int_array = Union[ \"ctypes.Array[ctypes._Pointer[ctypes.c_int32]]\", \"ctypes.Array[ctypes._Pointer[ctypes.c_int64]]\", ] _ctypes_float_ptr = Union[ \"ctypes._Pointer[ctypes.c_float]\", \"ctypes._Pointer[ctypes.c_double]\", ] _ctypes_float_array = Union[ \"ctypes.Array[ctypes._Pointer[ctypes.c_float]]\", \"ctypes.Array[ctypes._Pointer[ctypes.c_double]]\", ] _LGBM_EvalFunctionResultType = Tuple[str, float, bool] _LGBM_BoosterBestScoreType = Dict[str, Dict[str, float]] _LGBM_BoosterEvalMethodResultType = Tuple[str, str, float, bool] _LGBM_BoosterEvalMethodResultWithStandardDeviationType = Tuple[str, str, float, bool, float] _LGBM_CategoricalFeatureConfiguration = Union[List[str], List[int], \"Literal['auto']\"] _LGBM_FeatureNameConfiguration = Union[List[str], \"Literal['auto']\"] _LGBM_GroupType = Union[ List[float], List[int], np.ndarray, pd_Series, pa_Array, pa_ChunkedArray, ] _LGBM_PositionType = Union[ np.ndarray, pd_Series, ] _LGBM_InitScoreType = Union[ List[float], List[List[float]], np.ndarray, pd_Series, pd_DataFrame, pa_Table, pa_Array, pa_ChunkedArray, ] _LGBM_TrainDataType = Union[ str, Path, np.ndarray, pd_DataFrame, scipy.sparse.spmatrix, \"Sequence\", List[\"Sequence\"], List[np.ndarray], pa_Table, ] _LGBM_LabelType = Union[ List[float], List[int], np.ndarray, pd_Series, pd_DataFrame, pa_Array,", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_294", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "pa_ChunkedArray, ] _LGBM_PredictDataType = Union[ str, Path, np.ndarray, pd_DataFrame, scipy.sparse.spmatrix, pa_Table, ] _LGBM_WeightType = Union[ List[float], List[int], np.ndarray, pd_Series, pa_Array, pa_ChunkedArray, ] _LGBM_SetFieldType = Union[ List[List[float]], List[List[int]], List[float], List[int], np.ndarray, pd_Series, pd_DataFrame, pa_Table, pa_Array, pa_ChunkedArray, ] ZERO_THRESHOLD = 1e-35 _MULTICLASS_OBJECTIVES = {\"multiclass\", \"multiclassova\", \"multiclass_ova\", \"ova\", \"ovr\", \"softmax\"} class LightGBMError(Exception): \"\"\"Error thrown by LightGBM.\"\"\" pass def _is_zero(x: float) -> bool: return -ZERO_THRESHOLD <= x <= ZERO_THRESHOLD def _get_sample_count(total_nrow: int, params: str) -> int: sample_cnt = ctypes.c_int(0) _safe_call( _LIB.LGBM_GetSampleCount( ctypes.c_int32(total_nrow), _c_str(params), ctypes.byref(sample_cnt), ) ) return sample_cnt.value def _np2d_to_np1d(mat: np.ndarray) -> Tuple[np.ndarray, int]: if mat.dtype in (np.float32, np.float64): dtype = mat.dtype else: dtype = np.float32 if mat.flags[\"F_CONTIGUOUS\"]: order = \"F\" layout = _C_API_IS_COL_MAJOR else: order = \"C\" layout = _C_API_IS_ROW_MAJOR # ensure dtype and order, copies if either do not match data = np.asarray(mat, dtype=dtype, order=order) # flatten array without copying return data.ravel(order=order), layout class _MissingType(Enum): NONE = \"None\" NAN = \"NaN\" ZERO = \"Zero\" class _DummyLogger: def info(self, msg: str) -> None: print(msg) # noqa: T201 def warning(self, msg: str) -> None: warnings.warn(msg, stacklevel=3) _LOGGER: Any = _DummyLogger() _INFO_METHOD_NAME = \"info\" _WARNING_METHOD_NAME = \"warning\" def _has_method(logger: Any, method_name: str) -> bool: return callable(getattr(logger, method_name, None)) [docs] def register_logger( logger: Any, info_method_name: str = \"info\", warning_method_name: str = \"warning\", ) -> None: \"\"\"Register custom logger. Parameters ---------- logger : Any Custom logger. info_method_name : str, optional (default=\"info\") Method used to log info messages. warning_method_name : str, optional (default=\"warning\") Method used to log warning messages. \"\"\" if not _has_method(logger, info_method_name) or not _has_method(logger, warning_method_name): raise TypeError(f\"Logger must provide '{info_method_name}' and '{warning_method_name}' method\") global _LOGGER, _INFO_METHOD_NAME, _WARNING_METHOD_NAME _LOGGER = logger _INFO_METHOD_NAME = info_method_name _WARNING_METHOD_NAME = warning_method_name def _normalize_native_string(func: Callable[[str], None]) -> Callable[[str], None]: \"\"\"Join log messages from native library which come by chunks.\"\"\" msg_normalized: List[str] = [] @wraps(func) def wrapper(msg: str) -> None: nonlocal msg_normalized", "prev_chunk_id": "chunk_293", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_295", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "if msg.strip() == \"\": msg = \"\".join(msg_normalized) msg_normalized = [] return func(msg) else: msg_normalized.append(msg) return wrapper def _log_info(msg: str) -> None: getattr(_LOGGER, _INFO_METHOD_NAME)(msg) def _log_warning(msg: str) -> None: getattr(_LOGGER, _WARNING_METHOD_NAME)(msg) @_normalize_native_string def _log_native(msg: str) -> None: getattr(_LOGGER, _INFO_METHOD_NAME)(msg) def _log_callback(msg: bytes) -> None: \"\"\"Redirect logs from native library into Python.\"\"\" _log_native(str(msg.decode(\"utf-8\"))) # connect the Python logger to logging in lib_lightgbm if environ.get(\"LIGHTGBM_BUILD_DOC\", \"False\") != \"True\": _LIB.LGBM_GetLastError.restype = ctypes.c_char_p callback = ctypes.CFUNCTYPE(None, ctypes.c_char_p) _LIB.callback = callback(_log_callback) # type: ignore[attr-defined] if _LIB.LGBM_RegisterLogCallback(_LIB.callback) != 0: raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\")) _NUMERIC_TYPES = (int, float, bool) def _safe_call(ret: int) -> None: \"\"\"Check the return value from C API call. Parameters ---------- ret : int The return value from C API calls. \"\"\" if ret != 0: raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\")) def _is_numeric(obj: Any) -> bool: \"\"\"Check whether object is a number or not, include numpy number, etc.\"\"\" try: float(obj) return True except (TypeError, ValueError): # TypeError: obj is not a string or a number # ValueError: invalid literal return False def _is_numpy_1d_array(data: Any) -> bool: \"\"\"Check whether data is a numpy 1-D array.\"\"\" return isinstance(data, np.ndarray) and len(data.shape) == 1 def _is_numpy_column_array(data: Any) -> bool: \"\"\"Check whether data is a column numpy array.\"\"\" if not isinstance(data, np.ndarray): return False shape = data.shape return len(shape) == 2 and shape[1] == 1 def _cast_numpy_array_to_dtype(array: np.ndarray, dtype: \"np.typing.DTypeLike\") -> np.ndarray: \"\"\"Cast numpy array to given dtype.\"\"\" if array.dtype == dtype: return array return array.astype(dtype=dtype, copy=False) def _is_1d_list(data: Any) -> bool: \"\"\"Check whether data is a 1-D list.\"\"\" return isinstance(data, list) and (not data or _is_numeric(data[0])) def _is_list_of_numpy_arrays(data: Any) -> \"TypeGuard[List[np.ndarray]]\": return isinstance(data, list) and all(isinstance(x, np.ndarray) for x in data) def _is_list_of_sequences(data: Any) -> \"TypeGuard[List[Sequence]]\": return isinstance(data, list) and all(isinstance(x, Sequence) for x in data) def _is_1d_collection(data: Any) -> bool: \"\"\"Check whether data is a 1-D collection.\"\"\" return _is_numpy_1d_array(data) or _is_numpy_column_array(data) or", "prev_chunk_id": "chunk_294", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_296", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "_is_1d_list(data) or isinstance(data, pd_Series) def _list_to_1d_numpy( data: Any, dtype: \"np.typing.DTypeLike\", name: str, ) -> np.ndarray: \"\"\"Convert data to numpy 1-D array.\"\"\" if _is_numpy_1d_array(data): return _cast_numpy_array_to_dtype(data, dtype) elif _is_numpy_column_array(data): _log_warning(\"Converting column-vector to 1d array\") array = data.ravel() return _cast_numpy_array_to_dtype(array, dtype) elif _is_1d_list(data): return np.asarray(data, dtype=dtype) elif isinstance(data, pd_Series): _check_for_bad_pandas_dtypes(data.to_frame().dtypes) return np.asarray(data, dtype=dtype) # SparseArray should be supported as well else: raise TypeError( f\"Wrong type({type(data).__name__}) for {name}.\\nIt should be list, numpy 1-D array or pandas Series\" ) def _is_numpy_2d_array(data: Any) -> bool: \"\"\"Check whether data is a numpy 2-D array.\"\"\" return isinstance(data, np.ndarray) and len(data.shape) == 2 and data.shape[1] > 1 def _is_2d_list(data: Any) -> bool: \"\"\"Check whether data is a 2-D list.\"\"\" return isinstance(data, list) and len(data) > 0 and _is_1d_list(data[0]) def _is_2d_collection(data: Any) -> bool: \"\"\"Check whether data is a 2-D collection.\"\"\" return _is_numpy_2d_array(data) or _is_2d_list(data) or isinstance(data, pd_DataFrame) def _is_pyarrow_array(data: Any) -> \"TypeGuard[Union[pa_Array, pa_ChunkedArray]]\": \"\"\"Check whether data is a PyArrow array.\"\"\" return isinstance(data, (pa_Array, pa_ChunkedArray)) def _is_pyarrow_table(data: Any) -> bool: \"\"\"Check whether data is a PyArrow table.\"\"\" return isinstance(data, pa_Table) class _ArrowCArray: \"\"\"Simple wrapper around the C representation of an Arrow type.\"\"\" n_chunks: int chunks: arrow_cffi.CData schema: arrow_cffi.CData def __init__(self, n_chunks: int, chunks: arrow_cffi.CData, schema: arrow_cffi.CData): self.n_chunks = n_chunks self.chunks = chunks self.schema = schema @property def chunks_ptr(self) -> int: \"\"\"Returns the address of the pointer to the list of chunks making up the array.\"\"\" return int(arrow_cffi.cast(\"uintptr_t\", arrow_cffi.addressof(self.chunks[0]))) @property def schema_ptr(self) -> int: \"\"\"Returns the address of the pointer to the schema of the array.\"\"\" return int(arrow_cffi.cast(\"uintptr_t\", self.schema)) def _export_arrow_to_c(data: pa_Table) -> _ArrowCArray: \"\"\"Export an Arrow type to its C representation.\"\"\" # Obtain objects to export if isinstance(data, pa_Array): export_objects = [data] elif isinstance(data, pa_ChunkedArray): export_objects = data.chunks elif isinstance(data, pa_Table): export_objects = data.to_batches() else: raise ValueError(f\"data of type '{type(data)}' cannot be exported to Arrow\") # Prepare export chunks", "prev_chunk_id": "chunk_295", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_297", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "= arrow_cffi.new(\"struct ArrowArray[]\", len(export_objects)) schema = arrow_cffi.new(\"struct ArrowSchema*\") # Export all objects for i, obj in enumerate(export_objects): chunk_ptr = int(arrow_cffi.cast(\"uintptr_t\", arrow_cffi.addressof(chunks[i]))) if i == 0: schema_ptr = int(arrow_cffi.cast(\"uintptr_t\", schema)) obj._export_to_c(chunk_ptr, schema_ptr) else: obj._export_to_c(chunk_ptr) return _ArrowCArray(len(chunks), chunks, schema) def _data_to_2d_numpy( data: Any, dtype: \"np.typing.DTypeLike\", name: str, ) -> np.ndarray: \"\"\"Convert data to numpy 2-D array.\"\"\" if _is_numpy_2d_array(data): return _cast_numpy_array_to_dtype(data, dtype) if _is_2d_list(data): return np.array(data, dtype=dtype) if isinstance(data, pd_DataFrame): _check_for_bad_pandas_dtypes(data.dtypes) return _cast_numpy_array_to_dtype(data.values, dtype) raise TypeError( f\"Wrong type({type(data).__name__}) for {name}.\\n\" \"It should be list of lists, numpy 2-D array or pandas DataFrame\" ) def _cfloat32_array_to_numpy(*, cptr: \"ctypes._Pointer\", length: int) -> np.ndarray: \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_float)): return np.ctypeslib.as_array(cptr, shape=(length,)).copy() else: raise RuntimeError(\"Expected float pointer\") def _cfloat64_array_to_numpy(*, cptr: \"ctypes._Pointer\", length: int) -> np.ndarray: \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_double)): return np.ctypeslib.as_array(cptr, shape=(length,)).copy() else: raise RuntimeError(\"Expected double pointer\") def _cint32_array_to_numpy(*, cptr: \"ctypes._Pointer\", length: int) -> np.ndarray: \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)): return np.ctypeslib.as_array(cptr, shape=(length,)).copy() else: raise RuntimeError(\"Expected int32 pointer\") def _cint64_array_to_numpy(*, cptr: \"ctypes._Pointer\", length: int) -> np.ndarray: \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int64)): return np.ctypeslib.as_array(cptr, shape=(length,)).copy() else: raise RuntimeError(\"Expected int64 pointer\") def _c_str(string: str) -> ctypes.c_char_p: \"\"\"Convert a Python string to C string.\"\"\" return ctypes.c_char_p(string.encode(\"utf-8\")) def _c_array(ctype: type, values: List[Any]) -> ctypes.Array: \"\"\"Convert a Python array to C array.\"\"\" return (ctype * len(values))(*values) # type: ignore[operator] def _json_default_with_numpy(obj: Any) -> Any: \"\"\"Convert numpy classes to JSON serializable objects.\"\"\" if isinstance(obj, (np.integer, np.floating, np.bool_)): return obj.item() elif isinstance(obj, np.ndarray): return obj.tolist() else: return obj def _to_string(x: Union[int, float, str, List]) -> str: if isinstance(x, list): val_list = \",\".join(str(val) for val in x) return f\"[{val_list}]\" else: return str(x) def _param_dict_to_str(data: Optional[Dict[str, Any]]) -> str: \"\"\"Convert Python dictionary", "prev_chunk_id": "chunk_296", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_298", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "to string, which is passed to C API.\"\"\" if data is None or not data: return \"\" pairs = [] for key, val in data.items(): if isinstance(val, (list, tuple, set)) or _is_numpy_1d_array(val): pairs.append(f\"{key}={','.join(map(_to_string, val))}\") elif isinstance(val, (str, Path, _NUMERIC_TYPES)) or _is_numeric(val): pairs.append(f\"{key}={val}\") elif val is not None: raise TypeError(f\"Unknown type of parameter:{key}, got:{type(val).__name__}\") return \" \".join(pairs) class _TempFile: \"\"\"Proxy class to workaround errors on Windows.\"\"\" def __enter__(self) -> \"_TempFile\": with NamedTemporaryFile(prefix=\"lightgbm_tmp_\", delete=True) as f: self.name = f.name self.path = Path(self.name) return self def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: if self.path.is_file(): self.path.unlink() # DeprecationWarning is not shown by default, so let's create our own with higher level # ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning class LGBMDeprecationWarning(FutureWarning): \"\"\"Custom deprecation warning.\"\"\" pass class _ConfigAliases: # lazy evaluation to allow import without dynamic library, e.g., for docs generation aliases = None @staticmethod def _get_all_param_aliases() -> Dict[str, List[str]]: buffer_len = 1 << 20 tmp_out_len = ctypes.c_int64(0) string_buffer = ctypes.create_string_buffer(buffer_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_DumpParamAliases( ctypes.c_int64(buffer_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) actual_len = tmp_out_len.value # if buffer length is not long enough, re-allocate a buffer if actual_len > buffer_len: string_buffer = ctypes.create_string_buffer(actual_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_DumpParamAliases( ctypes.c_int64(actual_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) return json.loads( string_buffer.value.decode(\"utf-8\"), object_hook=lambda obj: {k: [k] + v for k, v in obj.items()} ) @classmethod def get(cls, *args: str) -> Set[str]: if cls.aliases is None: cls.aliases = cls._get_all_param_aliases() ret = set() for i in args: ret.update(cls.get_sorted(i)) return ret @classmethod def get_sorted(cls, name: str) -> List[str]: if cls.aliases is None: cls.aliases = cls._get_all_param_aliases() return cls.aliases.get(name, [name]) @classmethod def get_by_alias(cls, *args: str) -> Set[str]: if cls.aliases is None: cls.aliases = cls._get_all_param_aliases() ret = set(args) for arg in args: for aliases in cls.aliases.values(): if arg in aliases: ret.update(aliases) break return ret def _choose_param_value(main_param_name: str, params: Dict[str, Any], default_value: Any) -> Dict[str, Any]: \"\"\"Get a single parameter value,", "prev_chunk_id": "chunk_297", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_299", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "accounting for aliases. Parameters ---------- main_param_name : str Name of the main parameter to get a value for. One of the keys of ``_ConfigAliases``. params : dict Dictionary of LightGBM parameters. default_value : Any Default value to use for the parameter, if none is found in ``params``. Returns ------- params : dict A ``params`` dict with exactly one value for ``main_param_name``, and all aliases ``main_param_name`` removed. If both ``main_param_name`` and one or more aliases for it are found, the value of ``main_param_name`` will be preferred. \"\"\" # avoid side effects on passed-in parameters params = deepcopy(params) aliases = _ConfigAliases.get_sorted(main_param_name) aliases = [a for a in aliases if a != main_param_name] # if main_param_name was provided, keep that value and remove all aliases if main_param_name in params.keys(): for param in aliases: params.pop(param, None) return params # if main param name was not found, search for an alias for param in aliases: if param in params.keys(): params[main_param_name] = params[param] break if main_param_name in params.keys(): for param in aliases: params.pop(param, None) return params # neither of main_param_name, aliases were found params[main_param_name] = default_value return params _MAX_INT32 = (1 << 31) - 1 \"\"\"Macro definition of data type in C API of LightGBM\"\"\" _C_API_DTYPE_FLOAT32 = 0 _C_API_DTYPE_FLOAT64 = 1 _C_API_DTYPE_INT32 = 2 _C_API_DTYPE_INT64 = 3 \"\"\"Macro definition of data order in matrix\"\"\" _C_API_IS_COL_MAJOR = 0 _C_API_IS_ROW_MAJOR = 1 \"\"\"Macro definition of prediction type in C API of LightGBM\"\"\" _C_API_PREDICT_NORMAL = 0 _C_API_PREDICT_RAW_SCORE = 1 _C_API_PREDICT_LEAF_INDEX = 2 _C_API_PREDICT_CONTRIB = 3 \"\"\"Macro definition of sparse matrix type\"\"\" _C_API_MATRIX_TYPE_CSR = 0 _C_API_MATRIX_TYPE_CSC = 1 \"\"\"Macro definition of feature importance type\"\"\" _C_API_FEATURE_IMPORTANCE_SPLIT = 0 _C_API_FEATURE_IMPORTANCE_GAIN = 1 \"\"\"Data type of data field\"\"\" _FIELD_TYPE_MAPPER = { \"label\": _C_API_DTYPE_FLOAT32, \"weight\": _C_API_DTYPE_FLOAT32, \"init_score\": _C_API_DTYPE_FLOAT64, \"group\": _C_API_DTYPE_INT32, \"position\": _C_API_DTYPE_INT32, } \"\"\"String name to int feature importance type mapper\"\"\" _FEATURE_IMPORTANCE_TYPE_MAPPER = {", "prev_chunk_id": "chunk_298", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_300", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "\"split\": _C_API_FEATURE_IMPORTANCE_SPLIT, \"gain\": _C_API_FEATURE_IMPORTANCE_GAIN, } def _convert_from_sliced_object(data: np.ndarray) -> np.ndarray: \"\"\"Fix the memory of multi-dimensional sliced object.\"\"\" if isinstance(data, np.ndarray) and isinstance(data.base, np.ndarray): if not data.flags.c_contiguous: _log_warning( \"Usage of np.ndarray subset (sliced data) is not recommended \" \"due to it will double the peak memory cost in LightGBM.\" ) return np.copy(data) return data def _c_float_array(data: np.ndarray) -> Tuple[_ctypes_float_ptr, int, np.ndarray]: \"\"\"Get pointer of float numpy array / list.\"\"\" if _is_1d_list(data): data = np.asarray(data) if _is_numpy_1d_array(data): data = _convert_from_sliced_object(data) assert data.flags.c_contiguous ptr_data: _ctypes_float_ptr if data.dtype == np.float32: ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)) type_data = _C_API_DTYPE_FLOAT32 elif data.dtype == np.float64: ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_double)) type_data = _C_API_DTYPE_FLOAT64 else: raise TypeError(f\"Expected np.float32 or np.float64, met type({data.dtype})\") else: raise TypeError(f\"Unknown type({type(data).__name__})\") return (ptr_data, type_data, data) # return `data` to avoid the temporary copy is freed def _c_int_array(data: np.ndarray) -> Tuple[_ctypes_int_ptr, int, np.ndarray]: \"\"\"Get pointer of int numpy array / list.\"\"\" if _is_1d_list(data): data = np.asarray(data) if _is_numpy_1d_array(data): data = _convert_from_sliced_object(data) assert data.flags.c_contiguous ptr_data: _ctypes_int_ptr if data.dtype == np.int32: ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)) type_data = _C_API_DTYPE_INT32 elif data.dtype == np.int64: ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_int64)) type_data = _C_API_DTYPE_INT64 else: raise TypeError(f\"Expected np.int32 or np.int64, met type({data.dtype})\") else: raise TypeError(f\"Unknown type({type(data).__name__})\") return (ptr_data, type_data, data) # return `data` to avoid the temporary copy is freed def _is_allowed_numpy_dtype(dtype: type) -> bool: float128 = getattr(np, \"float128\", type(None)) return issubclass(dtype, (np.integer, np.floating, np.bool_)) and not issubclass(dtype, (np.timedelta64, float128)) def _check_for_bad_pandas_dtypes(pandas_dtypes_series: pd_Series) -> None: bad_pandas_dtypes = [ f\"{column_name}: {pandas_dtype}\" for column_name, pandas_dtype in pandas_dtypes_series.items() if not _is_allowed_numpy_dtype(pandas_dtype.type) ] if bad_pandas_dtypes: raise ValueError( f\"pandas dtypes must be int, float or bool.\\nFields with bad pandas dtypes: {', '.join(bad_pandas_dtypes)}\" ) def _pandas_to_numpy( data: pd_DataFrame, target_dtype: \"np.typing.DTypeLike\", ) -> np.ndarray: _check_for_bad_pandas_dtypes(data.dtypes) try: # most common case (no nullable dtypes) return data.to_numpy(dtype=target_dtype, copy=False) except TypeError: # 1.0 <= pd version < 1.1 and nullable dtypes, least common case # raises error", "prev_chunk_id": "chunk_299", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_301", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "because array is casted to type(pd.NA) and there's no na_value argument return data.astype(target_dtype, copy=False).values except ValueError: # data has nullable dtypes, but we can specify na_value argument and copy will be made return data.to_numpy(dtype=target_dtype, na_value=np.nan) def _data_from_pandas( data: pd_DataFrame, feature_name: _LGBM_FeatureNameConfiguration, categorical_feature: _LGBM_CategoricalFeatureConfiguration, pandas_categorical: Optional[List[List]], ) -> Tuple[np.ndarray, List[str], Union[List[str], List[int]], List[List]]: if len(data.shape) != 2 or data.shape[0] < 1: raise ValueError(\"Input data must be 2 dimensional and non empty.\") # take shallow copy in case we modify categorical columns # whole column modifications don't change the original df data = data.copy(deep=False) # determine feature names if feature_name == \"auto\": feature_name = [str(col) for col in data.columns] # determine categorical features cat_cols = [col for col, dtype in zip(data.columns, data.dtypes) if isinstance(dtype, pd_CategoricalDtype)] cat_cols_not_ordered: List[str] = [col for col in cat_cols if not data[col].cat.ordered] if pandas_categorical is None: # train dataset pandas_categorical = [list(data[col].cat.categories) for col in cat_cols] else: if len(cat_cols) != len(pandas_categorical): raise ValueError(\"train and valid dataset categorical_feature do not match.\") for col, category in zip(cat_cols, pandas_categorical): if list(data[col].cat.categories) != list(category): data[col] = data[col].cat.set_categories(category) if cat_cols: # cat_cols is list data[cat_cols] = data[cat_cols].apply(lambda x: x.cat.codes).replace({-1: np.nan}) # use cat cols from DataFrame if categorical_feature == \"auto\": categorical_feature = cat_cols_not_ordered df_dtypes = [dtype.type for dtype in data.dtypes] # so that the target dtype considers floats df_dtypes.append(np.float32) target_dtype = np.result_type(*df_dtypes) return ( _pandas_to_numpy(data, target_dtype=target_dtype), feature_name, categorical_feature, pandas_categorical, ) def _dump_pandas_categorical( pandas_categorical: Optional[List[List]], file_name: Optional[Union[str, Path]] = None, ) -> str: categorical_json = json.dumps(pandas_categorical, default=_json_default_with_numpy) pandas_str = f\"\\npandas_categorical:{categorical_json}\\n\" if file_name is not None: with open(file_name, \"a\") as f: f.write(pandas_str) return pandas_str def _load_pandas_categorical( file_name: Optional[Union[str, Path]] = None, model_str: Optional[str] = None, ) -> Optional[List[List]]: pandas_key = \"pandas_categorical:\" offset = -len(pandas_key) if file_name is not None: max_offset = -getsize(file_name) with open(file_name, \"rb\") as f: while True: offset = max(offset, max_offset) f.seek(offset, SEEK_END)", "prev_chunk_id": "chunk_300", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_302", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "lines = f.readlines() if len(lines) >= 2: break offset *= 2 last_line = lines[-1].decode(\"utf-8\").strip() if not last_line.startswith(pandas_key): last_line = lines[-2].decode(\"utf-8\").strip() elif model_str is not None: idx = model_str.rfind(\"\\n\", 0, offset) last_line = model_str[idx:].strip() if last_line.startswith(pandas_key): return json.loads(last_line[len(pandas_key) :]) else: return None [docs] class Sequence(abc.ABC): \"\"\" Generic data access interface. Object should support the following operations: .. code-block:: # Get total row number. >>> len(seq) # Random access by row index. Used for data sampling. >>> seq[10] # Range data access. Used to read data in batch when constructing Dataset. >>> seq[0:100] # Optionally specify batch_size to control range data read size. >>> seq.batch_size - With random access, **data sampling does not need to go through all data**. - With range data access, there's **no need to read all data into memory thus reduce memory usage**. .. versionadded:: 3.3.0 Attributes ---------- batch_size : int Default size of a batch. \"\"\" batch_size = 4096 # Defaults to read 4K rows in each batch. @abc.abstractmethod def __getitem__(self, idx: Union[int, slice, List[int]]) -> np.ndarray: \"\"\"Return data for given row index. A basic implementation should look like this: .. code-block:: python if isinstance(idx, numbers.Integral): return self._get_one_line(idx) elif isinstance(idx, slice): return np.stack([self._get_one_line(i) for i in range(idx.start, idx.stop)]) elif isinstance(idx, list): # Only required if using ``Dataset.subset()``. return np.array([self._get_one_line(i) for i in idx]) else: raise TypeError(f\"Sequence index must be integer, slice or list, got {type(idx).__name__}\") Parameters ---------- idx : int, slice[int], list[int] Item index. Returns ------- result : numpy 1-D array or numpy 2-D array 1-D array if idx is int, 2-D array if idx is slice or list. \"\"\" raise NotImplementedError(\"Sub-classes of lightgbm.Sequence must implement __getitem__()\") @abc.abstractmethod def __len__(self) -> int: \"\"\"Return row count of this sequence.\"\"\" raise NotImplementedError(\"Sub-classes of lightgbm.Sequence must implement __len__()\") class _InnerPredictor: \"\"\"_InnerPredictor of LightGBM. Not exposed to user. Used only for", "prev_chunk_id": "chunk_301", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_303", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "prediction, usually used for continued training. .. note:: Can be converted from Booster, but cannot be converted to Booster. \"\"\" def __init__( self, booster_handle: _BoosterHandle, pandas_categorical: Optional[List[List]], pred_parameter: Dict[str, Any], manage_handle: bool, ): \"\"\"Initialize the _InnerPredictor. Parameters ---------- booster_handle : object Handle of Booster. pandas_categorical : list of list, or None If provided, list of categories for ``pandas`` categorical columns. Where the ``i``th element of the list contains the categories for the ``i``th categorical feature. pred_parameter : dict Other parameters for the prediction. manage_handle : bool If ``True``, free the corresponding Booster on the C++ side when this Python object is deleted. \"\"\" self._handle = booster_handle self.__is_manage_handle = manage_handle self.pandas_categorical = pandas_categorical self.pred_parameter = _param_dict_to_str(pred_parameter) out_num_class = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetNumClasses( self._handle, ctypes.byref(out_num_class), ) ) self.num_class = out_num_class.value @classmethod def from_booster( cls, booster: \"Booster\", pred_parameter: Dict[str, Any], ) -> \"_InnerPredictor\": \"\"\"Initialize an ``_InnerPredictor`` from a ``Booster``. Parameters ---------- booster : Booster Booster. pred_parameter : dict Other parameters for the prediction. \"\"\" out_cur_iter = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetCurrentIteration( booster._handle, ctypes.byref(out_cur_iter), ) ) return cls( booster_handle=booster._handle, pandas_categorical=booster.pandas_categorical, pred_parameter=pred_parameter, manage_handle=False, ) @classmethod def from_model_file( cls, model_file: Union[str, Path], pred_parameter: Dict[str, Any], ) -> \"_InnerPredictor\": \"\"\"Initialize an ``_InnerPredictor`` from a text file containing a LightGBM model. Parameters ---------- model_file : str or pathlib.Path Path to the model file. pred_parameter : dict Other parameters for the prediction. \"\"\" booster_handle = ctypes.c_void_p() out_num_iterations = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterCreateFromModelfile( _c_str(str(model_file)), ctypes.byref(out_num_iterations), ctypes.byref(booster_handle), ) ) return cls( booster_handle=booster_handle, pandas_categorical=_load_pandas_categorical(file_name=model_file), pred_parameter=pred_parameter, manage_handle=True, ) def __del__(self) -> None: try: if self.__is_manage_handle: _safe_call(_LIB.LGBM_BoosterFree(self._handle)) except AttributeError: pass def __getstate__(self) -> Dict[str, Any]: this = self.__dict__.copy() this.pop(\"handle\", None) this.pop(\"_handle\", None) return this def predict( self, data: _LGBM_PredictDataType, start_iteration: int = 0, num_iteration: int = -1, raw_score: bool = False, pred_leaf: bool = False, pred_contrib: bool = False, data_has_header: bool = False, validate_features: bool = False, )", "prev_chunk_id": "chunk_302", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_304", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "-> Union[np.ndarray, scipy.sparse.spmatrix, List[scipy.sparse.spmatrix]]: \"\"\"Predict logic. Parameters ---------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse or pyarrow Table Data source for prediction. If str or pathlib.Path, it represents the path to a text file (CSV, TSV, or LibSVM). start_iteration : int, optional (default=0) Start index of the iteration to predict. num_iteration : int, optional (default=-1) Iteration used for prediction. raw_score : bool, optional (default=False) Whether to predict raw scores. pred_leaf : bool, optional (default=False) Whether to predict leaf index. pred_contrib : bool, optional (default=False) Whether to predict feature contributions. data_has_header : bool, optional (default=False) Whether data has header. Used only for txt data. validate_features : bool, optional (default=False) If True, ensure that the features used to predict match the ones used to train. Used only if data is pandas DataFrame. .. versionadded:: 4.0.0 Returns ------- result : numpy array, scipy.sparse or list of scipy.sparse Prediction result. Can be sparse or a list of sparse objects (each element represents predictions for one class) for feature contributions (when ``pred_contrib=True``). \"\"\" if isinstance(data, Dataset): raise TypeError(\"Cannot use Dataset instance for prediction, please use raw data instead\") if isinstance(data, pd_DataFrame) and validate_features: data_names = [str(x) for x in data.columns] ptr_names = (ctypes.c_char_p * len(data_names))() ptr_names[:] = [x.encode(\"utf-8\") for x in data_names] _safe_call( _LIB.LGBM_BoosterValidateFeatureNames( self._handle, ptr_names, ctypes.c_int(len(data_names)), ) ) if isinstance(data, pd_DataFrame): data = _data_from_pandas( data=data, feature_name=\"auto\", categorical_feature=\"auto\", pandas_categorical=self.pandas_categorical, )[0] predict_type = _C_API_PREDICT_NORMAL if raw_score: predict_type = _C_API_PREDICT_RAW_SCORE if pred_leaf: predict_type = _C_API_PREDICT_LEAF_INDEX if pred_contrib: predict_type = _C_API_PREDICT_CONTRIB if isinstance(data, (str, Path)): with _TempFile() as f: _safe_call( _LIB.LGBM_BoosterPredictForFile( self._handle, _c_str(str(data)), ctypes.c_int(data_has_header), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), _c_str(f.name), ) ) preds = np.loadtxt(f.name, dtype=np.float64) nrow = preds.shape[0] elif isinstance(data, scipy.sparse.csr_matrix): preds, nrow = self.__pred_for_csr( csr=data, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) elif isinstance(data, scipy.sparse.csc_matrix): preds, nrow = self.__pred_for_csc( csc=data, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) elif isinstance(data, np.ndarray): preds,", "prev_chunk_id": "chunk_303", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_305", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "nrow = self.__pred_for_np2d( mat=data, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) elif _is_pyarrow_table(data): preds, nrow = self.__pred_for_pyarrow_table( table=data, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) elif isinstance(data, list): try: data = np.array(data) except BaseException as err: raise ValueError(\"Cannot convert data list to numpy array.\") from err preds, nrow = self.__pred_for_np2d( mat=data, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) else: try: _log_warning(\"Converting data to scipy sparse matrix.\") csr = scipy.sparse.csr_matrix(data) except BaseException as err: raise TypeError(f\"Cannot predict data for type {type(data).__name__}\") from err preds, nrow = self.__pred_for_csr( csr=csr, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) if pred_leaf: preds = preds.astype(np.int32) is_sparse = isinstance(preds, (list, scipy.sparse.spmatrix)) if not is_sparse and (preds.size != nrow or pred_leaf or pred_contrib): if preds.size % nrow == 0: preds = preds.reshape(nrow, -1) else: raise ValueError(f\"Length of predict result ({preds.size}) cannot be divide nrow ({nrow})\") return preds def __get_num_preds( self, start_iteration: int, num_iteration: int, nrow: int, predict_type: int, ) -> int: \"\"\"Get size of prediction result.\"\"\" if nrow > _MAX_INT32: raise LightGBMError( \"LightGBM cannot perform prediction for data \" f\"with number of rows greater than MAX_INT32 ({_MAX_INT32}).\\n\" \"You can split your data into chunks \" \"and then concatenate predictions for them\" ) n_preds = ctypes.c_int64(0) _safe_call( _LIB.LGBM_BoosterCalcNumPredict( self._handle, ctypes.c_int(nrow), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.byref(n_preds), ) ) return n_preds.value def __inner_predict_np2d( self, mat: np.ndarray, start_iteration: int, num_iteration: int, predict_type: int, preds: Optional[np.ndarray], ) -> Tuple[np.ndarray, int]: data, layout = _np2d_to_np1d(mat) ptr_data, type_ptr_data, _ = _c_float_array(data) n_preds = self.__get_num_preds( start_iteration=start_iteration, num_iteration=num_iteration, nrow=mat.shape[0], predict_type=predict_type, ) if preds is None: preds = np.empty(n_preds, dtype=np.float64) elif len(preds.shape) != 1 or len(preds) != n_preds: raise ValueError(\"Wrong length of pre-allocated predict array\") out_num_preds = ctypes.c_int64(0) _safe_call( _LIB.LGBM_BoosterPredictForMat( self._handle, ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int32(mat.shape[0]), ctypes.c_int32(mat.shape[1]), ctypes.c_int(layout), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), ctypes.byref(out_num_preds), preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if n_preds != out_num_preds.value: raise ValueError(\"Wrong length for predict results\") return preds, mat.shape[0] def __pred_for_np2d( self, mat: np.ndarray, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[np.ndarray,", "prev_chunk_id": "chunk_304", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_306", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "int]: \"\"\"Predict for a 2-D numpy matrix.\"\"\" if len(mat.shape) != 2: raise ValueError(\"Input numpy.ndarray or list must be 2 dimensional\") nrow = mat.shape[0] if nrow > _MAX_INT32: sections = np.arange(start=_MAX_INT32, stop=nrow, step=_MAX_INT32) # __get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal n_preds = [ self.__get_num_preds(start_iteration, num_iteration, i, predict_type) for i in np.diff([0] + list(sections) + [nrow]) ] n_preds_sections = np.array([0] + n_preds, dtype=np.intp).cumsum() preds = np.empty(sum(n_preds), dtype=np.float64) for chunk, (start_idx_pred, end_idx_pred) in zip( np.array_split(mat, sections), zip(n_preds_sections, n_preds_sections[1:]) ): # avoid memory consumption by arrays concatenation operations self.__inner_predict_np2d( mat=chunk, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, preds=preds[start_idx_pred:end_idx_pred], ) return preds, nrow else: return self.__inner_predict_np2d( mat=mat, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, preds=None, ) def __create_sparse_native( self, cs: Union[scipy.sparse.csc_matrix, scipy.sparse.csr_matrix], out_shape: np.ndarray, out_ptr_indptr: \"ctypes._Pointer\", out_ptr_indices: \"ctypes._Pointer\", out_ptr_data: \"ctypes._Pointer\", indptr_type: int, data_type: int, is_csr: bool, ) -> Union[List[scipy.sparse.csc_matrix], List[scipy.sparse.csr_matrix]]: # create numpy array from output arrays data_indices_len = out_shape[0] indptr_len = out_shape[1] if indptr_type == _C_API_DTYPE_INT32: out_indptr = _cint32_array_to_numpy(cptr=out_ptr_indptr, length=indptr_len) elif indptr_type == _C_API_DTYPE_INT64: out_indptr = _cint64_array_to_numpy(cptr=out_ptr_indptr, length=indptr_len) else: raise TypeError(\"Expected int32 or int64 type for indptr\") if data_type == _C_API_DTYPE_FLOAT32: out_data = _cfloat32_array_to_numpy(cptr=out_ptr_data, length=data_indices_len) elif data_type == _C_API_DTYPE_FLOAT64: out_data = _cfloat64_array_to_numpy(cptr=out_ptr_data, length=data_indices_len) else: raise TypeError(\"Expected float32 or float64 type for data\") out_indices = _cint32_array_to_numpy(cptr=out_ptr_indices, length=data_indices_len) # break up indptr based on number of rows (note more than one matrix in multiclass case) per_class_indptr_shape = cs.indptr.shape[0] # for CSC there is extra column added if not is_csr: per_class_indptr_shape += 1 out_indptr_arrays = np.split(out_indptr, out_indptr.shape[0] / per_class_indptr_shape) # reformat output into a csr or csc matrix or list of csr or csc matrices cs_output_matrices = [] offset = 0 for cs_indptr in out_indptr_arrays: matrix_indptr_len = cs_indptr[cs_indptr.shape[0] - 1] cs_indices = out_indices[offset + cs_indptr[0] : offset + matrix_indptr_len] cs_data = out_data[offset + cs_indptr[0] : offset + matrix_indptr_len] offset += matrix_indptr_len # same shape as input csr", "prev_chunk_id": "chunk_305", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_307", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "or csc matrix except extra column for expected value cs_shape = [cs.shape[0], cs.shape[1] + 1] # note: make sure we copy data as it will be deallocated next if is_csr: cs_output_matrices.append(scipy.sparse.csr_matrix((cs_data, cs_indices, cs_indptr), cs_shape)) else: cs_output_matrices.append(scipy.sparse.csc_matrix((cs_data, cs_indices, cs_indptr), cs_shape)) # free the temporary native indptr, indices, and data _safe_call( _LIB.LGBM_BoosterFreePredictSparse( out_ptr_indptr, out_ptr_indices, out_ptr_data, ctypes.c_int(indptr_type), ctypes.c_int(data_type), ) ) if len(cs_output_matrices) == 1: return cs_output_matrices[0] return cs_output_matrices def __inner_predict_csr( self, csr: scipy.sparse.csr_matrix, start_iteration: int, num_iteration: int, predict_type: int, preds: Optional[np.ndarray], ) -> Tuple[np.ndarray, int]: nrow = len(csr.indptr) - 1 n_preds = self.__get_num_preds( start_iteration=start_iteration, num_iteration=num_iteration, nrow=nrow, predict_type=predict_type, ) if preds is None: preds = np.empty(n_preds, dtype=np.float64) elif len(preds.shape) != 1 or len(preds) != n_preds: raise ValueError(\"Wrong length of pre-allocated predict array\") out_num_preds = ctypes.c_int64(0) ptr_indptr, type_ptr_indptr, _ = _c_int_array(csr.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csr.data) assert csr.shape[1] <= _MAX_INT32 csr_indices = csr.indices.astype(np.int32, copy=False) _safe_call( _LIB.LGBM_BoosterPredictForCSR( self._handle, ptr_indptr, ctypes.c_int(type_ptr_indptr), csr_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csr.indptr)), ctypes.c_int64(len(csr.data)), ctypes.c_int64(csr.shape[1]), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), ctypes.byref(out_num_preds), preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if n_preds != out_num_preds.value: raise ValueError(\"Wrong length for predict results\") return preds, nrow def __inner_predict_csr_sparse( self, csr: scipy.sparse.csr_matrix, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[Union[List[scipy.sparse.csc_matrix], List[scipy.sparse.csr_matrix]], int]: ptr_indptr, type_ptr_indptr, __ = _c_int_array(csr.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csr.data) csr_indices = csr.indices.astype(np.int32, copy=False) matrix_type = _C_API_MATRIX_TYPE_CSR out_ptr_indptr: _ctypes_int_ptr if type_ptr_indptr == _C_API_DTYPE_INT32: out_ptr_indptr = ctypes.POINTER(ctypes.c_int32)() else: out_ptr_indptr = ctypes.POINTER(ctypes.c_int64)() out_ptr_indices = ctypes.POINTER(ctypes.c_int32)() out_ptr_data: _ctypes_float_ptr if type_ptr_data == _C_API_DTYPE_FLOAT32: out_ptr_data = ctypes.POINTER(ctypes.c_float)() else: out_ptr_data = ctypes.POINTER(ctypes.c_double)() out_shape = np.empty(2, dtype=np.int64) _safe_call( _LIB.LGBM_BoosterPredictSparseOutput( self._handle, ptr_indptr, ctypes.c_int(type_ptr_indptr), csr_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csr.indptr)), ctypes.c_int64(len(csr.data)), ctypes.c_int64(csr.shape[1]), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), ctypes.c_int(matrix_type), out_shape.ctypes.data_as(ctypes.POINTER(ctypes.c_int64)), ctypes.byref(out_ptr_indptr), ctypes.byref(out_ptr_indices), ctypes.byref(out_ptr_data), ) ) matrices = self.__create_sparse_native( cs=csr, out_shape=out_shape, out_ptr_indptr=out_ptr_indptr, out_ptr_indices=out_ptr_indices, out_ptr_data=out_ptr_data, indptr_type=type_ptr_indptr, data_type=type_ptr_data, is_csr=True, ) nrow = len(csr.indptr) - 1 return matrices, nrow def __pred_for_csr( self, csr: scipy.sparse.csr_matrix, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[np.ndarray, int]: \"\"\"Predict for a CSR data.\"\"\" if", "prev_chunk_id": "chunk_306", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_308", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "predict_type == _C_API_PREDICT_CONTRIB: return self.__inner_predict_csr_sparse( csr=csr, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) nrow = len(csr.indptr) - 1 if nrow > _MAX_INT32: sections = [0] + list(np.arange(start=_MAX_INT32, stop=nrow, step=_MAX_INT32)) + [nrow] # __get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal n_preds = [self.__get_num_preds(start_iteration, num_iteration, i, predict_type) for i in np.diff(sections)] n_preds_sections = np.array([0] + n_preds, dtype=np.intp).cumsum() preds = np.empty(sum(n_preds), dtype=np.float64) for (start_idx, end_idx), (start_idx_pred, end_idx_pred) in zip( zip(sections, sections[1:]), zip(n_preds_sections, n_preds_sections[1:]) ): # avoid memory consumption by arrays concatenation operations self.__inner_predict_csr( csr=csr[start_idx:end_idx], start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, preds=preds[start_idx_pred:end_idx_pred], ) return preds, nrow else: return self.__inner_predict_csr( csr=csr, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, preds=None, ) def __inner_predict_sparse_csc( self, csc: scipy.sparse.csc_matrix, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[Union[List[scipy.sparse.csc_matrix], List[scipy.sparse.csr_matrix]], int]: ptr_indptr, type_ptr_indptr, __ = _c_int_array(csc.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csc.data) csc_indices = csc.indices.astype(np.int32, copy=False) matrix_type = _C_API_MATRIX_TYPE_CSC out_ptr_indptr: _ctypes_int_ptr if type_ptr_indptr == _C_API_DTYPE_INT32: out_ptr_indptr = ctypes.POINTER(ctypes.c_int32)() else: out_ptr_indptr = ctypes.POINTER(ctypes.c_int64)() out_ptr_indices = ctypes.POINTER(ctypes.c_int32)() out_ptr_data: _ctypes_float_ptr if type_ptr_data == _C_API_DTYPE_FLOAT32: out_ptr_data = ctypes.POINTER(ctypes.c_float)() else: out_ptr_data = ctypes.POINTER(ctypes.c_double)() out_shape = np.empty(2, dtype=np.int64) _safe_call( _LIB.LGBM_BoosterPredictSparseOutput( self._handle, ptr_indptr, ctypes.c_int(type_ptr_indptr), csc_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csc.indptr)), ctypes.c_int64(len(csc.data)), ctypes.c_int64(csc.shape[0]), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), ctypes.c_int(matrix_type), out_shape.ctypes.data_as(ctypes.POINTER(ctypes.c_int64)), ctypes.byref(out_ptr_indptr), ctypes.byref(out_ptr_indices), ctypes.byref(out_ptr_data), ) ) matrices = self.__create_sparse_native( cs=csc, out_shape=out_shape, out_ptr_indptr=out_ptr_indptr, out_ptr_indices=out_ptr_indices, out_ptr_data=out_ptr_data, indptr_type=type_ptr_indptr, data_type=type_ptr_data, is_csr=False, ) nrow = csc.shape[0] return matrices, nrow def __pred_for_csc( self, csc: scipy.sparse.csc_matrix, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[np.ndarray, int]: \"\"\"Predict for a CSC data.\"\"\" nrow = csc.shape[0] if nrow > _MAX_INT32: return self.__pred_for_csr( csr=csc.tocsr(), start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) if predict_type == _C_API_PREDICT_CONTRIB: return self.__inner_predict_sparse_csc( csc=csc, start_iteration=start_iteration, num_iteration=num_iteration, predict_type=predict_type, ) n_preds = self.__get_num_preds( start_iteration=start_iteration, num_iteration=num_iteration, nrow=nrow, predict_type=predict_type, ) preds = np.empty(n_preds, dtype=np.float64) out_num_preds = ctypes.c_int64(0) ptr_indptr, type_ptr_indptr, __ = _c_int_array(csc.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csc.data) assert csc.shape[0] <= _MAX_INT32 csc_indices = csc.indices.astype(np.int32, copy=False) _safe_call( _LIB.LGBM_BoosterPredictForCSC( self._handle, ptr_indptr, ctypes.c_int(type_ptr_indptr), csc_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csc.indptr)), ctypes.c_int64(len(csc.data)), ctypes.c_int64(csc.shape[0]), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration),", "prev_chunk_id": "chunk_307", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_309", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "_c_str(self.pred_parameter), ctypes.byref(out_num_preds), preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if n_preds != out_num_preds.value: raise ValueError(\"Wrong length for predict results\") return preds, nrow def __pred_for_pyarrow_table( self, table: pa_Table, start_iteration: int, num_iteration: int, predict_type: int, ) -> Tuple[np.ndarray, int]: \"\"\"Predict for a PyArrow table.\"\"\" if not (PYARROW_INSTALLED and CFFI_INSTALLED): raise LightGBMError(\"Cannot predict from Arrow without 'pyarrow' and 'cffi' installed.\") # Check that the input is valid: we only handle numbers (for now) if not all(arrow_is_integer(t) or arrow_is_floating(t) or arrow_is_boolean(t) for t in table.schema.types): raise ValueError(\"Arrow table may only have integer or floating point datatypes\") # Prepare prediction output array n_preds = self.__get_num_preds( start_iteration=start_iteration, num_iteration=num_iteration, nrow=table.num_rows, predict_type=predict_type, ) preds = np.empty(n_preds, dtype=np.float64) out_num_preds = ctypes.c_int64(0) # Export Arrow table to C and run prediction c_array = _export_arrow_to_c(table) _safe_call( _LIB.LGBM_BoosterPredictForArrow( self._handle, ctypes.c_int64(c_array.n_chunks), ctypes.c_void_p(c_array.chunks_ptr), ctypes.c_void_p(c_array.schema_ptr), ctypes.c_int(predict_type), ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), _c_str(self.pred_parameter), ctypes.byref(out_num_preds), preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if n_preds != out_num_preds.value: raise ValueError(\"Wrong length for predict results\") return preds, table.num_rows def current_iteration(self) -> int: \"\"\"Get the index of the current iteration. Returns ------- cur_iter : int The index of the current iteration. \"\"\" out_cur_iter = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetCurrentIteration( self._handle, ctypes.byref(out_cur_iter), ) ) return out_cur_iter.value [docs] class Dataset: \"\"\" Dataset in LightGBM. LightGBM does not train on raw data. It discretizes continuous features into histogram bins, tries to combine categorical features, and automatically handles missing and infinite values. This class handles that preprocessing, and holds that alternative representation of the input data. \"\"\" [docs] def __init__( self, data: _LGBM_TrainDataType, label: Optional[_LGBM_LabelType] = None, reference: Optional[\"Dataset\"] = None, weight: Optional[_LGBM_WeightType] = None, group: Optional[_LGBM_GroupType] = None, init_score: Optional[_LGBM_InitScoreType] = None, feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", params: Optional[Dict[str, Any]] = None, free_raw_data: bool = True, position: Optional[_LGBM_PositionType] = None, ): \"\"\"Initialize Dataset. Parameters ---------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse, Sequence, list of Sequence, list of numpy array or pyarrow", "prev_chunk_id": "chunk_308", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_310", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "Table Data source of Dataset. If str or pathlib.Path, it represents the path to a text file (CSV, TSV, or LibSVM) or a LightGBM Dataset binary file. label : list, numpy 1-D array, pandas Series / one-column DataFrame, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Label of the data. reference : Dataset or None, optional (default=None) If this is Dataset for validation, training data should be used as reference. weight : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Weight for each instance. Weights should be non-negative. group : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. init_score : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None, optional (default=None) Init score for Dataset. feature_name : list of str, or 'auto', optional (default=\"auto\") Feature names. If 'auto' and data is pandas DataFrame or pyarrow Table, data columns names are used. categorical_feature : list of str or int, or 'auto', optional (default=\"auto\") Categorical features. If list of int, interpreted as indices. If list of str, interpreted as feature names (need to specify ``feature_name`` as well). If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647). Large values", "prev_chunk_id": "chunk_309", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_311", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "could be memory consuming. Consider using consecutive integers starting from zero. All negative values in categorical features will be treated as missing values. The output cannot be monotonically constrained with respect to a categorical feature. Floating point numbers in categorical features will be rounded towards 0. params : dict or None, optional (default=None) Other parameters for Dataset. free_raw_data : bool, optional (default=True) If True, raw data is freed after constructing inner Dataset. position : numpy 1-D array, pandas Series or None, optional (default=None) Position of items used in unbiased learning-to-rank task. \"\"\" self._handle: Optional[_DatasetHandle] = None self.data = data self.label = label self.reference = reference self.weight = weight self.group = group self.position = position self.init_score = init_score self.feature_name: _LGBM_FeatureNameConfiguration = feature_name self.categorical_feature: _LGBM_CategoricalFeatureConfiguration = categorical_feature self.params = deepcopy(params) self.free_raw_data = free_raw_data self.used_indices: Optional[List[int]] = None self._need_slice = True self._predictor: Optional[_InnerPredictor] = None self.pandas_categorical: Optional[List[List]] = None self._params_back_up: Optional[Dict[str, Any]] = None self.version = 0 self._start_row = 0 # Used when pushing rows one by one. def __del__(self) -> None: try: self._free_handle() except AttributeError: pass def _create_sample_indices(self, total_nrow: int) -> np.ndarray: \"\"\"Get an array of randomly chosen indices from this ``Dataset``. Indices are sampled without replacement. Parameters ---------- total_nrow : int Total number of rows to sample from. If this value is greater than the value of parameter ``bin_construct_sample_cnt``, only ``bin_construct_sample_cnt`` indices will be used. If Dataset has multiple input data, this should be the sum of rows of every file. Returns ------- indices : numpy array Indices for sampled data. \"\"\" param_str = _param_dict_to_str(self.get_params()) sample_cnt = _get_sample_count(total_nrow, param_str) indices = np.empty(sample_cnt, dtype=np.int32) ptr_data, _, _ = _c_int_array(indices) actual_sample_cnt = ctypes.c_int32(0) _safe_call( _LIB.LGBM_SampleIndices( ctypes.c_int32(total_nrow), _c_str(param_str), ptr_data, ctypes.byref(actual_sample_cnt), ) ) assert sample_cnt == actual_sample_cnt.value return indices def _init_from_ref_dataset( self, total_nrow: int, ref_dataset: _DatasetHandle, ) -> \"Dataset\": \"\"\"Create dataset from a reference dataset.", "prev_chunk_id": "chunk_310", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_312", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "Parameters ---------- total_nrow : int Number of rows expected to add to dataset. ref_dataset : object Handle of reference dataset to extract metadata from. Returns ------- self : Dataset Constructed Dataset object. \"\"\" self._handle = ctypes.c_void_p() _safe_call( _LIB.LGBM_DatasetCreateByReference( ref_dataset, ctypes.c_int64(total_nrow), ctypes.byref(self._handle), ) ) return self def _init_from_sample( self, sample_data: List[np.ndarray], sample_indices: List[np.ndarray], sample_cnt: int, total_nrow: int, ) -> \"Dataset\": \"\"\"Create Dataset from sampled data structures. Parameters ---------- sample_data : list of numpy array Sample data for each column. sample_indices : list of numpy array Sample data row index for each column. sample_cnt : int Number of samples. total_nrow : int Total number of rows for all input files. Returns ------- self : Dataset Constructed Dataset object. \"\"\" ncol = len(sample_indices) assert len(sample_data) == ncol, \"#sample data column != #column indices\" for i in range(ncol): if sample_data[i].dtype != np.double: raise ValueError(f\"sample_data[{i}] type {sample_data[i].dtype} is not double\") if sample_indices[i].dtype != np.int32: raise ValueError(f\"sample_indices[{i}] type {sample_indices[i].dtype} is not int32\") # c type: double** # each double* element points to start of each column of sample data. sample_col_ptr: _ctypes_float_array = (ctypes.POINTER(ctypes.c_double) * ncol)() # c type int** # each int* points to start of indices for each column indices_col_ptr: _ctypes_int_array = (ctypes.POINTER(ctypes.c_int32) * ncol)() for i in range(ncol): sample_col_ptr[i] = _c_float_array(sample_data[i])[0] indices_col_ptr[i] = _c_int_array(sample_indices[i])[0] num_per_col = np.array([len(d) for d in sample_indices], dtype=np.int32) num_per_col_ptr, _, _ = _c_int_array(num_per_col) self._handle = ctypes.c_void_p() params_str = _param_dict_to_str(self.get_params()) _safe_call( _LIB.LGBM_DatasetCreateFromSampledColumn( ctypes.cast(sample_col_ptr, ctypes.POINTER(ctypes.POINTER(ctypes.c_double))), ctypes.cast(indices_col_ptr, ctypes.POINTER(ctypes.POINTER(ctypes.c_int32))), ctypes.c_int32(ncol), num_per_col_ptr, ctypes.c_int32(sample_cnt), ctypes.c_int32(total_nrow), ctypes.c_int64(total_nrow), _c_str(params_str), ctypes.byref(self._handle), ) ) return self def _push_rows(self, data: np.ndarray) -> \"Dataset\": \"\"\"Add rows to Dataset. Parameters ---------- data : numpy 1-D array New data to add to the Dataset. Returns ------- self : Dataset Dataset object. \"\"\" nrow, ncol = data.shape data = data.reshape(data.size) data_ptr, data_type, _ = _c_float_array(data) _safe_call( _LIB.LGBM_DatasetPushRows( self._handle, data_ptr, data_type, ctypes.c_int32(nrow), ctypes.c_int32(ncol), ctypes.c_int32(self._start_row), ) )", "prev_chunk_id": "chunk_311", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_313", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "self._start_row += nrow return self [docs] def get_params(self) -> Dict[str, Any]: \"\"\"Get the used parameters in the Dataset. Returns ------- params : dict The used parameters in this Dataset object. \"\"\" if self.params is not None: # no min_data, nthreads and verbose in this function dataset_params = _ConfigAliases.get( \"bin_construct_sample_cnt\", \"categorical_feature\", \"data_random_seed\", \"enable_bundle\", \"feature_pre_filter\", \"forcedbins_filename\", \"group_column\", \"header\", \"ignore_column\", \"is_enable_sparse\", \"label_column\", \"linear_tree\", \"max_bin\", \"max_bin_by_feature\", \"min_data_in_bin\", \"pre_partition\", \"precise_float_parser\", \"two_round\", \"use_missing\", \"weight_column\", \"zero_as_missing\", ) return {k: v for k, v in self.params.items() if k in dataset_params} else: return {} def _free_handle(self) -> \"Dataset\": if self._handle is not None: _safe_call(_LIB.LGBM_DatasetFree(self._handle)) self._handle = None self._need_slice = True if self.used_indices is not None: self.data = None return self def _set_init_score_by_predictor( self, predictor: Optional[_InnerPredictor], data: _LGBM_TrainDataType, used_indices: Optional[Union[List[int], np.ndarray]], ) -> \"Dataset\": data_has_header = False if isinstance(data, (str, Path)) and self.params is not None: # check data has header or not data_has_header = any(self.params.get(alias, False) for alias in _ConfigAliases.get(\"header\")) num_data = self.num_data() if predictor is not None: init_score: Union[np.ndarray, scipy.sparse.spmatrix] = predictor.predict( data=data, raw_score=True, data_has_header=data_has_header, ) init_score = init_score.ravel() if used_indices is not None: assert not self._need_slice if isinstance(data, (str, Path)): sub_init_score = np.empty(num_data * predictor.num_class, dtype=np.float64) assert num_data == len(used_indices) for i in range(len(used_indices)): for j in range(predictor.num_class): sub_init_score[i * predictor.num_class + j] = init_score[ used_indices[i] * predictor.num_class + j ] init_score = sub_init_score if predictor.num_class > 1: # need to regroup init_score new_init_score = np.empty(init_score.size, dtype=np.float64) for i in range(num_data): for j in range(predictor.num_class): new_init_score[j * num_data + i] = init_score[i * predictor.num_class + j] init_score = new_init_score elif self.init_score is not None: init_score = np.full_like(self.init_score, fill_value=0.0, dtype=np.float64) else: return self self.set_init_score(init_score) return self def _lazy_init( self, data: Optional[_LGBM_TrainDataType], label: Optional[_LGBM_LabelType], reference: Optional[\"Dataset\"], weight: Optional[_LGBM_WeightType], group: Optional[_LGBM_GroupType], init_score: Optional[_LGBM_InitScoreType], predictor: Optional[_InnerPredictor], feature_name: _LGBM_FeatureNameConfiguration, categorical_feature: _LGBM_CategoricalFeatureConfiguration, params: Optional[Dict[str, Any]], position: Optional[_LGBM_PositionType], ) -> \"Dataset\": if data is", "prev_chunk_id": "chunk_312", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_314", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "None: self._handle = None return self if reference is not None: self.pandas_categorical = reference.pandas_categorical categorical_feature = reference.categorical_feature if isinstance(data, pd_DataFrame): data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas( data=data, feature_name=feature_name, categorical_feature=categorical_feature, pandas_categorical=self.pandas_categorical, ) elif _is_pyarrow_table(data) and feature_name == \"auto\": feature_name = data.column_names # process for args params = {} if params is None else params args_names = inspect.signature(self.__class__._lazy_init).parameters.keys() for key in params.keys(): if key in args_names: _log_warning( f\"{key} keyword has been found in `params` and will be ignored.\\n\" f\"Please use {key} argument of the Dataset constructor to pass this parameter.\" ) # get categorical features if isinstance(categorical_feature, list): categorical_indices = set() feature_dict = {} if isinstance(feature_name, list): feature_dict = {name: i for i, name in enumerate(feature_name)} for name in categorical_feature: if isinstance(name, str) and name in feature_dict: categorical_indices.add(feature_dict[name]) elif isinstance(name, int): categorical_indices.add(name) else: raise TypeError(f\"Wrong type({type(name).__name__}) or unknown name({name}) in categorical_feature\") if categorical_indices: for cat_alias in _ConfigAliases.get(\"categorical_feature\"): if cat_alias in params: # If the params[cat_alias] is equal to categorical_indices, do not report the warning. if not (isinstance(params[cat_alias], list) and set(params[cat_alias]) == categorical_indices): _log_warning(f\"{cat_alias} in param dict is overridden.\") params.pop(cat_alias, None) params[\"categorical_column\"] = sorted(categorical_indices) params_str = _param_dict_to_str(params) self.params = params # process for reference dataset ref_dataset = None if isinstance(reference, Dataset): ref_dataset = reference.construct()._handle elif reference is not None: raise TypeError(\"Reference dataset should be None or dataset instance\") # start construct data if isinstance(data, (str, Path)): self._handle = ctypes.c_void_p() _safe_call( _LIB.LGBM_DatasetCreateFromFile( _c_str(str(data)), _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) elif isinstance(data, scipy.sparse.csr_matrix): self.__init_from_csr(data, params_str, ref_dataset) elif isinstance(data, scipy.sparse.csc_matrix): self.__init_from_csc(data, params_str, ref_dataset) elif isinstance(data, np.ndarray): self.__init_from_np2d(data, params_str, ref_dataset) elif _is_pyarrow_table(data): self.__init_from_pyarrow_table(data, params_str, ref_dataset) elif isinstance(data, list) and len(data) > 0: if _is_list_of_numpy_arrays(data): self.__init_from_list_np2d(data, params_str, ref_dataset) elif _is_list_of_sequences(data): self.__init_from_seqs(data, ref_dataset) else: raise TypeError(\"Data list can only be of ndarray or Sequence\") elif isinstance(data, Sequence): self.__init_from_seqs([data], ref_dataset) else: try: csr = scipy.sparse.csr_matrix(data) self.__init_from_csr(csr, params_str, ref_dataset) except BaseException as", "prev_chunk_id": "chunk_313", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_315", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "err: raise TypeError(f\"Cannot initialize Dataset from {type(data).__name__}\") from err if label is not None: self.set_label(label) if self.get_label() is None: raise ValueError(\"Label should not be None\") if weight is not None: self.set_weight(weight) if group is not None: self.set_group(group) if position is not None: self.set_position(position) if isinstance(predictor, _InnerPredictor): if self._predictor is None and init_score is not None: _log_warning(\"The init_score will be overridden by the prediction of init_model.\") self._set_init_score_by_predictor(predictor=predictor, data=data, used_indices=None) elif init_score is not None: self.set_init_score(init_score) elif predictor is not None: raise TypeError(f\"Wrong predictor type {type(predictor).__name__}\") # set feature names return self.set_feature_name(feature_name) @staticmethod def _yield_row_from_seqlist(seqs: List[Sequence], indices: Iterable[int]) -> Iterator[np.ndarray]: offset = 0 seq_id = 0 seq = seqs[seq_id] for row_id in indices: assert row_id >= offset, \"sample indices are expected to be monotonic\" while row_id >= offset + len(seq): offset += len(seq) seq_id += 1 seq = seqs[seq_id] id_in_seq = row_id - offset row = seq[id_in_seq] yield row if row.flags[\"OWNDATA\"] else row.copy() def __sample(self, seqs: List[Sequence], total_nrow: int) -> Tuple[List[np.ndarray], List[np.ndarray]]: \"\"\"Sample data from seqs. Mimics behavior in c_api.cpp:LGBM_DatasetCreateFromMats() Returns ------- sampled_rows, sampled_row_indices \"\"\" indices = self._create_sample_indices(total_nrow) # Select sampled rows, transpose to column order. sampled = np.array(list(self._yield_row_from_seqlist(seqs, indices))) sampled = sampled.T filtered = [] filtered_idx = [] sampled_row_range = np.arange(len(indices), dtype=np.int32) for col in sampled: col_predicate = (np.abs(col) > ZERO_THRESHOLD) | np.isnan(col) filtered_col = col[col_predicate] filtered_row_idx = sampled_row_range[col_predicate] filtered.append(filtered_col) filtered_idx.append(filtered_row_idx) return filtered, filtered_idx def __init_from_seqs( self, seqs: List[Sequence], ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\" Initialize data from list of Sequence objects. Sequence: Generic Data Access Object Supports random access and access by batch if properly defined by user Data scheme uniformity are trusted, not checked \"\"\" total_nrow = sum(len(seq) for seq in seqs) # create validation dataset from ref_dataset if ref_dataset is not None: self._init_from_ref_dataset(total_nrow, ref_dataset) else: param_str = _param_dict_to_str(self.get_params()) sample_cnt = _get_sample_count(total_nrow, param_str) sample_data, col_indices = self.__sample(seqs, total_nrow) self._init_from_sample(sample_data,", "prev_chunk_id": "chunk_314", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_316", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "col_indices, sample_cnt, total_nrow) for seq in seqs: nrow = len(seq) batch_size = getattr(seq, \"batch_size\", None) or Sequence.batch_size for start in range(0, nrow, batch_size): end = min(start + batch_size, nrow) self._push_rows(seq[start:end]) return self def __init_from_np2d( self, mat: np.ndarray, params_str: str, ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\"Initialize data from a 2-D numpy matrix.\"\"\" if len(mat.shape) != 2: raise ValueError(\"Input numpy.ndarray must be 2 dimensional\") self._handle = ctypes.c_void_p() data, layout = _np2d_to_np1d(mat) ptr_data, type_ptr_data, _ = _c_float_array(data) _safe_call( _LIB.LGBM_DatasetCreateFromMat( ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int32(mat.shape[0]), ctypes.c_int32(mat.shape[1]), ctypes.c_int(layout), _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) return self def __init_from_list_np2d( self, mats: List[np.ndarray], params_str: str, ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\"Initialize data from a list of 2-D numpy matrices.\"\"\" ncol = mats[0].shape[1] nrow = np.empty((len(mats),), np.int32) ptr_data: _ctypes_float_array if mats[0].dtype == np.float64: ptr_data = (ctypes.POINTER(ctypes.c_double) * len(mats))() else: ptr_data = (ctypes.POINTER(ctypes.c_float) * len(mats))() layouts = (ctypes.c_int * len(mats))() holders = [] type_ptr_data = -1 for i, mat in enumerate(mats): if len(mat.shape) != 2: raise ValueError(\"Input numpy.ndarray must be 2 dimensional\") if mat.shape[1] != ncol: raise ValueError(\"Input arrays must have same number of columns\") nrow[i] = mat.shape[0] mat, layout = _np2d_to_np1d(mat) chunk_ptr_data, chunk_type_ptr_data, holder = _c_float_array(mat) if type_ptr_data != -1 and chunk_type_ptr_data != type_ptr_data: raise ValueError(\"Input chunks must have same type\") ptr_data[i] = chunk_ptr_data layouts[i] = layout type_ptr_data = chunk_type_ptr_data holders.append(holder) self._handle = ctypes.c_void_p() _safe_call( _LIB.LGBM_DatasetCreateFromMats( ctypes.c_int32(len(mats)), ctypes.cast(ptr_data, ctypes.POINTER(ctypes.POINTER(ctypes.c_double))), ctypes.c_int(type_ptr_data), nrow.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ctypes.c_int32(ncol), layouts, _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) return self def __init_from_csr( self, csr: scipy.sparse.csr_matrix, params_str: str, ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\"Initialize data from a CSR matrix.\"\"\" if len(csr.indices) != len(csr.data): raise ValueError(f\"Length mismatch: {len(csr.indices)} vs {len(csr.data)}\") self._handle = ctypes.c_void_p() ptr_indptr, type_ptr_indptr, __ = _c_int_array(csr.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csr.data) assert csr.shape[1] <= _MAX_INT32 csr_indices = csr.indices.astype(np.int32, copy=False) _safe_call( _LIB.LGBM_DatasetCreateFromCSR( ptr_indptr, ctypes.c_int(type_ptr_indptr), csr_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csr.indptr)), ctypes.c_int64(len(csr.data)), ctypes.c_int64(csr.shape[1]), _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) return self def __init_from_csc( self, csc: scipy.sparse.csc_matrix,", "prev_chunk_id": "chunk_315", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_317", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "params_str: str, ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\"Initialize data from a CSC matrix.\"\"\" if len(csc.indices) != len(csc.data): raise ValueError(f\"Length mismatch: {len(csc.indices)} vs {len(csc.data)}\") self._handle = ctypes.c_void_p() ptr_indptr, type_ptr_indptr, __ = _c_int_array(csc.indptr) ptr_data, type_ptr_data, _ = _c_float_array(csc.data) assert csc.shape[0] <= _MAX_INT32 csc_indices = csc.indices.astype(np.int32, copy=False) _safe_call( _LIB.LGBM_DatasetCreateFromCSC( ptr_indptr, ctypes.c_int(type_ptr_indptr), csc_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ptr_data, ctypes.c_int(type_ptr_data), ctypes.c_int64(len(csc.indptr)), ctypes.c_int64(len(csc.data)), ctypes.c_int64(csc.shape[0]), _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) return self def __init_from_pyarrow_table( self, table: pa_Table, params_str: str, ref_dataset: Optional[_DatasetHandle], ) -> \"Dataset\": \"\"\"Initialize data from a PyArrow table.\"\"\" if not (PYARROW_INSTALLED and CFFI_INSTALLED): raise LightGBMError(\"Cannot init Dataset from Arrow without 'pyarrow' and 'cffi' installed.\") # Check that the input is valid: we only handle numbers (for now) if not all(arrow_is_integer(t) or arrow_is_floating(t) or arrow_is_boolean(t) for t in table.schema.types): raise ValueError(\"Arrow table may only have integer or floating point datatypes\") # Export Arrow table to C c_array = _export_arrow_to_c(table) self._handle = ctypes.c_void_p() _safe_call( _LIB.LGBM_DatasetCreateFromArrow( ctypes.c_int64(c_array.n_chunks), ctypes.c_void_p(c_array.chunks_ptr), ctypes.c_void_p(c_array.schema_ptr), _c_str(params_str), ref_dataset, ctypes.byref(self._handle), ) ) return self @staticmethod def _compare_params_for_warning( params: Dict[str, Any], other_params: Dict[str, Any], ignore_keys: Set[str], ) -> bool: \"\"\"Compare two dictionaries with params ignoring some keys. It is only for the warning purpose. Parameters ---------- params : dict One dictionary with parameters to compare. other_params : dict Another dictionary with parameters to compare. ignore_keys : set Keys that should be ignored during comparing two dictionaries. Returns ------- compare_result : bool Returns whether two dictionaries with params are equal. \"\"\" for k, v in other_params.items(): if k not in ignore_keys: if k not in params or params[k] != v: return False for k, v in params.items(): if k not in ignore_keys: if k not in other_params or v != other_params[k]: return False return True [docs] def construct(self) -> \"Dataset\": \"\"\"Lazy init. Returns ------- self : Dataset Constructed Dataset object. \"\"\" if self._handle is None: if self.reference is not None: reference_params =", "prev_chunk_id": "chunk_316", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_318", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "self.reference.get_params() params = self.get_params() if params != reference_params: if not self._compare_params_for_warning( params=params, other_params=reference_params, ignore_keys=_ConfigAliases.get(\"categorical_feature\"), ): _log_warning(\"Overriding the parameters from Reference Dataset.\") self._update_params(reference_params) if self.used_indices is None: # create valid self._lazy_init( data=self.data, label=self.label, reference=self.reference, weight=self.weight, group=self.group, position=self.position, init_score=self.init_score, predictor=self._predictor, feature_name=self.feature_name, categorical_feature=\"auto\", params=self.params, ) else: # construct subset used_indices = _list_to_1d_numpy(self.used_indices, dtype=np.int32, name=\"used_indices\") assert used_indices.flags.c_contiguous if self.reference.group is not None: group_info = np.array(self.reference.group).astype(np.int32, copy=False) _, self.group = np.unique( np.repeat(range(len(group_info)), repeats=group_info)[self.used_indices], return_counts=True ) self._handle = ctypes.c_void_p() params_str = _param_dict_to_str(self.params) _safe_call( _LIB.LGBM_DatasetGetSubset( self.reference.construct()._handle, used_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)), ctypes.c_int32(used_indices.shape[0]), _c_str(params_str), ctypes.byref(self._handle), ) ) if not self.free_raw_data: self.get_data() if self.group is not None: self.set_group(self.group) if self.position is not None: self.set_position(self.position) if self.get_label() is None: raise ValueError(\"Label should not be None.\") if ( isinstance(self._predictor, _InnerPredictor) and self._predictor is not self.reference._predictor ): self.get_data() self._set_init_score_by_predictor( predictor=self._predictor, data=self.data, used_indices=used_indices ) else: # create train self._lazy_init( data=self.data, label=self.label, reference=None, weight=self.weight, group=self.group, init_score=self.init_score, predictor=self._predictor, feature_name=self.feature_name, categorical_feature=self.categorical_feature, params=self.params, position=self.position, ) if self.free_raw_data: self.data = None self.feature_name = self.get_feature_name() return self [docs] def create_valid( self, data: _LGBM_TrainDataType, label: Optional[_LGBM_LabelType] = None, weight: Optional[_LGBM_WeightType] = None, group: Optional[_LGBM_GroupType] = None, init_score: Optional[_LGBM_InitScoreType] = None, params: Optional[Dict[str, Any]] = None, position: Optional[_LGBM_PositionType] = None, ) -> \"Dataset\": \"\"\"Create validation data align with current Dataset. Parameters ---------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse, Sequence, list of Sequence, list of numpy array or pyarrow Table Data source of Dataset. If str or pathlib.Path, it represents the path to a text file (CSV, TSV, or LibSVM) or a LightGBM Dataset binary file. label : list, numpy 1-D array, pandas Series / one-column DataFrame, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Label of the data. weight : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Weight for each instance. Weights should be non-negative. group : list, numpy 1-D array, pandas Series, pyarrow Array,", "prev_chunk_id": "chunk_317", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_319", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "pyarrow ChunkedArray or None, optional (default=None) Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. init_score : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None, optional (default=None) Init score for Dataset. params : dict or None, optional (default=None) Other parameters for validation Dataset. position : numpy 1-D array, pandas Series or None, optional (default=None) Position of items used in unbiased learning-to-rank task. Returns ------- valid : Dataset Validation Dataset with reference to self. \"\"\" ret = Dataset( data, label=label, reference=self, weight=weight, group=group, position=position, init_score=init_score, params=params, free_raw_data=self.free_raw_data, ) ret._predictor = self._predictor ret.pandas_categorical = self.pandas_categorical return ret [docs] def subset( self, used_indices: List[int], params: Optional[Dict[str, Any]] = None, ) -> \"Dataset\": \"\"\"Get subset of current Dataset. Parameters ---------- used_indices : list of int Indices used to create the subset. params : dict or None, optional (default=None) These parameters will be passed to Dataset constructor. Returns ------- subset : Dataset Subset of the current Dataset. \"\"\" if params is None: params = self.params ret = Dataset( None, reference=self, feature_name=self.feature_name, categorical_feature=self.categorical_feature, params=params, free_raw_data=self.free_raw_data, ) ret._predictor = self._predictor ret.pandas_categorical = self.pandas_categorical ret.used_indices = sorted(used_indices) return ret [docs] def save_binary(self, filename: Union[str, Path]) -> \"Dataset\": \"\"\"Save Dataset to a binary file. .. note:: Please note that `init_score` is not saved in binary file. If you need it, please set it again after loading Dataset. Parameters ---------- filename : str or pathlib.Path Name of the output file.", "prev_chunk_id": "chunk_318", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_320", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "Returns ------- self : Dataset Returns self. \"\"\" _safe_call( _LIB.LGBM_DatasetSaveBinary( self.construct()._handle, _c_str(str(filename)), ) ) return self def _update_params(self, params: Optional[Dict[str, Any]]) -> \"Dataset\": if not params: return self params = deepcopy(params) def update() -> None: if not self.params: self.params = params else: self._params_back_up = deepcopy(self.params) self.params.update(params) if self._handle is None: update() elif params is not None: ret = _LIB.LGBM_DatasetUpdateParamChecking( _c_str(_param_dict_to_str(self.params)), _c_str(_param_dict_to_str(params)), ) if ret != 0: # could be updated if data is not freed if self.data is not None: update() self._free_handle() else: raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\")) return self def _reverse_update_params(self) -> \"Dataset\": if self._handle is None: self.params = deepcopy(self._params_back_up) self._params_back_up = None return self [docs] def set_field( self, field_name: str, data: Optional[_LGBM_SetFieldType], ) -> \"Dataset\": \"\"\"Set property into the Dataset. Parameters ---------- field_name : str The field name of the information. data : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None The data to be set. Returns ------- self : Dataset Dataset with set property. \"\"\" if self._handle is None: raise Exception(f\"Cannot set {field_name} before construct dataset\") if data is None: # set to None _safe_call( _LIB.LGBM_DatasetSetField( self._handle, _c_str(field_name), None, ctypes.c_int(0), ctypes.c_int(_FIELD_TYPE_MAPPER[field_name]), ) ) return self # If the data is a arrow data, we can just pass it to C if _is_pyarrow_array(data) or _is_pyarrow_table(data): # If a table is being passed, we concatenate the columns. This is only valid for # 'init_score'. if _is_pyarrow_table(data): if field_name != \"init_score\": raise ValueError(f\"pyarrow tables are not supported for field '{field_name}'\") data = pa_chunked_array( [ chunk for array in data.columns # type: ignore for chunk in array.chunks ] ) c_array = _export_arrow_to_c(data) _safe_call( _LIB.LGBM_DatasetSetFieldFromArrow( self._handle, _c_str(field_name), ctypes.c_int64(c_array.n_chunks), ctypes.c_void_p(c_array.chunks_ptr), ctypes.c_void_p(c_array.schema_ptr), ) ) self.version += 1 return self dtype: \"np.typing.DTypeLike\" if field_name == \"init_score\": dtype = np.float64", "prev_chunk_id": "chunk_319", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_321", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "if _is_1d_collection(data): data = _list_to_1d_numpy(data, dtype=dtype, name=field_name) elif _is_2d_collection(data): data = _data_to_2d_numpy(data, dtype=dtype, name=field_name) data = data.ravel(order=\"F\") else: raise TypeError( \"init_score must be list, numpy 1-D array or pandas Series.\\n\" \"In multiclass classification init_score can also be a list of lists, numpy 2-D array or pandas DataFrame.\" ) else: if field_name in {\"group\", \"position\"}: dtype = np.int32 else: dtype = np.float32 data = _list_to_1d_numpy(data, dtype=dtype, name=field_name) ptr_data: Union[_ctypes_float_ptr, _ctypes_int_ptr] if data.dtype == np.float32 or data.dtype == np.float64: ptr_data, type_data, _ = _c_float_array(data) elif data.dtype == np.int32: ptr_data, type_data, _ = _c_int_array(data) else: raise TypeError(f\"Expected np.float32/64 or np.int32, met type({data.dtype})\") if type_data != _FIELD_TYPE_MAPPER[field_name]: raise TypeError(\"Input type error for set_field\") _safe_call( _LIB.LGBM_DatasetSetField( self._handle, _c_str(field_name), ptr_data, ctypes.c_int(len(data)), ctypes.c_int(type_data), ) ) self.version += 1 return self [docs] def get_field(self, field_name: str) -> Optional[np.ndarray]: \"\"\"Get property from the Dataset. Can only be run on a constructed Dataset. Unlike ``get_group()``, ``get_init_score()``, ``get_label()``, ``get_position()``, and ``get_weight()``, this method ignores any raw data passed into ``lgb.Dataset()`` on the Python side, and will only read data from the constructed C++ ``Dataset`` object. Parameters ---------- field_name : str The field name of the information. Returns ------- info : numpy array or None A numpy array with information from the Dataset. \"\"\" if self._handle is None: raise Exception(f\"Cannot get {field_name} before construct Dataset\") tmp_out_len = ctypes.c_int(0) out_type = ctypes.c_int(0) ret = ctypes.POINTER(ctypes.c_void_p)() _safe_call( _LIB.LGBM_DatasetGetField( self._handle, _c_str(field_name), ctypes.byref(tmp_out_len), ctypes.byref(ret), ctypes.byref(out_type), ) ) if out_type.value != _FIELD_TYPE_MAPPER[field_name]: raise TypeError(\"Return type error for get_field\") if tmp_out_len.value == 0: return None if out_type.value == _C_API_DTYPE_INT32: arr = _cint32_array_to_numpy( cptr=ctypes.cast(ret, ctypes.POINTER(ctypes.c_int32)), length=tmp_out_len.value, ) elif out_type.value == _C_API_DTYPE_FLOAT32: arr = _cfloat32_array_to_numpy( cptr=ctypes.cast(ret, ctypes.POINTER(ctypes.c_float)), length=tmp_out_len.value, ) elif out_type.value == _C_API_DTYPE_FLOAT64: arr = _cfloat64_array_to_numpy( cptr=ctypes.cast(ret, ctypes.POINTER(ctypes.c_double)), length=tmp_out_len.value, ) else: raise TypeError(\"Unknown type\") if field_name == \"init_score\": num_data = self.num_data() num_classes = arr.size // num_data if num_classes > 1:", "prev_chunk_id": "chunk_320", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_322", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "arr = arr.reshape((num_data, num_classes), order=\"F\") return arr [docs] def set_categorical_feature( self, categorical_feature: _LGBM_CategoricalFeatureConfiguration, ) -> \"Dataset\": \"\"\"Set categorical features. Parameters ---------- categorical_feature : list of str or int, or 'auto' Names or indices of categorical features. Returns ------- self : Dataset Dataset with set categorical features. \"\"\" if self.categorical_feature == categorical_feature: return self if self.data is not None: if self.categorical_feature is None: self.categorical_feature = categorical_feature return self._free_handle() elif categorical_feature == \"auto\": return self else: if self.categorical_feature != \"auto\": _log_warning( \"categorical_feature in Dataset is overridden.\\n\" f\"New categorical_feature is {list(categorical_feature)}\" ) self.categorical_feature = categorical_feature return self._free_handle() else: raise LightGBMError( \"Cannot set categorical feature after freed raw data, \" \"set free_raw_data=False when construct Dataset to avoid this.\" ) def _set_predictor( self, predictor: Optional[_InnerPredictor], ) -> \"Dataset\": \"\"\"Set predictor for continued training. It is not recommended for user to call this function. Please use init_model argument in engine.train() or engine.cv() instead. \"\"\" if predictor is None and self._predictor is None: return self elif isinstance(predictor, _InnerPredictor) and isinstance(self._predictor, _InnerPredictor): if (predictor == self._predictor) and ( predictor.current_iteration() == self._predictor.current_iteration() ): return self if self._handle is None: self._predictor = predictor elif self.data is not None: self._predictor = predictor self._set_init_score_by_predictor( predictor=self._predictor, data=self.data, used_indices=None, ) elif self.used_indices is not None and self.reference is not None and self.reference.data is not None: self._predictor = predictor self._set_init_score_by_predictor( predictor=self._predictor, data=self.reference.data, used_indices=self.used_indices, ) else: raise LightGBMError( \"Cannot set predictor after freed raw data, \" \"set free_raw_data=False when construct Dataset to avoid this.\" ) return self [docs] def set_reference(self, reference: \"Dataset\") -> \"Dataset\": \"\"\"Set reference Dataset. Parameters ---------- reference : Dataset Reference that is used as a template to construct the current Dataset. Returns ------- self : Dataset Dataset with set reference. \"\"\" self.set_categorical_feature(reference.categorical_feature).set_feature_name( reference.feature_name )._set_predictor(reference._predictor) # we're done if self and reference share a common upstream reference if self.get_ref_chain().intersection(reference.get_ref_chain()): return self if self.data is", "prev_chunk_id": "chunk_321", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_323", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "not None: self.reference = reference return self._free_handle() else: raise LightGBMError( \"Cannot set reference after freed raw data, \" \"set free_raw_data=False when construct Dataset to avoid this.\" ) [docs] def set_feature_name(self, feature_name: _LGBM_FeatureNameConfiguration) -> \"Dataset\": \"\"\"Set feature name. Parameters ---------- feature_name : list of str Feature names. Returns ------- self : Dataset Dataset with set feature name. \"\"\" if feature_name != \"auto\": self.feature_name = feature_name if self._handle is not None and feature_name is not None and feature_name != \"auto\": if len(feature_name) != self.num_feature(): raise ValueError( f\"Length of feature_name({len(feature_name)}) and num_feature({self.num_feature()}) don't match\" ) c_feature_name = [_c_str(name) for name in feature_name] _safe_call( _LIB.LGBM_DatasetSetFeatureNames( self._handle, _c_array(ctypes.c_char_p, c_feature_name), ctypes.c_int(len(feature_name)), ) ) return self [docs] def set_label(self, label: Optional[_LGBM_LabelType]) -> \"Dataset\": \"\"\"Set label of Dataset. Parameters ---------- label : list, numpy 1-D array, pandas Series / one-column DataFrame, pyarrow Array, pyarrow ChunkedArray or None The label information to be set into Dataset. Returns ------- self : Dataset Dataset with set label. \"\"\" self.label = label if self._handle is not None: if isinstance(label, pd_DataFrame): if len(label.columns) > 1: raise ValueError(\"DataFrame for label cannot have multiple columns\") label_array = np.ravel(_pandas_to_numpy(label, target_dtype=np.float32)) elif _is_pyarrow_array(label): label_array = label else: label_array = _list_to_1d_numpy(label, dtype=np.float32, name=\"label\") self.set_field(\"label\", label_array) self.label = self.get_field(\"label\") # original values can be modified at cpp side return self [docs] def set_weight( self, weight: Optional[_LGBM_WeightType], ) -> \"Dataset\": \"\"\"Set weight of each instance. Parameters ---------- weight : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None Weight to be set for each data point. Weights should be non-negative. Returns ------- self : Dataset Dataset with set weight. \"\"\" # Check if the weight contains values other than one if weight is not None: if _is_pyarrow_array(weight): if pa_compute.all(pa_compute.equal(weight, 1)).as_py(): weight = None elif np.all(weight == 1): weight = None self.weight = weight # Set field if", "prev_chunk_id": "chunk_322", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_324", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "self._handle is not None and weight is not None: if not _is_pyarrow_array(weight): weight = _list_to_1d_numpy(weight, dtype=np.float32, name=\"weight\") self.set_field(\"weight\", weight) self.weight = self.get_field(\"weight\") # original values can be modified at cpp side return self [docs] def set_init_score( self, init_score: Optional[_LGBM_InitScoreType], ) -> \"Dataset\": \"\"\"Set init score of Booster to start from. Parameters ---------- init_score : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None Init score for Booster. Returns ------- self : Dataset Dataset with set init score. \"\"\" self.init_score = init_score if self._handle is not None and init_score is not None: self.set_field(\"init_score\", init_score) self.init_score = self.get_field(\"init_score\") # original values can be modified at cpp side return self [docs] def set_group( self, group: Optional[_LGBM_GroupType], ) -> \"Dataset\": \"\"\"Set group size of Dataset (used for ranking). Parameters ---------- group : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. Returns ------- self : Dataset Dataset with set group. \"\"\" self.group = group if self._handle is not None and group is not None: if not _is_pyarrow_array(group): group = _list_to_1d_numpy(group, dtype=np.int32, name=\"group\") self.set_field(\"group\", group) # original values can be modified at cpp side constructed_group = self.get_field(\"group\") if constructed_group is not None: self.group = np.diff(constructed_group) return self [docs] def set_position( self, position: Optional[_LGBM_PositionType], ) -> \"Dataset\": \"\"\"Set position of Dataset (used for ranking). Parameters ---------- position : numpy 1-D array, pandas Series or None,", "prev_chunk_id": "chunk_323", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_325", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "optional (default=None) Position of items used in unbiased learning-to-rank task. Returns ------- self : Dataset Dataset with set position. \"\"\" self.position = position if self._handle is not None and position is not None: position = _list_to_1d_numpy(position, dtype=np.int32, name=\"position\") self.set_field(\"position\", position) return self [docs] def get_feature_name(self) -> List[str]: \"\"\"Get the names of columns (features) in the Dataset. Returns ------- feature_names : list of str The names of columns (features) in the Dataset. \"\"\" if self._handle is None: raise LightGBMError(\"Cannot get feature_name before construct dataset\") num_feature = self.num_feature() tmp_out_len = ctypes.c_int(0) reserved_string_buffer_size = 255 required_string_buffer_size = ctypes.c_size_t(0) string_buffers = [ctypes.create_string_buffer(reserved_string_buffer_size) for _ in range(num_feature)] ptr_string_buffers = (ctypes.c_char_p * num_feature)(*map(ctypes.addressof, string_buffers)) # type: ignore[misc] _safe_call( _LIB.LGBM_DatasetGetFeatureNames( self._handle, ctypes.c_int(num_feature), ctypes.byref(tmp_out_len), ctypes.c_size_t(reserved_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) if num_feature != tmp_out_len.value: raise ValueError(\"Length of feature names doesn't equal with num_feature\") actual_string_buffer_size = required_string_buffer_size.value # if buffer length is not long enough, reallocate buffers if reserved_string_buffer_size < actual_string_buffer_size: string_buffers = [ctypes.create_string_buffer(actual_string_buffer_size) for _ in range(num_feature)] ptr_string_buffers = (ctypes.c_char_p * num_feature)(*map(ctypes.addressof, string_buffers)) # type: ignore[misc] _safe_call( _LIB.LGBM_DatasetGetFeatureNames( self._handle, ctypes.c_int(num_feature), ctypes.byref(tmp_out_len), ctypes.c_size_t(actual_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) return [string_buffers[i].value.decode(\"utf-8\") for i in range(num_feature)] [docs] def get_label(self) -> Optional[_LGBM_LabelType]: \"\"\"Get the label of the Dataset. Returns ------- label : list, numpy 1-D array, pandas Series / one-column DataFrame, pyarrow Array, pyarrow ChunkedArray or None The label information from the Dataset. For a constructed ``Dataset``, this will only return a numpy array. \"\"\" if self.label is None: self.label = self.get_field(\"label\") return self.label [docs] def get_weight(self) -> Optional[_LGBM_WeightType]: \"\"\"Get the weight of the Dataset. Returns ------- weight : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None Weight for each data point from the Dataset. Weights should be non-negative. For a constructed ``Dataset``, this will only return ``None`` or a numpy array. \"\"\" if self.weight is None: self.weight = self.get_field(\"weight\")", "prev_chunk_id": "chunk_324", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_326", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "return self.weight [docs] def get_init_score(self) -> Optional[_LGBM_InitScoreType]: \"\"\"Get the initial score of the Dataset. Returns ------- init_score : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None Init score of Booster. For a constructed ``Dataset``, this will only return ``None`` or a numpy array. \"\"\" if self.init_score is None: self.init_score = self.get_field(\"init_score\") return self.init_score [docs] def get_data(self) -> Optional[_LGBM_TrainDataType]: \"\"\"Get the raw data of the Dataset. Returns ------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse, Sequence, list of Sequence, list of numpy array, pyarrow Table or None Raw data used in the Dataset construction. \"\"\" if self._handle is None: raise Exception(\"Cannot get data before construct Dataset\") if self._need_slice and self.used_indices is not None and self.reference is not None: self.data = self.reference.data if self.data is not None: if isinstance(self.data, (np.ndarray, scipy.sparse.spmatrix)): self.data = self.data[self.used_indices, :] elif isinstance(self.data, pd_DataFrame): self.data = self.data.iloc[self.used_indices].copy() elif isinstance(self.data, Sequence): self.data = self.data[self.used_indices] elif _is_list_of_sequences(self.data) and len(self.data) > 0: self.data = np.array(list(self._yield_row_from_seqlist(self.data, self.used_indices))) else: _log_warning( f\"Cannot subset {type(self.data).__name__} type of raw data.\\nReturning original raw data\" ) self._need_slice = False if self.data is None: raise LightGBMError( \"Cannot call `get_data` after freed raw data, \" \"set free_raw_data=False when construct Dataset to avoid this.\" ) return self.data [docs] def get_group(self) -> Optional[_LGBM_GroupType]: \"\"\"Get the group of the Dataset. Returns ------- group : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in", "prev_chunk_id": "chunk_325", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_327", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "the third group, etc. For a constructed ``Dataset``, this will only return ``None`` or a numpy array. \"\"\" if self.group is None: self.group = self.get_field(\"group\") if self.group is not None: # group data from LightGBM is boundaries data, need to convert to group size self.group = np.diff(self.group) return self.group [docs] def get_position(self) -> Optional[_LGBM_PositionType]: \"\"\"Get the position of the Dataset. Returns ------- position : numpy 1-D array, pandas Series or None Position of items used in unbiased learning-to-rank task. For a constructed ``Dataset``, this will only return ``None`` or a numpy array. \"\"\" if self.position is None: self.position = self.get_field(\"position\") return self.position [docs] def num_data(self) -> int: \"\"\"Get the number of rows in the Dataset. Returns ------- number_of_rows : int The number of rows in the Dataset. \"\"\" if self._handle is not None: ret = ctypes.c_int(0) _safe_call( _LIB.LGBM_DatasetGetNumData( self._handle, ctypes.byref(ret), ) ) return ret.value else: raise LightGBMError(\"Cannot get num_data before construct dataset\") [docs] def num_feature(self) -> int: \"\"\"Get the number of columns (features) in the Dataset. Returns ------- number_of_columns : int The number of columns (features) in the Dataset. \"\"\" if self._handle is not None: ret = ctypes.c_int(0) _safe_call( _LIB.LGBM_DatasetGetNumFeature( self._handle, ctypes.byref(ret), ) ) return ret.value else: raise LightGBMError(\"Cannot get num_feature before construct dataset\") [docs] def feature_num_bin(self, feature: Union[int, str]) -> int: \"\"\"Get the number of bins for a feature. .. versionadded:: 4.0.0 Parameters ---------- feature : int or str Index or name of the feature. Returns ------- number_of_bins : int The number of constructed bins for the feature in the Dataset. \"\"\" if self._handle is not None: if isinstance(feature, str): feature_index = self.feature_name.index(feature) else: feature_index = feature ret = ctypes.c_int(0) _safe_call( _LIB.LGBM_DatasetGetFeatureNumBin( self._handle, ctypes.c_int(feature_index), ctypes.byref(ret), ) ) return ret.value else: raise LightGBMError(\"Cannot get feature_num_bin before construct dataset\") [docs] def get_ref_chain(self, ref_limit: int = 100) -> Set[\"Dataset\"]: \"\"\"Get a chain", "prev_chunk_id": "chunk_326", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_328", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "of Dataset objects. Starts with r, then goes to r.reference (if exists), then to r.reference.reference, etc. until we hit ``ref_limit`` or a reference loop. Parameters ---------- ref_limit : int, optional (default=100) The limit number of references. Returns ------- ref_chain : set of Dataset Chain of references of the Datasets. \"\"\" head = self ref_chain: Set[Dataset] = set() while len(ref_chain) < ref_limit: if isinstance(head, Dataset): ref_chain.add(head) if (head.reference is not None) and (head.reference not in ref_chain): head = head.reference else: break else: break return ref_chain [docs] def add_features_from(self, other: \"Dataset\") -> \"Dataset\": \"\"\"Add features from other Dataset to the current Dataset. Both Datasets must be constructed before calling this method. Parameters ---------- other : Dataset The Dataset to take features from. Returns ------- self : Dataset Dataset with the new features added. \"\"\" if self._handle is None or other._handle is None: raise ValueError(\"Both source and target Datasets must be constructed before adding features\") _safe_call( _LIB.LGBM_DatasetAddFeaturesFrom( self._handle, other._handle, ) ) was_none = self.data is None old_self_data_type = type(self.data).__name__ if other.data is None: self.data = None elif self.data is not None: if isinstance(self.data, np.ndarray): if isinstance(other.data, np.ndarray): self.data = np.hstack((self.data, other.data)) elif isinstance(other.data, scipy.sparse.spmatrix): self.data = np.hstack((self.data, other.data.toarray())) elif isinstance(other.data, pd_DataFrame): self.data = np.hstack((self.data, other.data.values)) else: self.data = None elif isinstance(self.data, scipy.sparse.spmatrix): sparse_format = self.data.getformat() if isinstance(other.data, (np.ndarray, scipy.sparse.spmatrix)): self.data = scipy.sparse.hstack((self.data, other.data), format=sparse_format) elif isinstance(other.data, pd_DataFrame): self.data = scipy.sparse.hstack((self.data, other.data.values), format=sparse_format) else: self.data = None elif isinstance(self.data, pd_DataFrame): if not PANDAS_INSTALLED: raise LightGBMError( \"Cannot add features to DataFrame type of raw data \" \"without pandas installed. \" \"Install pandas and restart your session.\" ) if isinstance(other.data, np.ndarray): self.data = concat((self.data, pd_DataFrame(other.data)), axis=1, ignore_index=True) elif isinstance(other.data, scipy.sparse.spmatrix): self.data = concat((self.data, pd_DataFrame(other.data.toarray())), axis=1, ignore_index=True) elif isinstance(other.data, pd_DataFrame): self.data = concat((self.data, other.data), axis=1, ignore_index=True) else: self.data = None else: self.data = None if self.data is", "prev_chunk_id": "chunk_327", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_329", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "None: err_msg = ( f\"Cannot add features from {type(other.data).__name__} type of raw data to \" f\"{old_self_data_type} type of raw data.\\n\" ) err_msg += ( \"Set free_raw_data=False when construct Dataset to avoid this\" if was_none else \"Freeing raw data\" ) _log_warning(err_msg) self.feature_name = self.get_feature_name() _log_warning( \"Resetting categorical features.\\n\" \"You can set new categorical features via ``set_categorical_feature`` method\" ) self.categorical_feature = \"auto\" self.pandas_categorical = None return self def _dump_text(self, filename: Union[str, Path]) -> \"Dataset\": \"\"\"Save Dataset to a text file. This format cannot be loaded back in by LightGBM, but is useful for debugging purposes. Parameters ---------- filename : str or pathlib.Path Name of the output file. Returns ------- self : Dataset Returns self. \"\"\" _safe_call( _LIB.LGBM_DatasetDumpText( self.construct()._handle, _c_str(str(filename)), ) ) return self _LGBM_CustomObjectiveFunction = Callable[ [np.ndarray, Dataset], Tuple[np.ndarray, np.ndarray], ] _LGBM_CustomEvalFunction = Union[ Callable[ [np.ndarray, Dataset], _LGBM_EvalFunctionResultType, ], Callable[ [np.ndarray, Dataset], List[_LGBM_EvalFunctionResultType], ], ] [docs] class Booster: \"\"\"Booster in LightGBM.\"\"\" [docs] def __init__( self, params: Optional[Dict[str, Any]] = None, train_set: Optional[Dataset] = None, model_file: Optional[Union[str, Path]] = None, model_str: Optional[str] = None, ): \"\"\"Initialize the Booster. Parameters ---------- params : dict or None, optional (default=None) Parameters for Booster. train_set : Dataset or None, optional (default=None) Training dataset. model_file : str, pathlib.Path or None, optional (default=None) Path to the model file. model_str : str or None, optional (default=None) Model will be loaded from this string. \"\"\" self._handle = ctypes.c_void_p() self._network = False self.__need_reload_eval_info = True self._train_data_name = \"training\" self.__set_objective_to_none = False self.best_iteration = -1 self.best_score: _LGBM_BoosterBestScoreType = {} params = {} if params is None else deepcopy(params) if train_set is not None: # Training task if not isinstance(train_set, Dataset): raise TypeError(f\"Training data should be Dataset instance, met {type(train_set).__name__}\") params = _choose_param_value( main_param_name=\"machines\", params=params, default_value=None, ) # if \"machines\" is given, assume user wants to do distributed learning, and set up network if", "prev_chunk_id": "chunk_328", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_330", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "params[\"machines\"] is None: params.pop(\"machines\", None) else: machines = params[\"machines\"] if isinstance(machines, str): num_machines_from_machine_list = len(machines.split(\",\")) elif isinstance(machines, (list, set)): num_machines_from_machine_list = len(machines) machines = \",\".join(machines) else: raise ValueError(\"Invalid machines in params.\") params = _choose_param_value( main_param_name=\"num_machines\", params=params, default_value=num_machines_from_machine_list, ) params = _choose_param_value( main_param_name=\"local_listen_port\", params=params, default_value=12400, ) self.set_network( machines=machines, local_listen_port=params[\"local_listen_port\"], listen_time_out=params.get(\"time_out\", 120), num_machines=params[\"num_machines\"], ) # construct booster object train_set.construct() # copy the parameters from train_set params.update(train_set.get_params()) params_str = _param_dict_to_str(params) _safe_call( _LIB.LGBM_BoosterCreate( train_set._handle, _c_str(params_str), ctypes.byref(self._handle), ) ) # save reference to data self.train_set = train_set self.valid_sets: List[Dataset] = [] self.name_valid_sets: List[str] = [] self.__num_dataset = 1 self.__init_predictor = train_set._predictor if self.__init_predictor is not None: _safe_call( _LIB.LGBM_BoosterMerge( self._handle, self.__init_predictor._handle, ) ) out_num_class = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetNumClasses( self._handle, ctypes.byref(out_num_class), ) ) self.__num_class = out_num_class.value # buffer for inner predict self.__inner_predict_buffer: List[Optional[np.ndarray]] = [None] self.__is_predicted_cur_iter = [False] self.__get_eval_info() self.pandas_categorical = train_set.pandas_categorical self.train_set_version = train_set.version elif model_file is not None: # Prediction task out_num_iterations = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterCreateFromModelfile( _c_str(str(model_file)), ctypes.byref(out_num_iterations), ctypes.byref(self._handle), ) ) out_num_class = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetNumClasses( self._handle, ctypes.byref(out_num_class), ) ) self.__num_class = out_num_class.value self.pandas_categorical = _load_pandas_categorical(file_name=model_file) if params: _log_warning(\"Ignoring params argument, using parameters from model file.\") params = self._get_loaded_param() elif model_str is not None: self.model_from_string(model_str) if params: _log_warning(\"Ignoring params argument, using parameters from model string.\") params = self._get_loaded_param() else: raise TypeError( \"Need at least one training dataset or model file or model string to create Booster instance\" ) self.params = params def __del__(self) -> None: try: if self._network: self.free_network() except AttributeError: pass try: if self._handle is not None: _safe_call(_LIB.LGBM_BoosterFree(self._handle)) except AttributeError: pass def __copy__(self) -> \"Booster\": return self.__deepcopy__(None) def __deepcopy__(self, *args: Any, **kwargs: Any) -> \"Booster\": model_str = self.model_to_string(num_iteration=-1) return Booster(model_str=model_str) def __getstate__(self) -> Dict[str, Any]: this = self.__dict__.copy() handle = this[\"_handle\"] this.pop(\"train_set\", None) this.pop(\"valid_sets\", None) if handle is not None: this[\"_handle\"] = self.model_to_string(num_iteration=-1) return this def __setstate__(self, state: Dict[str, Any]) -> None: model_str =", "prev_chunk_id": "chunk_329", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_331", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "state.get(\"_handle\", state.get(\"handle\", None)) if model_str is not None: handle = ctypes.c_void_p() out_num_iterations = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterLoadModelFromString( _c_str(model_str), ctypes.byref(out_num_iterations), ctypes.byref(handle), ) ) state[\"_handle\"] = handle self.__dict__.update(state) def _get_loaded_param(self) -> Dict[str, Any]: buffer_len = 1 << 20 tmp_out_len = ctypes.c_int64(0) string_buffer = ctypes.create_string_buffer(buffer_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterGetLoadedParam( self._handle, ctypes.c_int64(buffer_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) actual_len = tmp_out_len.value # if buffer length is not long enough, re-allocate a buffer if actual_len > buffer_len: string_buffer = ctypes.create_string_buffer(actual_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterGetLoadedParam( self._handle, ctypes.c_int64(actual_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) return json.loads(string_buffer.value.decode(\"utf-8\")) [docs] def free_dataset(self) -> \"Booster\": \"\"\"Free Booster's Datasets. Returns ------- self : Booster Booster without Datasets. \"\"\" self.__dict__.pop(\"train_set\", None) self.__dict__.pop(\"valid_sets\", None) self.__num_dataset = 0 return self def _free_buffer(self) -> \"Booster\": self.__inner_predict_buffer = [] self.__is_predicted_cur_iter = [] return self [docs] def set_network( self, machines: Union[List[str], Set[str], str], local_listen_port: int = 12400, listen_time_out: int = 120, num_machines: int = 1, ) -> \"Booster\": \"\"\"Set the network configuration. Parameters ---------- machines : list, set or str Names of machines. local_listen_port : int, optional (default=12400) TCP listen port for local machines. listen_time_out : int, optional (default=120) Socket time-out in minutes. num_machines : int, optional (default=1) The number of machines for distributed learning application. Returns ------- self : Booster Booster with set network. \"\"\" if isinstance(machines, (list, set)): machines = \",\".join(machines) _safe_call( _LIB.LGBM_NetworkInit( _c_str(machines), ctypes.c_int(local_listen_port), ctypes.c_int(listen_time_out), ctypes.c_int(num_machines), ) ) self._network = True return self [docs] def free_network(self) -> \"Booster\": \"\"\"Free Booster's network. Returns ------- self : Booster Booster with freed network. \"\"\" _safe_call(_LIB.LGBM_NetworkFree()) self._network = False return self [docs] def trees_to_dataframe(self) -> pd_DataFrame: \"\"\"Parse the fitted model and return in an easy-to-read pandas DataFrame. The returned DataFrame has the following columns. - ``tree_index`` : int64, which tree a node belongs to. 0-based, so a value of ``6``, for example, means \"this node is in the 7th tree\". - ``node_depth``", "prev_chunk_id": "chunk_330", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_332", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": ": int64, how far a node is from the root of the tree. The root node has a value of ``1``, its direct children are ``2``, etc. - ``node_index`` : str, unique identifier for a node. - ``left_child`` : str, ``node_index`` of the child node to the left of a split. ``None`` for leaf nodes. - ``right_child`` : str, ``node_index`` of the child node to the right of a split. ``None`` for leaf nodes. - ``parent_index`` : str, ``node_index`` of this node's parent. ``None`` for the root node. - ``split_feature`` : str, name of the feature used for splitting. ``None`` for leaf nodes. - ``split_gain`` : float64, gain from adding this split to the tree. ``NaN`` for leaf nodes. - ``threshold`` : float64, value of the feature used to decide which side of the split a record will go down. ``NaN`` for leaf nodes. - ``decision_type`` : str, logical operator describing how to compare a value to ``threshold``. For example, ``split_feature = \"Column_10\", threshold = 15, decision_type = \"<=\"`` means that records where ``Column_10 <= 15`` follow the left side of the split, otherwise follows the right side of the split. ``None`` for leaf nodes. - ``missing_direction`` : str, split direction that missing values should go to. ``None`` for leaf nodes. - ``missing_type`` : str, describes what types of values are treated as missing. - ``value`` : float64, predicted value for this leaf node, multiplied by the learning rate. - ``weight`` : float64 or int64, sum of Hessian (second-order derivative of objective), summed over observations that fall in this node. - ``count`` : int64, number of records in the training data that fall into this node. Returns ------- result : pandas DataFrame Returns a pandas DataFrame of the parsed model. \"\"\" if not PANDAS_INSTALLED: raise LightGBMError( \"This method cannot be", "prev_chunk_id": "chunk_331", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_333", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "run without pandas installed. \" \"You must install pandas and restart your session to use this method.\" ) if self.num_trees() == 0: raise LightGBMError(\"There are no trees in this Booster and thus nothing to parse\") def _is_split_node(tree: Dict[str, Any]) -> bool: return \"split_index\" in tree.keys() def create_node_record( tree: Dict[str, Any], node_depth: int = 1, tree_index: Optional[int] = None, feature_names: Optional[List[str]] = None, parent_node: Optional[str] = None, ) -> Dict[str, Any]: def _get_node_index( tree: Dict[str, Any], tree_index: Optional[int], ) -> str: tree_num = f\"{tree_index}-\" if tree_index is not None else \"\" is_split = _is_split_node(tree) node_type = \"S\" if is_split else \"L\" # if a single node tree it won't have `leaf_index` so return 0 node_num = tree.get(\"split_index\" if is_split else \"leaf_index\", 0) return f\"{tree_num}{node_type}{node_num}\" def _get_split_feature( tree: Dict[str, Any], feature_names: Optional[List[str]], ) -> Optional[str]: if _is_split_node(tree): if feature_names is not None: feature_name = feature_names[tree[\"split_feature\"]] else: feature_name = tree[\"split_feature\"] else: feature_name = None return feature_name def _is_single_node_tree(tree: Dict[str, Any]) -> bool: return set(tree.keys()) == {\"leaf_value\", \"leaf_count\"} # Create the node record, and populate universal data members node: Dict[str, Union[int, str, None]] = OrderedDict() node[\"tree_index\"] = tree_index node[\"node_depth\"] = node_depth node[\"node_index\"] = _get_node_index(tree, tree_index) node[\"left_child\"] = None node[\"right_child\"] = None node[\"parent_index\"] = parent_node node[\"split_feature\"] = _get_split_feature(tree, feature_names) node[\"split_gain\"] = None node[\"threshold\"] = None node[\"decision_type\"] = None node[\"missing_direction\"] = None node[\"missing_type\"] = None node[\"value\"] = None node[\"weight\"] = None node[\"count\"] = None # Update values to reflect node type (leaf or split) if _is_split_node(tree): node[\"left_child\"] = _get_node_index(tree[\"left_child\"], tree_index) node[\"right_child\"] = _get_node_index(tree[\"right_child\"], tree_index) node[\"split_gain\"] = tree[\"split_gain\"] node[\"threshold\"] = tree[\"threshold\"] node[\"decision_type\"] = tree[\"decision_type\"] node[\"missing_direction\"] = \"left\" if tree[\"default_left\"] else \"right\" node[\"missing_type\"] = tree[\"missing_type\"] node[\"value\"] = tree[\"internal_value\"] node[\"weight\"] = tree[\"internal_weight\"] node[\"count\"] = tree[\"internal_count\"] else: node[\"value\"] = tree[\"leaf_value\"] if not _is_single_node_tree(tree): node[\"weight\"] = tree[\"leaf_weight\"] node[\"count\"] = tree[\"leaf_count\"] return node def tree_dict_to_node_list( tree: Dict[str, Any], node_depth: int = 1,", "prev_chunk_id": "chunk_332", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_334", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "tree_index: Optional[int] = None, feature_names: Optional[List[str]] = None, parent_node: Optional[str] = None, ) -> List[Dict[str, Any]]: node = create_node_record( tree=tree, node_depth=node_depth, tree_index=tree_index, feature_names=feature_names, parent_node=parent_node, ) res = [node] if _is_split_node(tree): # traverse the next level of the tree children = [\"left_child\", \"right_child\"] for child in children: subtree_list = tree_dict_to_node_list( tree=tree[child], node_depth=node_depth + 1, tree_index=tree_index, feature_names=feature_names, parent_node=node[\"node_index\"], ) # In tree format, \"subtree_list\" is a list of node records (dicts), # and we add node to the list. res.extend(subtree_list) return res model_dict = self.dump_model() feature_names = model_dict[\"feature_names\"] model_list = [] for tree in model_dict[\"tree_info\"]: model_list.extend( tree_dict_to_node_list( tree=tree[\"tree_structure\"], tree_index=tree[\"tree_index\"], feature_names=feature_names ) ) return pd_DataFrame(model_list, columns=model_list[0].keys()) [docs] def set_train_data_name(self, name: str) -> \"Booster\": \"\"\"Set the name to the training Dataset. Parameters ---------- name : str Name for the training Dataset. Returns ------- self : Booster Booster with set training Dataset name. \"\"\" self._train_data_name = name return self [docs] def add_valid(self, data: Dataset, name: str) -> \"Booster\": \"\"\"Add validation data. Parameters ---------- data : Dataset Validation data. name : str Name of validation data. Returns ------- self : Booster Booster with set validation data. \"\"\" if not isinstance(data, Dataset): raise TypeError(f\"Validation data should be Dataset instance, met {type(data).__name__}\") if data._predictor is not self.__init_predictor: raise LightGBMError(\"Add validation data failed, you should use same predictor for these data\") _safe_call( _LIB.LGBM_BoosterAddValidData( self._handle, data.construct()._handle, ) ) self.valid_sets.append(data) self.name_valid_sets.append(name) self.__num_dataset += 1 self.__inner_predict_buffer.append(None) self.__is_predicted_cur_iter.append(False) return self [docs] def reset_parameter(self, params: Dict[str, Any]) -> \"Booster\": \"\"\"Reset parameters of Booster. Parameters ---------- params : dict New parameters for Booster. Returns ------- self : Booster Booster with new parameters. \"\"\" params_str = _param_dict_to_str(params) if params_str: _safe_call( _LIB.LGBM_BoosterResetParameter( self._handle, _c_str(params_str), ) ) self.params.update(params) return self [docs] def update( self, train_set: Optional[Dataset] = None, fobj: Optional[_LGBM_CustomObjectiveFunction] = None, ) -> bool: \"\"\"Update Booster for one iteration. Parameters ---------- train_set : Dataset or None, optional", "prev_chunk_id": "chunk_333", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_335", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "(default=None) Training data. If None, last training data is used. fobj : callable or None, optional (default=None) Customized objective function. Should accept two parameters: preds, train_data, and return (grad, hess). preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. Predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task. train_data : Dataset The training dataset. grad : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of preds for each sample point. hess : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of preds for each sample point. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. Returns ------- produced_empty_tree : bool ``True`` if the tree(s) produced by this iteration did not have any splits. This usually means that training is \"finished\" (calling ``update()`` again will not change the model's predictions). However, that is not always the case. For example, if you have added any randomness (like column sampling by setting ``feature_fraction_bynode < 1.0``), it is possible that another call to ``update()`` would produce a non-empty tree. \"\"\" # need reset training data if train_set is None and self.train_set_version != self.train_set.version: train_set = self.train_set is_the_same_train_set = False else: is_the_same_train_set = train_set is self.train_set and self.train_set_version == train_set.version if train_set is not None and not is_the_same_train_set: if not isinstance(train_set, Dataset): raise TypeError(f\"Training data should be Dataset instance, met {type(train_set).__name__}\") if train_set._predictor is not self.__init_predictor: raise LightGBMError(\"Replace training data failed, you should use same predictor", "prev_chunk_id": "chunk_334", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_336", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "for these data\") self.train_set = train_set _safe_call( _LIB.LGBM_BoosterResetTrainingData( self._handle, self.train_set.construct()._handle, ) ) self.__inner_predict_buffer[0] = None self.train_set_version = self.train_set.version produced_empty_tree = ctypes.c_int(0) if fobj is None: if self.__set_objective_to_none: raise LightGBMError(\"Cannot update due to null objective function.\") _safe_call( _LIB.LGBM_BoosterUpdateOneIter( self._handle, ctypes.byref(produced_empty_tree), ) ) self.__is_predicted_cur_iter = [False for _ in range(self.__num_dataset)] return produced_empty_tree.value == 1 else: if not self.__set_objective_to_none: self.reset_parameter({\"objective\": \"none\"}).__set_objective_to_none = True grad, hess = fobj(self.__inner_predict(0), self.train_set) return self.__boost(grad, hess) def __boost( self, grad: np.ndarray, hess: np.ndarray, ) -> bool: \"\"\"Boost Booster for one iteration with customized gradient statistics. .. note:: Score is returned before any transformation, e.g. it is raw margin instead of probability of positive class for binary task. For multi-class task, score are numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. Parameters ---------- grad : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of score for each sample point. hess : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of score for each sample point. Returns ------- produced_empty_tree : bool ``True`` if the tree(s) produced by this iteration did not have any splits. This usually means that training is \"finished\" (calling ``__boost()`` again will not change the model's predictions). However, that is not always the case. For example, if you have added any randomness (like column sampling by setting ``feature_fraction_bynode < 1.0``), it is possible that another call to ``__boost()`` would produce a non-empty tree. \"\"\" if self.__num_class > 1: grad = grad.ravel(order=\"F\") hess = hess.ravel(order=\"F\") grad = _list_to_1d_numpy(grad, dtype=np.float32, name=\"gradient\") hess = _list_to_1d_numpy(hess, dtype=np.float32, name=\"hessian\") assert grad.flags.c_contiguous assert hess.flags.c_contiguous if len(grad)", "prev_chunk_id": "chunk_335", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_337", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "!= len(hess): raise ValueError(f\"Lengths of gradient ({len(grad)}) and Hessian ({len(hess)}) don't match\") num_train_data = self.train_set.num_data() if len(grad) != num_train_data * self.__num_class: raise ValueError( f\"Lengths of gradient ({len(grad)}) and Hessian ({len(hess)}) \" f\"don't match training data length ({num_train_data}) * \" f\"number of models per one iteration ({self.__num_class})\" ) produced_empty_tree = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterUpdateOneIterCustom( self._handle, grad.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), hess.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), ctypes.byref(produced_empty_tree), ) ) self.__is_predicted_cur_iter = [False for _ in range(self.__num_dataset)] return produced_empty_tree.value == 1 [docs] def rollback_one_iter(self) -> \"Booster\": \"\"\"Rollback one iteration. Returns ------- self : Booster Booster with rolled back one iteration. \"\"\" _safe_call(_LIB.LGBM_BoosterRollbackOneIter(self._handle)) self.__is_predicted_cur_iter = [False for _ in range(self.__num_dataset)] return self [docs] def current_iteration(self) -> int: \"\"\"Get the index of the current iteration. Returns ------- cur_iter : int The index of the current iteration. \"\"\" out_cur_iter = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetCurrentIteration( self._handle, ctypes.byref(out_cur_iter), ) ) return out_cur_iter.value [docs] def num_model_per_iteration(self) -> int: \"\"\"Get number of models per iteration. Returns ------- model_per_iter : int The number of models per iteration. \"\"\" model_per_iter = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterNumModelPerIteration( self._handle, ctypes.byref(model_per_iter), ) ) return model_per_iter.value [docs] def num_trees(self) -> int: \"\"\"Get number of weak sub-models. Returns ------- num_trees : int The number of weak sub-models. \"\"\" num_trees = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterNumberOfTotalModel( self._handle, ctypes.byref(num_trees), ) ) return num_trees.value [docs] def upper_bound(self) -> float: \"\"\"Get upper bound value of a model. Returns ------- upper_bound : float Upper bound value of the model. \"\"\" ret = ctypes.c_double(0) _safe_call( _LIB.LGBM_BoosterGetUpperBoundValue( self._handle, ctypes.byref(ret), ) ) return ret.value [docs] def lower_bound(self) -> float: \"\"\"Get lower bound value of a model. Returns ------- lower_bound : float Lower bound value of the model. \"\"\" ret = ctypes.c_double(0) _safe_call( _LIB.LGBM_BoosterGetLowerBoundValue( self._handle, ctypes.byref(ret), ) ) return ret.value [docs] def eval( self, data: Dataset, name: str, feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] = None, ) -> List[_LGBM_BoosterEvalMethodResultType]: \"\"\"Evaluate for data. Parameters ---------- data : Dataset Data for the evaluating. name :", "prev_chunk_id": "chunk_336", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_338", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "str Name of the data. feval : callable, list of callable, or None, optional (default=None) Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples. preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes]. If custom objective function is used, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task in this case. eval_data : Dataset A ``Dataset`` to evaluate. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. Returns ------- result : list List with (dataset_name, eval_name, eval_result, is_higher_better) tuples. \"\"\" if not isinstance(data, Dataset): raise TypeError(\"Can only eval for Dataset instance\") data_idx = -1 if data is self.train_set: data_idx = 0 else: for i in range(len(self.valid_sets)): if data is self.valid_sets[i]: data_idx = i + 1 break # need to push new valid data if data_idx == -1: self.add_valid(data, name) data_idx = self.__num_dataset - 1 return self.__inner_eval(data_name=name, data_idx=data_idx, feval=feval) [docs] def eval_train( self, feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] = None, ) -> List[_LGBM_BoosterEvalMethodResultType]: \"\"\"Evaluate for training data. Parameters ---------- feval : callable, list of callable, or None, optional (default=None) Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples. preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes]. If custom objective function is used, predicted values are returned before any transformation, e.g. they are raw margin instead of probability", "prev_chunk_id": "chunk_337", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_339", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "of positive class for binary task in this case. eval_data : Dataset The training dataset. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. Returns ------- result : list List with (train_dataset_name, eval_name, eval_result, is_higher_better) tuples. \"\"\" return self.__inner_eval(data_name=self._train_data_name, data_idx=0, feval=feval) [docs] def eval_valid( self, feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] = None, ) -> List[_LGBM_BoosterEvalMethodResultType]: \"\"\"Evaluate for validation data. Parameters ---------- feval : callable, list of callable, or None, optional (default=None) Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples. preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes]. If custom objective function is used, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task in this case. eval_data : Dataset The validation dataset. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. Returns ------- result : list List with (validation_dataset_name, eval_name, eval_result, is_higher_better) tuples. \"\"\" return [ item for i in range(1, self.__num_dataset) for item in self.__inner_eval(data_name=self.name_valid_sets[i - 1], data_idx=i, feval=feval) ] [docs] def save_model( self, filename: Union[str, Path], num_iteration: Optional[int] = None, start_iteration: int = 0, importance_type: str = \"split\", ) -> \"Booster\": \"\"\"Save Booster to file. Parameters ---------- filename : str or pathlib.Path Filename to save Booster. num_iteration : int or None, optional (default=None) Index of the iteration that should be saved. If None, if the best iteration exists, it is saved; otherwise, all iterations are saved. If", "prev_chunk_id": "chunk_338", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_340", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "<= 0, all iterations are saved. start_iteration : int, optional (default=0) Start index of the iteration that should be saved. importance_type : str, optional (default=\"split\") What type of feature importance should be saved. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. Returns ------- self : Booster Returns self. \"\"\" if num_iteration is None: num_iteration = self.best_iteration importance_type_int = _FEATURE_IMPORTANCE_TYPE_MAPPER[importance_type] _safe_call( _LIB.LGBM_BoosterSaveModel( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.c_int(importance_type_int), _c_str(str(filename)), ) ) _dump_pandas_categorical(self.pandas_categorical, filename) return self [docs] def shuffle_models( self, start_iteration: int = 0, end_iteration: int = -1, ) -> \"Booster\": \"\"\"Shuffle models. Parameters ---------- start_iteration : int, optional (default=0) The first iteration that will be shuffled. end_iteration : int, optional (default=-1) The last iteration that will be shuffled. If <= 0, means the last available iteration. Returns ------- self : Booster Booster with shuffled models. \"\"\" _safe_call( _LIB.LGBM_BoosterShuffleModels( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(end_iteration), ) ) return self [docs] def model_from_string(self, model_str: str) -> \"Booster\": \"\"\"Load Booster from a string. Parameters ---------- model_str : str Model will be loaded from this string. Returns ------- self : Booster Loaded Booster object. \"\"\" # ensure that existing Booster is freed before replacing it # with a new one createdfrom file _safe_call(_LIB.LGBM_BoosterFree(self._handle)) self._free_buffer() self._handle = ctypes.c_void_p() out_num_iterations = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterLoadModelFromString( _c_str(model_str), ctypes.byref(out_num_iterations), ctypes.byref(self._handle), ) ) out_num_class = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetNumClasses( self._handle, ctypes.byref(out_num_class), ) ) self.__num_class = out_num_class.value self.pandas_categorical = _load_pandas_categorical(model_str=model_str) return self [docs] def model_to_string( self, num_iteration: Optional[int] = None, start_iteration: int = 0, importance_type: str = \"split\", ) -> str: \"\"\"Save Booster to string. Parameters ---------- num_iteration : int or None, optional (default=None) Index of the iteration that should be saved. If None, if the best iteration exists, it is saved; otherwise, all iterations are saved. If <= 0,", "prev_chunk_id": "chunk_339", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_341", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "all iterations are saved. start_iteration : int, optional (default=0) Start index of the iteration that should be saved. importance_type : str, optional (default=\"split\") What type of feature importance should be saved. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. Returns ------- str_repr : str String representation of Booster. \"\"\" if num_iteration is None: num_iteration = self.best_iteration importance_type_int = _FEATURE_IMPORTANCE_TYPE_MAPPER[importance_type] buffer_len = 1 << 20 tmp_out_len = ctypes.c_int64(0) string_buffer = ctypes.create_string_buffer(buffer_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterSaveModelToString( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.c_int(importance_type_int), ctypes.c_int64(buffer_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) actual_len = tmp_out_len.value # if buffer length is not long enough, re-allocate a buffer if actual_len > buffer_len: string_buffer = ctypes.create_string_buffer(actual_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterSaveModelToString( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.c_int(importance_type_int), ctypes.c_int64(actual_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) ret = string_buffer.value.decode(\"utf-8\") ret += _dump_pandas_categorical(self.pandas_categorical) return ret [docs] def dump_model( self, num_iteration: Optional[int] = None, start_iteration: int = 0, importance_type: str = \"split\", object_hook: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None, ) -> Dict[str, Any]: \"\"\"Dump Booster to JSON format. Parameters ---------- num_iteration : int or None, optional (default=None) Index of the iteration that should be dumped. If None, if the best iteration exists, it is dumped; otherwise, all iterations are dumped. If <= 0, all iterations are dumped. start_iteration : int, optional (default=0) Start index of the iteration that should be dumped. importance_type : str, optional (default=\"split\") What type of feature importance should be dumped. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. object_hook : callable or None, optional (default=None) If not None, ``object_hook`` is a function called while parsing the json string returned by the C API. It may be used to alter", "prev_chunk_id": "chunk_340", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_342", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "the json, to store specific values while building the json structure. It avoids walking through the structure again. It saves a significant amount of time if the number of trees is huge. Signature is ``def object_hook(node: dict) -> dict``. None is equivalent to ``lambda node: node``. See documentation of ``json.loads()`` for further details. Returns ------- json_repr : dict JSON format of Booster. \"\"\" if num_iteration is None: num_iteration = self.best_iteration importance_type_int = _FEATURE_IMPORTANCE_TYPE_MAPPER[importance_type] buffer_len = 1 << 20 tmp_out_len = ctypes.c_int64(0) string_buffer = ctypes.create_string_buffer(buffer_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterDumpModel( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.c_int(importance_type_int), ctypes.c_int64(buffer_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) actual_len = tmp_out_len.value # if buffer length is not long enough, reallocate a buffer if actual_len > buffer_len: string_buffer = ctypes.create_string_buffer(actual_len) ptr_string_buffer = ctypes.c_char_p(ctypes.addressof(string_buffer)) _safe_call( _LIB.LGBM_BoosterDumpModel( self._handle, ctypes.c_int(start_iteration), ctypes.c_int(num_iteration), ctypes.c_int(importance_type_int), ctypes.c_int64(actual_len), ctypes.byref(tmp_out_len), ptr_string_buffer, ) ) ret = json.loads(string_buffer.value.decode(\"utf-8\"), object_hook=object_hook) ret[\"pandas_categorical\"] = json.loads( json.dumps( self.pandas_categorical, default=_json_default_with_numpy, ) ) return ret [docs] def predict( self, data: _LGBM_PredictDataType, start_iteration: int = 0, num_iteration: Optional[int] = None, raw_score: bool = False, pred_leaf: bool = False, pred_contrib: bool = False, data_has_header: bool = False, validate_features: bool = False, **kwargs: Any, ) -> Union[np.ndarray, scipy.sparse.spmatrix, List[scipy.sparse.spmatrix]]: \"\"\"Make a prediction. Parameters ---------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse or pyarrow Table Data source for prediction. If str or pathlib.Path, it represents the path to a text file (CSV, TSV, or LibSVM). start_iteration : int, optional (default=0) Start index of the iteration to predict. If <= 0, starts from the first iteration. num_iteration : int or None, optional (default=None) Total number of iterations used in the prediction. If None, if the best iteration exists and start_iteration <= 0, the best iteration is used; otherwise, all iterations from ``start_iteration`` are used (no limits). If <= 0, all iterations from ``start_iteration`` are used (no limits). raw_score : bool, optional", "prev_chunk_id": "chunk_341", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_343", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "(default=False) Whether to predict raw scores. pred_leaf : bool, optional (default=False) Whether to predict leaf index. pred_contrib : bool, optional (default=False) Whether to predict feature contributions. .. note:: If you want to get more explanations for your model's predictions using SHAP values, like SHAP interaction values, you can install the shap package (https://github.com/slundberg/shap). Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra column, where the last column is the expected value. data_has_header : bool, optional (default=False) Whether the data has header. Used only if data is str. validate_features : bool, optional (default=False) If True, ensure that the features used to predict match the ones used to train. Used only if data is pandas DataFrame. **kwargs Other parameters for the prediction. Returns ------- result : numpy array, scipy.sparse or list of scipy.sparse Prediction result. Can be sparse or a list of sparse objects (each element represents predictions for one class) for feature contributions (when ``pred_contrib=True``). \"\"\" predictor = _InnerPredictor.from_booster( booster=self, pred_parameter=deepcopy(kwargs), ) if num_iteration is None: if start_iteration <= 0: num_iteration = self.best_iteration else: num_iteration = -1 return predictor.predict( data=data, start_iteration=start_iteration, num_iteration=num_iteration, raw_score=raw_score, pred_leaf=pred_leaf, pred_contrib=pred_contrib, data_has_header=data_has_header, validate_features=validate_features, ) [docs] def refit( self, data: _LGBM_TrainDataType, label: _LGBM_LabelType, decay_rate: float = 0.9, reference: Optional[Dataset] = None, weight: Optional[_LGBM_WeightType] = None, group: Optional[_LGBM_GroupType] = None, init_score: Optional[_LGBM_InitScoreType] = None, feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", dataset_params: Optional[Dict[str, Any]] = None, free_raw_data: bool = True, validate_features: bool = False, **kwargs: Any, ) -> \"Booster\": \"\"\"Refit the existing Booster by new data. Parameters ---------- data : str, pathlib.Path, numpy array, pandas DataFrame, scipy.sparse, Sequence, list of Sequence, list of numpy array or pyarrow Table Data source for refit. If str or pathlib.Path, it represents the path to a text file (CSV, TSV, or LibSVM). label : list, numpy", "prev_chunk_id": "chunk_342", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_344", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "1-D array, pandas Series / one-column DataFrame, pyarrow Array or pyarrow ChunkedArray Label for refit. decay_rate : float, optional (default=0.9) Decay rate of refit, will use ``leaf_output = decay_rate * old_leaf_output + (1.0 - decay_rate) * new_leaf_output`` to refit trees. reference : Dataset or None, optional (default=None) Reference for ``data``. .. versionadded:: 4.0.0 weight : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Weight for each ``data`` instance. Weights should be non-negative. .. versionadded:: 4.0.0 group : list, numpy 1-D array, pandas Series, pyarrow Array, pyarrow ChunkedArray or None, optional (default=None) Group/query size for ``data``. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. .. versionadded:: 4.0.0 init_score : list, list of lists (for multi-class task), numpy array, pandas Series, pandas DataFrame (for multi-class task), pyarrow Array, pyarrow ChunkedArray, pyarrow Table (for multi-class task) or None, optional (default=None) Init score for ``data``. .. versionadded:: 4.0.0 feature_name : list of str, or 'auto', optional (default=\"auto\") Feature names for ``data``. If 'auto' and data is pandas DataFrame, data columns names are used. .. versionadded:: 4.0.0 categorical_feature : list of str or int, or 'auto', optional (default=\"auto\") Categorical features for ``data``. If list of int, interpreted as indices. If list of str, interpreted as feature names (need to specify ``feature_name`` as well). If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647). Large values could", "prev_chunk_id": "chunk_343", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_345", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "be memory consuming. Consider using consecutive integers starting from zero. All negative values in categorical features will be treated as missing values. The output cannot be monotonically constrained with respect to a categorical feature. Floating point numbers in categorical features will be rounded towards 0. .. versionadded:: 4.0.0 dataset_params : dict or None, optional (default=None) Other parameters for Dataset ``data``. .. versionadded:: 4.0.0 free_raw_data : bool, optional (default=True) If True, raw data is freed after constructing inner Dataset for ``data``. .. versionadded:: 4.0.0 validate_features : bool, optional (default=False) If True, ensure that the features used to refit the model match the original ones. Used only if data is pandas DataFrame. .. versionadded:: 4.0.0 **kwargs Other parameters for refit. These parameters will be passed to ``predict`` method. Returns ------- result : Booster Refitted Booster. \"\"\" if self.__set_objective_to_none: raise LightGBMError(\"Cannot refit due to null objective function.\") if dataset_params is None: dataset_params = {} predictor = _InnerPredictor.from_booster(booster=self, pred_parameter=deepcopy(kwargs)) leaf_preds: np.ndarray = predictor.predict( # type: ignore[assignment] data=data, start_iteration=-1, pred_leaf=True, validate_features=validate_features, ) nrow, ncol = leaf_preds.shape out_is_linear = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetLinear( self._handle, ctypes.byref(out_is_linear), ) ) new_params = _choose_param_value( main_param_name=\"linear_tree\", params=self.params, default_value=None, ) new_params[\"linear_tree\"] = bool(out_is_linear.value) new_params.update(dataset_params) train_set = Dataset( data=data, label=label, reference=reference, weight=weight, group=group, init_score=init_score, feature_name=feature_name, categorical_feature=categorical_feature, params=new_params, free_raw_data=free_raw_data, ) new_params[\"refit_decay_rate\"] = decay_rate new_booster = Booster(new_params, train_set) # Copy models _safe_call( _LIB.LGBM_BoosterMerge( new_booster._handle, predictor._handle, ) ) leaf_preds = leaf_preds.reshape(-1) ptr_data, _, _ = _c_int_array(leaf_preds) _safe_call( _LIB.LGBM_BoosterRefit( new_booster._handle, ptr_data, ctypes.c_int32(nrow), ctypes.c_int32(ncol), ) ) new_booster._network = self._network return new_booster [docs] def get_leaf_output(self, tree_id: int, leaf_id: int) -> float: \"\"\"Get the output of a leaf. Parameters ---------- tree_id : int The index of the tree. leaf_id : int The index of the leaf in the tree. Returns ------- result : float The output of the leaf. \"\"\" ret = ctypes.c_double(0) _safe_call( _LIB.LGBM_BoosterGetLeafValue( self._handle, ctypes.c_int(tree_id), ctypes.c_int(leaf_id), ctypes.byref(ret), ) )", "prev_chunk_id": "chunk_344", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_346", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "return ret.value [docs] def set_leaf_output( self, tree_id: int, leaf_id: int, value: float, ) -> \"Booster\": \"\"\"Set the output of a leaf. .. versionadded:: 4.0.0 Parameters ---------- tree_id : int The index of the tree. leaf_id : int The index of the leaf in the tree. value : float Value to set as the output of the leaf. Returns ------- self : Booster Booster with the leaf output set. \"\"\" _safe_call( _LIB.LGBM_BoosterSetLeafValue( self._handle, ctypes.c_int(tree_id), ctypes.c_int(leaf_id), ctypes.c_double(value), ) ) return self [docs] def num_feature(self) -> int: \"\"\"Get number of features. Returns ------- num_feature : int The number of features. \"\"\" out_num_feature = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetNumFeature( self._handle, ctypes.byref(out_num_feature), ) ) return out_num_feature.value [docs] def feature_name(self) -> List[str]: \"\"\"Get names of features. Returns ------- result : list of str List with names of features. \"\"\" num_feature = self.num_feature() # Get name of features tmp_out_len = ctypes.c_int(0) reserved_string_buffer_size = 255 required_string_buffer_size = ctypes.c_size_t(0) string_buffers = [ctypes.create_string_buffer(reserved_string_buffer_size) for _ in range(num_feature)] ptr_string_buffers = (ctypes.c_char_p * num_feature)(*map(ctypes.addressof, string_buffers)) # type: ignore[misc] _safe_call( _LIB.LGBM_BoosterGetFeatureNames( self._handle, ctypes.c_int(num_feature), ctypes.byref(tmp_out_len), ctypes.c_size_t(reserved_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) if num_feature != tmp_out_len.value: raise ValueError(\"Length of feature names doesn't equal with num_feature\") actual_string_buffer_size = required_string_buffer_size.value # if buffer length is not long enough, reallocate buffers if reserved_string_buffer_size < actual_string_buffer_size: string_buffers = [ctypes.create_string_buffer(actual_string_buffer_size) for _ in range(num_feature)] ptr_string_buffers = (ctypes.c_char_p * num_feature)(*map(ctypes.addressof, string_buffers)) # type: ignore[misc] _safe_call( _LIB.LGBM_BoosterGetFeatureNames( self._handle, ctypes.c_int(num_feature), ctypes.byref(tmp_out_len), ctypes.c_size_t(actual_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) return [string_buffers[i].value.decode(\"utf-8\") for i in range(num_feature)] [docs] def feature_importance( self, importance_type: str = \"split\", iteration: Optional[int] = None, ) -> np.ndarray: \"\"\"Get feature importances. Parameters ---------- importance_type : str, optional (default=\"split\") How the importance is calculated. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. iteration : int or None, optional (default=None) Limit", "prev_chunk_id": "chunk_345", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_347", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "number of iterations in the feature importance calculation. If None, if the best iteration exists, it is used; otherwise, all trees are used. If <= 0, all trees are used (no limits). Returns ------- result : numpy array Array with feature importances. \"\"\" if iteration is None: iteration = self.best_iteration importance_type_int = _FEATURE_IMPORTANCE_TYPE_MAPPER[importance_type] result = np.empty(self.num_feature(), dtype=np.float64) _safe_call( _LIB.LGBM_BoosterFeatureImportance( self._handle, ctypes.c_int(iteration), ctypes.c_int(importance_type_int), result.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if importance_type_int == _C_API_FEATURE_IMPORTANCE_SPLIT: return result.astype(np.int32) else: return result [docs] def get_split_value_histogram( self, feature: Union[int, str], bins: Optional[Union[int, str]] = None, xgboost_style: bool = False, ) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray, pd_DataFrame]: \"\"\"Get split value histogram for the specified feature. Parameters ---------- feature : int or str The feature name or index the histogram is calculated for. If int, interpreted as index. If str, interpreted as name. .. warning:: Categorical features are not supported. bins : int, str or None, optional (default=None) The maximum number of bins. If None, or int and > number of unique split values and ``xgboost_style=True``, the number of bins equals number of unique split values. If str, it should be one from the list of the supported values by ``numpy.histogram()`` function. xgboost_style : bool, optional (default=False) Whether the returned result should be in the same form as it is in XGBoost. If False, the returned value is tuple of 2 numpy arrays as it is in ``numpy.histogram()`` function. If True, the returned value is matrix, in which the first column is the right edges of non-empty bins and the second one is the histogram values. Returns ------- result_tuple : tuple of 2 numpy arrays If ``xgboost_style=False``, the values of the histogram of used splitting values for the specified feature and the bin edges. result_array_like : numpy array or pandas DataFrame (if pandas is installed) If ``xgboost_style=True``, the histogram of used splitting", "prev_chunk_id": "chunk_346", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_348", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "values for the specified feature. \"\"\" def add(root: Dict[str, Any]) -> None: \"\"\"Recursively add thresholds.\"\"\" if \"split_index\" in root: # non-leaf if feature_names is not None and isinstance(feature, str): split_feature = feature_names[root[\"split_feature\"]] else: split_feature = root[\"split_feature\"] if split_feature == feature: if isinstance(root[\"threshold\"], str): raise LightGBMError(\"Cannot compute split value histogram for the categorical feature\") values.append(root[\"threshold\"]) add(root[\"left_child\"]) add(root[\"right_child\"]) model = self.dump_model() feature_names = model.get(\"feature_names\") tree_infos = model[\"tree_info\"] values: List[float] = [] for tree_info in tree_infos: add(tree_info[\"tree_structure\"]) if bins is None or isinstance(bins, int) and xgboost_style: n_unique = len(np.unique(values)) bins = max(min(n_unique, bins) if bins is not None else n_unique, 1) hist, bin_edges = np.histogram(values, bins=bins) if xgboost_style: ret = np.column_stack((bin_edges[1:], hist)) ret = ret[ret[:, 1] > 0] if PANDAS_INSTALLED: return pd_DataFrame(ret, columns=[\"SplitValue\", \"Count\"]) else: return ret else: return hist, bin_edges def __inner_eval( self, *, data_name: str, data_idx: int, feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]], ) -> List[_LGBM_BoosterEvalMethodResultType]: \"\"\"Evaluate training or validation data.\"\"\" if data_idx >= self.__num_dataset: raise ValueError(\"Data_idx should be smaller than number of dataset\") self.__get_eval_info() ret = [] if self.__num_inner_eval > 0: result = np.empty(self.__num_inner_eval, dtype=np.float64) tmp_out_len = ctypes.c_int(0) _safe_call( _LIB.LGBM_BoosterGetEval( self._handle, ctypes.c_int(data_idx), ctypes.byref(tmp_out_len), result.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ) ) if tmp_out_len.value != self.__num_inner_eval: raise ValueError(\"Wrong length of eval results\") for i in range(self.__num_inner_eval): ret.append((data_name, self.__name_inner_eval[i], result[i], self.__higher_better_inner_eval[i])) if callable(feval): feval = [feval] if feval is not None: if data_idx == 0: cur_data = self.train_set else: cur_data = self.valid_sets[data_idx - 1] for eval_function in feval: if eval_function is None: continue feval_ret = eval_function(self.__inner_predict(data_idx), cur_data) if isinstance(feval_ret, list): for eval_name, val, is_higher_better in feval_ret: ret.append((data_name, eval_name, val, is_higher_better)) else: eval_name, val, is_higher_better = feval_ret ret.append((data_name, eval_name, val, is_higher_better)) return ret def __inner_predict(self, data_idx: int) -> np.ndarray: \"\"\"Predict for training and validation dataset.\"\"\" if data_idx >= self.__num_dataset: raise ValueError(\"Data_idx should be smaller than number of dataset\") if self.__inner_predict_buffer[data_idx] is None: if data_idx == 0: n_preds = self.train_set.num_data() * self.__num_class", "prev_chunk_id": "chunk_347", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_349", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/basic.html", "title": "Source code for lightgbm.basic", "page_title": "lightgbm.basic — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.basic", "content": "else: n_preds = self.valid_sets[data_idx - 1].num_data() * self.__num_class self.__inner_predict_buffer[data_idx] = np.empty(n_preds, dtype=np.float64) # avoid to predict many time in one iteration if not self.__is_predicted_cur_iter[data_idx]: tmp_out_len = ctypes.c_int64(0) data_ptr = self.__inner_predict_buffer[data_idx].ctypes.data_as(ctypes.POINTER(ctypes.c_double)) # type: ignore[union-attr] _safe_call( _LIB.LGBM_BoosterGetPredict( self._handle, ctypes.c_int(data_idx), ctypes.byref(tmp_out_len), data_ptr, ) ) if tmp_out_len.value != len(self.__inner_predict_buffer[data_idx]): # type: ignore[arg-type] raise ValueError(f\"Wrong length of predict results for data {data_idx}\") self.__is_predicted_cur_iter[data_idx] = True result: np.ndarray = self.__inner_predict_buffer[data_idx] # type: ignore[assignment] if self.__num_class > 1: num_data = result.size // self.__num_class result = result.reshape(num_data, self.__num_class, order=\"F\") return result def __get_eval_info(self) -> None: \"\"\"Get inner evaluation count and names.\"\"\" if self.__need_reload_eval_info: self.__need_reload_eval_info = False out_num_eval = ctypes.c_int(0) # Get num of inner evals _safe_call( _LIB.LGBM_BoosterGetEvalCounts( self._handle, ctypes.byref(out_num_eval), ) ) self.__num_inner_eval = out_num_eval.value if self.__num_inner_eval > 0: # Get name of eval metrics tmp_out_len = ctypes.c_int(0) reserved_string_buffer_size = 255 required_string_buffer_size = ctypes.c_size_t(0) string_buffers = [ ctypes.create_string_buffer(reserved_string_buffer_size) for _ in range(self.__num_inner_eval) ] ptr_string_buffers = (ctypes.c_char_p * self.__num_inner_eval)(*map(ctypes.addressof, string_buffers)) # type: ignore[misc] _safe_call( _LIB.LGBM_BoosterGetEvalNames( self._handle, ctypes.c_int(self.__num_inner_eval), ctypes.byref(tmp_out_len), ctypes.c_size_t(reserved_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) if self.__num_inner_eval != tmp_out_len.value: raise ValueError(\"Length of eval names doesn't equal with num_evals\") actual_string_buffer_size = required_string_buffer_size.value # if buffer length is not long enough, reallocate buffers if reserved_string_buffer_size < actual_string_buffer_size: string_buffers = [ ctypes.create_string_buffer(actual_string_buffer_size) for _ in range(self.__num_inner_eval) ] ptr_string_buffers = (ctypes.c_char_p * self.__num_inner_eval)( *map(ctypes.addressof, string_buffers) ) # type: ignore[misc] _safe_call( _LIB.LGBM_BoosterGetEvalNames( self._handle, ctypes.c_int(self.__num_inner_eval), ctypes.byref(tmp_out_len), ctypes.c_size_t(actual_string_buffer_size), ctypes.byref(required_string_buffer_size), ptr_string_buffers, ) ) self.__name_inner_eval = [string_buffers[i].value.decode(\"utf-8\") for i in range(self.__num_inner_eval)] self.__higher_better_inner_eval = [ name.startswith((\"auc\", \"ndcg@\", \"map@\", \"average_precision\")) for name in self.__name_inner_eval ]", "prev_chunk_id": "chunk_348", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_350", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "Source code for lightgbm.plotting # coding: utf-8 \"\"\"Plotting library.\"\"\" import math from copy import deepcopy from io import BytesIO from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union import numpy as np from .basic import Booster, _data_from_pandas, _is_zero, _log_warning, _MissingType from .compat import GRAPHVIZ_INSTALLED, MATPLOTLIB_INSTALLED, pd_DataFrame from .sklearn import LGBMModel __all__ = [ \"create_tree_digraph\", \"plot_importance\", \"plot_metric\", \"plot_split_value_histogram\", \"plot_tree\", ] if TYPE_CHECKING: import matplotlib def _check_not_tuple_of_2_elements(obj: Any, obj_name: str) -> None: \"\"\"Check object is not tuple or does not have 2 elements.\"\"\" if not isinstance(obj, tuple) or len(obj) != 2: raise TypeError(f\"{obj_name} must be a tuple of 2 elements.\") def _float2str(value: float, precision: Optional[int]) -> str: return f\"{value:.{precision}f}\" if precision is not None and not isinstance(value, str) else str(value) [docs] def plot_importance( booster: Union[Booster, LGBMModel], ax: \"Optional[matplotlib.axes.Axes]\" = None, height: float = 0.2, xlim: Optional[Tuple[float, float]] = None, ylim: Optional[Tuple[float, float]] = None, title: Optional[str] = \"Feature importance\", xlabel: Optional[str] = \"Feature importance\", ylabel: Optional[str] = \"Features\", importance_type: str = \"auto\", max_num_features: Optional[int] = None, ignore_zero: bool = True, figsize: Optional[Tuple[float, float]] = None, dpi: Optional[int] = None, grid: bool = True, precision: Optional[int] = 3, **kwargs: Any, ) -> Any: \"\"\"Plot model's feature importances. Parameters ---------- booster : Booster or LGBMModel Booster or LGBMModel instance which feature importance should be plotted. ax : matplotlib.axes.Axes or None, optional (default=None) Target axes instance. If None, new figure and axes will be created. height : float, optional (default=0.2) Bar height, passed to ``ax.barh()``. xlim : tuple of 2 elements or None, optional (default=None) Tuple passed to ``ax.xlim()``. ylim : tuple of 2 elements or None, optional (default=None) Tuple passed to ``ax.ylim()``. title : str or None, optional (default=\"Feature importance\") Axes title. If None, title is disabled. xlabel : str or None, optional (default=\"Feature importance\") X-axis title label. If None, title is disabled.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_351", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "@importance_type@ placeholder can be used, and it will be replaced with the value of ``importance_type`` parameter. ylabel : str or None, optional (default=\"Features\") Y-axis title label. If None, title is disabled. importance_type : str, optional (default=\"auto\") How the importance is calculated. If \"auto\", if ``booster`` parameter is LGBMModel, ``booster.importance_type`` attribute is used; \"split\" otherwise. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. max_num_features : int or None, optional (default=None) Max number of top features displayed on plot. If None or <1, all features will be displayed. ignore_zero : bool, optional (default=True) Whether to ignore features with zero importance. figsize : tuple of 2 elements or None, optional (default=None) Figure size. dpi : int or None, optional (default=None) Resolution of the figure. grid : bool, optional (default=True) Whether to add a grid for axes. precision : int or None, optional (default=3) Used to restrict the display of floating point values to a certain precision. **kwargs Other parameters passed to ``ax.barh()``. Returns ------- ax : matplotlib.axes.Axes The plot with model's feature importances. \"\"\" if MATPLOTLIB_INSTALLED: import matplotlib.pyplot as plt # noqa: PLC0415 else: raise ImportError(\"You must install matplotlib and restart your session to plot importance.\") if isinstance(booster, LGBMModel): if importance_type == \"auto\": importance_type = booster.importance_type booster = booster.booster_ elif isinstance(booster, Booster): if importance_type == \"auto\": importance_type = \"split\" else: raise TypeError(\"booster must be Booster or LGBMModel.\") importance = booster.feature_importance(importance_type=importance_type) feature_name = booster.feature_name() if not len(importance): raise ValueError(\"Booster's feature_importance is empty.\") tuples = sorted(zip(feature_name, importance), key=lambda x: x[1]) if ignore_zero: tuples = [x for x in tuples if x[1] > 0] if max_num_features is not None and max_num_features > 0: tuples = tuples[-max_num_features:] labels, values = zip(*tuples) if ax is None: if figsize is not", "prev_chunk_id": "chunk_350", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_352", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "None: _check_not_tuple_of_2_elements(figsize, \"figsize\") _, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi) ylocs = np.arange(len(values)) ax.barh(ylocs, values, align=\"center\", height=height, **kwargs) for x, y in zip(values, ylocs): ax.text(x + 1, y, _float2str(x, precision) if importance_type == \"gain\" else x, va=\"center\") ax.set_yticks(ylocs) ax.set_yticklabels(labels) if xlim is not None: _check_not_tuple_of_2_elements(xlim, \"xlim\") else: xlim = (0, max(values) * 1.1) ax.set_xlim(xlim) if ylim is not None: _check_not_tuple_of_2_elements(ylim, \"ylim\") else: ylim = (-1, len(values)) ax.set_ylim(ylim) if title is not None: ax.set_title(title) if xlabel is not None: xlabel = xlabel.replace(\"@importance_type@\", importance_type) ax.set_xlabel(xlabel) if ylabel is not None: ax.set_ylabel(ylabel) ax.grid(grid) return ax [docs] def plot_split_value_histogram( booster: Union[Booster, LGBMModel], feature: Union[int, str], bins: Union[int, str, None] = None, ax: \"Optional[matplotlib.axes.Axes]\" = None, width_coef: float = 0.8, xlim: Optional[Tuple[float, float]] = None, ylim: Optional[Tuple[float, float]] = None, title: Optional[str] = \"Split value histogram for feature with @index/name@ @feature@\", xlabel: Optional[str] = \"Feature split value\", ylabel: Optional[str] = \"Count\", figsize: Optional[Tuple[float, float]] = None, dpi: Optional[int] = None, grid: bool = True, **kwargs: Any, ) -> Any: \"\"\"Plot split value histogram for the specified feature of the model. Parameters ---------- booster : Booster or LGBMModel Booster or LGBMModel instance of which feature split value histogram should be plotted. feature : int or str The feature name or index the histogram is plotted for. If int, interpreted as index. If str, interpreted as name. bins : int, str or None, optional (default=None) The maximum number of bins. If None, the number of bins equals number of unique split values. If str, it should be one from the list of the supported values by ``numpy.histogram()`` function. ax : matplotlib.axes.Axes or None, optional (default=None) Target axes instance. If None, new figure and axes will be created. width_coef : float, optional (default=0.8) Coefficient for histogram bar width. xlim : tuple of 2 elements or None, optional (default=None)", "prev_chunk_id": "chunk_351", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_353", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "Tuple passed to ``ax.xlim()``. ylim : tuple of 2 elements or None, optional (default=None) Tuple passed to ``ax.ylim()``. title : str or None, optional (default=\"Split value histogram for feature with @index/name@ @feature@\") Axes title. If None, title is disabled. @feature@ placeholder can be used, and it will be replaced with the value of ``feature`` parameter. @index/name@ placeholder can be used, and it will be replaced with ``index`` word in case of ``int`` type ``feature`` parameter or ``name`` word in case of ``str`` type ``feature`` parameter. xlabel : str or None, optional (default=\"Feature split value\") X-axis title label. If None, title is disabled. ylabel : str or None, optional (default=\"Count\") Y-axis title label. If None, title is disabled. figsize : tuple of 2 elements or None, optional (default=None) Figure size. dpi : int or None, optional (default=None) Resolution of the figure. grid : bool, optional (default=True) Whether to add a grid for axes. **kwargs Other parameters passed to ``ax.bar()``. Returns ------- ax : matplotlib.axes.Axes The plot with specified model's feature split value histogram. \"\"\" if MATPLOTLIB_INSTALLED: import matplotlib.pyplot as plt # noqa: PLC0415 from matplotlib.ticker import MaxNLocator # noqa: PLC0415 else: raise ImportError(\"You must install matplotlib and restart your session to plot split value histogram.\") if isinstance(booster, LGBMModel): booster = booster.booster_ elif not isinstance(booster, Booster): raise TypeError(\"booster must be Booster or LGBMModel.\") hist, split_bins = booster.get_split_value_histogram(feature=feature, bins=bins, xgboost_style=False) if np.count_nonzero(hist) == 0: raise ValueError(f\"Cannot plot split value histogram, because feature {feature} was not used in splitting\") width = width_coef * (split_bins[1] - split_bins[0]) centred = (split_bins[:-1] + split_bins[1:]) / 2 if ax is None: if figsize is not None: _check_not_tuple_of_2_elements(figsize, \"figsize\") _, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi) ax.bar(centred, hist, align=\"center\", width=width, **kwargs) if xlim is not None: _check_not_tuple_of_2_elements(xlim, \"xlim\") else: range_result = split_bins[-1] - split_bins[0] xlim = (split_bins[0] - range_result", "prev_chunk_id": "chunk_352", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_354", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "* 0.2, split_bins[-1] + range_result * 0.2) ax.set_xlim(xlim) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) if ylim is not None: _check_not_tuple_of_2_elements(ylim, \"ylim\") else: ylim = (0, max(hist) * 1.1) ax.set_ylim(ylim) if title is not None: title = title.replace(\"@feature@\", str(feature)) title = title.replace(\"@index/name@\", (\"name\" if isinstance(feature, str) else \"index\")) ax.set_title(title) if xlabel is not None: ax.set_xlabel(xlabel) if ylabel is not None: ax.set_ylabel(ylabel) ax.grid(grid) return ax [docs] def plot_metric( booster: Union[Dict, LGBMModel], metric: Optional[str] = None, dataset_names: Optional[List[str]] = None, ax: \"Optional[matplotlib.axes.Axes]\" = None, xlim: Optional[Tuple[float, float]] = None, ylim: Optional[Tuple[float, float]] = None, title: Optional[str] = \"Metric during training\", xlabel: Optional[str] = \"Iterations\", ylabel: Optional[str] = \"@metric@\", figsize: Optional[Tuple[float, float]] = None, dpi: Optional[int] = None, grid: bool = True, ) -> Any: \"\"\"Plot one metric during training. Parameters ---------- booster : dict or LGBMModel Dictionary returned from ``lightgbm.train()`` or LGBMModel instance. metric : str or None, optional (default=None) The metric name to plot. Only one metric supported because different metrics have various scales. If None, first metric picked from dictionary (according to hashcode). dataset_names : list of str, or None, optional (default=None) List of the dataset names which are used to calculate metric to plot. If None, all datasets are used. ax : matplotlib.axes.Axes or None, optional (default=None) Target axes instance. If None, new figure and axes will be created. xlim : tuple of 2 elements or None, optional (default=None) Tuple passed to ``ax.xlim()``. ylim : tuple of 2 elements or None, optional (default=None) Tuple passed to ``ax.ylim()``. title : str or None, optional (default=\"Metric during training\") Axes title. If None, title is disabled. xlabel : str or None, optional (default=\"Iterations\") X-axis title label. If None, title is disabled. ylabel : str or None, optional (default=\"@metric@\") Y-axis title label. If 'auto', metric name is used. If None, title is disabled. @metric@ placeholder can be used, and", "prev_chunk_id": "chunk_353", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_355", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "it will be replaced with metric name. figsize : tuple of 2 elements or None, optional (default=None) Figure size. dpi : int or None, optional (default=None) Resolution of the figure. grid : bool, optional (default=True) Whether to add a grid for axes. Returns ------- ax : matplotlib.axes.Axes The plot with metric's history over the training. \"\"\" if MATPLOTLIB_INSTALLED: import matplotlib.pyplot as plt # noqa: PLC0415 else: raise ImportError(\"You must install matplotlib and restart your session to plot metric.\") if isinstance(booster, LGBMModel): eval_results = deepcopy(booster.evals_result_) elif isinstance(booster, dict): eval_results = deepcopy(booster) elif isinstance(booster, Booster): raise TypeError( \"booster must be dict or LGBMModel. To use plot_metric with Booster type, first record the metrics using record_evaluation callback then pass that to plot_metric as argument `booster`\" ) else: raise TypeError(\"booster must be dict or LGBMModel.\") num_data = len(eval_results) if not num_data: raise ValueError(\"eval results cannot be empty.\") if ax is None: if figsize is not None: _check_not_tuple_of_2_elements(figsize, \"figsize\") _, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi) if dataset_names is None: dataset_names_iter = iter(eval_results.keys()) elif not isinstance(dataset_names, (list, tuple, set)) or not dataset_names: raise ValueError(\"dataset_names should be iterable and cannot be empty\") else: dataset_names_iter = iter(dataset_names) name = next(dataset_names_iter) # take one as sample metrics_for_one = eval_results[name] num_metric = len(metrics_for_one) if metric is None: if num_metric > 1: _log_warning(\"More than one metric available, picking one to plot.\") metric, results = metrics_for_one.popitem() else: if metric not in metrics_for_one: raise KeyError(\"No given metric in eval results.\") results = metrics_for_one[metric] num_iteration = len(results) max_result = max(results) min_result = min(results) x_ = range(num_iteration) ax.plot(x_, results, label=name) for name in dataset_names_iter: metrics_for_one = eval_results[name] results = metrics_for_one[metric] max_result = max(*results, max_result) min_result = min(*results, min_result) ax.plot(x_, results, label=name) ax.legend(loc=\"best\") if xlim is not None: _check_not_tuple_of_2_elements(xlim, \"xlim\") else: xlim = (0, num_iteration) ax.set_xlim(xlim) if ylim is not None: _check_not_tuple_of_2_elements(ylim, \"ylim\") else: range_result", "prev_chunk_id": "chunk_354", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_356", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "= max_result - min_result ylim = (min_result - range_result * 0.2, max_result + range_result * 0.2) ax.set_ylim(ylim) if title is not None: ax.set_title(title) if xlabel is not None: ax.set_xlabel(xlabel) if ylabel is not None: ylabel = ylabel.replace(\"@metric@\", metric) ax.set_ylabel(ylabel) ax.grid(grid) return ax def _determine_direction_for_numeric_split( *, fval: float, threshold: float, missing_type_str: str, default_left: bool, ) -> str: missing_type = _MissingType(missing_type_str) if math.isnan(fval) and missing_type != _MissingType.NAN: fval = 0.0 if (missing_type == _MissingType.ZERO and _is_zero(fval)) or ( missing_type == _MissingType.NAN and math.isnan(fval) ): direction = \"left\" if default_left else \"right\" else: direction = \"left\" if fval <= threshold else \"right\" return direction def _determine_direction_for_categorical_split(fval: float, thresholds: str) -> str: if math.isnan(fval) or int(fval) < 0: return \"right\" int_thresholds = {int(t) for t in thresholds.split(\"||\")} return \"left\" if int(fval) in int_thresholds else \"right\" def _to_graphviz( *, tree_info: Dict[str, Any], show_info: List[str], feature_names: Union[List[str], None], precision: Optional[int], orientation: str, constraints: Optional[List[int]], example_case: Optional[Union[np.ndarray, pd_DataFrame]], max_category_values: int, **kwargs: Any, ) -> Any: \"\"\"Convert specified tree to graphviz instance. See: - https://graphviz.readthedocs.io/en/stable/api.html#digraph \"\"\" if GRAPHVIZ_INSTALLED: from graphviz import Digraph # noqa: PLC0415 else: raise ImportError(\"You must install graphviz and restart your session to plot tree.\") def add( root: Dict[str, Any], total_count: int, parent: Optional[str], decision: Optional[str], highlight: bool ) -> None: \"\"\"Recursively add node or edge.\"\"\" fillcolor = \"white\" style = \"\" tooltip = None if highlight: color = \"blue\" penwidth = \"3\" else: color = \"black\" penwidth = \"1\" if \"split_index\" in root: # non-leaf shape = \"rectangle\" l_dec = \"yes\" r_dec = \"no\" threshold = root[\"threshold\"] if root[\"decision_type\"] == \"<=\": operator = \"&#8804;\" elif root[\"decision_type\"] == \"==\": operator = \"=\" else: raise ValueError(\"Invalid decision type in tree model.\") name = f\"split{root['split_index']}\" split_feature = root[\"split_feature\"] if feature_names is not None: label = f\"<B>{feature_names[split_feature]}</B> {operator}\" else: label = f\"feature <B>{split_feature}</B> {operator} \" direction = None", "prev_chunk_id": "chunk_355", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_357", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "if example_case is not None: if root[\"decision_type\"] == \"==\": direction = _determine_direction_for_categorical_split( fval=example_case[split_feature], thresholds=root[\"threshold\"] ) else: direction = _determine_direction_for_numeric_split( fval=example_case[split_feature], threshold=root[\"threshold\"], missing_type_str=root[\"missing_type\"], default_left=root[\"default_left\"], ) if root[\"decision_type\"] == \"==\": category_values = root[\"threshold\"].split(\"||\") if len(category_values) > max_category_values: tooltip = root[\"threshold\"] threshold = \"||\".join(category_values[:2]) + \"||...||\" + category_values[-1] label += f\"<B>{_float2str(threshold, precision)}</B>\" for info in [\"split_gain\", \"internal_value\", \"internal_weight\", \"internal_count\", \"data_percentage\"]: if info in show_info: output = info.split(\"_\")[-1] if info in {\"split_gain\", \"internal_value\", \"internal_weight\"}: label += f\"<br/>{_float2str(root[info], precision)} {output}\" elif info == \"internal_count\": label += f\"<br/>{output}: {root[info]}\" elif info == \"data_percentage\": label += f\"<br/>{_float2str(root['internal_count'] / total_count * 100, 2)}% of data\" if constraints: if constraints[root[\"split_feature\"]] == 1: fillcolor = \"#ddffdd\" # light green if constraints[root[\"split_feature\"]] == -1: fillcolor = \"#ffdddd\" # light red style = \"filled\" label = f\"<{label}>\" add( root=root[\"left_child\"], total_count=total_count, parent=name, decision=l_dec, highlight=highlight and direction == \"left\", ) add( root=root[\"right_child\"], total_count=total_count, parent=name, decision=r_dec, highlight=highlight and direction == \"right\", ) else: # leaf shape = \"ellipse\" name = f\"leaf{root['leaf_index']}\" label = f\"leaf {root['leaf_index']}: \" label += f\"<B>{_float2str(root['leaf_value'], precision)}</B>\" if \"leaf_weight\" in show_info: label += f\"<br/>{_float2str(root['leaf_weight'], precision)} weight\" if \"leaf_count\" in show_info: label += f\"<br/>count: {root['leaf_count']}\" if \"data_percentage\" in show_info: label += f\"<br/>{_float2str(root['leaf_count'] / total_count * 100, 2)}% of data\" label = f\"<{label}>\" graph.node( name, label=label, shape=shape, style=style, fillcolor=fillcolor, color=color, penwidth=penwidth, tooltip=tooltip, ) if parent is not None: graph.edge(parent, name, decision, color=color, penwidth=penwidth) graph = Digraph(**kwargs) rankdir = \"LR\" if orientation == \"horizontal\" else \"TB\" graph.attr(\"graph\", nodesep=\"0.05\", ranksep=\"0.3\", rankdir=rankdir) if \"internal_count\" in tree_info[\"tree_structure\"]: add( root=tree_info[\"tree_structure\"], total_count=tree_info[\"tree_structure\"][\"internal_count\"], parent=None, decision=None, highlight=example_case is not None, ) else: raise Exception(\"Cannot plot trees with no split\") if constraints: # \"#ddffdd\" is light green, \"#ffdddd\" is light red legend = \"\"\"< <TABLE BORDER=\"0\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\"> <TR> <TD COLSPAN=\"2\"><B>Monotone constraints</B></TD> </TR> <TR> <TD>Increasing</TD> <TD BGCOLOR=\"#ddffdd\"></TD> </TR> <TR> <TD>Decreasing</TD> <TD BGCOLOR=\"#ffdddd\"></TD> </TR> </TABLE> >\"\"\" graph.node(\"legend\", label=legend, shape=\"rectangle\", color=\"white\") return graph [docs]", "prev_chunk_id": "chunk_356", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_358", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "def create_tree_digraph( booster: Union[Booster, LGBMModel], tree_index: int = 0, show_info: Optional[List[str]] = None, precision: Optional[int] = 3, orientation: str = \"horizontal\", example_case: Optional[Union[np.ndarray, pd_DataFrame]] = None, max_category_values: int = 10, **kwargs: Any, ) -> Any: \"\"\"Create a digraph representation of specified tree. Each node in the graph represents a node in the tree. Non-leaf nodes have labels like ``Column_10 <= 875.9``, which means \"this node splits on the feature named \"Column_10\", with threshold 875.9\". Leaf nodes have labels like ``leaf 2: 0.422``, which means \"this node is a leaf node, and the predicted value for records that fall into this node is 0.422\". The number (``2``) is an internal unique identifier and doesn't have any special meaning. .. note:: For more information please visit https://graphviz.readthedocs.io/en/stable/api.html#digraph. Parameters ---------- booster : Booster or LGBMModel Booster or LGBMModel instance to be converted. tree_index : int, optional (default=0) The index of a target tree to convert. show_info : list of str, or None, optional (default=None) What information should be shown in nodes. - ``'split_gain'`` : gain from adding this split to the model - ``'internal_value'`` : raw predicted value that would be produced by this node if it was a leaf node - ``'internal_count'`` : number of records from the training data that fall into this non-leaf node - ``'internal_weight'`` : total weight of all nodes that fall into this non-leaf node - ``'leaf_count'`` : number of records from the training data that fall into this leaf node - ``'leaf_weight'`` : total weight (sum of Hessian) of all observations that fall into this leaf node - ``'data_percentage'`` : percentage of training data that fall into this node precision : int or None, optional (default=3) Used to restrict the display of floating point values to a certain precision. orientation : str, optional (default='horizontal') Orientation of", "prev_chunk_id": "chunk_357", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_359", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "the tree. Can be 'horizontal' or 'vertical'. example_case : numpy 2-D array, pandas DataFrame or None, optional (default=None) Single row with the same structure as the training data. If not None, the plot will highlight the path that sample takes through the tree. .. versionadded:: 4.0.0 max_category_values : int, optional (default=10) The maximum number of category values to display in tree nodes, if the number of thresholds is greater than this value, thresholds will be collapsed and displayed on the label tooltip instead. .. warning:: Consider wrapping the SVG string of the tree graph with ``IPython.display.HTML`` when running on JupyterLab to get the `tooltip <https://graphviz.org/docs/attrs/tooltip>`_ working right. Example: .. code-block:: python from IPython.display import HTML graph = lgb.create_tree_digraph(clf, max_category_values=5) HTML(graph._repr_image_svg_xml()) .. versionadded:: 4.0.0 **kwargs Other parameters passed to ``Digraph`` constructor. Check https://graphviz.readthedocs.io/en/stable/api.html#digraph for the full list of supported parameters. Returns ------- graph : graphviz.Digraph The digraph representation of specified tree. \"\"\" if isinstance(booster, LGBMModel): booster = booster.booster_ elif not isinstance(booster, Booster): raise TypeError(\"booster must be Booster or LGBMModel.\") model = booster.dump_model() tree_infos = model[\"tree_info\"] feature_names = model.get(\"feature_names\", None) monotone_constraints = model.get(\"monotone_constraints\", None) if tree_index < len(tree_infos): tree_info = tree_infos[tree_index] else: raise IndexError(\"tree_index is out of range.\") if show_info is None: show_info = [] if example_case is not None: if not isinstance(example_case, (np.ndarray, pd_DataFrame)) or example_case.ndim != 2: raise ValueError(\"example_case must be a numpy 2-D array or a pandas DataFrame\") if example_case.shape[0] != 1: raise ValueError(\"example_case must have a single row.\") if isinstance(example_case, pd_DataFrame): example_case = _data_from_pandas( data=example_case, feature_name=\"auto\", categorical_feature=\"auto\", pandas_categorical=booster.pandas_categorical, )[0] example_case = example_case[0] return _to_graphviz( tree_info=tree_info, show_info=show_info, feature_names=feature_names, precision=precision, orientation=orientation, constraints=monotone_constraints, example_case=example_case, max_category_values=max_category_values, **kwargs, ) [docs] def plot_tree( booster: Union[Booster, LGBMModel], ax: \"Optional[matplotlib.axes.Axes]\" = None, tree_index: int = 0, figsize: Optional[Tuple[float, float]] = None, dpi: Optional[int] = None, show_info: Optional[List[str]] = None, precision: Optional[int] = 3, orientation: str =", "prev_chunk_id": "chunk_358", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_360", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "\"horizontal\", example_case: Optional[Union[np.ndarray, pd_DataFrame]] = None, **kwargs: Any, ) -> Any: \"\"\"Plot specified tree. Each node in the graph represents a node in the tree. Non-leaf nodes have labels like ``Column_10 <= 875.9``, which means \"this node splits on the feature named \"Column_10\", with threshold 875.9\". Leaf nodes have labels like ``leaf 2: 0.422``, which means \"this node is a leaf node, and the predicted value for records that fall into this node is 0.422\". The number (``2``) is an internal unique identifier and doesn't have any special meaning. .. note:: It is preferable to use ``create_tree_digraph()`` because of its lossless quality and returned objects can be also rendered and displayed directly inside a Jupyter notebook. Parameters ---------- booster : Booster or LGBMModel Booster or LGBMModel instance to be plotted. ax : matplotlib.axes.Axes or None, optional (default=None) Target axes instance. If None, new figure and axes will be created. tree_index : int, optional (default=0) The index of a target tree to plot. figsize : tuple of 2 elements or None, optional (default=None) Figure size. dpi : int or None, optional (default=None) Resolution of the figure. show_info : list of str, or None, optional (default=None) What information should be shown in nodes. - ``'split_gain'`` : gain from adding this split to the model - ``'internal_value'`` : raw predicted value that would be produced by this node if it was a leaf node - ``'internal_count'`` : number of records from the training data that fall into this non-leaf node - ``'internal_weight'`` : total weight of all nodes that fall into this non-leaf node - ``'leaf_count'`` : number of records from the training data that fall into this leaf node - ``'leaf_weight'`` : total weight (sum of Hessian) of all observations that fall into this leaf node - ``'data_percentage'`` : percentage of training", "prev_chunk_id": "chunk_359", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_361", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html", "title": "Source code for lightgbm.plotting", "page_title": "lightgbm.plotting — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.plotting", "content": "data that fall into this node precision : int or None, optional (default=3) Used to restrict the display of floating point values to a certain precision. orientation : str, optional (default='horizontal') Orientation of the tree. Can be 'horizontal' or 'vertical'. example_case : numpy 2-D array, pandas DataFrame or None, optional (default=None) Single row with the same structure as the training data. If not None, the plot will highlight the path that sample takes through the tree. .. versionadded:: 4.0.0 **kwargs Other parameters passed to ``Digraph`` constructor. Check https://graphviz.readthedocs.io/en/stable/api.html#digraph for the full list of supported parameters. Returns ------- ax : matplotlib.axes.Axes The plot with single tree. \"\"\" if MATPLOTLIB_INSTALLED: import matplotlib.image # noqa: PLC0415 import matplotlib.pyplot as plt # noqa: PLC0415 else: raise ImportError(\"You must install matplotlib and restart your session to plot tree.\") if ax is None: if figsize is not None: _check_not_tuple_of_2_elements(figsize, \"figsize\") _, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi) graph = create_tree_digraph( booster=booster, tree_index=tree_index, show_info=show_info, precision=precision, orientation=orientation, example_case=example_case, **kwargs, ) s = BytesIO() s.write(graph.pipe(format=\"png\")) s.seek(0) img = matplotlib.image.imread(s) ax.imshow(img) ax.axis(\"off\") return ax", "prev_chunk_id": "chunk_360", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_362", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "Source code for lightgbm.callback # coding: utf-8 \"\"\"Callbacks library.\"\"\" from collections import OrderedDict from dataclasses import dataclass from functools import partial from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union from .basic import ( Booster, _ConfigAliases, _LGBM_BoosterEvalMethodResultType, _LGBM_BoosterEvalMethodResultWithStandardDeviationType, _log_info, _log_warning, ) if TYPE_CHECKING: from .engine import CVBooster __all__ = [ \"EarlyStopException\", \"early_stopping\", \"log_evaluation\", \"record_evaluation\", \"reset_parameter\", ] _EvalResultDict = Dict[str, Dict[str, List[Any]]] _EvalResultTuple = Union[ _LGBM_BoosterEvalMethodResultType, _LGBM_BoosterEvalMethodResultWithStandardDeviationType, ] _ListOfEvalResultTuples = Union[ List[_LGBM_BoosterEvalMethodResultType], List[_LGBM_BoosterEvalMethodResultWithStandardDeviationType], ] class EarlyStopException(Exception): \"\"\"Exception of early stopping. Raise this from a callback passed in via keyword argument ``callbacks`` in ``cv()`` or ``train()`` to trigger early stopping. \"\"\" def __init__(self, best_iteration: int, best_score: _ListOfEvalResultTuples) -> None: \"\"\"Create early stopping exception. Parameters ---------- best_iteration : int The best iteration stopped. 0-based... pass ``best_iteration=2`` to indicate that the third iteration was the best one. best_score : list of (eval_name, metric_name, eval_result, is_higher_better) tuple or (eval_name, metric_name, eval_result, is_higher_better, stdv) tuple Scores for each metric, on each validation set, as of the best iteration. \"\"\" super().__init__() self.best_iteration = best_iteration self.best_score = best_score # Callback environment used by callbacks @dataclass class CallbackEnv: model: Union[Booster, \"CVBooster\"] params: Dict[str, Any] iteration: int begin_iteration: int end_iteration: int evaluation_result_list: Optional[_ListOfEvalResultTuples] def _is_using_cv(env: CallbackEnv) -> bool: \"\"\"Check if model in callback env is a CVBooster.\"\"\" # this import is here to avoid a circular import from .engine import CVBooster # noqa: PLC0415 return isinstance(env.model, CVBooster) def _format_eval_result(value: _EvalResultTuple, show_stdv: bool) -> str: \"\"\"Format metric string.\"\"\" dataset_name, metric_name, metric_value, *_ = value out = f\"{dataset_name}'s {metric_name}: {metric_value:g}\" # tuples from cv() sometimes have a 5th item, with standard deviation of # the evaluation metric (taken over all cross-validation folds) if show_stdv and len(value) == 5: out += f\" + {value[4]:g}\" return out class _LogEvaluationCallback: \"\"\"Internal log evaluation callable class.\"\"\" def __init__(self, period: int = 1, show_stdv: bool =", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_363", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "True) -> None: self.order = 10 self.before_iteration = False self.period = period self.show_stdv = show_stdv def __call__(self, env: CallbackEnv) -> None: if self.period > 0 and env.evaluation_result_list and (env.iteration + 1) % self.period == 0: result = \"\\t\".join([_format_eval_result(x, self.show_stdv) for x in env.evaluation_result_list]) _log_info(f\"[{env.iteration + 1}]\\t{result}\") [docs] def log_evaluation(period: int = 1, show_stdv: bool = True) -> _LogEvaluationCallback: \"\"\"Create a callback that logs the evaluation results. By default, standard output resource is used. Use ``register_logger()`` function to register a custom logger. Note ---- Requires at least one validation data. Parameters ---------- period : int, optional (default=1) The period to log the evaluation results. The last boosting stage or the boosting stage found by using ``early_stopping`` callback is also logged. show_stdv : bool, optional (default=True) Whether to log stdv (if provided). Returns ------- callback : _LogEvaluationCallback The callback that logs the evaluation results every ``period`` boosting iteration(s). \"\"\" return _LogEvaluationCallback(period=period, show_stdv=show_stdv) class _RecordEvaluationCallback: \"\"\"Internal record evaluation callable class.\"\"\" def __init__(self, eval_result: _EvalResultDict) -> None: self.order = 20 self.before_iteration = False if not isinstance(eval_result, dict): raise TypeError(\"eval_result should be a dictionary\") self.eval_result = eval_result def _init(self, env: CallbackEnv) -> None: if env.evaluation_result_list is None: raise RuntimeError( \"record_evaluation() callback enabled but no evaluation results found. This is a probably bug in LightGBM. \" \"Please report it at https://github.com/microsoft/LightGBM/issues\" ) self.eval_result.clear() for item in env.evaluation_result_list: dataset_name, metric_name, *_ = item self.eval_result.setdefault(dataset_name, OrderedDict()) if len(item) == 4: self.eval_result[dataset_name].setdefault(metric_name, []) else: self.eval_result[dataset_name].setdefault(f\"{metric_name}-mean\", []) self.eval_result[dataset_name].setdefault(f\"{metric_name}-stdv\", []) def __call__(self, env: CallbackEnv) -> None: if env.iteration == env.begin_iteration: self._init(env) if env.evaluation_result_list is None: raise RuntimeError( \"record_evaluation() callback enabled but no evaluation results found. This is a probably bug in LightGBM. \" \"Please report it at https://github.com/microsoft/LightGBM/issues\" ) for item in env.evaluation_result_list: # for cv(), 'metric_value' is actually a mean of metric values over all CV folds dataset_name, metric_name, metric_value,", "prev_chunk_id": "chunk_362", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_364", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "*_ = item if len(item) == 4: # train() self.eval_result[dataset_name][metric_name].append(metric_value) else: # cv() metric_std_dev = item[4] # type: ignore[misc] self.eval_result[dataset_name][f\"{metric_name}-mean\"].append(metric_value) self.eval_result[dataset_name][f\"{metric_name}-stdv\"].append(metric_std_dev) [docs] def record_evaluation(eval_result: Dict[str, Dict[str, List[Any]]]) -> Callable: \"\"\"Create a callback that records the evaluation history into ``eval_result``. Parameters ---------- eval_result : dict Dictionary used to store all evaluation results of all validation sets. This should be initialized outside of your call to ``record_evaluation()`` and should be empty. Any initial contents of the dictionary will be deleted. .. rubric:: Example With two validation sets named 'eval' and 'train', and one evaluation metric named 'logloss' this dictionary after finishing a model training process will have the following structure: .. code-block:: { 'train': { 'logloss': [0.48253, 0.35953, ...] }, 'eval': { 'logloss': [0.480385, 0.357756, ...] } } Returns ------- callback : _RecordEvaluationCallback The callback that records the evaluation history into the passed dictionary. \"\"\" return _RecordEvaluationCallback(eval_result=eval_result) class _ResetParameterCallback: \"\"\"Internal reset parameter callable class.\"\"\" def __init__(self, **kwargs: Union[list, Callable]) -> None: self.order = 10 self.before_iteration = True self.kwargs = kwargs def __call__(self, env: CallbackEnv) -> None: new_parameters = {} for key, value in self.kwargs.items(): if isinstance(value, list): if len(value) != env.end_iteration - env.begin_iteration: raise ValueError(f\"Length of list {key!r} has to be equal to 'num_boost_round'.\") new_param = value[env.iteration - env.begin_iteration] elif callable(value): new_param = value(env.iteration - env.begin_iteration) else: raise ValueError( \"Only list and callable values are supported \" \"as a mapping from boosting round index to new parameter value.\" ) if new_param != env.params.get(key, None): new_parameters[key] = new_param if new_parameters: if isinstance(env.model, Booster): env.model.reset_parameter(new_parameters) else: # CVBooster holds a list of Booster objects, each needs to be updated for booster in env.model.boosters: booster.reset_parameter(new_parameters) env.params.update(new_parameters) [docs] def reset_parameter(**kwargs: Union[list, Callable]) -> Callable: \"\"\"Create a callback that resets the parameter after the first iteration. .. note:: The initial parameter will still take in-effect on first", "prev_chunk_id": "chunk_363", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_365", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "iteration. Parameters ---------- **kwargs : value should be list or callable List of parameters for each boosting round or a callable that calculates the parameter in terms of current number of round (e.g. yields learning rate decay). If list lst, parameter = lst[current_round]. If callable func, parameter = func(current_round). Returns ------- callback : _ResetParameterCallback The callback that resets the parameter after the first iteration. \"\"\" return _ResetParameterCallback(**kwargs) class _EarlyStoppingCallback: \"\"\"Internal early stopping callable class.\"\"\" def __init__( self, stopping_rounds: int, first_metric_only: bool = False, verbose: bool = True, min_delta: Union[float, List[float]] = 0.0, ) -> None: self.enabled = _should_enable_early_stopping(stopping_rounds) self.order = 30 self.before_iteration = False self.stopping_rounds = stopping_rounds self.first_metric_only = first_metric_only self.verbose = verbose self.min_delta = min_delta self._reset_storages() def _reset_storages(self) -> None: self.best_score: List[float] = [] self.best_iter: List[int] = [] self.best_score_list: List[_ListOfEvalResultTuples] = [] self.cmp_op: List[Callable[[float, float], bool]] = [] self.first_metric = \"\" def _gt_delta(self, curr_score: float, best_score: float, delta: float) -> bool: return curr_score > best_score + delta def _lt_delta(self, curr_score: float, best_score: float, delta: float) -> bool: return curr_score < best_score - delta def _is_train_set(self, dataset_name: str, env: CallbackEnv) -> bool: \"\"\"Check, by name, if a given Dataset is the training data.\"\"\" # for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set # and those metrics are considered for early stopping if _is_using_cv(env) and dataset_name == \"train\": return True # for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name if isinstance(env.model, Booster) and dataset_name == env.model._train_data_name: return True return False def _init(self, env: CallbackEnv) -> None: if env.evaluation_result_list is None or env.evaluation_result_list == []: raise ValueError(\"For early stopping, at least one dataset and eval metric is required for evaluation\") is_dart = any(env.params.get(alias, \"\") == \"dart\" for alias in _ConfigAliases.get(\"boosting\")) if is_dart: self.enabled = False _log_warning(\"Early stopping is not available in dart", "prev_chunk_id": "chunk_364", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_366", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "mode\") return # get details of the first dataset first_dataset_name, first_metric_name, *_ = env.evaluation_result_list[0] # validation sets are guaranteed to not be identical to the training data in cv() if isinstance(env.model, Booster): only_train_set = len(env.evaluation_result_list) == 1 and self._is_train_set( dataset_name=first_dataset_name, env=env, ) if only_train_set: self.enabled = False _log_warning(\"Only training set found, disabling early stopping.\") return if self.verbose: _log_info(f\"Training until validation scores don't improve for {self.stopping_rounds} rounds\") self._reset_storages() n_metrics = len({m[1] for m in env.evaluation_result_list}) n_datasets = len(env.evaluation_result_list) // n_metrics if isinstance(self.min_delta, list): if not all(t >= 0 for t in self.min_delta): raise ValueError(\"Values for early stopping min_delta must be non-negative.\") if len(self.min_delta) == 0: if self.verbose: _log_info(\"Disabling min_delta for early stopping.\") deltas = [0.0] * n_datasets * n_metrics elif len(self.min_delta) == 1: if self.verbose: _log_info(f\"Using {self.min_delta[0]} as min_delta for all metrics.\") deltas = self.min_delta * n_datasets * n_metrics else: if len(self.min_delta) != n_metrics: raise ValueError(\"Must provide a single value for min_delta or as many as metrics.\") if self.first_metric_only and self.verbose: _log_info(f\"Using only {self.min_delta[0]} as early stopping min_delta.\") deltas = self.min_delta * n_datasets else: if self.min_delta < 0: raise ValueError(\"Early stopping min_delta must be non-negative.\") if self.min_delta > 0 and n_metrics > 1 and not self.first_metric_only and self.verbose: _log_info(f\"Using {self.min_delta} as min_delta for all metrics.\") deltas = [self.min_delta] * n_datasets * n_metrics self.first_metric = first_metric_name for eval_ret, delta in zip(env.evaluation_result_list, deltas): self.best_iter.append(0) if eval_ret[3]: # greater is better self.best_score.append(float(\"-inf\")) self.cmp_op.append(partial(self._gt_delta, delta=delta)) else: self.best_score.append(float(\"inf\")) self.cmp_op.append(partial(self._lt_delta, delta=delta)) def _final_iteration_check(self, *, env: CallbackEnv, metric_name: str, i: int) -> None: if env.iteration == env.end_iteration - 1: if self.verbose: best_score_str = \"\\t\".join([_format_eval_result(x, show_stdv=True) for x in self.best_score_list[i]]) _log_info( f\"Did not meet early stopping. Best iteration is:\\n[{self.best_iter[i] + 1}]\\t{best_score_str}\" ) if self.first_metric_only: _log_info(f\"Evaluated only: {metric_name}\") raise EarlyStopException(self.best_iter[i], self.best_score_list[i]) def __call__(self, env: CallbackEnv) -> None: if env.iteration == env.begin_iteration: self._init(env) if not self.enabled: return if env.evaluation_result_list is None:", "prev_chunk_id": "chunk_365", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_367", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "raise RuntimeError( \"early_stopping() callback enabled but no evaluation results found. This is a probably bug in LightGBM. \" \"Please report it at https://github.com/microsoft/LightGBM/issues\" ) # self.best_score_list is initialized to an empty list first_time_updating_best_score_list = self.best_score_list == [] for i in range(len(env.evaluation_result_list)): dataset_name, metric_name, metric_value, *_ = env.evaluation_result_list[i] if first_time_updating_best_score_list or self.cmp_op[i](metric_value, self.best_score[i]): self.best_score[i] = metric_value self.best_iter[i] = env.iteration if first_time_updating_best_score_list: self.best_score_list.append(env.evaluation_result_list) else: self.best_score_list[i] = env.evaluation_result_list if self.first_metric_only and self.first_metric != metric_name: continue # use only the first metric for early stopping if self._is_train_set( dataset_name=dataset_name, env=env, ): continue # train data for lgb.cv or sklearn wrapper (underlying lgb.train) elif env.iteration - self.best_iter[i] >= self.stopping_rounds: if self.verbose: eval_result_str = \"\\t\".join( [_format_eval_result(x, show_stdv=True) for x in self.best_score_list[i]] ) _log_info(f\"Early stopping, best iteration is:\\n[{self.best_iter[i] + 1}]\\t{eval_result_str}\") if self.first_metric_only: _log_info(f\"Evaluated only: {metric_name}\") raise EarlyStopException(self.best_iter[i], self.best_score_list[i]) self._final_iteration_check(env=env, metric_name=metric_name, i=i) def _should_enable_early_stopping(stopping_rounds: Any) -> bool: \"\"\"Check if early stopping should be activated. This function will evaluate to True if the early stopping callback should be activated (i.e. stopping_rounds > 0). It also provides an informative error if the type is not int. \"\"\" if not isinstance(stopping_rounds, int): raise TypeError(f\"early_stopping_round should be an integer. Got '{type(stopping_rounds).__name__}'\") return stopping_rounds > 0 [docs] def early_stopping( stopping_rounds: int, first_metric_only: bool = False, verbose: bool = True, min_delta: Union[float, List[float]] = 0.0, ) -> _EarlyStoppingCallback: \"\"\"Create a callback that activates early stopping. Activates early stopping. The model will train until the validation score doesn't improve by at least ``min_delta``. Validation score needs to improve at least every ``stopping_rounds`` round(s) to continue training. Requires at least one validation data and one metric. If there's more than one, will check all of them. But the training data is ignored anyway. To check only the first metric set ``first_metric_only`` to True. The index of iteration that has the best performance will be saved in the", "prev_chunk_id": "chunk_366", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_368", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/callback.html", "title": "Source code for lightgbm.callback", "page_title": "lightgbm.callback — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.callback", "content": "``best_iteration`` attribute of a model. .. note:: If using ``boosting_type=\"dart\"``, this callback has no effect and early stopping will not be performed. Parameters ---------- stopping_rounds : int The possible number of rounds without the trend occurrence. first_metric_only : bool, optional (default=False) Whether to use only the first metric for early stopping. verbose : bool, optional (default=True) Whether to log message with early stopping information. By default, standard output resource is used. Use ``register_logger()`` function to register a custom logger. min_delta : float or list of float, optional (default=0.0) Minimum improvement in score to keep training. If float, this single value is used for all metrics. If list, its length should match the total number of metrics. .. versionadded:: 4.0.0 Returns ------- callback : _EarlyStoppingCallback The callback that activates early stopping. \"\"\" return _EarlyStoppingCallback( stopping_rounds=stopping_rounds, first_metric_only=first_metric_only, verbose=verbose, min_delta=min_delta, )", "prev_chunk_id": "chunk_367", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_369", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "Source code for lightgbm.sklearn # coding: utf-8 \"\"\"Scikit-learn wrapper interface for LightGBM.\"\"\" import copy from inspect import signature from pathlib import Path from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union import numpy as np import scipy.sparse from .basic import ( _MULTICLASS_OBJECTIVES, Booster, Dataset, LightGBMError, _choose_param_value, _ConfigAliases, _LGBM_BoosterBestScoreType, _LGBM_CategoricalFeatureConfiguration, _LGBM_EvalFunctionResultType, _LGBM_FeatureNameConfiguration, _LGBM_GroupType, _LGBM_InitScoreType, _LGBM_LabelType, _LGBM_WeightType, _log_warning, ) from .callback import _EvalResultDict, record_evaluation from .compat import ( SKLEARN_INSTALLED, LGBMNotFittedError, _LGBMAssertAllFinite, _LGBMCheckClassificationTargets, _LGBMCheckSampleWeight, _LGBMClassifierBase, _LGBMComputeSampleWeight, _LGBMCpuCount, _LGBMLabelEncoder, _LGBMModelBase, _LGBMRegressorBase, _LGBMValidateData, _sklearn_version, pa_Table, pd_DataFrame, ) from .engine import train if TYPE_CHECKING: from .compat import _sklearn_Tags __all__ = [ \"LGBMClassifier\", \"LGBMModel\", \"LGBMRanker\", \"LGBMRegressor\", ] _LGBM_ScikitMatrixLike = Union[ List[Union[List[float], List[int]]], np.ndarray, pd_DataFrame, pa_Table, scipy.sparse.spmatrix, ] _LGBM_ScikitCustomObjectiveFunction = Union[ # f(labels, preds) Callable[ [Optional[np.ndarray], np.ndarray], Tuple[np.ndarray, np.ndarray], ], # f(labels, preds, weights) Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray]], Tuple[np.ndarray, np.ndarray], ], # f(labels, preds, weights, group) Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray], Optional[np.ndarray]], Tuple[np.ndarray, np.ndarray], ], ] _LGBM_ScikitCustomEvalFunction = Union[ # f(labels, preds) Callable[ [Optional[np.ndarray], np.ndarray], _LGBM_EvalFunctionResultType, ], Callable[ [Optional[np.ndarray], np.ndarray], List[_LGBM_EvalFunctionResultType], ], # f(labels, preds, weights) Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray]], _LGBM_EvalFunctionResultType, ], Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray]], List[_LGBM_EvalFunctionResultType], ], # f(labels, preds, weights, group) Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray], Optional[np.ndarray]], _LGBM_EvalFunctionResultType, ], Callable[ [Optional[np.ndarray], np.ndarray, Optional[np.ndarray], Optional[np.ndarray]], List[_LGBM_EvalFunctionResultType], ], ] _LGBM_ScikitEvalMetricType = Union[ str, _LGBM_ScikitCustomEvalFunction, List[Union[str, _LGBM_ScikitCustomEvalFunction]], ] _LGBM_ScikitValidSet = Tuple[_LGBM_ScikitMatrixLike, _LGBM_LabelType] def _get_group_from_constructed_dataset(dataset: Dataset) -> Optional[np.ndarray]: group = dataset.get_group() error_msg = ( \"Estimators in lightgbm.sklearn should only retrieve query groups from a constructed Dataset. \" \"If you're seeing this message, it's a bug in lightgbm. Please report it at https://github.com/microsoft/LightGBM/issues.\" ) assert group is None or isinstance(group, np.ndarray), error_msg return group def _get_label_from_constructed_dataset(dataset: Dataset) -> np.ndarray: label = dataset.get_label() error_msg = ( \"Estimators in lightgbm.sklearn should only retrieve labels from a constructed Dataset. \" \"If you're seeing this message, it's a bug in lightgbm. Please report it at", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_370", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "https://github.com/microsoft/LightGBM/issues.\" ) assert isinstance(label, np.ndarray), error_msg return label def _get_weight_from_constructed_dataset(dataset: Dataset) -> Optional[np.ndarray]: weight = dataset.get_weight() error_msg = ( \"Estimators in lightgbm.sklearn should only retrieve weights from a constructed Dataset. \" \"If you're seeing this message, it's a bug in lightgbm. Please report it at https://github.com/microsoft/LightGBM/issues.\" ) assert weight is None or isinstance(weight, np.ndarray), error_msg return weight class _ObjectiveFunctionWrapper: \"\"\"Proxy class for objective function.\"\"\" def __init__(self, func: _LGBM_ScikitCustomObjectiveFunction): \"\"\"Construct a proxy class. This class transforms objective function to match objective function with signature ``new_func(preds, dataset)`` as expected by ``lightgbm.engine.train``. Parameters ---------- func : callable Expects a callable with following signatures: ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or ``func(y_true, y_pred, weight, group)`` and returns (grad, hess): y_true : numpy 1-D array of shape = [n_samples] The target values. y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The predicted values. Predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task. weight : numpy 1-D array of shape = [n_samples] The weight of samples. Weights should be non-negative. group : numpy 1-D array Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape [n_samples, n_classes] (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of y_pred for each sample point. hess : numpy 1-D array", "prev_chunk_id": "chunk_369", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_371", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of y_pred for each sample point. .. note:: For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. \"\"\" self.func = func def __call__( self, preds: np.ndarray, dataset: Dataset, ) -> Tuple[np.ndarray, np.ndarray]: \"\"\"Call passed function with appropriate arguments. Parameters ---------- preds : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The predicted values. dataset : Dataset The training dataset. Returns ------- grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of preds for each sample point. hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of preds for each sample point. \"\"\" labels = _get_label_from_constructed_dataset(dataset) argc = len(signature(self.func).parameters) if argc == 2: grad, hess = self.func(labels, preds) # type: ignore[call-arg] return grad, hess weight = _get_weight_from_constructed_dataset(dataset) if argc == 3: grad, hess = self.func(labels, preds, weight) # type: ignore[call-arg] return grad, hess if argc == 4: group = _get_group_from_constructed_dataset(dataset) return self.func(labels, preds, weight, group) # type: ignore[call-arg] raise TypeError(f\"Self-defined objective function should have 2, 3 or 4 arguments, got {argc}\") class _EvalFunctionWrapper: \"\"\"Proxy class for evaluation function.\"\"\" def __init__(self, func: _LGBM_ScikitCustomEvalFunction): \"\"\"Construct a proxy class. This class transforms evaluation function to match evaluation function with", "prev_chunk_id": "chunk_370", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_372", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "signature ``new_func(preds, dataset)`` as expected by ``lightgbm.engine.train``. Parameters ---------- func : callable Expects a callable with following signatures: ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or ``func(y_true, y_pred, weight, group)`` and returns (eval_name, eval_result, is_higher_better) or list of (eval_name, eval_result, is_higher_better): y_true : numpy 1-D array of shape = [n_samples] The target values. y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array shape = [n_samples, n_classes] (for multi-class task) The predicted values. In case of custom ``objective``, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task in this case. weight : numpy 1-D array of shape = [n_samples] The weight of samples. Weights should be non-negative. group : numpy 1-D array Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. \"\"\" self.func = func def __call__( self, preds: np.ndarray, dataset: Dataset, ) -> Union[_LGBM_EvalFunctionResultType, List[_LGBM_EvalFunctionResultType]]: \"\"\"Call passed function with appropriate arguments. Parameters ---------- preds : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The predicted values. dataset : Dataset The training dataset. Returns ------- eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. \"\"\" labels = _get_label_from_constructed_dataset(dataset)", "prev_chunk_id": "chunk_371", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_373", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "argc = len(signature(self.func).parameters) if argc == 2: return self.func(labels, preds) # type: ignore[call-arg] weight = _get_weight_from_constructed_dataset(dataset) if argc == 3: return self.func(labels, preds, weight) # type: ignore[call-arg] if argc == 4: group = _get_group_from_constructed_dataset(dataset) return self.func(labels, preds, weight, group) # type: ignore[call-arg] raise TypeError(f\"Self-defined eval function should have 2, 3 or 4 arguments, got {argc}\") # documentation templates for LGBMModel methods are shared between the classes in # this module and those in the ``dask`` module _lgbmmodel_doc_fit = \"\"\" Build a gradient boosting model from the training set (X, y). Parameters ---------- X : {X_shape} Input feature matrix. y : {y_shape} The target values (class labels in classification, real numbers in regression). sample_weight : {sample_weight_shape} Weights of training data. Weights should be non-negative. init_score : {init_score_shape} Init score of training data. group : {group_shape} Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. eval_set : list or None, optional (default=None) A list of (X, y) tuple pairs to use as validation sets. eval_names : list of str, or None, optional (default=None) Names of eval_set. eval_sample_weight : {eval_sample_weight_shape} Weights of eval data. Weights should be non-negative. eval_class_weight : list or None, optional (default=None) Class weights of eval data. eval_init_score : {eval_init_score_shape} Init score of eval data. eval_group : {eval_group_shape} Group data of eval data. eval_metric : str, callable, list or None, optional (default=None) If str, it should be a built-in evaluation metric to use. If callable, it should be a custom evaluation metric, see note below for more details. If", "prev_chunk_id": "chunk_372", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_374", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both. In either case, the ``metric`` from the model parameters will be evaluated and used as well. Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker. feature_name : list of str, or 'auto', optional (default='auto') Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. categorical_feature : list of str or int, or 'auto', optional (default='auto') Categorical features. If list of int, interpreted as indices. If list of str, interpreted as feature names (need to specify ``feature_name`` as well). If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647). Large values could be memory consuming. Consider using consecutive integers starting from zero. All negative values in categorical features will be treated as missing values. The output cannot be monotonically constrained with respect to a categorical feature. Floating point numbers in categorical features will be rounded towards 0. callbacks : list of callable, or None, optional (default=None) List of callback functions that are applied at each iteration. See Callbacks in Python API for more information. init_model : str, pathlib.Path, Booster, LGBMModel or None, optional (default=None) Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training. Returns ------- self : LGBMModel Returns self. \"\"\" _lgbmmodel_doc_custom_eval_note = \"\"\" Note ---- Custom eval function expects a callable with following signatures: ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or ``func(y_true, y_pred, weight, group)`` and returns (eval_name, eval_result, is_higher_better) or list of (eval_name, eval_result, is_higher_better): y_true : numpy 1-D array of shape = [n_samples] The target values. y_pred : numpy 1-D array of shape = [n_samples] or numpy", "prev_chunk_id": "chunk_373", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_375", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "2-D array of shape = [n_samples, n_classes] (for multi-class task) The predicted values. In case of custom ``objective``, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task in this case. weight : numpy 1-D array of shape = [n_samples] The weight of samples. Weights should be non-negative. group : numpy 1-D array Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. \"\"\" _lgbmmodel_doc_predict = \"\"\" {description} Parameters ---------- X : {X_shape} Input features matrix. raw_score : bool, optional (default=False) Whether to predict raw scores. start_iteration : int, optional (default=0) Start index of the iteration to predict. If <= 0, starts from the first iteration. num_iteration : int or None, optional (default=None) Total number of iterations used in the prediction. If None, if the best iteration exists and start_iteration <= 0, the best iteration is used; otherwise, all iterations from ``start_iteration`` are used (no limits). If <= 0, all iterations from ``start_iteration`` are used (no limits). pred_leaf : bool, optional (default=False) Whether to predict leaf index. pred_contrib : bool, optional (default=False) Whether to predict feature contributions. .. note:: If you want to get more explanations for your model's predictions using SHAP values, like SHAP interaction values, you can install the shap package (https://github.com/slundberg/shap). Note that unlike the shap package, with", "prev_chunk_id": "chunk_374", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_376", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "``pred_contrib`` we return a matrix with an extra column, where the last column is the expected value. validate_features : bool, optional (default=False) If True, ensure that the features used to predict match the ones used to train. Used only if data is pandas DataFrame. **kwargs Other parameters for the prediction. Returns ------- {output_name} : {predicted_result_shape} The predicted values. X_leaves : {X_leaves_shape} If ``pred_leaf=True``, the predicted leaf of every tree for each sample. X_SHAP_values : {X_SHAP_values_shape} If ``pred_contrib=True``, the feature contributions for each sample. \"\"\" def _extract_evaluation_meta_data( *, collection: Optional[Union[Dict[Any, Any], List[Any]]], name: str, i: int, ) -> Optional[Any]: \"\"\"Try to extract the ith element of one of the ``eval_*`` inputs.\"\"\" if collection is None: return None elif isinstance(collection, list): # It's possible, for example, to pass 3 eval sets through `eval_set`, # but only 1 init_score through `eval_init_score`. # # This if-else accounts for that possibility. if len(collection) > i: return collection[i] else: return None elif isinstance(collection, dict): return collection.get(i, None) else: raise TypeError(f\"{name} should be dict or list\") [docs] class LGBMModel(_LGBMModelBase): \"\"\"Implementation of the scikit-learn API for LightGBM.\"\"\" [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[Dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, np.random.Generator]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", **kwargs: Any, ): r\"\"\"Construct a gradient boosting model. Parameters ---------- boosting_type : str, optional (default='gbdt') 'gbdt', traditional Gradient Boosting Decision Tree. 'dart', Dropouts meet Multiple Additive Regression Trees. 'rf', Random Forest. num_leaves : int, optional (default=31)", "prev_chunk_id": "chunk_375", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_377", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "Maximum tree leaves for base learners. max_depth : int, optional (default=-1) Maximum tree depth for base learners, <=0 means no limit. If setting this to a positive value, consider also changing ``num_leaves`` to ``<= 2^max_depth``. learning_rate : float, optional (default=0.1) Boosting learning rate. You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate in training using ``reset_parameter`` callback. Note, that this will ignore the ``learning_rate`` argument in training. n_estimators : int, optional (default=100) Number of boosted trees to fit. subsample_for_bin : int, optional (default=200000) Number of samples for constructing bins. objective : str, callable or None, optional (default=None) Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below). Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker. class_weight : dict, 'balanced' or None, optional (default=None) Weights associated with classes in the form ``{class_label: weight}``. Use this parameter only for multi-class classification task; for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters. Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities. You may want to consider performing probability calibration (https://scikit-learn.org/stable/modules/calibration.html) of your model. The 'balanced' mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method) if ``sample_weight`` is specified. min_split_gain : float, optional (default=0.) Minimum loss reduction required to make a further partition on a leaf node of the tree. min_child_weight : float, optional (default=1e-3) Minimum sum of instance weight (Hessian) needed in a child (leaf). min_child_samples : int, optional (default=20) Minimum number of data needed", "prev_chunk_id": "chunk_376", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_378", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "in a child (leaf). subsample : float, optional (default=1.) Subsample ratio of the training instance. subsample_freq : int, optional (default=0) Frequency of subsample, <=0 means no enable. colsample_bytree : float, optional (default=1.) Subsample ratio of columns when constructing each tree. reg_alpha : float, optional (default=0.) L1 regularization term on weights. reg_lambda : float, optional (default=0.) L2 regularization term on weights. random_state : int, RandomState object or None, optional (default=None) Random number seed. If int, this number is used to seed the C++ code. If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code. If None, default seeds in C++ code are used. n_jobs : int or None, optional (default=None) Number of parallel threads to use for training (can be changed at prediction time by passing it as an extra keyword argument). For better performance, it is recommended to set this to the number of physical cores in the CPU. Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds to using the number of physical cores in the system (its correct detection requires either the ``joblib`` or the ``psutil`` util libraries to be installed). .. versionchanged:: 4.0.0 importance_type : str, optional (default='split') The type of feature importance to be filled into ``feature_importances_``. If 'split', result contains numbers of times the feature is used in a model. If 'gain', result contains total gains of splits which use the feature. **kwargs Other parameters for the model. Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters. .. warning:: \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues. Note ----", "prev_chunk_id": "chunk_377", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_379", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "A custom objective function can be provided for the ``objective`` parameter. In this case, it should have the signature ``objective(y_true, y_pred) -> grad, hess``, ``objective(y_true, y_pred, weight) -> grad, hess`` or ``objective(y_true, y_pred, weight, group) -> grad, hess``: y_true : numpy 1-D array of shape = [n_samples] The target values. y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The predicted values. Predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task. weight : numpy 1-D array of shape = [n_samples] The weight of samples. Weights should be non-negative. group : numpy 1-D array Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of y_pred for each sample point. hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of y_pred for each sample point. For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. \"\"\" if not SKLEARN_INSTALLED: raise LightGBMError( \"scikit-learn is required for lightgbm.sklearn. \"", "prev_chunk_id": "chunk_378", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_380", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "\"You must install scikit-learn and restart your session to use this module.\" ) self.boosting_type = boosting_type self.objective = objective self.num_leaves = num_leaves self.max_depth = max_depth self.learning_rate = learning_rate self.n_estimators = n_estimators self.subsample_for_bin = subsample_for_bin self.min_split_gain = min_split_gain self.min_child_weight = min_child_weight self.min_child_samples = min_child_samples self.subsample = subsample self.subsample_freq = subsample_freq self.colsample_bytree = colsample_bytree self.reg_alpha = reg_alpha self.reg_lambda = reg_lambda self.random_state = random_state self.n_jobs = n_jobs self.importance_type = importance_type self._Booster: Optional[Booster] = None self._evals_result: _EvalResultDict = {} self._best_score: _LGBM_BoosterBestScoreType = {} self._best_iteration: int = -1 self._other_params: Dict[str, Any] = {} self._objective = objective self.class_weight = class_weight self._class_weight: Optional[Union[Dict, str]] = None self._class_map: Optional[Dict[int, int]] = None self._n_features: int = -1 self._n_features_in: int = -1 self._classes: Optional[np.ndarray] = None self._n_classes: int = -1 self.set_params(**kwargs) # scikit-learn 1.6 introduced an __sklearn__tags() method intended to replace _more_tags(). # _more_tags() can be removed whenever lightgbm's minimum supported scikit-learn version # is >=1.6. # ref: https://github.com/microsoft/LightGBM/pull/6651 def _more_tags(self) -> Dict[str, Any]: check_sample_weight_str = ( \"In LightGBM, setting a sample's weight to 0 can produce a different result than omitting the sample. \" \"Such samples intentionally still affect count-based measures like 'min_data_in_leaf' \" \"(https://github.com/microsoft/LightGBM/issues/5626#issuecomment-1712706678) and the estimated distribution \" \"of features for Dataset construction (see https://github.com/microsoft/LightGBM/issues/5553).\" ) # \"check_sample_weight_equivalence\" can be removed when lightgbm's # minimum supported scikit-learn version is at least 1.6 # ref: https://github.com/scikit-learn/scikit-learn/pull/30137 return { \"allow_nan\": True, \"X_types\": [\"2darray\", \"sparse\", \"1dlabels\"], \"_xfail_checks\": { \"check_no_attributes_set_in_init\": \"scikit-learn incorrectly asserts that private attributes \" \"cannot be set in __init__: \" \"(see https://github.com/microsoft/LightGBM/issues/2628)\", \"check_sample_weight_equivalence\": check_sample_weight_str, \"check_sample_weight_equivalence_on_dense_data\": check_sample_weight_str, \"check_sample_weight_equivalence_on_sparse_data\": check_sample_weight_str, }, } @staticmethod def _update_sklearn_tags_from_dict( *, tags: \"_sklearn_Tags\", tags_dict: Dict[str, Any], ) -> \"_sklearn_Tags\": \"\"\"Update ``sklearn.utils.Tags`` inherited from ``scikit-learn`` base classes. ``scikit-learn`` 1.6 introduced a dataclass-based interface for estimator tags. ref: https://github.com/scikit-learn/scikit-learn/pull/29677 This method handles updating that instance based on the value in ``self._more_tags()``. \"\"\" tags.input_tags.allow_nan = tags_dict[\"allow_nan\"] tags.input_tags.sparse =", "prev_chunk_id": "chunk_379", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_381", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "\"sparse\" in tags_dict[\"X_types\"] tags.target_tags.one_d_labels = \"1dlabels\" in tags_dict[\"X_types\"] return tags def __sklearn_tags__(self) -> Optional[\"_sklearn_Tags\"]: # _LGBMModelBase.__sklearn_tags__() cannot be called unconditionally, # because that method isn't defined for scikit-learn<1.6 if not hasattr(_LGBMModelBase, \"__sklearn_tags__\"): err_msg = ( \"__sklearn_tags__() should not be called when using scikit-learn<1.6. \" f\"Detected version: {_sklearn_version}\" ) raise AttributeError(err_msg) # take whatever tags are provided by BaseEstimator, then modify # them with LightGBM-specific values return self._update_sklearn_tags_from_dict( tags=super().__sklearn_tags__(), tags_dict=self._more_tags(), ) def __sklearn_is_fitted__(self) -> bool: return getattr(self, \"fitted_\", False) [docs] def get_params(self, deep: bool = True) -> Dict[str, Any]: \"\"\"Get parameters for this estimator. Parameters ---------- deep : bool, optional (default=True) If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : dict Parameter names mapped to their values. \"\"\" # Based on: https://github.com/dmlc/xgboost/blob/bd92b1c9c0db3e75ec3dfa513e1435d518bb535d/python-package/xgboost/sklearn.py#L941 # which was based on: https://stackoverflow.com/questions/59248211 # # `get_params()` flows like this: # # 0. Get parameters in subclass (self.__class__) first, by using inspect. # 1. Get parameters in all parent classes (especially `LGBMModel`). # 2. Get whatever was passed via `**kwargs`. # 3. Merge them. # # This needs to accommodate being called recursively in the following # inheritance graphs (and similar for classification and ranking): # # DaskLGBMRegressor -> LGBMRegressor -> LGBMModel -> BaseEstimator # (custom subclass) -> LGBMRegressor -> LGBMModel -> BaseEstimator # LGBMRegressor -> LGBMModel -> BaseEstimator # (custom subclass) -> LGBMModel -> BaseEstimator # LGBMModel -> BaseEstimator # params = super().get_params(deep=deep) cp = copy.copy(self) # If the immediate parent defines get_params(), use that. if callable(getattr(cp.__class__.__bases__[0], \"get_params\", None)): cp.__class__ = cp.__class__.__bases__[0] # Otherwise, skip it and assume the next class will have it. # This is here primarily for cases where the first class in MRO is a scikit-learn mixin. else: cp.__class__ = cp.__class__.__bases__[1] params.update(cp.__class__.get_params(cp, deep)) params.update(self._other_params) return params [docs] def set_params(self, **params: Any) ->", "prev_chunk_id": "chunk_380", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_382", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "\"LGBMModel\": \"\"\"Set the parameters of this estimator. Parameters ---------- **params Parameter names with their new values. Returns ------- self : object Returns self. \"\"\" for key, value in params.items(): setattr(self, key, value) if hasattr(self, f\"_{key}\"): setattr(self, f\"_{key}\", value) self._other_params[key] = value return self def _process_params(self, stage: str) -> Dict[str, Any]: \"\"\"Process the parameters of this estimator based on its type, parameter aliases, etc. Parameters ---------- stage : str Name of the stage (can be ``fit`` or ``predict``) this method is called from. Returns ------- processed_params : dict Processed parameter names mapped to their values. \"\"\" assert stage in {\"fit\", \"predict\"} params = self.get_params() params.pop(\"objective\", None) for alias in _ConfigAliases.get(\"objective\"): if alias in params: obj = params.pop(alias) _log_warning(f\"Found '{alias}' in params. Will use it instead of 'objective' argument\") if stage == \"fit\": self._objective = obj if stage == \"fit\": if self._objective is None: if isinstance(self, LGBMRegressor): self._objective = \"regression\" elif isinstance(self, LGBMClassifier): if self._n_classes > 2: self._objective = \"multiclass\" else: self._objective = \"binary\" elif isinstance(self, LGBMRanker): self._objective = \"lambdarank\" else: raise ValueError(\"Unknown LGBMModel type.\") if callable(self._objective): if stage == \"fit\": params[\"objective\"] = _ObjectiveFunctionWrapper(self._objective) else: params[\"objective\"] = \"None\" else: params[\"objective\"] = self._objective params.pop(\"importance_type\", None) params.pop(\"n_estimators\", None) params.pop(\"class_weight\", None) if isinstance(params[\"random_state\"], np.random.RandomState): params[\"random_state\"] = params[\"random_state\"].randint(np.iinfo(np.int32).max) elif isinstance(params[\"random_state\"], np.random.Generator): params[\"random_state\"] = int(params[\"random_state\"].integers(np.iinfo(np.int32).max)) if self._n_classes > 2: for alias in _ConfigAliases.get(\"num_class\"): params.pop(alias, None) params[\"num_class\"] = self._n_classes if hasattr(self, \"_eval_at\"): eval_at = self._eval_at for alias in _ConfigAliases.get(\"eval_at\"): if alias in params: _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\") eval_at = params.pop(alias) params[\"eval_at\"] = eval_at # register default metric for consistency with callable eval_metric case original_metric = self._objective if isinstance(self._objective, str) else None if original_metric is None: # try to deduce from class instance if isinstance(self, LGBMRegressor): original_metric = \"l2\" elif isinstance(self, LGBMClassifier): original_metric = \"multi_logloss\" if self._n_classes > 2 else \"binary_logloss\" elif", "prev_chunk_id": "chunk_381", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_383", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "isinstance(self, LGBMRanker): original_metric = \"ndcg\" # overwrite default metric by explicitly set metric params = _choose_param_value(\"metric\", params, original_metric) # use joblib conventions for negative n_jobs, just like scikit-learn # at predict time, this is handled later due to the order of parameter updates if stage == \"fit\": params = _choose_param_value(\"num_threads\", params, self.n_jobs) params[\"num_threads\"] = self._process_n_jobs(params[\"num_threads\"]) return params def _process_n_jobs(self, n_jobs: Optional[int]) -> int: \"\"\"Convert special values of n_jobs to their actual values according to the formulas that apply. Parameters ---------- n_jobs : int or None The original value of n_jobs, potentially having special values such as 'None' or negative integers. Returns ------- n_jobs : int The value of n_jobs with special values converted to actual number of threads. \"\"\" if n_jobs is None: n_jobs = _LGBMCpuCount(only_physical_cores=True) elif n_jobs < 0: n_jobs = max(_LGBMCpuCount(only_physical_cores=False) + 1 + n_jobs, 1) return n_jobs [docs] def fit( self, X: _LGBM_ScikitMatrixLike, y: _LGBM_LabelType, sample_weight: Optional[_LGBM_WeightType] = None, init_score: Optional[_LGBM_InitScoreType] = None, group: Optional[_LGBM_GroupType] = None, eval_set: Optional[List[_LGBM_ScikitValidSet]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_LGBM_WeightType]] = None, eval_class_weight: Optional[List[float]] = None, eval_init_score: Optional[List[_LGBM_InitScoreType]] = None, eval_group: Optional[List[_LGBM_GroupType]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", callbacks: Optional[List[Callable]] = None, init_model: Optional[Union[str, Path, Booster, \"LGBMModel\"]] = None, ) -> \"LGBMModel\": \"\"\"Docstring is set after definition, using a template.\"\"\" params = self._process_params(stage=\"fit\") # Do not modify original args in fit function # Refer to https://github.com/microsoft/LightGBM/pull/2619 eval_metric_list: List[Union[str, _LGBM_ScikitCustomEvalFunction]] if eval_metric is None: eval_metric_list = [] elif isinstance(eval_metric, list): eval_metric_list = copy.deepcopy(eval_metric) else: eval_metric_list = [copy.deepcopy(eval_metric)] # Separate built-in from callable evaluation metrics eval_metrics_callable = [_EvalFunctionWrapper(f) for f in eval_metric_list if callable(f)] eval_metrics_builtin = [m for m in eval_metric_list if isinstance(m, str)] # concatenate metric from params (or default if not provided in params) and eval_metric params[\"metric\"] = [params[\"metric\"]] if isinstance(params[\"metric\"], (str,", "prev_chunk_id": "chunk_382", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_384", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "type(None))) else params[\"metric\"] params[\"metric\"] = [e for e in eval_metrics_builtin if e not in params[\"metric\"]] + params[\"metric\"] params[\"metric\"] = [metric for metric in params[\"metric\"] if metric is not None] if not isinstance(X, (pd_DataFrame, pa_Table)): _X, _y = _LGBMValidateData( self, X, y, reset=True, # allow any input type (this validation is done further down, in lgb.Dataset()) accept_sparse=True, # do not raise an error if Inf of NaN values are found (LightGBM handles these internally) ensure_all_finite=False, # raise an error on 0-row and 1-row inputs ensure_min_samples=2, ) if sample_weight is not None: sample_weight = _LGBMCheckSampleWeight(sample_weight, _X) else: _X, _y = X, y # for other data types, setting n_features_in_ is handled by _LGBMValidateData() in the branch above self.n_features_in_ = _X.shape[1] if self._class_weight is None: self._class_weight = self.class_weight if self._class_weight is not None: class_sample_weight = _LGBMComputeSampleWeight(self._class_weight, y) if sample_weight is None or len(sample_weight) == 0: sample_weight = class_sample_weight else: sample_weight = np.multiply(sample_weight, class_sample_weight) train_set = Dataset( data=_X, label=_y, weight=sample_weight, group=group, init_score=init_score, categorical_feature=categorical_feature, feature_name=feature_name, params=params, ) valid_sets: List[Dataset] = [] if eval_set is not None: if isinstance(eval_set, tuple): eval_set = [eval_set] for i, valid_data in enumerate(eval_set): # reduce cost for prediction training data if valid_data[0] is X and valid_data[1] is y: valid_set = train_set else: valid_weight = _extract_evaluation_meta_data( collection=eval_sample_weight, name=\"eval_sample_weight\", i=i, ) valid_class_weight = _extract_evaluation_meta_data( collection=eval_class_weight, name=\"eval_class_weight\", i=i, ) if valid_class_weight is not None: if isinstance(valid_class_weight, dict) and self._class_map is not None: valid_class_weight = {self._class_map[k]: v for k, v in valid_class_weight.items()} valid_class_sample_weight = _LGBMComputeSampleWeight(valid_class_weight, valid_data[1]) if valid_weight is None or len(valid_weight) == 0: valid_weight = valid_class_sample_weight else: valid_weight = np.multiply(valid_weight, valid_class_sample_weight) valid_init_score = _extract_evaluation_meta_data( collection=eval_init_score, name=\"eval_init_score\", i=i, ) valid_group = _extract_evaluation_meta_data( collection=eval_group, name=\"eval_group\", i=i, ) valid_set = Dataset( data=valid_data[0], label=valid_data[1], weight=valid_weight, group=valid_group, init_score=valid_init_score, categorical_feature=\"auto\", params=params, ) valid_sets.append(valid_set) if isinstance(init_model, LGBMModel): init_model = init_model.booster_ if callbacks is None: callbacks = [] else: callbacks = copy.copy(callbacks)", "prev_chunk_id": "chunk_383", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_385", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "# don't use deepcopy here to allow non-serializable objects evals_result: _EvalResultDict = {} callbacks.append(record_evaluation(evals_result)) self._Booster = train( params=params, train_set=train_set, num_boost_round=self.n_estimators, valid_sets=valid_sets, valid_names=eval_names, feval=eval_metrics_callable, # type: ignore[arg-type] init_model=init_model, callbacks=callbacks, ) # This populates the property self.n_features_, the number of features in the fitted model, # and so should only be set after fitting. # # The related property self._n_features_in, which populates self.n_features_in_, # is set BEFORE fitting. self._n_features = self._Booster.num_feature() self._evals_result = evals_result self._best_iteration = self._Booster.best_iteration self._best_score = self._Booster.best_score self.fitted_ = True # free dataset self._Booster.free_dataset() del train_set, valid_sets return self fit.__doc__ = ( _lgbmmodel_doc_fit.format( X_shape=\"numpy array, pandas DataFrame, pyarrow Table, scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\", y_shape=\"numpy array, pandas DataFrame, pandas Series, list of int or float, pyarrow Array, pyarrow ChunkedArray of shape = [n_samples]\", sample_weight_shape=\"numpy array, pandas Series, list of int or float, pyarrow Array, pyarrow ChunkedArray of shape = [n_samples] or None, optional (default=None)\", init_score_shape=\"numpy array, pandas DataFrame, pandas Series, list of int or float, list of lists, pyarrow Array, pyarrow ChunkedArray, pyarrow Table of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)\", group_shape=\"numpy array, pandas Series, pyarrow Array, pyarrow ChunkedArray, list of int or float, or None, optional (default=None)\", eval_sample_weight_shape=\"list of array (same types as ``sample_weight`` supports), or None, optional (default=None)\", eval_init_score_shape=\"list of array (same types as ``init_score`` supports), or None, optional (default=None)\", eval_group_shape=\"list of array (same types as ``group`` supports), or None, optional (default=None)\", ) + \"\\n\\n\" + _lgbmmodel_doc_custom_eval_note ) [docs] def predict( self, X: _LGBM_ScikitMatrixLike, raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ): \"\"\"Docstring is set after definition, using a template.\"\"\" if not", "prev_chunk_id": "chunk_384", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_386", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"Estimator not fitted, call fit before exploiting the model.\") if not isinstance(X, (pd_DataFrame, pa_Table)): X = _LGBMValidateData( self, X, # 'y' being omitted = run scikit-learn's check_array() instead of check_X_y() # # Prevent scikit-learn from deleting or modifying attributes like 'feature_names_in_' and 'n_features_in_'. # These shouldn't be changed at predict() time. reset=False, # allow any input type (this validation is done further down, in lgb.Dataset()) accept_sparse=True, # do not raise an error if Inf of NaN values are found (LightGBM handles these internally) ensure_all_finite=False, # raise an error on 0-row inputs ensure_min_samples=1, ) # retrieve original params that possibly can be used in both training and prediction # and then overwrite them (considering aliases) with params that were passed directly in prediction predict_params = self._process_params(stage=\"predict\") for alias in _ConfigAliases.get_by_alias( \"data\", \"X\", \"raw_score\", \"start_iteration\", \"num_iteration\", \"pred_leaf\", \"pred_contrib\", *kwargs.keys(), ): predict_params.pop(alias, None) predict_params.update(kwargs) # number of threads can have values with special meaning which is only applied # in the scikit-learn interface, these should not reach the c++ side as-is predict_params = _choose_param_value(\"num_threads\", predict_params, self.n_jobs) predict_params[\"num_threads\"] = self._process_n_jobs(predict_params[\"num_threads\"]) return self._Booster.predict( # type: ignore[union-attr] X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **predict_params, ) predict.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted value for each sample.\", X_shape=\"numpy array, pandas DataFrame, scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\", output_name=\"predicted_result\", predicted_result_shape=\"array-like of shape = [n_samples] or shape = [n_samples, n_classes]\", X_leaves_shape=\"array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\", X_SHAP_values_shape=\"array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\", ) @property def n_features_(self) -> int: \"\"\":obj:`int`: The number of features of fitted model.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No n_features found. Need to call fit beforehand.\") return self._n_features @property def n_features_in_(self) ->", "prev_chunk_id": "chunk_385", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_387", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "int: \"\"\":obj:`int`: The number of features of fitted model.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No n_features_in found. Need to call fit beforehand.\") return self._n_features_in @n_features_in_.setter def n_features_in_(self, value: int) -> None: \"\"\"Set number of features found in passed-in dataset. Starting with ``scikit-learn`` 1.6, ``scikit-learn`` expects to be able to directly set this property in functions like ``validate_data()``. .. note:: Do not call ``estimator.n_features_in_ = some_int`` or anything else that invokes this method. It is only here for compatibility with ``scikit-learn`` validation functions used internally in ``lightgbm``. \"\"\" self._n_features_in = value @property def best_score_(self) -> _LGBM_BoosterBestScoreType: \"\"\":obj:`dict`: The best score of fitted model.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No best_score found. Need to call fit beforehand.\") return self._best_score @property def best_iteration_(self) -> int: \"\"\":obj:`int`: The best iteration of fitted model if ``early_stopping()`` callback has been specified.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError( \"No best_iteration found. Need to call fit with early_stopping callback beforehand.\" ) return self._best_iteration @property def objective_(self) -> Union[str, _LGBM_ScikitCustomObjectiveFunction]: \"\"\":obj:`str` or :obj:`callable`: The concrete objective used while fitting this model.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No objective found. Need to call fit beforehand.\") return self._objective # type: ignore[return-value] @property def n_estimators_(self) -> int: \"\"\":obj:`int`: True number of boosting iterations performed. This might be less than parameter ``n_estimators`` if early stopping was enabled or if boosting stopped early due to limits on complexity like ``min_gain_to_split``. .. versionadded:: 4.0.0 \"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No n_estimators found. Need to call fit beforehand.\") return self._Booster.current_iteration() # type: ignore @property def n_iter_(self) -> int: \"\"\":obj:`int`: True number of boosting iterations performed. This might be less than parameter ``n_estimators`` if early stopping was enabled or if boosting stopped early due to limits on complexity like ``min_gain_to_split``. .. versionadded:: 4.0.0 \"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No n_iter found. Need to call fit beforehand.\") return self._Booster.current_iteration() # type: ignore", "prev_chunk_id": "chunk_386", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_388", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "@property def booster_(self) -> Booster: \"\"\"Booster: The underlying Booster of this model.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No booster found. Need to call fit beforehand.\") return self._Booster # type: ignore[return-value] @property def evals_result_(self) -> _EvalResultDict: \"\"\":obj:`dict`: The evaluation results if validation sets have been specified.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No results found. Need to call fit with eval_set beforehand.\") return self._evals_result @property def feature_importances_(self) -> np.ndarray: \"\"\":obj:`array` of shape = [n_features]: The feature importances (the higher, the more important). .. note:: ``importance_type`` attribute is passed to the function to configure the type of importance values to be extracted. \"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No feature_importances found. Need to call fit beforehand.\") return self._Booster.feature_importance(importance_type=self.importance_type) # type: ignore[union-attr] @property def feature_name_(self) -> List[str]: \"\"\":obj:`list` of shape = [n_features]: The names of features. .. note:: If input does not contain feature names, they will be added during fitting in the format ``Column_0``, ``Column_1``, ..., ``Column_N``. \"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No feature_name found. Need to call fit beforehand.\") return self._Booster.feature_name() # type: ignore[union-attr] @property def feature_names_in_(self) -> np.ndarray: \"\"\":obj:`array` of shape = [n_features]: scikit-learn compatible version of ``.feature_name_``. .. versionadded:: 4.5.0 \"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No feature_names_in_ found. Need to call fit beforehand.\") return np.array(self.feature_name_) @feature_names_in_.deleter def feature_names_in_(self) -> None: \"\"\"Intercept calls to delete ``feature_names_in_``. Some code paths in ``scikit-learn`` try to delete the ``feature_names_in_`` attribute on estimators when a new training dataset that doesn't have features is passed. LightGBM automatically assigns feature names to such datasets (like ``Column_0``, ``Column_1``, etc.) and so does not want that behavior. However, that behavior is coupled to ``scikit-learn`` automatically updating ``n_features_in_`` in those same code paths, which is necessary for compliance with its API (via argument ``reset`` to functions like ``validate_data()`` and ``check_array()``). .. note:: Do not call ``del estimator.feature_names_in_`` or anything else that invokes this", "prev_chunk_id": "chunk_387", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_389", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "method. It is only here for compatibility with ``scikit-learn`` validation functions used internally in ``lightgbm``. \"\"\" pass [docs] class LGBMRegressor(_LGBMRegressorBase, LGBMModel): \"\"\"LightGBM regressor.\"\"\" # NOTE: all args from LGBMModel.__init__() are intentionally repeated here for # docs, help(), and tab completion. [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[Dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, np.random.Generator]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", **kwargs: Any, ) -> None: super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) __init__.__doc__ = LGBMModel.__init__.__doc__ def _more_tags(self) -> Dict[str, Any]: # handle the case where RegressorMixin possibly provides _more_tags() if callable(getattr(_LGBMRegressorBase, \"_more_tags\", None)): tags = _LGBMRegressorBase._more_tags(self) else: tags = {} # override those with LightGBM-specific preferences tags.update(LGBMModel._more_tags(self)) return tags def __sklearn_tags__(self) -> \"_sklearn_Tags\": return super().__sklearn_tags__() [docs] def fit( # type: ignore[override] self, X: _LGBM_ScikitMatrixLike, y: _LGBM_LabelType, sample_weight: Optional[_LGBM_WeightType] = None, init_score: Optional[_LGBM_InitScoreType] = None, eval_set: Optional[List[_LGBM_ScikitValidSet]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_LGBM_WeightType]] = None, eval_init_score: Optional[List[_LGBM_InitScoreType]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", callbacks: Optional[List[Callable]] = None, init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None, ) -> \"LGBMRegressor\": \"\"\"Docstring is inherited from the LGBMModel.\"\"\" super().fit( X, y, sample_weight=sample_weight, init_score=init_score, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_init_score=eval_init_score, eval_metric=eval_metric, feature_name=feature_name, categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model, ) return self _base_doc = LGBMModel.fit.__doc__.replace(\"self : LGBMModel\", \"self : LGBMRegressor\") # type: ignore _base_doc = ( _base_doc[: _base_doc.find(\"group :\")] # type: ignore +", "prev_chunk_id": "chunk_388", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_390", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "_base_doc[_base_doc.find(\"eval_set :\") :] ) # type: ignore _base_doc = _base_doc[: _base_doc.find(\"eval_class_weight :\")] + _base_doc[_base_doc.find(\"eval_init_score :\") :] fit.__doc__ = _base_doc[: _base_doc.find(\"eval_group :\")] + _base_doc[_base_doc.find(\"eval_metric :\") :] [docs] class LGBMClassifier(_LGBMClassifierBase, LGBMModel): \"\"\"LightGBM classifier.\"\"\" # NOTE: all args from LGBMModel.__init__() are intentionally repeated here for # docs, help(), and tab completion. [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[Dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, np.random.Generator]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", **kwargs: Any, ) -> None: super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) __init__.__doc__ = LGBMModel.__init__.__doc__ def _more_tags(self) -> Dict[str, Any]: # handle the case where ClassifierMixin possibly provides _more_tags() if callable(getattr(_LGBMClassifierBase, \"_more_tags\", None)): tags = _LGBMClassifierBase._more_tags(self) else: tags = {} # override those with LightGBM-specific preferences tags.update(LGBMModel._more_tags(self)) return tags def __sklearn_tags__(self) -> \"_sklearn_Tags\": tags = super().__sklearn_tags__() tags.classifier_tags.multi_class = True tags.classifier_tags.multi_label = False return tags [docs] def fit( # type: ignore[override] self, X: _LGBM_ScikitMatrixLike, y: _LGBM_LabelType, sample_weight: Optional[_LGBM_WeightType] = None, init_score: Optional[_LGBM_InitScoreType] = None, eval_set: Optional[List[_LGBM_ScikitValidSet]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_LGBM_WeightType]] = None, eval_class_weight: Optional[List[float]] = None, eval_init_score: Optional[List[_LGBM_InitScoreType]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", callbacks: Optional[List[Callable]] = None, init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None, ) -> \"LGBMClassifier\": \"\"\"Docstring is inherited from the LGBMModel.\"\"\" _LGBMAssertAllFinite(y) _LGBMCheckClassificationTargets(y) self._le = _LGBMLabelEncoder().fit(y) _y = self._le.transform(y) self._class_map = dict(zip(self._le.classes_, self._le.transform(self._le.classes_))) if isinstance(self.class_weight, dict): self._class_weight =", "prev_chunk_id": "chunk_389", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_391", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "{self._class_map[k]: v for k, v in self.class_weight.items()} self._classes = self._le.classes_ self._n_classes = len(self._classes) # type: ignore[arg-type] if self.objective is None: self._objective = None # adjust eval metrics to match whether binary or multiclass # classification is being performed if not callable(eval_metric): if isinstance(eval_metric, list): eval_metric_list = eval_metric elif isinstance(eval_metric, str): eval_metric_list = [eval_metric] else: eval_metric_list = [] if self.__is_multiclass: for index, metric in enumerate(eval_metric_list): if metric in {\"logloss\", \"binary_logloss\"}: eval_metric_list[index] = \"multi_logloss\" elif metric in {\"error\", \"binary_error\"}: eval_metric_list[index] = \"multi_error\" else: for index, metric in enumerate(eval_metric_list): if metric in {\"logloss\", \"multi_logloss\"}: eval_metric_list[index] = \"binary_logloss\" elif metric in {\"error\", \"multi_error\"}: eval_metric_list[index] = \"binary_error\" eval_metric = eval_metric_list # do not modify args, as it causes errors in model selection tools valid_sets: Optional[List[_LGBM_ScikitValidSet]] = None if eval_set is not None: if isinstance(eval_set, tuple): eval_set = [eval_set] valid_sets = [] for valid_x, valid_y in eval_set: if valid_x is X and valid_y is y: valid_sets.append((valid_x, _y)) else: valid_sets.append((valid_x, self._le.transform(valid_y))) super().fit( X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_class_weight=eval_class_weight, eval_init_score=eval_init_score, eval_metric=eval_metric, feature_name=feature_name, categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model, ) return self _base_doc = LGBMModel.fit.__doc__.replace(\"self : LGBMModel\", \"self : LGBMClassifier\") # type: ignore _base_doc = ( _base_doc[: _base_doc.find(\"group :\")] # type: ignore + _base_doc[_base_doc.find(\"eval_set :\") :] ) # type: ignore fit.__doc__ = _base_doc[: _base_doc.find(\"eval_group :\")] + _base_doc[_base_doc.find(\"eval_metric :\") :] [docs] def predict( self, X: _LGBM_ScikitMatrixLike, raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ): \"\"\"Docstring is inherited from the LGBMModel.\"\"\" result = self.predict_proba( X=X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) if callable(self._objective) or raw_score or pred_leaf or pred_contrib: return result else: class_index = np.argmax(result, axis=1) return self._le.inverse_transform(class_index) predict.__doc__ = LGBMModel.predict.__doc__ [docs] def predict_proba( self, X: _LGBM_ScikitMatrixLike, raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool", "prev_chunk_id": "chunk_390", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_392", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "= False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ): \"\"\"Docstring is set after definition, using a template.\"\"\" result = super().predict( X=X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) if callable(self._objective) and not (raw_score or pred_leaf or pred_contrib): _log_warning( \"Cannot compute class probabilities or labels \" \"due to the usage of customized objective function.\\n\" \"Returning raw scores instead.\" ) return result elif self.__is_multiclass or raw_score or pred_leaf or pred_contrib: # type: ignore [operator] return result else: return np.vstack((1.0 - result, result)).transpose() predict_proba.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted probability for each class for each sample.\", X_shape=\"numpy array, pandas DataFrame, scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\", output_name=\"predicted_probability\", predicted_result_shape=\"array-like of shape = [n_samples] or shape = [n_samples, n_classes]\", X_leaves_shape=\"array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\", X_SHAP_values_shape=\"array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\", ) @property def classes_(self) -> np.ndarray: \"\"\":obj:`array` of shape = [n_classes]: The class label array.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No classes found. Need to call fit beforehand.\") return self._classes # type: ignore[return-value] @property def n_classes_(self) -> int: \"\"\":obj:`int`: The number of classes.\"\"\" if not self.__sklearn_is_fitted__(): raise LGBMNotFittedError(\"No classes found. Need to call fit beforehand.\") return self._n_classes @property def __is_multiclass(self) -> bool: \"\"\":obj:`bool`: Indicator of whether the classifier is used for multiclass.\"\"\" return self._n_classes > 2 or (isinstance(self._objective, str) and self._objective in _MULTICLASS_OBJECTIVES) [docs] class LGBMRanker(LGBMModel): \"\"\"LightGBM ranker. .. warning:: scikit-learn doesn't support ranking applications yet, therefore this class is not really compatible with the sklearn ecosystem. Please use this class mainly for training and applying ranking models in common sklearnish way. \"\"\" # NOTE: all args from LGBMModel.__init__() are intentionally repeated here for # docs, help(),", "prev_chunk_id": "chunk_391", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_393", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "and tab completion. [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[Dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, np.random.Generator]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", **kwargs: Any, ) -> None: super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) __init__.__doc__ = LGBMModel.__init__.__doc__ [docs] def fit( # type: ignore[override] self, X: _LGBM_ScikitMatrixLike, y: _LGBM_LabelType, sample_weight: Optional[_LGBM_WeightType] = None, init_score: Optional[_LGBM_InitScoreType] = None, group: Optional[_LGBM_GroupType] = None, eval_set: Optional[List[_LGBM_ScikitValidSet]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_LGBM_WeightType]] = None, eval_init_score: Optional[List[_LGBM_InitScoreType]] = None, eval_group: Optional[List[_LGBM_GroupType]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, eval_at: Union[List[int], Tuple[int, ...]] = (1, 2, 3, 4, 5), feature_name: _LGBM_FeatureNameConfiguration = \"auto\", categorical_feature: _LGBM_CategoricalFeatureConfiguration = \"auto\", callbacks: Optional[List[Callable]] = None, init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None, ) -> \"LGBMRanker\": \"\"\"Docstring is inherited from the LGBMModel.\"\"\" # check group data if group is None: raise ValueError(\"Should set group for ranking task\") if eval_set is not None: if eval_group is None: raise ValueError(\"Eval_group cannot be None when eval_set is not None\") if len(eval_group) != len(eval_set): raise ValueError(\"Length of eval_group should be equal to eval_set\") if ( isinstance(eval_group, dict) and any(i not in eval_group or eval_group[i] is None for i in range(len(eval_group))) or isinstance(eval_group, list) and any(group is None for group in eval_group) ): raise ValueError( \"Should set group for all eval datasets for ranking task; \" \"if you use dict, the index should start", "prev_chunk_id": "chunk_392", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_394", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html", "title": "Source code for lightgbm.sklearn", "page_title": "lightgbm.sklearn — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.sklearn", "content": "from 0\" ) self._eval_at = eval_at super().fit( X, y, sample_weight=sample_weight, init_score=init_score, group=group, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_init_score=eval_init_score, eval_group=eval_group, eval_metric=eval_metric, feature_name=feature_name, categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model, ) return self _base_doc = LGBMModel.fit.__doc__.replace(\"self : LGBMModel\", \"self : LGBMRanker\") # type: ignore fit.__doc__ = ( _base_doc[: _base_doc.find(\"eval_class_weight :\")] # type: ignore + _base_doc[_base_doc.find(\"eval_init_score :\") :] ) # type: ignore _base_doc = fit.__doc__ _before_feature_name, _feature_name, _after_feature_name = _base_doc.partition(\"feature_name :\") fit.__doc__ = f\"\"\"{_before_feature_name}eval_at : list or tuple of int, optional (default=(1, 2, 3, 4, 5)) The evaluation positions of the specified metric. {_feature_name}{_after_feature_name}\"\"\"", "prev_chunk_id": "chunk_393", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_395", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "Source code for lightgbm.dask # coding: utf-8 \"\"\"Distributed training with LightGBM and dask.distributed. This module enables you to perform distributed training with LightGBM on dask.Array and dask.DataFrame collections. It is based on dask-lightgbm, which was based on dask-xgboost. \"\"\" import operator import socket from collections import defaultdict from copy import deepcopy from enum import Enum, auto from functools import partial from typing import Any, Dict, Iterable, List, Optional, Tuple, Type, Union from urllib.parse import urlparse import numpy as np import scipy.sparse as ss from .basic import LightGBMError, _choose_param_value, _ConfigAliases, _log_info, _log_warning from .compat import ( DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED, Client, Future, LGBMNotFittedError, concat, dask_Array, dask_array_from_delayed, dask_bag_from_delayed, dask_DataFrame, dask_Series, default_client, delayed, pd_DataFrame, pd_Series, wait, ) from .sklearn import ( LGBMClassifier, LGBMModel, LGBMRanker, LGBMRegressor, _LGBM_ScikitCustomObjectiveFunction, _LGBM_ScikitEvalMetricType, _lgbmmodel_doc_custom_eval_note, _lgbmmodel_doc_fit, _lgbmmodel_doc_predict, ) __all__ = [ \"DaskLGBMClassifier\", \"DaskLGBMRanker\", \"DaskLGBMRegressor\", ] _DaskCollection = Union[dask_Array, dask_DataFrame, dask_Series] _DaskMatrixLike = Union[dask_Array, dask_DataFrame] _DaskVectorLike = Union[dask_Array, dask_Series] _DaskPart = Union[np.ndarray, pd_DataFrame, pd_Series, ss.spmatrix] _PredictionDtype = Union[Type[np.float32], Type[np.float64], Type[np.int32], Type[np.int64]] class _RemoteSocket: def acquire(self) -> int: self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.socket.bind((\"\", 0)) return self.socket.getsockname()[1] def release(self) -> None: self.socket.close() def _acquire_port() -> Tuple[_RemoteSocket, int]: s = _RemoteSocket() port = s.acquire() return s, port class _DatasetNames(Enum): \"\"\"Placeholder names used by lightgbm.dask internals to say 'also evaluate the training data'. Avoid duplicating the training data when the validation set refers to elements of training data. \"\"\" TRAINSET = auto() SAMPLE_WEIGHT = auto() INIT_SCORE = auto() GROUP = auto() def _get_dask_client(client: Optional[Client]) -> Client: \"\"\"Choose a Dask client to use. Parameters ---------- client : dask.distributed.Client or None Dask client. Returns ------- client : dask.distributed.Client A Dask client. \"\"\" if client is None: return default_client() else: return client def _assign_open_ports_to_workers( client: Client, workers: List[str], ) -> Tuple[Dict[str, Future], Dict[str, int]]: \"\"\"Assign an open port to each worker. Returns ------- worker_to_socket_future: dict mapping", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_396", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "from worker address to a future pointing to the remote socket. worker_to_port: dict mapping from worker address to an open port in the worker's host. \"\"\" # Acquire port in worker worker_to_future = {} for worker in workers: worker_to_future[worker] = client.submit( _acquire_port, workers=[worker], allow_other_workers=False, pure=False, ) # schedule futures to retrieve each element of the tuple worker_to_socket_future = {} worker_to_port_future = {} for worker, socket_future in worker_to_future.items(): worker_to_socket_future[worker] = client.submit(operator.itemgetter(0), socket_future) worker_to_port_future[worker] = client.submit(operator.itemgetter(1), socket_future) # retrieve ports worker_to_port = client.gather(worker_to_port_future) return worker_to_socket_future, worker_to_port def _concat(seq: List[_DaskPart]) -> _DaskPart: if isinstance(seq[0], np.ndarray): return np.concatenate(seq, axis=0) elif isinstance(seq[0], (pd_DataFrame, pd_Series)): return concat(seq, axis=0) elif isinstance(seq[0], ss.spmatrix): return ss.vstack(seq, format=\"csr\") else: raise TypeError( f\"Data must be one of: numpy arrays, pandas dataframes, sparse matrices (from scipy). Got {type(seq[0]).__name__}.\" ) def _remove_list_padding(*args: Any) -> List[List[Any]]: return [[z for z in arg if z is not None] for arg in args] def _pad_eval_names(lgbm_model: LGBMModel, required_names: List[str]) -> LGBMModel: \"\"\"Append missing (key, value) pairs to a LightGBM model's evals_result_ and best_score_ OrderedDict attrs based on a set of required eval_set names. Allows users to rely on expected eval_set names being present when fitting DaskLGBM estimators with ``eval_set``. \"\"\" for eval_name in required_names: if eval_name not in lgbm_model.evals_result_: lgbm_model.evals_result_[eval_name] = {} if eval_name not in lgbm_model.best_score_: lgbm_model.best_score_[eval_name] = {} return lgbm_model def _train_part( *, params: Dict[str, Any], model_factory: Type[LGBMModel], list_of_parts: List[Dict[str, _DaskPart]], machines: str, local_listen_port: int, num_machines: int, return_model: bool, time_out: int, remote_socket: _RemoteSocket, **kwargs: Any, ) -> Optional[LGBMModel]: network_params = { \"machines\": machines, \"local_listen_port\": local_listen_port, \"time_out\": time_out, \"num_machines\": num_machines, } params.update(network_params) is_ranker = issubclass(model_factory, LGBMRanker) # Concatenate many parts into one data = _concat([x[\"data\"] for x in list_of_parts]) label = _concat([x[\"label\"] for x in list_of_parts]) if \"weight\" in list_of_parts[0]: weight = _concat([x[\"weight\"] for x in list_of_parts]) else: weight = None if \"group\" in list_of_parts[0]: group", "prev_chunk_id": "chunk_395", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_397", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "= _concat([x[\"group\"] for x in list_of_parts]) else: group = None if \"init_score\" in list_of_parts[0]: init_score = _concat([x[\"init_score\"] for x in list_of_parts]) else: init_score = None # construct local eval_set data. n_evals = max(len(x.get(\"eval_set\", [])) for x in list_of_parts) eval_names = kwargs.pop(\"eval_names\", None) eval_class_weight = kwargs.get(\"eval_class_weight\") local_eval_set = None local_eval_names = None local_eval_sample_weight = None local_eval_init_score = None local_eval_group = None if n_evals: has_eval_sample_weight = any(x.get(\"eval_sample_weight\") is not None for x in list_of_parts) has_eval_init_score = any(x.get(\"eval_init_score\") is not None for x in list_of_parts) local_eval_set = [] evals_result_names = [] if has_eval_sample_weight: local_eval_sample_weight = [] if has_eval_init_score: local_eval_init_score = [] if is_ranker: local_eval_group = [] # store indices of eval_set components that were not contained within local parts. missing_eval_component_idx = [] # consolidate parts of each individual eval component. for i in range(n_evals): x_e = [] y_e = [] w_e = [] init_score_e = [] g_e = [] for part in list_of_parts: if not part.get(\"eval_set\"): continue # require that eval_name exists in evaluated result data in case dropped due to padding. # in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'. if eval_names: evals_result_name = eval_names[i] else: evals_result_name = f\"valid_{i}\" eval_set = part[\"eval_set\"][i] if eval_set is _DatasetNames.TRAINSET: x_e.append(part[\"data\"]) y_e.append(part[\"label\"]) else: x_e.extend(eval_set[0]) y_e.extend(eval_set[1]) if evals_result_name not in evals_result_names: evals_result_names.append(evals_result_name) eval_weight = part.get(\"eval_sample_weight\") if eval_weight: if eval_weight[i] is _DatasetNames.SAMPLE_WEIGHT: w_e.append(part[\"weight\"]) else: w_e.extend(eval_weight[i]) eval_init_score = part.get(\"eval_init_score\") if eval_init_score: if eval_init_score[i] is _DatasetNames.INIT_SCORE: init_score_e.append(part[\"init_score\"]) else: init_score_e.extend(eval_init_score[i]) eval_group = part.get(\"eval_group\") if eval_group: if eval_group[i] is _DatasetNames.GROUP: g_e.append(part[\"group\"]) else: g_e.extend(eval_group[i]) # filter padding from eval parts then _concat each eval_set component. x_e, y_e, w_e, init_score_e, g_e = _remove_list_padding(x_e, y_e, w_e, init_score_e, g_e) if x_e: local_eval_set.append((_concat(x_e), _concat(y_e))) else: missing_eval_component_idx.append(i) continue if w_e: local_eval_sample_weight.append(_concat(w_e)) if init_score_e: local_eval_init_score.append(_concat(init_score_e)) if g_e: local_eval_group.append(_concat(g_e)) # reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker. eval_component_idx =", "prev_chunk_id": "chunk_396", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_398", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "[i for i in range(n_evals) if i not in missing_eval_component_idx] if eval_names: local_eval_names = [eval_names[i] for i in eval_component_idx] if eval_class_weight: kwargs[\"eval_class_weight\"] = [eval_class_weight[i] for i in eval_component_idx] model = model_factory(**params) if remote_socket is not None: remote_socket.release() try: if is_ranker: model.fit( data, label, sample_weight=weight, init_score=init_score, group=group, eval_set=local_eval_set, eval_sample_weight=local_eval_sample_weight, eval_init_score=local_eval_init_score, eval_group=local_eval_group, eval_names=local_eval_names, **kwargs, ) else: model.fit( data, label, sample_weight=weight, init_score=init_score, eval_set=local_eval_set, eval_sample_weight=local_eval_sample_weight, eval_init_score=local_eval_init_score, eval_names=local_eval_names, **kwargs, ) finally: if getattr(model, \"fitted_\", False): model.booster_.free_network() if n_evals: # ensure that expected keys for evals_result_ and best_score_ exist regardless of padding. model = _pad_eval_names(model, required_names=evals_result_names) return model if return_model else None def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]: parts = data.to_delayed() if isinstance(parts, np.ndarray): if is_matrix: assert parts.shape[1] == 1 else: assert parts.ndim == 1 or parts.shape[1] == 1 parts = parts.flatten().tolist() return parts def _machines_to_worker_map(machines: str, worker_addresses: Iterable[str]) -> Dict[str, int]: \"\"\"Create a worker_map from machines list. Given ``machines`` and a list of Dask worker addresses, return a mapping where the keys are ``worker_addresses`` and the values are ports from ``machines``. Parameters ---------- machines : str A comma-delimited list of workers, of the form ``ip1:port,ip2:port``. worker_addresses : list of str An iterable of Dask worker addresses, of the form ``{protocol}{hostname}:{port}``, where ``port`` is the port Dask's scheduler uses to talk to that worker. Returns ------- result : Dict[str, int] Dictionary where keys are work addresses in the form expected by Dask and values are a port for LightGBM to use. \"\"\" machine_addresses = machines.split(\",\") if len(set(machine_addresses)) != len(machine_addresses): raise ValueError( f\"Found duplicates in 'machines' ({machines}). Each entry in 'machines' must be a unique IP-port combination.\" ) machine_to_port = defaultdict(set) for address in machine_addresses: host, port = address.split(\":\") machine_to_port[host].add(int(port)) out = {} for address in worker_addresses: worker_host = urlparse(address).hostname if not worker_host: raise ValueError(f\"Could not parse host name from worker address '{address}'\") out[address] = machine_to_port[worker_host].pop()", "prev_chunk_id": "chunk_397", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_399", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "return out def _train( *, client: Client, data: _DaskMatrixLike, label: _DaskCollection, params: Dict[str, Any], model_factory: Type[LGBMModel], sample_weight: Optional[_DaskVectorLike] = None, init_score: Optional[_DaskCollection] = None, group: Optional[_DaskVectorLike] = None, eval_set: Optional[List[Tuple[_DaskMatrixLike, _DaskCollection]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_DaskVectorLike]] = None, eval_class_weight: Optional[List[Union[dict, str]]] = None, eval_init_score: Optional[List[_DaskCollection]] = None, eval_group: Optional[List[_DaskVectorLike]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, eval_at: Optional[Union[List[int], Tuple[int, ...]]] = None, **kwargs: Any, ) -> LGBMModel: \"\"\"Inner train routine. Parameters ---------- client : dask.distributed.Client Dask client. data : Dask Array or Dask DataFrame of shape = [n_samples, n_features] Input feature matrix. label : Dask Array, Dask DataFrame or Dask Series of shape = [n_samples] The target values (class labels in classification, real numbers in regression). params : dict Parameters passed to constructor of the local underlying model. model_factory : lightgbm.LGBMClassifier, lightgbm.LGBMRegressor, or lightgbm.LGBMRanker class Class of the local underlying model. sample_weight : Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None) Weights of training data. Weights should be non-negative. init_score : Dask Array or Dask Series of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task), or Dask Array or Dask DataFrame of shape = [n_samples, n_classes] (for multi-class task), or None, optional (default=None) Init score of training data. group : Dask Array or Dask Series or None, optional (default=None) Group/query data. Only used in the learning-to-rank task. sum(group) = n_samples. For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups, where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc. eval_set : list of (X, y) tuples of Dask data collections, or None, optional (default=None) List of (X, y) tuple", "prev_chunk_id": "chunk_398", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_400", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "pairs to use as validation sets. Note, that not all workers may receive chunks of every eval set within ``eval_set``. When the returned lightgbm estimator is not trained using any chunks of a particular eval set, its corresponding component of ``evals_result_`` and ``best_score_`` will be empty dictionaries. eval_names : list of str, or None, optional (default=None) Names of eval_set. eval_sample_weight : list of Dask Array or Dask Series, or None, optional (default=None) Weights for each validation set in eval_set. Weights should be non-negative. eval_class_weight : list of dict or str, or None, optional (default=None) Class weights, one dict or str for each validation set in eval_set. eval_init_score : list of Dask Array, Dask Series or Dask DataFrame (for multi-class task), or None, optional (default=None) Initial model score for each validation set in eval_set. eval_group : list of Dask Array or Dask Series, or None, optional (default=None) Group/query for each validation set in eval_set. eval_metric : str, callable, list or None, optional (default=None) If str, it should be a built-in evaluation metric to use. If callable, it should be a custom evaluation metric, see note below for more details. If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both. In either case, the ``metric`` from the Dask model parameters (or inferred from the objective) will be evaluated and used as well. Default: 'l2' for DaskLGBMRegressor, 'binary(multi)_logloss' for DaskLGBMClassifier, 'ndcg' for DaskLGBMRanker. eval_at : list or tuple of int, optional (default=None) The evaluation positions of the specified ranking metric. **kwargs Other parameters passed to ``fit`` method of the local underlying model. Returns ------- model : lightgbm.LGBMClassifier, lightgbm.LGBMRegressor, or lightgbm.LGBMRanker class Returns fitted underlying model. Note ---- This method handles setting up the following network parameters based on information about the Dask", "prev_chunk_id": "chunk_399", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_401", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "cluster referenced by ``client``. * ``local_listen_port``: port that each LightGBM worker opens a listening socket on, to accept connections from other workers. This can differ from LightGBM worker to LightGBM worker, but does not have to. * ``machines``: a comma-delimited list of all workers in the cluster, in the form ``ip:port,ip:port``. If running multiple Dask workers on the same host, use different ports for each worker. For example, for ``LocalCluster(n_workers=3)``, you might pass ``\"127.0.0.1:12400,127.0.0.1:12401,127.0.0.1:12402\"``. * ``num_machines``: number of LightGBM workers. * ``timeout``: time in minutes to wait before closing unused sockets. The default behavior of this function is to generate ``machines`` from the list of Dask workers which hold some piece of the training data, and to search for an open port on each worker to be used as ``local_listen_port``. If ``machines`` is provided explicitly in ``params``, this function uses the hosts and ports in that list directly, and does not do any searching. This means that if any of the Dask workers are missing from the list or any of those ports are not free when training starts, training will fail. If ``local_listen_port`` is provided in ``params`` and ``machines`` is not, this function constructs ``machines`` from the list of Dask workers which hold some piece of the training data, assuming that each one will use the same ``local_listen_port``. \"\"\" params = deepcopy(params) # capture whether local_listen_port or its aliases were provided listen_port_in_params = any(alias in params for alias in _ConfigAliases.get(\"local_listen_port\")) # capture whether machines or its aliases were provided machines_in_params = any(alias in params for alias in _ConfigAliases.get(\"machines\")) params = _choose_param_value( main_param_name=\"tree_learner\", params=params, default_value=\"data\", ) allowed_tree_learners = { \"data\", \"data_parallel\", \"feature\", \"feature_parallel\", \"voting\", \"voting_parallel\", } if params[\"tree_learner\"] not in allowed_tree_learners: _log_warning( f'Parameter tree_learner set to {params[\"tree_learner\"]}, which is not allowed. Using \"data\" as default' ) params[\"tree_learner\"] = \"data\" #", "prev_chunk_id": "chunk_400", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_402", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "Some passed-in parameters can be removed: # * 'num_machines': set automatically from Dask worker list # * 'num_threads': overridden to match nthreads on each Dask process for param_alias in _ConfigAliases.get(\"num_machines\", \"num_threads\"): if param_alias in params: _log_warning(f\"Parameter {param_alias} will be ignored.\") params.pop(param_alias) # Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality data_parts = _split_to_parts(data=data, is_matrix=True) label_parts = _split_to_parts(data=label, is_matrix=False) parts = [{\"data\": x, \"label\": y} for (x, y) in zip(data_parts, label_parts)] n_parts = len(parts) if sample_weight is not None: weight_parts = _split_to_parts(data=sample_weight, is_matrix=False) for i in range(n_parts): parts[i][\"weight\"] = weight_parts[i] if group is not None: group_parts = _split_to_parts(data=group, is_matrix=False) for i in range(n_parts): parts[i][\"group\"] = group_parts[i] if init_score is not None: init_score_parts = _split_to_parts(data=init_score, is_matrix=False) for i in range(n_parts): parts[i][\"init_score\"] = init_score_parts[i] # evals_set will to be re-constructed into smaller lists of (X, y) tuples, where # X and y are each delayed sub-lists of original eval dask Collections. if eval_set: # find maximum number of parts in an individual eval set so that we can # pad eval sets when they come in different sizes. n_largest_eval_parts = max(x[0].npartitions for x in eval_set) eval_sets: Dict[ int, List[Union[_DatasetNames, Tuple[List[Optional[_DaskMatrixLike]], List[Optional[_DaskVectorLike]]]]] ] = defaultdict(list) if eval_sample_weight: eval_sample_weights: Dict[int, List[Union[_DatasetNames, List[Optional[_DaskVectorLike]]]]] = defaultdict( list ) if eval_group: eval_groups: Dict[int, List[Union[_DatasetNames, List[Optional[_DaskVectorLike]]]]] = defaultdict(list) if eval_init_score: eval_init_scores: Dict[int, List[Union[_DatasetNames, List[Optional[_DaskMatrixLike]]]]] = defaultdict(list) for i, (X_eval, y_eval) in enumerate(eval_set): n_this_eval_parts = X_eval.npartitions # when individual eval set is equivalent to training data, skip recomputing parts. if X_eval is data and y_eval is label: for parts_idx in range(n_parts): eval_sets[parts_idx].append(_DatasetNames.TRAINSET) else: eval_x_parts = _split_to_parts(data=X_eval, is_matrix=True) eval_y_parts = _split_to_parts(data=y_eval, is_matrix=False) for j in range(n_largest_eval_parts): parts_idx = j % n_parts # add None-padding for individual eval_set member if it is smaller than the largest member. if j < n_this_eval_parts: x_e = eval_x_parts[j] y_e = eval_y_parts[j] else: x_e", "prev_chunk_id": "chunk_401", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_403", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "= None y_e = None if j < n_parts: # first time a chunk of this eval set is added to this part. eval_sets[parts_idx].append(([x_e], [y_e])) else: # append additional chunks of this eval set to this part. eval_sets[parts_idx][-1][0].append(x_e) # type: ignore[index, union-attr] eval_sets[parts_idx][-1][1].append(y_e) # type: ignore[index, union-attr] if eval_sample_weight: if eval_sample_weight[i] is sample_weight: for parts_idx in range(n_parts): eval_sample_weights[parts_idx].append(_DatasetNames.SAMPLE_WEIGHT) else: eval_w_parts = _split_to_parts(data=eval_sample_weight[i], is_matrix=False) # ensure that all evaluation parts map uniquely to one part. for j in range(n_largest_eval_parts): if j < n_this_eval_parts: w_e = eval_w_parts[j] else: w_e = None parts_idx = j % n_parts if j < n_parts: eval_sample_weights[parts_idx].append([w_e]) else: eval_sample_weights[parts_idx][-1].append(w_e) # type: ignore[union-attr] if eval_init_score: if eval_init_score[i] is init_score: for parts_idx in range(n_parts): eval_init_scores[parts_idx].append(_DatasetNames.INIT_SCORE) else: eval_init_score_parts = _split_to_parts(data=eval_init_score[i], is_matrix=False) for j in range(n_largest_eval_parts): if j < n_this_eval_parts: init_score_e = eval_init_score_parts[j] else: init_score_e = None parts_idx = j % n_parts if j < n_parts: eval_init_scores[parts_idx].append([init_score_e]) else: eval_init_scores[parts_idx][-1].append(init_score_e) # type: ignore[union-attr] if eval_group: if eval_group[i] is group: for parts_idx in range(n_parts): eval_groups[parts_idx].append(_DatasetNames.GROUP) else: eval_g_parts = _split_to_parts(data=eval_group[i], is_matrix=False) for j in range(n_largest_eval_parts): if j < n_this_eval_parts: g_e = eval_g_parts[j] else: g_e = None parts_idx = j % n_parts if j < n_parts: eval_groups[parts_idx].append([g_e]) else: eval_groups[parts_idx][-1].append(g_e) # type: ignore[union-attr] # assign sub-eval_set components to worker parts. for parts_idx, e_set in eval_sets.items(): parts[parts_idx][\"eval_set\"] = e_set if eval_sample_weight: parts[parts_idx][\"eval_sample_weight\"] = eval_sample_weights[parts_idx] if eval_init_score: parts[parts_idx][\"eval_init_score\"] = eval_init_scores[parts_idx] if eval_group: parts[parts_idx][\"eval_group\"] = eval_groups[parts_idx] # Start computation in the background parts = list(map(delayed, parts)) parts = client.compute(parts) wait(parts) for part in parts: if part.status == \"error\": # type: ignore # trigger error locally return part # type: ignore[return-value] # Find locations of all parts and map them to particular Dask workers key_to_part_dict = {part.key: part for part in parts} # type: ignore who_has = client.who_has(parts) worker_map = defaultdict(list) for key, workers in who_has.items(): worker_map[next(iter(workers))].append(key_to_part_dict[key]) # Check that all", "prev_chunk_id": "chunk_402", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_404", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "workers were provided some of eval_set. Otherwise warn user that validation # data artifacts may not be populated depending on worker returning final estimator. if eval_set: for worker in worker_map: has_eval_set = False for part in worker_map[worker]: if \"eval_set\" in part.result(): # type: ignore[attr-defined] has_eval_set = True break if not has_eval_set: _log_warning( f\"Worker {worker} was not allocated eval_set data. Therefore evals_result_ and best_score_ data may be unreliable. \" \"Try rebalancing data across workers.\" ) # assign general validation set settings to fit kwargs. if eval_names: kwargs[\"eval_names\"] = eval_names if eval_class_weight: kwargs[\"eval_class_weight\"] = eval_class_weight if eval_metric: kwargs[\"eval_metric\"] = eval_metric if eval_at: kwargs[\"eval_at\"] = eval_at master_worker = next(iter(worker_map)) worker_ncores = client.ncores() # resolve aliases for network parameters and pop the result off params. # these values are added back in calls to `_train_part()` params = _choose_param_value( main_param_name=\"local_listen_port\", params=params, default_value=12400, ) local_listen_port = params.pop(\"local_listen_port\") params = _choose_param_value( main_param_name=\"machines\", params=params, default_value=None, ) machines = params.pop(\"machines\") # figure out network params worker_to_socket_future: Dict[str, Future] = {} worker_addresses = worker_map.keys() if machines is not None: _log_info(\"Using passed-in 'machines' parameter\") worker_address_to_port = _machines_to_worker_map( machines=machines, worker_addresses=worker_addresses, ) else: if listen_port_in_params: _log_info(\"Using passed-in 'local_listen_port' for all workers\") unique_hosts = {urlparse(a).hostname for a in worker_addresses} if len(unique_hosts) < len(worker_addresses): msg = ( \"'local_listen_port' was provided in Dask training parameters, but at least one \" \"machine in the cluster has multiple Dask worker processes running on it. Please omit \" \"'local_listen_port' or pass 'machines'.\" ) raise LightGBMError(msg) worker_address_to_port = dict.fromkeys(worker_addresses, local_listen_port) else: _log_info(\"Finding random open ports for workers\") worker_to_socket_future, worker_address_to_port = _assign_open_ports_to_workers( client, list(worker_map.keys()) ) machines = \",\".join( [f\"{urlparse(worker_address).hostname}:{port}\" for worker_address, port in worker_address_to_port.items()] ) num_machines = len(worker_address_to_port) # Tell each worker to train on the parts that it has locally # # This code treats ``_train_part()`` calls as not \"pure\" because: # 1. there is randomness in the training process", "prev_chunk_id": "chunk_403", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_405", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "unless parameters ``seed`` # and ``deterministic`` are set # 2. even with those parameters set, the output of one ``_train_part()`` call # relies on global state (it and all the other LightGBM training processes # coordinate with each other) futures_classifiers = [ client.submit( _train_part, model_factory=model_factory, params={**params, \"num_threads\": worker_ncores[worker]}, list_of_parts=list_of_parts, machines=machines, local_listen_port=worker_address_to_port[worker], num_machines=num_machines, time_out=params.get(\"time_out\", 120), remote_socket=worker_to_socket_future.get(worker, None), return_model=(worker == master_worker), workers=[worker], allow_other_workers=False, pure=False, **kwargs, ) for worker, list_of_parts in worker_map.items() ] results = client.gather(futures_classifiers) results = [v for v in results if v] model = results[0] # if network parameters were changed during training, remove them from the # returned model so that they're generated dynamically on every run based # on the Dask cluster you're connected to and which workers have pieces of # the training data if not listen_port_in_params: for param in _ConfigAliases.get(\"local_listen_port\"): model._other_params.pop(param, None) if not machines_in_params: for param in _ConfigAliases.get(\"machines\"): model._other_params.pop(param, None) for param in _ConfigAliases.get(\"num_machines\", \"timeout\"): model._other_params.pop(param, None) return model def _predict_part( part: _DaskPart, *, model: LGBMModel, raw_score: bool, pred_proba: bool, pred_leaf: bool, pred_contrib: bool, **kwargs: Any, ) -> _DaskPart: result: _DaskPart if part.shape[0] == 0: result = np.array([]) elif pred_proba: result = model.predict_proba( part, raw_score=raw_score, pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs, ) else: result = model.predict( part, raw_score=raw_score, pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs, ) # dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series if isinstance(part, pd_DataFrame): if len(result.shape) == 2: result = pd_DataFrame(result, index=part.index) else: result = pd_Series(result, index=part.index, name=\"predictions\") return result def _predict( *, model: LGBMModel, data: _DaskMatrixLike, client: Client, raw_score: bool = False, pred_proba: bool = False, pred_leaf: bool = False, pred_contrib: bool = False, dtype: _PredictionDtype = np.float32, **kwargs: Any, ) -> Union[dask_Array, List[dask_Array]]: \"\"\"Inner predict routine. Parameters ---------- model : lightgbm.LGBMClassifier, lightgbm.LGBMRegressor, or lightgbm.LGBMRanker class Fitted underlying model. data : Dask Array or Dask DataFrame of shape = [n_samples, n_features] Input feature matrix.", "prev_chunk_id": "chunk_404", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_406", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "raw_score : bool, optional (default=False) Whether to predict raw scores. pred_proba : bool, optional (default=False) Should method return results of ``predict_proba`` (``pred_proba=True``) or ``predict`` (``pred_proba=False``). pred_leaf : bool, optional (default=False) Whether to predict leaf index. pred_contrib : bool, optional (default=False) Whether to predict feature contributions. dtype : np.dtype, optional (default=np.float32) Dtype of the output. **kwargs Other parameters passed to ``predict`` or ``predict_proba`` method. Returns ------- predicted_result : Dask Array of shape = [n_samples] or shape = [n_samples, n_classes] The predicted values. X_leaves : Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes] If ``pred_leaf=True``, the predicted leaf of every tree for each sample. X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or (if multi-class and using sparse inputs) a list of ``n_classes`` Dask Arrays of shape = [n_samples, n_features + 1] If ``pred_contrib=True``, the feature contributions for each sample. \"\"\" if not all((DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED)): raise LightGBMError(\"dask, pandas and scikit-learn are required for lightgbm.dask\") if isinstance(data, dask_DataFrame): return data.map_partitions( _predict_part, model=model, raw_score=raw_score, pred_proba=pred_proba, pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs, ).values elif isinstance(data, dask_Array): # for multi-class classification with sparse matrices, pred_contrib predictions # are returned as a list of sparse matrices (one per class) num_classes = model._n_classes if num_classes > 2 and pred_contrib and isinstance(data._meta, ss.spmatrix): predict_function = partial( _predict_part, model=model, raw_score=False, pred_proba=pred_proba, pred_leaf=False, pred_contrib=True, **kwargs, ) delayed_chunks = data.to_delayed() bag = dask_bag_from_delayed(delayed_chunks[:, 0]) @delayed def _extract(items: List[Any], i: int) -> Any: return items[i] preds = bag.map_partitions(predict_function) # pred_contrib output will have one column per feature, # plus one more for the base value num_cols = model.n_features_ + 1 nrows_per_chunk = data.chunks[0] out: List[List[dask_Array]] = [[] for _ in range(num_classes)] # need to tell Dask the expected type and shape of individual preds pred_meta = data._meta", "prev_chunk_id": "chunk_405", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_407", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "for j, partition in enumerate(preds.to_delayed()): for i in range(num_classes): part = dask_array_from_delayed( value=_extract(partition, i), shape=(nrows_per_chunk[j], num_cols), meta=pred_meta, ) out[i].append(part) # by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix # the code below is used instead to ensure that the sparse type is preserved during concatenation if isinstance(pred_meta, ss.csr_matrix): concat_fn = partial(ss.vstack, format=\"csr\") elif isinstance(pred_meta, ss.csc_matrix): concat_fn = partial(ss.vstack, format=\"csc\") else: concat_fn = ss.vstack # At this point, `out` is a list of lists of delayeds (each of which points to a matrix). # Concatenate them to return a list of Dask Arrays. out_arrays: List[dask_Array] = [] for i in range(num_classes): out_arrays.append( dask_array_from_delayed( value=delayed(concat_fn)(out[i]), shape=(data.shape[0], num_cols), meta=pred_meta, ) ) return out_arrays data_row = client.compute(data[[0]]).result() predict_fn = partial( _predict_part, model=model, raw_score=raw_score, pred_proba=pred_proba, pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs, ) pred_row = predict_fn(data_row) chunks: Tuple[int, ...] = (data.chunks[0],) map_blocks_kwargs = {} if len(pred_row.shape) > 1: chunks += (pred_row.shape[1],) else: map_blocks_kwargs[\"drop_axis\"] = 1 return data.map_blocks( predict_fn, chunks=chunks, meta=pred_row, dtype=dtype, **map_blocks_kwargs, ) else: raise TypeError(f\"Data must be either Dask Array or Dask DataFrame. Got {type(data).__name__}.\") class _DaskLGBMModel: @property def client_(self) -> Client: \"\"\":obj:`dask.distributed.Client`: Dask client. This property can be passed in the constructor or updated with ``model.set_params(client=client)``. \"\"\" if not getattr(self, \"fitted_\", False): raise LGBMNotFittedError(\"Cannot access property client_ before calling fit().\") return _get_dask_client(client=self.client) def _lgb_dask_getstate(self) -> Dict[Any, Any]: \"\"\"Remove un-picklable attributes before serialization.\"\"\" client = self.__dict__.pop(\"client\", None) self._other_params.pop(\"client\", None) # type: ignore[attr-defined] out = deepcopy(self.__dict__) out.update({\"client\": None}) self.client = client return out def _lgb_dask_fit( self, *, model_factory: Type[LGBMModel], X: _DaskMatrixLike, y: _DaskCollection, sample_weight: Optional[_DaskVectorLike] = None, init_score: Optional[_DaskCollection] = None, group: Optional[_DaskVectorLike] = None, eval_set: Optional[List[Tuple[_DaskMatrixLike, _DaskCollection]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_DaskVectorLike]] = None, eval_class_weight: Optional[List[Union[dict, str]]] = None, eval_init_score: Optional[List[_DaskCollection]] = None, eval_group: Optional[List[_DaskVectorLike]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, eval_at: Optional[Union[List[int], Tuple[int, ...]]] = None, **kwargs: Any, ) -> \"_DaskLGBMModel\": if", "prev_chunk_id": "chunk_406", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_408", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "not DASK_INSTALLED: raise LightGBMError(\"dask is required for lightgbm.dask\") if not all((DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED)): raise LightGBMError(\"dask, pandas and scikit-learn are required for lightgbm.dask\") params = self.get_params(True) # type: ignore[attr-defined] params.pop(\"client\", None) model = _train( client=_get_dask_client(self.client), data=X, label=y, params=params, model_factory=model_factory, sample_weight=sample_weight, init_score=init_score, group=group, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_class_weight=eval_class_weight, eval_init_score=eval_init_score, eval_group=eval_group, eval_metric=eval_metric, eval_at=eval_at, **kwargs, ) self.set_params(**model.get_params()) # type: ignore[attr-defined] self._lgb_dask_copy_extra_params(model, self) # type: ignore[attr-defined] return self def _lgb_dask_to_local(self, model_factory: Type[LGBMModel]) -> LGBMModel: params = self.get_params() # type: ignore[attr-defined] params.pop(\"client\", None) model = model_factory(**params) self._lgb_dask_copy_extra_params(self, model) model._other_params.pop(\"client\", None) return model @staticmethod def _lgb_dask_copy_extra_params( source: Union[\"_DaskLGBMModel\", LGBMModel], dest: Union[\"_DaskLGBMModel\", LGBMModel], ) -> None: params = source.get_params() # type: ignore[union-attr] attributes = source.__dict__ extra_param_names = set(attributes.keys()).difference(params.keys()) for name in extra_param_names: setattr(dest, name, attributes[name]) [docs] class DaskLGBMClassifier(LGBMClassifier, _DaskLGBMModel): \"\"\"Distributed version of lightgbm.LGBMClassifier.\"\"\" [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, \"np.random.Generator\"]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", client: Optional[Client] = None, **kwargs: Any, ): \"\"\"Docstring is inherited from the lightgbm.LGBMClassifier.__init__.\"\"\" self.client = client super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) _base_doc = LGBMClassifier.__init__.__doc__ _before_kwargs, _kwargs, _after_kwargs = _base_doc.partition(\"**kwargs\") # type: ignore __init__.__doc__ = f\"\"\" {_before_kwargs}client : dask.distributed.Client or None, optional (default=None) {\" \":4}Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled. {_kwargs}{_after_kwargs} \"\"\" def __getstate__(self)", "prev_chunk_id": "chunk_407", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_409", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "-> Dict[Any, Any]: return self._lgb_dask_getstate() [docs] def fit( # type: ignore[override] self, X: _DaskMatrixLike, y: _DaskCollection, sample_weight: Optional[_DaskVectorLike] = None, init_score: Optional[_DaskCollection] = None, eval_set: Optional[List[Tuple[_DaskMatrixLike, _DaskCollection]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_DaskVectorLike]] = None, eval_class_weight: Optional[List[Union[dict, str]]] = None, eval_init_score: Optional[List[_DaskCollection]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, **kwargs: Any, ) -> \"DaskLGBMClassifier\": \"\"\"Docstring is inherited from the lightgbm.LGBMClassifier.fit.\"\"\" self._lgb_dask_fit( model_factory=LGBMClassifier, X=X, y=y, sample_weight=sample_weight, init_score=init_score, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_class_weight=eval_class_weight, eval_init_score=eval_init_score, eval_metric=eval_metric, **kwargs, ) return self _base_doc = _lgbmmodel_doc_fit.format( X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", y_shape=\"Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\", sample_weight_shape=\"Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None)\", init_score_shape=\"Dask Array or Dask Series of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task), or Dask Array or Dask DataFrame of shape = [n_samples, n_classes] (for multi-class task), or None, optional (default=None)\", group_shape=\"Dask Array or Dask Series or None, optional (default=None)\", eval_sample_weight_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", eval_init_score_shape=\"list of Dask Array, Dask Series or Dask DataFrame (for multi-class task), or None, optional (default=None)\", eval_group_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", ) # DaskLGBMClassifier does not support group, eval_group. _base_doc = _base_doc[: _base_doc.find(\"group :\")] + _base_doc[_base_doc.find(\"eval_set :\") :] _base_doc = _base_doc[: _base_doc.find(\"eval_group :\")] + _base_doc[_base_doc.find(\"eval_metric :\") :] # DaskLGBMClassifier support for callbacks and init_model is not tested fit.__doc__ = f\"\"\"{_base_doc[: _base_doc.find(\"callbacks :\")]}**kwargs Other parameters passed through to ``LGBMClassifier.fit()``. Returns ------- self : lightgbm.DaskLGBMClassifier Returns self. {_lgbmmodel_doc_custom_eval_note} \"\"\" [docs] def predict( self, X: _DaskMatrixLike, # type: ignore[override] raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ) -> dask_Array: \"\"\"Docstring is inherited from the lightgbm.LGBMClassifier.predict.\"\"\" return _predict( model=self.to_local(),", "prev_chunk_id": "chunk_408", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_410", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "data=X, dtype=self.classes_.dtype, client=_get_dask_client(self.client), raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) predict.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted value for each sample.\", X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", output_name=\"predicted_result\", predicted_result_shape=\"Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]\", X_leaves_shape=\"Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\", X_SHAP_values_shape=\"Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or (if multi-class and using sparse inputs) a list of ``n_classes`` Dask Arrays of shape = [n_samples, n_features + 1]\", ) [docs] def predict_proba( self, X: _DaskMatrixLike, # type: ignore[override] raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ) -> dask_Array: \"\"\"Docstring is inherited from the lightgbm.LGBMClassifier.predict_proba.\"\"\" return _predict( model=self.to_local(), data=X, pred_proba=True, client=_get_dask_client(self.client), raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) predict_proba.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted probability for each class for each sample.\", X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", output_name=\"predicted_probability\", predicted_result_shape=\"Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]\", X_leaves_shape=\"Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\", X_SHAP_values_shape=\"Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or (if multi-class and using sparse inputs) a list of ``n_classes`` Dask Arrays of shape = [n_samples, n_features + 1]\", ) [docs] def to_local(self) -> LGBMClassifier: \"\"\"Create regular version of lightgbm.LGBMClassifier from the distributed version. Returns ------- model : lightgbm.LGBMClassifier Local underlying model. \"\"\" return self._lgb_dask_to_local(LGBMClassifier) [docs] class DaskLGBMRegressor(LGBMRegressor, _DaskLGBMModel): \"\"\"Distributed version of lightgbm.LGBMRegressor.\"\"\" [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100,", "prev_chunk_id": "chunk_409", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_411", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, \"np.random.Generator\"]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", client: Optional[Client] = None, **kwargs: Any, ): \"\"\"Docstring is inherited from the lightgbm.LGBMRegressor.__init__.\"\"\" self.client = client super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) _base_doc = LGBMRegressor.__init__.__doc__ _before_kwargs, _kwargs, _after_kwargs = _base_doc.partition(\"**kwargs\") # type: ignore __init__.__doc__ = f\"\"\" {_before_kwargs}client : dask.distributed.Client or None, optional (default=None) {\" \":4}Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled. {_kwargs}{_after_kwargs} \"\"\" def __getstate__(self) -> Dict[Any, Any]: return self._lgb_dask_getstate() [docs] def fit( # type: ignore[override] self, X: _DaskMatrixLike, y: _DaskCollection, sample_weight: Optional[_DaskVectorLike] = None, init_score: Optional[_DaskVectorLike] = None, eval_set: Optional[List[Tuple[_DaskMatrixLike, _DaskCollection]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_DaskVectorLike]] = None, eval_init_score: Optional[List[_DaskVectorLike]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, **kwargs: Any, ) -> \"DaskLGBMRegressor\": \"\"\"Docstring is inherited from the lightgbm.LGBMRegressor.fit.\"\"\" self._lgb_dask_fit( model_factory=LGBMRegressor, X=X, y=y, sample_weight=sample_weight, init_score=init_score, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_init_score=eval_init_score, eval_metric=eval_metric, **kwargs, ) return self _base_doc = _lgbmmodel_doc_fit.format( X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", y_shape=\"Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\", sample_weight_shape=\"Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None)\", init_score_shape=\"Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None)\", group_shape=\"Dask Array or Dask Series or None, optional (default=None)\", eval_sample_weight_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", eval_init_score_shape=\"list of Dask Array or Dask Series, or", "prev_chunk_id": "chunk_410", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_412", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "None, optional (default=None)\", eval_group_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", ) # DaskLGBMRegressor does not support group, eval_class_weight, eval_group. _base_doc = _base_doc[: _base_doc.find(\"group :\")] + _base_doc[_base_doc.find(\"eval_set :\") :] _base_doc = _base_doc[: _base_doc.find(\"eval_class_weight :\")] + _base_doc[_base_doc.find(\"eval_init_score :\") :] _base_doc = _base_doc[: _base_doc.find(\"eval_group :\")] + _base_doc[_base_doc.find(\"eval_metric :\") :] # DaskLGBMRegressor support for callbacks and init_model is not tested fit.__doc__ = f\"\"\"{_base_doc[: _base_doc.find(\"callbacks :\")]}**kwargs Other parameters passed through to ``LGBMRegressor.fit()``. Returns ------- self : lightgbm.DaskLGBMRegressor Returns self. {_lgbmmodel_doc_custom_eval_note} \"\"\" [docs] def predict( self, X: _DaskMatrixLike, # type: ignore[override] raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ) -> dask_Array: \"\"\"Docstring is inherited from the lightgbm.LGBMRegressor.predict.\"\"\" return _predict( model=self.to_local(), data=X, client=_get_dask_client(self.client), raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) predict.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted value for each sample.\", X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", output_name=\"predicted_result\", predicted_result_shape=\"Dask Array of shape = [n_samples]\", X_leaves_shape=\"Dask Array of shape = [n_samples, n_trees]\", X_SHAP_values_shape=\"Dask Array of shape = [n_samples, n_features + 1]\", ) [docs] def to_local(self) -> LGBMRegressor: \"\"\"Create regular version of lightgbm.LGBMRegressor from the distributed version. Returns ------- model : lightgbm.LGBMRegressor Local underlying model. \"\"\" return self._lgb_dask_to_local(LGBMRegressor) [docs] class DaskLGBMRanker(LGBMRanker, _DaskLGBMModel): \"\"\"Distributed version of lightgbm.LGBMRanker.\"\"\" [docs] def __init__( self, *, boosting_type: str = \"gbdt\", num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Optional[Union[str, _LGBM_ScikitCustomObjectiveFunction]] = None, class_weight: Optional[Union[dict, str]] = None, min_split_gain: float = 0.0, min_child_weight: float = 1e-3, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Optional[Union[int, np.random.RandomState, \"np.random.Generator\"]] = None, n_jobs: Optional[int] = None, importance_type: str = \"split\", client:", "prev_chunk_id": "chunk_411", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_413", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "Optional[Client] = None, **kwargs: Any, ): \"\"\"Docstring is inherited from the lightgbm.LGBMRanker.__init__.\"\"\" self.client = client super().__init__( boosting_type=boosting_type, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, objective=objective, class_weight=class_weight, min_split_gain=min_split_gain, min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, subsample_freq=subsample_freq, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=random_state, n_jobs=n_jobs, importance_type=importance_type, **kwargs, ) _base_doc = LGBMRanker.__init__.__doc__ _before_kwargs, _kwargs, _after_kwargs = _base_doc.partition(\"**kwargs\") # type: ignore __init__.__doc__ = f\"\"\" {_before_kwargs}client : dask.distributed.Client or None, optional (default=None) {\" \":4}Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled. {_kwargs}{_after_kwargs} \"\"\" def __getstate__(self) -> Dict[Any, Any]: return self._lgb_dask_getstate() [docs] def fit( # type: ignore[override] self, X: _DaskMatrixLike, y: _DaskCollection, sample_weight: Optional[_DaskVectorLike] = None, init_score: Optional[_DaskVectorLike] = None, group: Optional[_DaskVectorLike] = None, eval_set: Optional[List[Tuple[_DaskMatrixLike, _DaskCollection]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[_DaskVectorLike]] = None, eval_init_score: Optional[List[_DaskVectorLike]] = None, eval_group: Optional[List[_DaskVectorLike]] = None, eval_metric: Optional[_LGBM_ScikitEvalMetricType] = None, eval_at: Union[List[int], Tuple[int, ...]] = (1, 2, 3, 4, 5), **kwargs: Any, ) -> \"DaskLGBMRanker\": \"\"\"Docstring is inherited from the lightgbm.LGBMRanker.fit.\"\"\" self._lgb_dask_fit( model_factory=LGBMRanker, X=X, y=y, sample_weight=sample_weight, init_score=init_score, group=group, eval_set=eval_set, eval_names=eval_names, eval_sample_weight=eval_sample_weight, eval_init_score=eval_init_score, eval_group=eval_group, eval_metric=eval_metric, eval_at=eval_at, **kwargs, ) return self _base_doc = _lgbmmodel_doc_fit.format( X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", y_shape=\"Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\", sample_weight_shape=\"Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None)\", init_score_shape=\"Dask Array or Dask Series of shape = [n_samples] or None, optional (default=None)\", group_shape=\"Dask Array or Dask Series or None, optional (default=None)\", eval_sample_weight_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", eval_init_score_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", eval_group_shape=\"list of Dask Array or Dask Series, or None, optional (default=None)\", ) # DaskLGBMRanker does not support eval_class_weight or early stopping _base_doc = _base_doc[: _base_doc.find(\"eval_class_weight :\")] + _base_doc[_base_doc.find(\"eval_init_score :\") :] _base_doc = ( _base_doc[: _base_doc.find(\"feature_name :\")] +", "prev_chunk_id": "chunk_412", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_414", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/dask.html", "title": "Source code for lightgbm.dask", "page_title": "lightgbm.dask — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.dask", "content": "\"eval_at : list or tuple of int, optional (default=(1, 2, 3, 4, 5))\\n\" + f\"{' ':8}The evaluation positions of the specified metric.\\n\" + f\"{' ':4}{_base_doc[_base_doc.find('feature_name :') :]}\" ) # DaskLGBMRanker support for callbacks and init_model is not tested fit.__doc__ = f\"\"\"{_base_doc[: _base_doc.find(\"callbacks :\")]}**kwargs Other parameters passed through to ``LGBMRanker.fit()``. Returns ------- self : lightgbm.DaskLGBMRanker Returns self. {_lgbmmodel_doc_custom_eval_note} \"\"\" [docs] def predict( self, X: _DaskMatrixLike, # type: ignore[override] raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any, ) -> dask_Array: \"\"\"Docstring is inherited from the lightgbm.LGBMRanker.predict.\"\"\" return _predict( model=self.to_local(), data=X, client=_get_dask_client(self.client), raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration, pred_leaf=pred_leaf, pred_contrib=pred_contrib, validate_features=validate_features, **kwargs, ) predict.__doc__ = _lgbmmodel_doc_predict.format( description=\"Return the predicted value for each sample.\", X_shape=\"Dask Array or Dask DataFrame of shape = [n_samples, n_features]\", output_name=\"predicted_result\", predicted_result_shape=\"Dask Array of shape = [n_samples]\", X_leaves_shape=\"Dask Array of shape = [n_samples, n_trees]\", X_SHAP_values_shape=\"Dask Array of shape = [n_samples, n_features + 1]\", ) [docs] def to_local(self) -> LGBMRanker: \"\"\"Create regular version of lightgbm.LGBMRanker from the distributed version. Returns ------- model : lightgbm.LGBMRanker Local underlying model. \"\"\" return self._lgb_dask_to_local(LGBMRanker)", "prev_chunk_id": "chunk_413", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_415", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/index.html", "title": "All modules for which code is available", "page_title": "Overview: module code — LightGBM 4.6.0.99 documentation", "breadcrumbs": "All modules for which code is available", "content": "All modules for which code is available - lightgbm.basic - lightgbm.callback - lightgbm.dask - lightgbm.engine - lightgbm.plotting - lightgbm.sklearn", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_416", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "Source code for lightgbm.engine # coding: utf-8 \"\"\"Library with training routines of LightGBM.\"\"\" import copy import json from collections import OrderedDict, defaultdict from operator import attrgetter from pathlib import Path from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union import numpy as np from . import callback from .basic import ( Booster, Dataset, LightGBMError, _choose_param_value, _ConfigAliases, _InnerPredictor, _LGBM_BoosterEvalMethodResultType, _LGBM_BoosterEvalMethodResultWithStandardDeviationType, _LGBM_CustomObjectiveFunction, _LGBM_EvalFunctionResultType, _log_warning, ) from .compat import SKLEARN_INSTALLED, _LGBMBaseCrossValidator, _LGBMGroupKFold, _LGBMStratifiedKFold __all__ = [ \"cv\", \"CVBooster\", \"train\", ] _LGBM_CustomMetricFunction = Union[ Callable[ [np.ndarray, Dataset], _LGBM_EvalFunctionResultType, ], Callable[ [np.ndarray, Dataset], List[_LGBM_EvalFunctionResultType], ], ] _LGBM_PreprocFunction = Callable[ [Dataset, Dataset, Dict[str, Any]], Tuple[Dataset, Dataset, Dict[str, Any]], ] def _choose_num_iterations(*, num_boost_round_kwarg: int, params: Dict[str, Any]) -> Dict[str, Any]: \"\"\"Choose number of boosting rounds. In ``train()`` and ``cv()``, there are multiple ways to provide configuration for the number of boosting rounds to perform: * the ``num_boost_round`` keyword argument * any of the ``num_iterations`` or its aliases via the ``params`` dictionary These should be preferred in the following order (first one found wins): 1. ``num_iterations`` provided via ``params`` (because it's the main parameter name) 2. any other aliases of ``num_iterations`` provided via ``params`` 3. the ``num_boost_round`` keyword argument This function handles that choice, and issuing helpful warnings in the cases where the result might be surprising. Returns ------- params : dict Parameters, with ``\"num_iterations\"`` set to the preferred value and all other aliases of ``num_iterations`` removed. \"\"\" num_iteration_configs_provided = { alias: params[alias] for alias in _ConfigAliases.get(\"num_iterations\") if alias in params } # now that the relevant information has been pulled out of params, it's safe to overwrite it # with the content that should be used for training (i.e. with aliases resolved) params = _choose_param_value( main_param_name=\"num_iterations\", params=params, default_value=num_boost_round_kwarg, ) # if there were not multiple boosting rounds configurations provided in params, # then by definition", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_417", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "they cannot have conflicting values... no need to warn if len(num_iteration_configs_provided) <= 1: return params # if all the aliases have the same value, no need to warn if len(set(num_iteration_configs_provided.values())) <= 1: return params # if this line is reached, lightgbm should warn value_string = \", \".join(f\"{alias}={val}\" for alias, val in num_iteration_configs_provided.items()) _log_warning( f\"Found conflicting values for num_iterations provided via 'params': {value_string}. \" f\"LightGBM will perform up to {params['num_iterations']} boosting rounds. \" \"To be confident in the maximum number of boosting rounds LightGBM will perform and to \" \"suppress this warning, modify 'params' so that only one of those is present.\" ) return params [docs] def train( params: Dict[str, Any], train_set: Dataset, num_boost_round: int = 100, valid_sets: Optional[List[Dataset]] = None, valid_names: Optional[List[str]] = None, feval: Optional[Union[_LGBM_CustomMetricFunction, List[_LGBM_CustomMetricFunction]]] = None, init_model: Optional[Union[str, Path, Booster]] = None, keep_training_booster: bool = False, callbacks: Optional[List[Callable]] = None, ) -> Booster: \"\"\"Perform the training with given parameters. Parameters ---------- params : dict Parameters for training. Values passed through ``params`` take precedence over those supplied via arguments. train_set : Dataset Data to be trained on. num_boost_round : int, optional (default=100) Number of boosting iterations. valid_sets : list of Dataset, or None, optional (default=None) List of data to be evaluated on during training. valid_names : list of str, or None, optional (default=None) Names of ``valid_sets``. feval : callable, list of callable, or None, optional (default=None) Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples. preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes]. If custom objective function is used, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of", "prev_chunk_id": "chunk_416", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_418", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "positive class for binary task in this case. eval_data : Dataset A ``Dataset`` to evaluate. eval_name : str The name of evaluation function (without whitespaces). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. To ignore the default metric corresponding to the used objective, set the ``metric`` parameter to the string ``\"None\"`` in ``params``. init_model : str, pathlib.Path, Booster or None, optional (default=None) Filename of LightGBM model or Booster instance used for continue training. keep_training_booster : bool, optional (default=False) Whether the returned Booster will be used to keep training. If False, the returned value will be converted into _InnerPredictor before returning. This means you won't be able to use ``eval``, ``eval_train`` or ``eval_valid`` methods of the returned Booster. When your model is very large and cause the memory error, you can try to set this param to ``True`` to avoid the model conversion performed during the internal call of ``model_to_string``. You can still use _InnerPredictor as ``init_model`` for future continue training. callbacks : list of callable, or None, optional (default=None) List of callback functions that are applied at each iteration. See Callbacks in Python API for more information. Note ---- A custom objective function can be provided for the ``objective`` parameter. It should accept two parameters: preds, train_data and return (grad, hess). preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. Predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task. train_data : Dataset The training dataset. grad : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of preds for each sample point. hess : numpy", "prev_chunk_id": "chunk_417", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_419", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "1-D array or numpy 2-D array (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of preds for each sample point. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. Returns ------- booster : Booster The trained Booster model. \"\"\" if not isinstance(train_set, Dataset): raise TypeError(f\"train() only accepts Dataset object, train_set has type '{type(train_set).__name__}'.\") if isinstance(valid_sets, list): for i, valid_item in enumerate(valid_sets): if not isinstance(valid_item, Dataset): raise TypeError( \"Every item in valid_sets must be a Dataset object. \" f\"Item {i} has type '{type(valid_item).__name__}'.\" ) # create predictor first params = copy.deepcopy(params) params = _choose_param_value( main_param_name=\"objective\", params=params, default_value=None, ) fobj: Optional[_LGBM_CustomObjectiveFunction] = None if callable(params[\"objective\"]): fobj = params[\"objective\"] params[\"objective\"] = \"none\" params = _choose_num_iterations(num_boost_round_kwarg=num_boost_round, params=params) num_boost_round = params[\"num_iterations\"] if num_boost_round <= 0: raise ValueError(f\"Number of boosting rounds must be greater than 0. Got {num_boost_round}.\") # setting early stopping via global params should be possible params = _choose_param_value( main_param_name=\"early_stopping_round\", params=params, default_value=None, ) if params[\"early_stopping_round\"] is None: params.pop(\"early_stopping_round\") first_metric_only = params.get(\"first_metric_only\", False) predictor: Optional[_InnerPredictor] = None if isinstance(init_model, (str, Path)): predictor = _InnerPredictor.from_model_file(model_file=init_model, pred_parameter=params) elif isinstance(init_model, Booster): predictor = _InnerPredictor.from_booster(booster=init_model, pred_parameter=dict(init_model.params, **params)) if predictor is not None: init_iteration = predictor.current_iteration() else: init_iteration = 0 train_set._update_params(params)._set_predictor(predictor) is_valid_contain_train = False train_data_name = \"training\" reduced_valid_sets = [] name_valid_sets = [] if valid_sets is not None: if isinstance(valid_sets, Dataset): valid_sets = [valid_sets] if isinstance(valid_names, str): valid_names = [valid_names] for i, valid_data in enumerate(valid_sets): # reduce cost for prediction training data if valid_data is train_set: is_valid_contain_train = True if valid_names is not None: train_data_name = valid_names[i] continue reduced_valid_sets.append(valid_data._update_params(params).set_reference(train_set)) if valid_names is not None and len(valid_names) > i: name_valid_sets.append(valid_names[i]) else: name_valid_sets.append(f\"valid_{i}\") # process callbacks if callbacks is None: callbacks_set = set() else: for i,", "prev_chunk_id": "chunk_418", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_420", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "cb in enumerate(callbacks): cb.__dict__.setdefault(\"order\", i - len(callbacks)) callbacks_set = set(callbacks) if callback._should_enable_early_stopping(params.get(\"early_stopping_round\", 0)): callbacks_set.add( callback.early_stopping( stopping_rounds=params[\"early_stopping_round\"], # type: ignore[arg-type] first_metric_only=first_metric_only, min_delta=params.get(\"early_stopping_min_delta\", 0.0), verbose=_choose_param_value( main_param_name=\"verbosity\", params=params, default_value=1, ).pop(\"verbosity\") > 0, ) ) callbacks_before_iter_set = {cb for cb in callbacks_set if getattr(cb, \"before_iteration\", False)} callbacks_after_iter_set = callbacks_set - callbacks_before_iter_set callbacks_before_iter = sorted(callbacks_before_iter_set, key=attrgetter(\"order\")) callbacks_after_iter = sorted(callbacks_after_iter_set, key=attrgetter(\"order\")) # construct booster try: booster = Booster(params=params, train_set=train_set) if is_valid_contain_train: booster.set_train_data_name(train_data_name) for valid_set, name_valid_set in zip(reduced_valid_sets, name_valid_sets): booster.add_valid(valid_set, name_valid_set) finally: train_set._reverse_update_params() for valid_set in reduced_valid_sets: valid_set._reverse_update_params() booster.best_iteration = 0 # start training for i in range(init_iteration, init_iteration + num_boost_round): for cb in callbacks_before_iter: cb( callback.CallbackEnv( model=booster, params=params, iteration=i, begin_iteration=init_iteration, end_iteration=init_iteration + num_boost_round, evaluation_result_list=None, ) ) booster.update(fobj=fobj) evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = [] # check evaluation result. if valid_sets is not None: if is_valid_contain_train: evaluation_result_list.extend(booster.eval_train(feval)) evaluation_result_list.extend(booster.eval_valid(feval)) try: for cb in callbacks_after_iter: cb( callback.CallbackEnv( model=booster, params=params, iteration=i, begin_iteration=init_iteration, end_iteration=init_iteration + num_boost_round, evaluation_result_list=evaluation_result_list, ) ) except callback.EarlyStopException as earlyStopException: booster.best_iteration = earlyStopException.best_iteration + 1 evaluation_result_list = earlyStopException.best_score break booster.best_score = defaultdict(OrderedDict) for dataset_name, eval_name, score, _ in evaluation_result_list: booster.best_score[dataset_name][eval_name] = score if not keep_training_booster: booster.model_from_string(booster.model_to_string()).free_dataset() return booster [docs] class CVBooster: \"\"\"CVBooster in LightGBM. Auxiliary data structure to hold and redirect all boosters of ``cv()`` function. This class has the same methods as Booster class. All method calls, except for the following methods, are actually performed for underlying Boosters and then all returned results are returned in a list. - ``model_from_string()`` - ``model_to_string()`` - ``save_model()`` Attributes ---------- boosters : list of Booster The list of underlying fitted models. best_iteration : int The best iteration of fitted model. \"\"\" [docs] def __init__( self, model_file: Optional[Union[str, Path]] = None, ): \"\"\"Initialize the CVBooster. Parameters ---------- model_file : str, pathlib.Path or None, optional (default=None) Path to the CVBooster model file. \"\"\" self.boosters: List[Booster] = [] self.best_iteration = -1 if model_file is not None: with", "prev_chunk_id": "chunk_419", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_421", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "open(model_file, \"r\") as file: self._from_dict(json.load(file)) def _from_dict(self, models: Dict[str, Any]) -> None: \"\"\"Load CVBooster from dict.\"\"\" self.best_iteration = models[\"best_iteration\"] self.boosters = [] for model_str in models[\"boosters\"]: self.boosters.append(Booster(model_str=model_str)) def _to_dict( self, *, num_iteration: Optional[int], start_iteration: int, importance_type: str, ) -> Dict[str, Any]: \"\"\"Serialize CVBooster to dict.\"\"\" models_str = [] for booster in self.boosters: models_str.append( booster.model_to_string( num_iteration=num_iteration, start_iteration=start_iteration, importance_type=importance_type ) ) return {\"boosters\": models_str, \"best_iteration\": self.best_iteration} def __getattr__(self, name: str) -> Callable[[Any, Any], List[Any]]: \"\"\"Redirect methods call of CVBooster.\"\"\" def handler_function(*args: Any, **kwargs: Any) -> List[Any]: \"\"\"Call methods with each booster, and concatenate their results.\"\"\" ret = [] for booster in self.boosters: ret.append(getattr(booster, name)(*args, **kwargs)) return ret return handler_function def __getstate__(self) -> Dict[str, Any]: return vars(self) def __setstate__(self, state: Dict[str, Any]) -> None: vars(self).update(state) [docs] def model_from_string(self, model_str: str) -> \"CVBooster\": \"\"\"Load CVBooster from a string. Parameters ---------- model_str : str Model will be loaded from this string. Returns ------- self : CVBooster Loaded CVBooster object. \"\"\" self._from_dict(json.loads(model_str)) return self [docs] def model_to_string( self, num_iteration: Optional[int] = None, start_iteration: int = 0, importance_type: str = \"split\", ) -> str: \"\"\"Save CVBooster to JSON string. Parameters ---------- num_iteration : int or None, optional (default=None) Index of the iteration that should be saved. If None, if the best iteration exists, it is saved; otherwise, all iterations are saved. If <= 0, all iterations are saved. start_iteration : int, optional (default=0) Start index of the iteration that should be saved. importance_type : str, optional (default=\"split\") What type of feature importance should be saved. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. Returns ------- str_repr : str JSON string representation of CVBooster. \"\"\" return json.dumps( self._to_dict(num_iteration=num_iteration, start_iteration=start_iteration, importance_type=importance_type) ) [docs] def save_model( self, filename: Union[str, Path], num_iteration: Optional[int]", "prev_chunk_id": "chunk_420", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_422", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "= None, start_iteration: int = 0, importance_type: str = \"split\", ) -> \"CVBooster\": \"\"\"Save CVBooster to a file as JSON text. Parameters ---------- filename : str or pathlib.Path Filename to save CVBooster. num_iteration : int or None, optional (default=None) Index of the iteration that should be saved. If None, if the best iteration exists, it is saved; otherwise, all iterations are saved. If <= 0, all iterations are saved. start_iteration : int, optional (default=0) Start index of the iteration that should be saved. importance_type : str, optional (default=\"split\") What type of feature importance should be saved. If \"split\", result contains numbers of times the feature is used in a model. If \"gain\", result contains total gains of splits which use the feature. Returns ------- self : CVBooster Returns self. \"\"\" with open(filename, \"w\") as file: json.dump( self._to_dict( num_iteration=num_iteration, start_iteration=start_iteration, importance_type=importance_type ), file, ) return self def _make_n_folds( *, full_data: Dataset, folds: Optional[Union[Iterable[Tuple[np.ndarray, np.ndarray]], _LGBMBaseCrossValidator]], nfold: int, params: Dict[str, Any], seed: int, fpreproc: Optional[_LGBM_PreprocFunction], stratified: bool, shuffle: bool, eval_train_metric: bool, ) -> CVBooster: \"\"\"Make a n-fold list of Booster from random indices.\"\"\" full_data = full_data.construct() num_data = full_data.num_data() if folds is not None: if not hasattr(folds, \"__iter__\") and not hasattr(folds, \"split\"): raise AttributeError( \"folds should be a generator or iterator of (train_idx, test_idx) tuples \" \"or scikit-learn splitter object with split method\" ) if hasattr(folds, \"split\"): group_info = full_data.get_group() if group_info is not None: group_info = np.asarray(group_info, dtype=np.int32) flatted_group = np.repeat(range(len(group_info)), repeats=group_info) else: flatted_group = np.zeros(num_data, dtype=np.int32) folds = folds.split(X=np.empty(num_data), y=full_data.get_label(), groups=flatted_group) else: if any( params.get(obj_alias, \"\") in {\"lambdarank\", \"rank_xendcg\", \"xendcg\", \"xe_ndcg\", \"xe_ndcg_mart\", \"xendcg_mart\"} for obj_alias in _ConfigAliases.get(\"objective\") ): if not SKLEARN_INSTALLED: raise LightGBMError(\"scikit-learn is required for ranking cv\") # ranking task, split according to groups group_info = np.asarray(full_data.get_group(), dtype=np.int32) flatted_group = np.repeat(range(len(group_info)), repeats=group_info) group_kfold = _LGBMGroupKFold(n_splits=nfold) folds = group_kfold.split(X=np.empty(num_data), groups=flatted_group)", "prev_chunk_id": "chunk_421", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_423", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "elif stratified: if not SKLEARN_INSTALLED: raise LightGBMError(\"scikit-learn is required for stratified cv\") skf = _LGBMStratifiedKFold(n_splits=nfold, shuffle=shuffle, random_state=seed) folds = skf.split(X=np.empty(num_data), y=full_data.get_label()) else: if shuffle: randidx = np.random.RandomState(seed).permutation(num_data) else: randidx = np.arange(num_data) kstep = int(num_data / nfold) test_id = [randidx[i : i + kstep] for i in range(0, num_data, kstep)] train_id = [np.concatenate([test_id[i] for i in range(nfold) if k != i]) for k in range(nfold)] folds = zip(train_id, test_id) ret = CVBooster() for train_idx, test_idx in folds: train_set = full_data.subset(sorted(train_idx)) valid_set = full_data.subset(sorted(test_idx)) # run preprocessing on the data set if needed if fpreproc is not None: train_set, valid_set, tparam = fpreproc(train_set, valid_set, params.copy()) else: tparam = params booster_for_fold = Booster(tparam, train_set) if eval_train_metric: booster_for_fold.add_valid(train_set, \"train\") booster_for_fold.add_valid(valid_set, \"valid\") ret.boosters.append(booster_for_fold) return ret def _agg_cv_result( raw_results: List[List[_LGBM_BoosterEvalMethodResultType]], ) -> List[_LGBM_BoosterEvalMethodResultWithStandardDeviationType]: \"\"\"Aggregate cross-validation results.\"\"\" # build up 2 maps, of the form: # # OrderedDict{ # (<dataset_name>, <metric_name>): <is_higher_better> # } # # OrderedDict{ # (<dataset_name>, <metric_name>): list[<metric_value>] # } # metric_types: Dict[Tuple[str, str], bool] = OrderedDict() metric_values: Dict[Tuple[str, str], List[float]] = OrderedDict() for one_result in raw_results: for dataset_name, metric_name, metric_value, is_higher_better in one_result: key = (dataset_name, metric_name) metric_types[key] = is_higher_better metric_values.setdefault(key, []) metric_values[key].append(metric_value) # turn that into a list of tuples of the form: # # [ # (<dataset_name>, <metric_name>, mean(<values>), <is_higher_better>, std_dev(<values>)) # ] return [(k[0], k[1], float(np.mean(v)), metric_types[k], float(np.std(v))) for k, v in metric_values.items()] [docs] def cv( params: Dict[str, Any], train_set: Dataset, num_boost_round: int = 100, folds: Optional[Union[Iterable[Tuple[np.ndarray, np.ndarray]], _LGBMBaseCrossValidator]] = None, nfold: int = 5, stratified: bool = True, shuffle: bool = True, metrics: Optional[Union[str, List[str]]] = None, feval: Optional[Union[_LGBM_CustomMetricFunction, List[_LGBM_CustomMetricFunction]]] = None, init_model: Optional[Union[str, Path, Booster]] = None, fpreproc: Optional[_LGBM_PreprocFunction] = None, seed: int = 0, callbacks: Optional[List[Callable]] = None, eval_train_metric: bool = False, return_cvbooster: bool = False, ) -> Dict[str, Union[List[float], CVBooster]]: \"\"\"Perform the cross-validation with given parameters.", "prev_chunk_id": "chunk_422", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_424", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "Parameters ---------- params : dict Parameters for training. Values passed through ``params`` take precedence over those supplied via arguments. train_set : Dataset Data to be trained on. num_boost_round : int, optional (default=100) Number of boosting iterations. folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None) If generator or iterator, it should yield the train and test indices for each fold. If object, it should be one of the scikit-learn splitter classes (https://scikit-learn.org/stable/modules/classes.html#splitter-classes) and have ``split`` method. This argument has highest priority over other data split arguments. nfold : int, optional (default=5) Number of folds in CV. stratified : bool, optional (default=True) Whether to perform stratified sampling. shuffle : bool, optional (default=True) Whether to shuffle before splitting data. metrics : str, list of str, or None, optional (default=None) Evaluation metrics to be monitored while CV. If not None, the metric in ``params`` will be overridden. feval : callable, list of callable, or None, optional (default=None) Customized evaluation function. Each evaluation function should accept two parameters: preds, eval_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples. preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes]. If custom objective function is used, predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task in this case. eval_data : Dataset A ``Dataset`` to evaluate. eval_name : str The name of evaluation function (without whitespace). eval_result : float The eval result. is_higher_better : bool Is eval result higher better, e.g. AUC is ``is_higher_better``. To ignore the default metric corresponding to the used objective, set ``metrics`` to the string ``\"None\"``. init_model : str, pathlib.Path, Booster or None, optional", "prev_chunk_id": "chunk_423", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_425", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "(default=None) Filename of LightGBM model or Booster instance used for continue training. fpreproc : callable or None, optional (default=None) Preprocessing function that takes (dtrain, dtest, params) and returns transformed versions of those. seed : int, optional (default=0) Seed used to generate the folds (passed to numpy.random.seed). callbacks : list of callable, or None, optional (default=None) List of callback functions that are applied at each iteration. See Callbacks in Python API for more information. eval_train_metric : bool, optional (default=False) Whether to display the train metric in progress. The score of the metric is calculated again after each training step, so there is some impact on performance. return_cvbooster : bool, optional (default=False) Whether to return Booster models trained on each fold through ``CVBooster``. Note ---- A custom objective function can be provided for the ``objective`` parameter. It should accept two parameters: preds, train_data and return (grad, hess). preds : numpy 1-D array or numpy 2-D array (for multi-class task) The predicted values. Predicted values are returned before any transformation, e.g. they are raw margin instead of probability of positive class for binary task. train_data : Dataset The training dataset. grad : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the first order derivative (gradient) of the loss with respect to the elements of preds for each sample point. hess : numpy 1-D array or numpy 2-D array (for multi-class task) The value of the second order derivative (Hessian) of the loss with respect to the elements of preds for each sample point. For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes], and grad and hess should be returned in the same format. Returns ------- eval_results : dict History of evaluation results of each metric. The dictionary has the following format: {'valid metric1-mean': [values],", "prev_chunk_id": "chunk_424", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_426", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "'valid metric1-stdv': [values], 'valid metric2-mean': [values], 'valid metric2-stdv': [values], ...}. If ``return_cvbooster=True``, also returns trained boosters wrapped in a ``CVBooster`` object via ``cvbooster`` key. If ``eval_train_metric=True``, also returns the train metric history. In this case, the dictionary has the following format: {'train metric1-mean': [values], 'valid metric1-mean': [values], 'train metric2-mean': [values], 'valid metric2-mean': [values], ...}. \"\"\" if not isinstance(train_set, Dataset): raise TypeError(f\"cv() only accepts Dataset object, train_set has type '{type(train_set).__name__}'.\") params = copy.deepcopy(params) params = _choose_param_value( main_param_name=\"objective\", params=params, default_value=None, ) fobj: Optional[_LGBM_CustomObjectiveFunction] = None if callable(params[\"objective\"]): fobj = params[\"objective\"] params[\"objective\"] = \"none\" params = _choose_num_iterations(num_boost_round_kwarg=num_boost_round, params=params) num_boost_round = params[\"num_iterations\"] if num_boost_round <= 0: raise ValueError(f\"Number of boosting rounds must be greater than 0. Got {num_boost_round}.\") # setting early stopping via global params should be possible params = _choose_param_value( main_param_name=\"early_stopping_round\", params=params, default_value=None, ) if params[\"early_stopping_round\"] is None: params.pop(\"early_stopping_round\") first_metric_only = params.get(\"first_metric_only\", False) if isinstance(init_model, (str, Path)): predictor = _InnerPredictor.from_model_file( model_file=init_model, pred_parameter=params, ) elif isinstance(init_model, Booster): predictor = _InnerPredictor.from_booster( booster=init_model, pred_parameter=dict(init_model.params, **params), ) else: predictor = None if metrics is not None: for metric_alias in _ConfigAliases.get(\"metric\"): params.pop(metric_alias, None) params[\"metric\"] = metrics train_set._update_params(params)._set_predictor(predictor) results = defaultdict(list) cvbooster = _make_n_folds( full_data=train_set, folds=folds, nfold=nfold, params=params, seed=seed, fpreproc=fpreproc, stratified=stratified, shuffle=shuffle, eval_train_metric=eval_train_metric, ) # setup callbacks if callbacks is None: callbacks_set = set() else: for i, cb in enumerate(callbacks): cb.__dict__.setdefault(\"order\", i - len(callbacks)) callbacks_set = set(callbacks) if callback._should_enable_early_stopping(params.get(\"early_stopping_round\", 0)): callbacks_set.add( callback.early_stopping( stopping_rounds=params[\"early_stopping_round\"], # type: ignore[arg-type] first_metric_only=first_metric_only, min_delta=params.get(\"early_stopping_min_delta\", 0.0), verbose=_choose_param_value( main_param_name=\"verbosity\", params=params, default_value=1, ).pop(\"verbosity\") > 0, ) ) callbacks_before_iter_set = {cb for cb in callbacks_set if getattr(cb, \"before_iteration\", False)} callbacks_after_iter_set = callbacks_set - callbacks_before_iter_set callbacks_before_iter = sorted(callbacks_before_iter_set, key=attrgetter(\"order\")) callbacks_after_iter = sorted(callbacks_after_iter_set, key=attrgetter(\"order\")) for i in range(num_boost_round): for cb in callbacks_before_iter: cb( callback.CallbackEnv( model=cvbooster, params=params, iteration=i, begin_iteration=0, end_iteration=num_boost_round, evaluation_result_list=None, ) ) cvbooster.update(fobj=fobj) # type: ignore[call-arg] res = _agg_cv_result(cvbooster.eval_valid(feval)) # type: ignore[call-arg] for dataset_name, metric_name, metric_mean, _, metric_std_dev in res: results[f\"{dataset_name}", "prev_chunk_id": "chunk_425", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_427", "url": "https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/engine.html", "title": "Source code for lightgbm.engine", "page_title": "lightgbm.engine — LightGBM 4.6.0.99 documentation", "breadcrumbs": "Source code for lightgbm.engine", "content": "{metric_name}-mean\"].append(metric_mean) results[f\"{dataset_name} {metric_name}-stdv\"].append(metric_std_dev) try: for cb in callbacks_after_iter: cb( callback.CallbackEnv( model=cvbooster, params=params, iteration=i, begin_iteration=0, end_iteration=num_boost_round, evaluation_result_list=res, ) ) except callback.EarlyStopException as earlyStopException: cvbooster.best_iteration = earlyStopException.best_iteration + 1 for bst in cvbooster.boosters: bst.best_iteration = cvbooster.best_iteration for k in results: results[k] = results[k][: cvbooster.best_iteration] break if return_cvbooster: results[\"cvbooster\"] = cvbooster # type: ignore[assignment] return dict(results)", "prev_chunk_id": "chunk_426", "next_chunk_id": null, "type": "section"}
]