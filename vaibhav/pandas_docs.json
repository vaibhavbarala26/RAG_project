[
{"chunk_id": "chunk_0", "url": "https://pandas.pydata.org/docs/user_guide/index.html", "title": "User Guide#", "page_title": "User Guide — pandas 2.3.1 documentation", "breadcrumbs": "User Guide#", "content": "User Guide# The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as “working with missing data”), and discusses how pandas approaches the problem, with many examples throughout. Users brand-new to pandas should start with 10 minutes to pandas. For a high level summary of the pandas fundamentals, see Intro to data structures and Essential basic functionality. Further information on any specific method can be obtained in the API reference.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1", "url": "https://pandas.pydata.org/docs/user_guide/index.html", "title": "How to read these guides#", "page_title": "User Guide — pandas 2.3.1 documentation", "breadcrumbs": "How to read these guides#", "content": "How to read these guides# In these guides you will see input code inside code blocks such as: import pandas as pd pd.DataFrame({'A': [1, 2, 3]}) or: import pandas as pd pd.DataFrame({'A': [1, 2, 3]}) A 0 1 1 2 2 3 The first block is a standard python input, while in the second the indicates the input is inside a notebook. In Jupyter Notebooks the last line is printed and plots are shown inline. For example: a = 1 a 1 is equivalent to: a = 1 print(a)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_2", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "10 minutes to pandas#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "10 minutes to pandas#", "content": "10 minutes to pandas# This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the Cookbook. Customarily, we import as follows: import numpy as np import pandas as pd", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_3", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Basic data structures in pandas#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Basic data structures in pandas#", "content": "Basic data structures in pandas# Pandas provides two types of classes for handling data: - Series: a one-dimensional labeled array holding data of any typesuch as integers, strings, Python objects etc. - DataFrame: a two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns.", "prev_chunk_id": "chunk_2", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_4", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Object creation#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Object creation#", "content": "Object creation# See the Intro to data structures section. Creating a Series by passing a list of values, letting pandas create a default RangeIndex. s = pd.Series([1, 3, 5, np.nan, 6, 8]) s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 Creating a DataFrame by passing a NumPy array with a datetime index using date_range() and labeled columns: dates = pd.date_range(\"20130101\", periods=6) dates DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\")) df A B C D 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 2013-01-05 -0.424972 0.567020 0.276232 -1.087401 2013-01-06 -0.673690 0.113648 -1.478427 0.524988 Creating a DataFrame by passing a dictionary of objects where the keys are the column labels and the values are the column values. df2 = pd.DataFrame( ...: { ...: \"A\": 1.0, ...: \"B\": pd.Timestamp(\"20130102\"), ...: \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"), ...: \"D\": np.array([3] * 4, dtype=\"int32\"), ...: \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]), ...: \"F\": \"foo\", ...: } ...: ) ...: df2 A B C D E F 0 1.0 2013-01-02 1.0 3 test foo 1 1.0 2013-01-02 1.0 3 train foo 2 1.0 2013-01-02 1.0 3 test foo 3 1.0 2013-01-02 1.0 3 train foo The columns of the resulting DataFrame have different dtypes: df2.dtypes A float64 B datetime64[s] C float32 D int32 E category F object dtype: object If you’re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here’s a subset of the attributes that will be completed: df2.<TAB> # noqa: E225, E999 df2.A df2.bool df2.abs df2.boxplot df2.add df2.C df2.add_prefix df2.clip df2.add_suffix df2.columns df2.align df2.copy df2.all df2.count df2.any df2.combine df2.append df2.D df2.apply df2.describe df2.applymap df2.diff df2.B df2.duplicated As you can see, the columns A, B,", "prev_chunk_id": "chunk_3", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_5", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Object creation#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Object creation#", "content": "C, and D are automatically tab completed. E and F are there as well; the rest of the attributes have been truncated for brevity.", "prev_chunk_id": "chunk_4", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_6", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Viewing data#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Viewing data#", "content": "Viewing data# See the Essentially basics functionality section. Use DataFrame.head() and DataFrame.tail() to view the top and bottom rows of the frame respectively: df.head() A B C D 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 2013-01-05 -0.424972 0.567020 0.276232 -1.087401 df.tail(3) A B C D 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 2013-01-05 -0.424972 0.567020 0.276232 -1.087401 2013-01-06 -0.673690 0.113648 -1.478427 0.524988 Display the DataFrame.index or DataFrame.columns: df.index DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') df.columns Index(['A', 'B', 'C', 'D'], dtype='object') Return a NumPy representation of the underlying data with DataFrame.to_numpy() without the index or column labels: df.to_numpy() array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) describe() shows a quick statistic summary of your data: df.describe() A B C D count 6.000000 6.000000 6.000000 6.000000 mean 0.073711 -0.431125 -0.687758 -0.233103 std 0.843157 0.922818 0.779887 0.973118 min -0.861849 -2.104569 -1.509059 -1.135632 25% -0.611510 -0.600794 -1.368714 -1.076610 50% 0.022070 -0.228039 -0.767252 -0.386188 75% 0.658444 0.041933 -0.034326 0.461706 max 1.212112 0.567020 0.276232 1.071804 Transposing your data: df.T 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06 A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690 B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648 C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427 D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 DataFrame.sort_index() sorts by an axis: df.sort_index(axis=1, ascending=False) D C B A 2013-01-01 -1.135632 -1.509059 -0.282863 0.469112 2013-01-02 -1.044236 0.119209 -0.173215 1.212112 2013-01-03 1.071804 -0.494929 -2.104569 -0.861849 2013-01-04 0.271860 -1.039575 -0.706771 0.721555 2013-01-05 -1.087401 0.276232 0.567020 -0.424972 2013-01-06 0.524988 -1.478427 0.113648 -0.673690 DataFrame.sort_values() sorts by values: df.sort_values(by=\"B\") A B C D 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 2013-01-02 1.212112 -0.173215", "prev_chunk_id": "chunk_5", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_7", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Viewing data#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Viewing data#", "content": "0.119209 -1.044236 2013-01-06 -0.673690 0.113648 -1.478427 0.524988 2013-01-05 -0.424972 0.567020 0.276232 -1.087401", "prev_chunk_id": "chunk_6", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_8", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Selection#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Selection#", "content": "Selection# See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing.", "prev_chunk_id": "chunk_7", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_9", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Getitem ([])#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Getitem ([])#", "content": "Getitem ([])# For a DataFrame, passing a single label selects a columns and yields a Series equivalent to df.A: df[\"A\"] 2013-01-01 0.469112 2013-01-02 1.212112 2013-01-03 -0.861849 2013-01-04 0.721555 2013-01-05 -0.424972 2013-01-06 -0.673690 Freq: D, Name: A, dtype: float64 For a DataFrame, passing a slice : selects matching rows: df[0:3] A B C D 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 df[\"20130102\":\"20130104\"] A B C D 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 2013-01-04 0.721555 -0.706771 -1.039575 0.271860", "prev_chunk_id": "chunk_8", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_10", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Selection by label#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Selection by label#", "content": "Selection by label# See more in Selection by Label using DataFrame.loc() or DataFrame.at(). Selecting a row matching a label: df.loc[dates[0]] A 0.469112 B -0.282863 C -1.509059 D -1.135632 Name: 2013-01-01 00:00:00, dtype: float64 Selecting all rows (:) with a select column labels: df.loc[:, [\"A\", \"B\"]] A B 2013-01-01 0.469112 -0.282863 2013-01-02 1.212112 -0.173215 2013-01-03 -0.861849 -2.104569 2013-01-04 0.721555 -0.706771 2013-01-05 -0.424972 0.567020 2013-01-06 -0.673690 0.113648 For label slicing, both endpoints are included: df.loc[\"20130102\":\"20130104\", [\"A\", \"B\"]] A B 2013-01-02 1.212112 -0.173215 2013-01-03 -0.861849 -2.104569 2013-01-04 0.721555 -0.706771 Selecting a single row and column label returns a scalar: df.loc[dates[0], \"A\"] 0.4691122999071863 For getting fast access to a scalar (equivalent to the prior method): df.at[dates[0], \"A\"] 0.4691122999071863", "prev_chunk_id": "chunk_9", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_11", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Selection by position#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Selection by position#", "content": "Selection by position# See more in Selection by Position using DataFrame.iloc() or DataFrame.iat(). Select via the position of the passed integers: df.iloc[3] A 0.721555 B -0.706771 C -1.039575 D 0.271860 Name: 2013-01-04 00:00:00, dtype: float64 Integer slices acts similar to NumPy/Python: df.iloc[3:5, 0:2] A B 2013-01-04 0.721555 -0.706771 2013-01-05 -0.424972 0.567020 Lists of integer position locations: df.iloc[[1, 2, 4], [0, 2]] A C 2013-01-02 1.212112 0.119209 2013-01-03 -0.861849 -0.494929 2013-01-05 -0.424972 0.276232 For slicing rows explicitly: df.iloc[1:3, :] A B C D 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 For slicing columns explicitly: df.iloc[:, 1:3] B C 2013-01-01 -0.282863 -1.509059 2013-01-02 -0.173215 0.119209 2013-01-03 -2.104569 -0.494929 2013-01-04 -0.706771 -1.039575 2013-01-05 0.567020 0.276232 2013-01-06 0.113648 -1.478427 For getting a value explicitly: df.iloc[1, 1] -0.17321464905330858 For getting fast access to a scalar (equivalent to the prior method): df.iat[1, 1] -0.17321464905330858", "prev_chunk_id": "chunk_10", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_12", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Boolean indexing#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Boolean indexing#", "content": "Boolean indexing# Select rows where df.A is greater than 0. df[df[\"A\"] > 0] A B C D 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 Selecting values from a DataFrame where a boolean condition is met: df[df > 0] A B C D 2013-01-01 0.469112 NaN NaN NaN 2013-01-02 1.212112 NaN 0.119209 NaN 2013-01-03 NaN NaN NaN 1.071804 2013-01-04 0.721555 NaN NaN 0.271860 2013-01-05 NaN 0.567020 0.276232 NaN 2013-01-06 NaN 0.113648 NaN 0.524988 Using isin() method for filtering: df2 = df.copy() df2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"] df2 A B C D E 2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one 2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two 2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three 2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four 2013-01-06 -0.673690 0.113648 -1.478427 0.524988 three df2[df2[\"E\"].isin([\"two\", \"four\"])] A B C D E 2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two 2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four", "prev_chunk_id": "chunk_11", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_13", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Setting#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Setting#", "content": "Setting# Setting a new column automatically aligns the data by the indexes: s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range(\"20130102\", periods=6)) s1 2013-01-02 1 2013-01-03 2 2013-01-04 3 2013-01-05 4 2013-01-06 5 2013-01-07 6 Freq: D, dtype: int64 df[\"F\"] = s1 Setting values by label: df.at[dates[0], \"A\"] = 0 Setting values by position: df.iat[0, 1] = 0 Setting by assigning with a NumPy array: df.loc[:, \"D\"] = np.array([5] * len(df)) The result of the prior setting operations: df A B C D F 2013-01-01 0.000000 0.000000 -1.509059 5.0 NaN 2013-01-02 1.212112 -0.173215 0.119209 5.0 1.0 2013-01-03 -0.861849 -2.104569 -0.494929 5.0 2.0 2013-01-04 0.721555 -0.706771 -1.039575 5.0 3.0 2013-01-05 -0.424972 0.567020 0.276232 5.0 4.0 2013-01-06 -0.673690 0.113648 -1.478427 5.0 5.0 A where operation with setting: df2 = df.copy() df2[df2 > 0] = -df2 df2 A B C D F 2013-01-01 0.000000 0.000000 -1.509059 -5.0 NaN 2013-01-02 -1.212112 -0.173215 -0.119209 -5.0 -1.0 2013-01-03 -0.861849 -2.104569 -0.494929 -5.0 -2.0 2013-01-04 -0.721555 -0.706771 -1.039575 -5.0 -3.0 2013-01-05 -0.424972 -0.567020 -0.276232 -5.0 -4.0 2013-01-06 -0.673690 -0.113648 -1.478427 -5.0 -5.0", "prev_chunk_id": "chunk_12", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_14", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Missing data#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Missing data#", "content": "Missing data# For NumPy data types, np.nan represents missing data. It is by default not included in computations. See the Missing Data section. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\"E\"]) df1.loc[dates[0] : dates[1], \"E\"] = 1 df1 A B C D F E 2013-01-01 0.000000 0.000000 -1.509059 5.0 NaN 1.0 2013-01-02 1.212112 -0.173215 0.119209 5.0 1.0 1.0 2013-01-03 -0.861849 -2.104569 -0.494929 5.0 2.0 NaN 2013-01-04 0.721555 -0.706771 -1.039575 5.0 3.0 NaN DataFrame.dropna() drops any rows that have missing data: df1.dropna(how=\"any\") A B C D F E 2013-01-02 1.212112 -0.173215 0.119209 5.0 1.0 1.0 DataFrame.fillna() fills missing data: df1.fillna(value=5) A B C D F E 2013-01-01 0.000000 0.000000 -1.509059 5.0 5.0 1.0 2013-01-02 1.212112 -0.173215 0.119209 5.0 1.0 1.0 2013-01-03 -0.861849 -2.104569 -0.494929 5.0 2.0 5.0 2013-01-04 0.721555 -0.706771 -1.039575 5.0 3.0 5.0 isna() gets the boolean mask where values are nan: pd.isna(df1) A B C D F E 2013-01-01 False False False False True False 2013-01-02 False False False False False False 2013-01-03 False False False False False True 2013-01-04 False False False False False True", "prev_chunk_id": "chunk_13", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_15", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Operations#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# See the Basic section on Binary Ops.", "prev_chunk_id": "chunk_14", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_16", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Stats#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Stats#", "content": "Stats# Operations in general exclude missing data. Calculate the mean value for each column: df.mean() A -0.004474 B -0.383981 C -0.687758 D 5.000000 F 3.000000 dtype: float64 Calculate the mean value for each row: df.mean(axis=1) 2013-01-01 0.872735 2013-01-02 1.431621 2013-01-03 0.707731 2013-01-04 1.395042 2013-01-05 1.883656 2013-01-06 1.592306 Freq: D, dtype: float64 Operating with another Series or DataFrame with a different index or column will align the result with the union of the index or column labels. In addition, pandas automatically broadcasts along the specified dimension and will fill unaligned labels with np.nan. s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2) s 2013-01-01 NaN 2013-01-02 NaN 2013-01-03 1.0 2013-01-04 3.0 2013-01-05 5.0 2013-01-06 NaN Freq: D, dtype: float64 df.sub(s, axis=\"index\") A B C D F 2013-01-01 NaN NaN NaN NaN NaN 2013-01-02 NaN NaN NaN NaN NaN 2013-01-03 -1.861849 -3.104569 -1.494929 4.0 1.0 2013-01-04 -2.278445 -3.706771 -4.039575 2.0 0.0 2013-01-05 -5.424972 -4.432980 -4.723768 0.0 -1.0 2013-01-06 NaN NaN NaN NaN NaN", "prev_chunk_id": "chunk_15", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_17", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "User defined functions#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "User defined functions#", "content": "User defined functions# DataFrame.agg() and DataFrame.transform() applies a user defined function that reduces or broadcasts its result respectively. df.agg(lambda x: np.mean(x) * 5.6) A -0.025054 B -2.150294 C -3.851445 D 28.000000 F 16.800000 dtype: float64 df.transform(lambda x: x * 101.2) A B C D F 2013-01-01 0.000000 0.000000 -152.716721 506.0 NaN 2013-01-02 122.665737 -17.529322 12.063922 506.0 101.2 2013-01-03 -87.219115 -212.982405 -50.086843 506.0 202.4 2013-01-04 73.021382 -71.525239 -105.204988 506.0 303.6 2013-01-05 -43.007200 57.382459 27.954680 506.0 404.8 2013-01-06 -68.177398 11.501219 -149.616767 506.0 506.0", "prev_chunk_id": "chunk_16", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_18", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Value Counts#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Value Counts#", "content": "Value Counts# See more at Histogramming and Discretization. s = pd.Series(np.random.randint(0, 7, size=10)) s 0 4 1 2 2 1 3 2 4 6 5 4 6 4 7 6 8 4 9 4 dtype: int64 s.value_counts() 4 5 2 2 6 2 1 1 Name: count, dtype: int64", "prev_chunk_id": "chunk_17", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_19", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "String Methods#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "String Methods#", "content": "String Methods# Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. See more at Vectorized String Methods. s = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"]) s.str.lower() 0 a 1 b 2 c 3 aaba 4 baca 5 NaN 6 caba 7 dog 8 cat dtype: object", "prev_chunk_id": "chunk_18", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_20", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Concat#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Concat#", "content": "Concat# pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. See the Merging section. Concatenating pandas objects together row-wise with concat(): df = pd.DataFrame(np.random.randn(10, 4)) df 0 1 2 3 0 -0.548702 1.467327 -1.015962 -0.483075 1 1.637550 -1.217659 -0.291519 -1.745505 2 -0.263952 0.991460 -0.919069 0.266046 3 -0.709661 1.669052 1.037882 -1.705775 4 -0.919854 -0.042379 1.247642 -0.009920 5 0.290213 0.495767 0.362949 1.548106 6 -1.131345 -0.089329 0.337863 -0.945867 7 -0.932132 1.956030 0.017587 -0.016692 8 -0.575247 0.254161 -1.143704 0.215897 9 1.193555 -0.077118 -0.408530 -0.862495 # break it into pieces pieces = [df[:3], df[3:7], df[7:]] pd.concat(pieces) 0 1 2 3 0 -0.548702 1.467327 -1.015962 -0.483075 1 1.637550 -1.217659 -0.291519 -1.745505 2 -0.263952 0.991460 -0.919069 0.266046 3 -0.709661 1.669052 1.037882 -1.705775 4 -0.919854 -0.042379 1.247642 -0.009920 5 0.290213 0.495767 0.362949 1.548106 6 -1.131345 -0.089329 0.337863 -0.945867 7 -0.932132 1.956030 0.017587 -0.016692 8 -0.575247 0.254161 -1.143704 0.215897 9 1.193555 -0.077118 -0.408530 -0.862495", "prev_chunk_id": "chunk_19", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_21", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Join#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Join#", "content": "Join# merge() enables SQL style join types along specific columns. See the Database style joining section. left = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"lval\": [1, 2]}) right = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"rval\": [4, 5]}) left key lval 0 foo 1 1 foo 2 right key rval 0 foo 4 1 foo 5 pd.merge(left, right, on=\"key\") key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 merge() on unique keys: left = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"lval\": [1, 2]}) right = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"rval\": [4, 5]}) left key lval 0 foo 1 1 bar 2 right key rval 0 foo 4 1 bar 5 pd.merge(left, right, on=\"key\") key lval rval 0 foo 1 4 1 bar 2 5", "prev_chunk_id": "chunk_20", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_22", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Grouping#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Grouping#", "content": "Grouping# By “group by” we are referring to a process involving one or more of the following steps: - Splittingthe data into groups based on some criteria - Applyinga function to each group independently - Combiningthe results into a data structure See the Grouping section. df = pd.DataFrame( ....: { ....: \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"], ....: \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"], ....: \"C\": np.random.randn(8), ....: \"D\": np.random.randn(8), ....: } ....: ) ....: df A B C D 0 foo one 1.346061 -1.577585 1 bar one 1.511763 0.396823 2 foo two 1.627081 -0.105381 3 bar three -0.990582 -0.532532 4 foo two -0.441652 1.453749 5 bar two 1.211526 1.208843 6 foo one 0.268520 -0.080952 7 foo three 0.024580 -0.264610 Grouping by a column label, selecting column labels, and then applying the DataFrameGroupBy.sum() function to the resulting groups: df.groupby(\"A\")[[\"C\", \"D\"]].sum() C D A bar 1.732707 1.073134 foo 2.824590 -0.574779 Grouping by multiple columns label forms MultiIndex. df.groupby([\"A\", \"B\"]).sum() C D A B bar one 1.511763 0.396823 three -0.990582 -0.532532 two 1.211526 1.208843 foo one 1.614581 -1.658537 three 0.024580 -0.264610 two 1.185429 1.348368", "prev_chunk_id": "chunk_21", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_23", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Reshaping#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Reshaping#", "content": "Reshaping# See the sections on Hierarchical Indexing and Reshaping.", "prev_chunk_id": "chunk_22", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_24", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Stack#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Stack#", "content": "Stack# arrays = [ ....: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ....: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ....: ] ....: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"]) df2 = df[:4] df2 A B first second bar one -0.727965 -0.589346 two 0.339969 -0.693205 baz one -0.339355 0.593616 two 0.884345 1.591431 The stack() method “compresses” a level in the DataFrame’s columns: stacked = df2.stack(future_stack=True) stacked first second bar one A -0.727965 B -0.589346 two A 0.339969 B -0.693205 baz one A -0.339355 B 0.593616 two A 0.884345 B 1.591431 dtype: float64 With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level: stacked.unstack() A B first second bar one -0.727965 -0.589346 two 0.339969 -0.693205 baz one -0.339355 0.593616 two 0.884345 1.591431 stacked.unstack(1) second one two first bar A -0.727965 0.339969 B -0.589346 -0.693205 baz A -0.339355 0.884345 B 0.593616 1.591431 stacked.unstack(0) first bar baz second one A -0.727965 -0.339355 B -0.589346 0.593616 two A 0.339969 0.884345 B -0.693205 1.591431", "prev_chunk_id": "chunk_23", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_25", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Pivot tables#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Pivot tables#", "content": "Pivot tables# See the section on Pivot Tables. df = pd.DataFrame( .....: { .....: \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3, .....: \"B\": [\"A\", \"B\", \"C\"] * 4, .....: \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2, .....: \"D\": np.random.randn(12), .....: \"E\": np.random.randn(12), .....: } .....: ) .....: df A B C D E 0 one A foo -1.202872 0.047609 1 one B foo -1.814470 -0.136473 2 two C foo 1.018601 -0.561757 3 three A bar -0.595447 -1.623033 4 one B bar 1.395433 0.029399 5 one C bar -0.392670 -0.542108 6 two A foo 0.007207 0.282696 7 three B foo 1.928123 -0.087302 8 one C foo -0.055224 -1.575170 9 one A bar 2.395985 1.771208 10 two B bar 1.552825 0.816482 11 three C bar 0.166599 1.100230 pivot_table() pivots a DataFrame specifying the values, index and columns pd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"]) C bar foo A B one A 2.395985 -1.202872 B 1.395433 -1.814470 C -0.392670 -0.055224 three A -0.595447 NaN B NaN 1.928123 C 0.166599 NaN two A NaN 0.007207 B 1.552825 NaN C NaN 1.018601", "prev_chunk_id": "chunk_24", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_26", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Time series#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Time series#", "content": "Time series# pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the Time Series section. rng = pd.date_range(\"1/1/2012\", periods=100, freq=\"s\") ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample(\"5Min\").sum() 2012-01-01 24182 Freq: 5min, dtype: int64 Series.tz_localize() localizes a time series to a time zone: rng = pd.date_range(\"3/6/2012 00:00\", periods=5, freq=\"D\") ts = pd.Series(np.random.randn(len(rng)), rng) ts 2012-03-06 1.857704 2012-03-07 -1.193545 2012-03-08 0.677510 2012-03-09 -0.153931 2012-03-10 0.520091 Freq: D, dtype: float64 ts_utc = ts.tz_localize(\"UTC\") ts_utc 2012-03-06 00:00:00+00:00 1.857704 2012-03-07 00:00:00+00:00 -1.193545 2012-03-08 00:00:00+00:00 0.677510 2012-03-09 00:00:00+00:00 -0.153931 2012-03-10 00:00:00+00:00 0.520091 Freq: D, dtype: float64 Series.tz_convert() converts a timezones aware time series to another time zone: ts_utc.tz_convert(\"US/Eastern\") 2012-03-05 19:00:00-05:00 1.857704 2012-03-06 19:00:00-05:00 -1.193545 2012-03-07 19:00:00-05:00 0.677510 2012-03-08 19:00:00-05:00 -0.153931 2012-03-09 19:00:00-05:00 0.520091 Freq: D, dtype: float64 Adding a non-fixed duration (BusinessDay) to a time series: rng DatetimeIndex(['2012-03-06', '2012-03-07', '2012-03-08', '2012-03-09', '2012-03-10'], dtype='datetime64[ns]', freq='D') rng + pd.offsets.BusinessDay(5) DatetimeIndex(['2012-03-13', '2012-03-14', '2012-03-15', '2012-03-16', '2012-03-16'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_25", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_27", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Categoricals#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Categoricals#", "content": "Categoricals# pandas can include categorical data in a DataFrame. For full docs, see the categorical introduction and the API documentation. df = pd.DataFrame( .....: {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]} .....: ) .....: Converting the raw grades to a categorical data type: df[\"grade\"] = df[\"raw_grade\"].astype(\"category\") df[\"grade\"] 0 a 1 b 2 b 3 a 4 a 5 e Name: grade, dtype: category Categories (3, object): ['a', 'b', 'e'] Rename the categories to more meaningful names: new_categories = [\"very good\", \"good\", \"very bad\"] df[\"grade\"] = df[\"grade\"].cat.rename_categories(new_categories) Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default): df[\"grade\"] = df[\"grade\"].cat.set_categories( .....: [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"] .....: ) .....: df[\"grade\"] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good'] Sorting is per order in the categories, not lexical order: df.sort_values(by=\"grade\") id raw_grade grade 5 6 e very bad 1 2 b good 2 3 b good 0 1 a very good 3 4 a very good 4 5 a very good Grouping by a categorical column with observed=False also shows empty categories: df.groupby(\"grade\", observed=False).size() grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64", "prev_chunk_id": "chunk_26", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_28", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Plotting#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Plotting#", "content": "Plotting# See the Plotting docs. We use the standard convention for referencing the matplotlib API: import matplotlib.pyplot as plt plt.close(\"all\") The plt.close method is used to close a figure window: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000)) ts = ts.cumsum() ts.plot(); plot() plots all columns: df = pd.DataFrame( .....: np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"] .....: ) .....: df = df.cumsum() plt.figure(); df.plot(); plt.legend(loc='best');", "prev_chunk_id": "chunk_27", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_29", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Importing and exporting data#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Importing and exporting data#", "content": "Importing and exporting data# See the IO Tools section.", "prev_chunk_id": "chunk_28", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_30", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "CSV#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "CSV#", "content": "CSV# Writing to a csv file: using DataFrame.to_csv() df = pd.DataFrame(np.random.randint(0, 5, (10, 5))) df.to_csv(\"foo.csv\") Reading from a csv file: using read_csv() pd.read_csv(\"foo.csv\") Unnamed: 0 0 1 2 3 4 0 0 4 3 1 1 2 1 1 1 0 2 3 2 2 2 1 4 2 1 2 3 3 0 4 0 2 2 4 4 4 2 2 3 4 5 5 4 0 4 3 1 6 6 2 1 2 0 3 7 7 4 0 4 4 4 8 8 4 4 1 0 1 9 9 0 4 3 0 3", "prev_chunk_id": "chunk_29", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_31", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Parquet#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Parquet#", "content": "Parquet# Writing to a Parquet file: df.to_parquet(\"foo.parquet\") Reading from a Parquet file Store using read_parquet(): pd.read_parquet(\"foo.parquet\") 0 1 2 3 4 0 4 3 1 1 2 1 1 0 2 3 2 2 1 4 2 1 2 3 0 4 0 2 2 4 4 2 2 3 4 5 4 0 4 3 1 6 2 1 2 0 3 7 4 0 4 4 4 8 4 4 1 0 1 9 0 4 3 0 3", "prev_chunk_id": "chunk_30", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_32", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Excel#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Excel#", "content": "Excel# Reading and writing to Excel. Writing to an excel file using DataFrame.to_excel(): df.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\") Reading from an excel file using read_excel(): pd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) Unnamed: 0 0 1 2 3 4 0 0 4 3 1 1 2 1 1 1 0 2 3 2 2 2 1 4 2 1 2 3 3 0 4 0 2 2 4 4 4 2 2 3 4 5 5 4 0 4 3 1 6 6 2 1 2 0 3 7 7 4 0 4 4 4 8 8 4 4 1 0 1 9 9 0 4 3 0 3", "prev_chunk_id": "chunk_31", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_33", "url": "https://pandas.pydata.org/docs/user_guide/10min.html", "title": "Gotchas#", "page_title": "10 minutes to pandas — pandas 2.3.1 documentation", "breadcrumbs": "Gotchas#", "content": "Gotchas# If you are attempting to perform a boolean operation on a Series or DataFrame you might see an exception like: if pd.Series([False, True, False]): .....: print(\"I was true\") .....: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-141-b27eb9c1dfc0> in ?() ----> 1 if pd.Series([False, True, False]): 2 print(\"I was true\") ~/work/pandas/pandas/pandas/core/generic.py in ?(self) 1575 @final 1576 def __nonzero__(self) -> NoReturn: -> 1577 raise ValueError( 1578 f\"The truth value of a {type(self).__name__} is ambiguous. \" 1579 \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\" 1580 ) ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). See Comparisons and Gotchas for an explanation and what to do.", "prev_chunk_id": "chunk_32", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_34", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Intro to data structures#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Intro to data structures#", "content": "Intro to data structures# We’ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, axis labeling, and alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace: import numpy as np import pandas as pd Fundamentally, data alignment is intrinsic. The link between labels and data will not be broken unless done so explicitly by you. We’ll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_35", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Series#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Series#", "content": "Series# Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call: s = pd.Series(data, index=index) Here, data can be many different things: - a Python dict - an ndarray - a scalar value (like 5) The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is: From ndarray If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1]. s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) s a 0.469112 b -0.282863 c -1.509059 d -1.135632 e 1.212112 dtype: float64 s.index Index(['a', 'b', 'c', 'd', 'e'], dtype='object') pd.Series(np.random.randn(5)) 0 -0.173215 1 0.119209 2 -1.044236 3 -0.861849 4 -2.104569 dtype: float64 From dict Series can be instantiated from dicts: d = {\"b\": 1, \"a\": 0, \"c\": 2} pd.Series(d) b 1 a 0 c 2 dtype: int64 If an index is passed, the values in data corresponding to the labels in the index will be pulled out. d = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0} pd.Series(d) a 0.0 b 1.0 c 2.0 dtype: float64 pd.Series(d, index=[\"b\", \"c\", \"d\", \"a\"]) b 1.0 c 2.0 d NaN a 0.0 dtype: float64 From scalar value If data is a scalar value, an index must be provided. The value will be repeated to match the length of index. pd.Series(5.0, index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) a 5.0 b 5.0 c 5.0 d 5.0 e 5.0 dtype: float64", "prev_chunk_id": "chunk_34", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_36", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Series is ndarray-like#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Series is ndarray-like#", "content": "Series is ndarray-like# Series acts very similarly to a ndarray and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index. s.iloc[0] 0.4691122999071863 s.iloc[:3] a 0.469112 b -0.282863 c -1.509059 dtype: float64 s[s > s.median()] a 0.469112 e 1.212112 dtype: float64 s.iloc[[4, 3, 1]] e 1.212112 d -1.135632 b -0.282863 dtype: float64 np.exp(s) a 1.598575 b 0.753623 c 0.221118 d 0.321219 e 3.360575 dtype: float64 Like a NumPy array, a pandas Series has a single dtype. s.dtype dtype('float64') This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy’s type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. See dtypes for more. If you need the actual array backing a Series, use Series.array. s.array <NumpyExtensionArray> [ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124, -1.1356323710171934, 1.2121120250208506] Length: 5, dtype: float64 Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example). Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame. See dtypes for more. While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy(). s.to_numpy() array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.", "prev_chunk_id": "chunk_35", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_37", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Series is dict-like#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Series is dict-like#", "content": "Series is dict-like# A Series is also like a fixed-size dict in that you can get and set values by index label: s[\"a\"] 0.4691122999071863 s[\"e\"] = 12.0 s a 0.469112 b -0.282863 c -1.509059 d -1.135632 e 12.000000 dtype: float64 \"e\" in s True \"f\" in s False If a label is not contained in the index, an exception is raised: s[\"f\"] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key) 3811 try: -> 3812 return self._engine.get_loc(casted_key) 3813 except KeyError as err: File ~/work/pandas/pandas/pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc() File ~/work/pandas/pandas/pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc() File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item() File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: 'f' The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) Cell In[26], line 1 ----> 1 s[\"f\"] File ~/work/pandas/pandas/pandas/core/series.py:1130, in Series.__getitem__(self, key) 1127 return self._values[key] 1129 elif key_is_scalar: -> 1130 return self._get_value(key) 1132 # Convert generator to list before going through hashable part 1133 # (We will iterate through the generator there to check for slices) 1134 if is_iterator(key): File ~/work/pandas/pandas/pandas/core/series.py:1246, in Series._get_value(self, label, takeable) 1243 return self._values[label] 1245 # Similar to Index.get_value, but we do not fall back to positional -> 1246 loc = self.index.get_loc(label) 1248 if is_integer(loc): 1249 return self._values[loc] File ~/work/pandas/pandas/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key) 3814 if isinstance(casted_key, slice) or ( 3815 isinstance(casted_key, abc.Iterable) 3816 and any(isinstance(x, slice) for x in casted_key) 3817 ): 3818 raise InvalidIndexError(key) -> 3819 raise KeyError(key) from err 3820 except TypeError: 3821 # If we have a listlike key, _check_indexing_error will raise 3822 # InvalidIndexError. Otherwise we fall through and re-raise 3823 # the TypeError. 3824 self._check_indexing_error(key) KeyError: 'f' Using the Series.get() method, a missing label will return None or specified default: s.get(\"f\") s.get(\"f\", np.nan) nan These labels can also be accessed by attribute.", "prev_chunk_id": "chunk_36", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_38", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Vectorized operations and label alignment with Series#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Vectorized operations and label alignment with Series#", "content": "Vectorized operations and label alignment with Series# When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray. s + s a 0.938225 b -0.565727 c -3.018117 d -2.271265 e 24.000000 dtype: float64 s * 2 a 0.938225 b -0.565727 c -3.018117 d -2.271265 e 24.000000 dtype: float64 np.exp(s) a 1.598575 b 0.753623 c 0.221118 d 0.321219 e 162754.791419 dtype: float64 A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels. s.iloc[1:] + s.iloc[:-1] a NaN b -0.565727 c -3.018117 d -2.271265 e NaN dtype: float64 The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.", "prev_chunk_id": "chunk_37", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_39", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Name attribute#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Name attribute#", "content": "Name attribute# Series also has a name attribute: s = pd.Series(np.random.randn(5), name=\"something\") s 0 -0.494929 1 1.071804 2 0.721555 3 -0.706771 4 -1.039575 Name: something, dtype: float64 s.name 'something' The Series name can be assigned automatically in many cases, in particular, when selecting a single column from a DataFrame, the name will be assigned the column label. You can rename a Series with the pandas.Series.rename() method. s2 = s.rename(\"different\") s2.name 'different' Note that s and s2 refer to different objects.", "prev_chunk_id": "chunk_38", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_40", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "DataFrame#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame#", "content": "DataFrame# DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input: - Dict of 1D ndarrays, lists, dicts, orSeries - 2-D numpy.ndarray - Structured or recordndarray - ASeries - AnotherDataFrame Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index. If axis labels are not passed, they will be constructed from the input data based on common sense rules.", "prev_chunk_id": "chunk_39", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_41", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From dict of Series or dicts#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From dict of Series or dicts#", "content": "From dict of Series or dicts# The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys. d = { ....: \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]), ....: \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]), ....: } ....: df = pd.DataFrame(d) df one two a 1.0 1.0 b 2.0 2.0 c 3.0 3.0 d NaN 4.0 pd.DataFrame(d, index=[\"d\", \"b\", \"a\"]) one two d NaN 4.0 b 2.0 2.0 a 1.0 1.0 pd.DataFrame(d, index=[\"d\", \"b\", \"a\"], columns=[\"two\", \"three\"]) two three d 4.0 NaN b 2.0 NaN a 1.0 NaN The row and column labels can be accessed respectively by accessing the index and columns attributes: df.index Index(['a', 'b', 'c', 'd'], dtype='object') df.columns Index(['one', 'two'], dtype='object')", "prev_chunk_id": "chunk_40", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_42", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From dict of ndarrays / lists#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From dict of ndarrays / lists#", "content": "From dict of ndarrays / lists# All ndarrays must share the same length. If an index is passed, it must also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length. d = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]} pd.DataFrame(d) one two 0 1.0 4.0 1 2.0 3.0 2 3.0 2.0 3 4.0 1.0 pd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"]) one two a 1.0 4.0 b 2.0 3.0 c 3.0 2.0 d 4.0 1.0", "prev_chunk_id": "chunk_41", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_43", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From structured or record array#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From structured or record array#", "content": "From structured or record array# This case is handled identically to a dict of arrays. data = np.zeros((2,), dtype=[(\"A\", \"i4\"), (\"B\", \"f4\"), (\"C\", \"a10\")]) data[:] = [(1, 2.0, \"Hello\"), (2, 3.0, \"World\")] pd.DataFrame(data) A B C 0 1 2.0 b'Hello' 1 2 3.0 b'World' pd.DataFrame(data, index=[\"first\", \"second\"]) A B C first 1 2.0 b'Hello' second 2 3.0 b'World' pd.DataFrame(data, columns=[\"C\", \"A\", \"B\"]) C A B 0 b'Hello' 1 2.0 1 b'World' 2 3.0", "prev_chunk_id": "chunk_42", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_44", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From a list of dicts#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From a list of dicts#", "content": "From a list of dicts# data2 = [{\"a\": 1, \"b\": 2}, {\"a\": 5, \"b\": 10, \"c\": 20}] pd.DataFrame(data2) a b c 0 1 2 NaN 1 5 10 20.0 pd.DataFrame(data2, index=[\"first\", \"second\"]) a b c first 1 2 NaN second 5 10 20.0 pd.DataFrame(data2, columns=[\"a\", \"b\"]) a b 0 1 2 1 5 10", "prev_chunk_id": "chunk_43", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_45", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From a dict of tuples#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From a dict of tuples#", "content": "From a dict of tuples# You can automatically create a MultiIndexed frame by passing a tuples dictionary. pd.DataFrame( ....: { ....: (\"a\", \"b\"): {(\"A\", \"B\"): 1, (\"A\", \"C\"): 2}, ....: (\"a\", \"a\"): {(\"A\", \"C\"): 3, (\"A\", \"B\"): 4}, ....: (\"a\", \"c\"): {(\"A\", \"B\"): 5, (\"A\", \"C\"): 6}, ....: (\"b\", \"a\"): {(\"A\", \"C\"): 7, (\"A\", \"B\"): 8}, ....: (\"b\", \"b\"): {(\"A\", \"D\"): 9, (\"A\", \"B\"): 10}, ....: } ....: ) ....: a b b a c a b A B 1.0 4.0 5.0 8.0 10.0 C 2.0 3.0 6.0 7.0 NaN D NaN NaN NaN NaN 9.0", "prev_chunk_id": "chunk_44", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_46", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From a Series#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From a Series#", "content": "From a Series# The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided). ser = pd.Series(range(3), index=list(\"abc\"), name=\"ser\") pd.DataFrame(ser) ser a 0 b 1 c 2", "prev_chunk_id": "chunk_45", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_47", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From a list of namedtuples#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From a list of namedtuples#", "content": "From a list of namedtuples# The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised. from collections import namedtuple Point = namedtuple(\"Point\", \"x y\") pd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)]) x y 0 0 0 1 0 3 2 2 3 Point3D = namedtuple(\"Point3D\", \"x y z\") pd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)]) x y z 0 0 0 0.0 1 0 3 5.0 2 2 3 NaN", "prev_chunk_id": "chunk_46", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_48", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "From a list of dataclasses#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "From a list of dataclasses#", "content": "From a list of dataclasses# Data Classes as introduced in PEP557, can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries. Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError. from dataclasses import make_dataclass Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)]) pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)]) x y 0 0 0 1 0 3 2 2 3 Missing data To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing. See Missing data for more.", "prev_chunk_id": "chunk_47", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_49", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Alternate constructors#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Alternate constructors#", "content": "Alternate constructors# DataFrame.from_dict DataFrame.from_dict() takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels. pd.DataFrame.from_dict(dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])])) A B 0 1 4 1 2 5 2 3 6 If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names: pd.DataFrame.from_dict( ....: dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]), ....: orient=\"index\", ....: columns=[\"one\", \"two\", \"three\"], ....: ) ....: one two three A 1 2 3 B 4 5 6 DataFrame.from_records DataFrame.from_records() takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. data array([(1, 2., b'Hello'), (2, 3., b'World')], dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')]) pd.DataFrame.from_records(data, index=\"C\") A B C b'Hello' 1 2.0 b'World' 2 3.0", "prev_chunk_id": "chunk_48", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_50", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Column selection, addition, deletion#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Column selection, addition, deletion#", "content": "Column selection, addition, deletion# You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations: df[\"one\"] a 1.0 b 2.0 c 3.0 d NaN Name: one, dtype: float64 df[\"three\"] = df[\"one\"] * df[\"two\"] df[\"flag\"] = df[\"one\"] > 2 df one two three flag a 1.0 1.0 1.0 False b 2.0 2.0 4.0 False c 3.0 3.0 9.0 True d NaN 4.0 NaN False Columns can be deleted or popped like with a dict: del df[\"two\"] three = df.pop(\"three\") df one flag a 1.0 False b 2.0 False c 3.0 True d NaN False When inserting a scalar value, it will naturally be propagated to fill the column: df[\"foo\"] = \"bar\" df one flag foo a 1.0 False bar b 2.0 False bar c 3.0 True bar d NaN False bar When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame’s index: df[\"one_trunc\"] = df[\"one\"][:2] df one flag foo one_trunc a 1.0 False bar 1.0 b 2.0 False bar 2.0 c 3.0 True bar NaN d NaN False bar NaN You can insert raw ndarrays but their length must match the length of the DataFrame’s index. By default, columns get inserted at the end. DataFrame.insert() inserts at a particular location in the columns: df.insert(1, \"bar\", df[\"one\"]) df one bar flag foo one_trunc a 1.0 1.0 False bar 1.0 b 2.0 2.0 False bar 2.0 c 3.0 3.0 True bar NaN d NaN NaN False bar NaN", "prev_chunk_id": "chunk_49", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_51", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Assigning new columns in method chains#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Assigning new columns in method chains#", "content": "Assigning new columns in method chains# Inspired by dplyr’s mutate verb, DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns. iris = pd.read_csv(\"data/iris.data\") iris.head() SepalLength SepalWidth PetalLength PetalWidth Name 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa iris.assign(sepal_ratio=iris[\"SepalWidth\"] / iris[\"SepalLength\"]).head() SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio 0 5.1 3.5 1.4 0.2 Iris-setosa 0.686275 1 4.9 3.0 1.4 0.2 Iris-setosa 0.612245 2 4.7 3.2 1.3 0.2 Iris-setosa 0.680851 3 4.6 3.1 1.5 0.2 Iris-setosa 0.673913 4 5.0 3.6 1.4 0.2 Iris-setosa 0.720000 In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to. iris.assign(sepal_ratio=lambda x: (x[\"SepalWidth\"] / x[\"SepalLength\"])).head() SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio 0 5.1 3.5 1.4 0.2 Iris-setosa 0.686275 1 4.9 3.0 1.4 0.2 Iris-setosa 0.612245 2 4.7 3.2 1.3 0.2 Iris-setosa 0.680851 3 4.6 3.1 1.5 0.2 Iris-setosa 0.673913 4 5.0 3.6 1.4 0.2 Iris-setosa 0.720000 assign() always returns a copy of the data, leaving the original DataFrame untouched. Passing a callable, as opposed to an actual value to be inserted, is useful when you don’t have a reference to the DataFrame at hand. This is common when using assign() in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot: ( ....: iris.query(\"SepalLength > 5\") ....: .assign( ....: SepalRatio=lambda x: x.SepalWidth / x.SepalLength, ....: PetalRatio=lambda x: x.PetalWidth / x.PetalLength, ....: ) ....: .plot(kind=\"scatter\", x=\"SepalRatio\", y=\"PetalRatio\") ....: ) ....: <Axes: xlabel='SepalRatio', ylabel='PetalRatio'> Since a function is passed in, the function is", "prev_chunk_id": "chunk_50", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_52", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Assigning new columns in method chains#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Assigning new columns in method chains#", "content": "computed on the DataFrame being assigned to. Importantly, this is the DataFrame that’s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn’t have a reference to the filtered DataFrame available. The function signature for assign() is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted. The order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign(). dfa = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) dfa.assign(C=lambda x: x[\"A\"] + x[\"B\"], D=lambda x: x[\"A\"] + x[\"C\"]) A B C D 0 1 4 5 6 1 2 5 7 9 2 3 6 9 12 In the second expression, x['C'] will refer to the newly created column, that’s equal to dfa['A'] + dfa['B'].", "prev_chunk_id": "chunk_51", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_53", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Indexing / selection#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Indexing / selection#", "content": "Indexing / selection# The basics of indexing are as follows: Operation | Syntax | Result Select column | df[col] | Series Select row by label | df.loc[label] | Series Select row by integer location | df.iloc[loc] | Series Slice rows | df[5:10] | DataFrame Select rows by boolean vector | df[bool_vec] | DataFrame Row selection, for example, returns a Series whose index is the columns of the DataFrame: df.loc[\"b\"] one 2.0 bar 2.0 flag False foo bar one_trunc 2.0 Name: b, dtype: object df.iloc[2] one 3.0 bar 3.0 flag True foo bar one_trunc NaN Name: c, dtype: object For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the section on indexing. We will address the fundamentals of reindexing / conforming to new sets of labels in the section on reindexing.", "prev_chunk_id": "chunk_52", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_54", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Data alignment and arithmetic#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Data alignment and arithmetic#", "content": "Data alignment and arithmetic# Data alignment between DataFrame objects automatically align on both the columns and the index (row labels). Again, the resulting object will have the union of the column and row labels. df = pd.DataFrame(np.random.randn(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"]) df2 = pd.DataFrame(np.random.randn(7, 3), columns=[\"A\", \"B\", \"C\"]) df + df2 A B C D 0 0.045691 -0.014138 1.380871 NaN 1 -0.955398 -1.501007 0.037181 NaN 2 -0.662690 1.534833 -0.859691 NaN 3 -2.452949 1.237274 -0.133712 NaN 4 1.414490 1.951676 -2.320422 NaN 5 -0.494922 -1.649727 -1.084601 NaN 6 -1.047551 -0.748572 -0.805479 NaN 7 NaN NaN NaN NaN 8 NaN NaN NaN NaN 9 NaN NaN NaN NaN When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example: df - df.iloc[0] A B C D 0 0.000000 0.000000 0.000000 0.000000 1 -1.359261 -0.248717 -0.453372 -1.754659 2 0.253128 0.829678 0.010026 -1.991234 3 -1.311128 0.054325 -1.724913 -1.620544 4 0.573025 1.500742 -0.676070 1.367331 5 -1.741248 0.781993 -1.241620 -2.053136 6 -1.240774 -0.869551 -0.153282 0.000430 7 -0.743894 0.411013 -0.929563 -0.282386 8 -1.194921 1.320690 0.238224 -1.482644 9 2.293786 1.856228 0.773289 -1.446531 For explicit control over the matching and broadcasting behavior, see the section on flexible binary operations. Arithmetic operations with scalars operate element-wise: df * 5 + 2 A B C D 0 3.359299 -0.124862 4.835102 3.381160 1 -3.437003 -1.368449 2.568242 -5.392133 2 4.624938 4.023526 4.885230 -6.575010 3 -3.196342 0.146766 -3.789461 -4.721559 4 6.224426 7.378849 1.454750 10.217815 5 -5.346940 3.785103 -1.373001 -6.884519 6 -2.844569 -4.472618 4.068691 3.383309 7 -0.360173 1.930201 0.187285 1.969232 8 -2.615303 6.478587 6.026220 -4.032059 9 14.828230 9.156280 8.701544 -3.851494 1 / df A B C D 0 3.678365 -2.353094 1.763605 3.620145 1 -0.919624 -1.484363 8.799067 -0.676395 2 1.904807 2.470934 1.732964 -0.583090 3 -0.962215 -2.697986 -0.863638 -0.743875 4 1.183593 0.929567", "prev_chunk_id": "chunk_53", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_55", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Data alignment and arithmetic#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Data alignment and arithmetic#", "content": "-9.170108 0.608434 5 -0.680555 2.800959 -1.482360 -0.562777 6 -1.032084 -0.772485 2.416988 3.614523 7 -2.118489 -71.634509 -2.758294 -162.507295 8 -1.083352 1.116424 1.241860 -0.828904 9 0.389765 0.698687 0.746097 -0.854483 df ** 4 A B C D 0 0.005462 3.261689e-02 0.103370 5.822320e-03 1 1.398165 2.059869e-01 0.000167 4.777482e+00 2 0.075962 2.682596e-02 0.110877 8.650845e+00 3 1.166571 1.887302e-02 1.797515 3.265879e+00 4 0.509555 1.339298e+00 0.000141 7.297019e+00 5 4.661717 1.624699e-02 0.207103 9.969092e+00 6 0.881334 2.808277e+00 0.029302 5.858632e-03 7 0.049647 3.797614e-08 0.017276 1.433866e-09 8 0.725974 6.437005e-01 0.420446 2.118275e+00 9 43.329821 4.196326e+00 3.227153 1.875802e+00 Boolean operators operate element-wise as well: df1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [0, 1, 1]}, dtype=bool) df2 = pd.DataFrame({\"a\": [0, 1, 1], \"b\": [1, 1, 0]}, dtype=bool) df1 & df2 a b 0 False False 1 False True 2 True False df1 | df2 a b 0 True True 1 True True 2 True True df1 ^ df2 a b 0 True True 1 True False 2 False True -df1 a b 0 False True 1 True False 2 False False", "prev_chunk_id": "chunk_54", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_56", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Transposing#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Transposing#", "content": "Transposing# To transpose, access the T attribute or DataFrame.transpose(), similar to an ndarray: # only show the first 5 rows df[:5].T 0 1 2 3 4 A 0.271860 -1.087401 0.524988 -1.039268 0.844885 B -0.424972 -0.673690 0.404705 -0.370647 1.075770 C 0.567020 0.113648 0.577046 -1.157892 -0.109050 D 0.276232 -1.478427 -1.715002 -1.344312 1.643563", "prev_chunk_id": "chunk_55", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_57", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "DataFrame interoperability with NumPy functions#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame interoperability with NumPy functions#", "content": "DataFrame interoperability with NumPy functions# Most NumPy functions can be called directly on Series and DataFrame. np.exp(df) A B C D 0 1.312403 0.653788 1.763006 1.318154 1 0.337092 0.509824 1.120358 0.227996 2 1.690438 1.498861 1.780770 0.179963 3 0.353713 0.690288 0.314148 0.260719 4 2.327710 2.932249 0.896686 5.173571 5 0.230066 1.429065 0.509360 0.169161 6 0.379495 0.274028 1.512461 1.318720 7 0.623732 0.986137 0.695904 0.993865 8 0.397301 2.449092 2.237242 0.299269 9 13.009059 4.183951 3.820223 0.310274 np.asarray(df) array([[ 0.2719, -0.425 , 0.567 , 0.2762], [-1.0874, -0.6737, 0.1136, -1.4784], [ 0.525 , 0.4047, 0.577 , -1.715 ], [-1.0393, -0.3706, -1.1579, -1.3443], [ 0.8449, 1.0758, -0.109 , 1.6436], [-1.4694, 0.357 , -0.6746, -1.7769], [-0.9689, -1.2945, 0.4137, 0.2767], [-0.472 , -0.014 , -0.3625, -0.0062], [-0.9231, 0.8957, 0.8052, -1.2064], [ 2.5656, 1.4313, 1.3403, -1.1703]]) DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array. Series implements __array_ufunc__, which allows it to work with NumPy’s universal functions. The ufunc is applied to the underlying array in a Series. ser = pd.Series([1, 2, 3, 4]) np.exp(ser) 0 2.718282 1 7.389056 2 20.085537 3 54.598150 dtype: float64 When multiple Series are passed to a ufunc, they are aligned before performing the operation. Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation. ser1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"]) ser2 = pd.Series([1, 3, 5], index=[\"b\", \"a\", \"c\"]) ser1 a 1 b 2 c 3 dtype: int64 ser2 b 1 a 3 c 5 dtype: int64 np.remainder(ser1, ser2) a 1 b 0 c 3 dtype: int64 As usual, the union of the two indices is taken, and non-overlapping values", "prev_chunk_id": "chunk_56", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_58", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "DataFrame interoperability with NumPy functions#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame interoperability with NumPy functions#", "content": "are filled with missing values. ser3 = pd.Series([2, 4, 6], index=[\"b\", \"c\", \"d\"]) ser3 b 2 c 4 d 6 dtype: int64 np.remainder(ser1, ser3) a NaN b 0.0 c 3.0 d NaN dtype: float64 When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned. ser = pd.Series([1, 2, 3]) idx = pd.Index([4, 5, 6]) np.maximum(ser, idx) 0 4 1 5 2 6 dtype: int64 NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray (see Sparse calculation). If possible, the ufunc is applied without converting the underlying data to an ndarray.", "prev_chunk_id": "chunk_57", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_59", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Console display#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Console display#", "content": "Console display# A very large DataFrame will be truncated to display them in the console. You can also get a summary using info(). (The baseball dataset is from the plyr R package): baseball = pd.read_csv(\"data/baseball.csv\") print(baseball) id player year stint team lg ... so ibb hbp sh sf gidp 0 88641 womacto01 2006 2 CHN NL ... 4.0 0.0 0.0 3.0 0.0 0.0 1 88643 schilcu01 2006 1 BOS AL ... 1.0 0.0 0.0 0.0 0.0 0.0 .. ... ... ... ... ... .. ... ... ... ... ... ... ... 98 89533 aloumo01 2007 1 NYN NL ... 30.0 5.0 2.0 0.0 3.0 13.0 99 89534 alomasa02 2007 1 NYN NL ... 3.0 0.0 0.0 0.0 0.0 0.0 [100 rows x 23 columns] baseball.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 100 entries, 0 to 99 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 100 non-null int64 1 player 100 non-null object 2 year 100 non-null int64 3 stint 100 non-null int64 4 team 100 non-null object 5 lg 100 non-null object 6 g 100 non-null int64 7 ab 100 non-null int64 8 r 100 non-null int64 9 h 100 non-null int64 10 X2b 100 non-null int64 11 X3b 100 non-null int64 12 hr 100 non-null int64 13 rbi 100 non-null float64 14 sb 100 non-null float64 15 cs 100 non-null float64 16 bb 100 non-null int64 17 so 100 non-null float64 18 ibb 100 non-null float64 19 hbp 100 non-null float64 20 sh 100 non-null float64 21 sf 100 non-null float64 22 gidp 100 non-null float64 dtypes: float64(9), int64(11), object(3) memory usage: 18.1+ KB However, using DataFrame.to_string() will return a string representation of the DataFrame in tabular form, though it won’t always fit the console width: print(baseball.iloc[-20:, :12].to_string()) id player year stint team lg", "prev_chunk_id": "chunk_58", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_60", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Console display#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Console display#", "content": "g ab r h X2b X3b 80 89474 finlest01 2007 1 COL NL 43 94 9 17 3 0 81 89480 embreal01 2007 1 OAK AL 4 0 0 0 0 0 82 89481 edmonji01 2007 1 SLN NL 117 365 39 92 15 2 83 89482 easleda01 2007 1 NYN NL 76 193 24 54 6 0 84 89489 delgaca01 2007 1 NYN NL 139 538 71 139 30 0 85 89493 cormirh01 2007 1 CIN NL 6 0 0 0 0 0 86 89494 coninje01 2007 2 NYN NL 21 41 2 8 2 0 87 89495 coninje01 2007 1 CIN NL 80 215 23 57 11 1 88 89497 clemero02 2007 1 NYA AL 2 2 0 1 0 0 89 89498 claytro01 2007 2 BOS AL 8 6 1 0 0 0 90 89499 claytro01 2007 1 TOR AL 69 189 23 48 14 0 91 89501 cirilje01 2007 2 ARI NL 28 40 6 8 4 0 92 89502 cirilje01 2007 1 MIN AL 50 153 18 40 9 2 93 89521 bondsba01 2007 1 SFN NL 126 340 75 94 14 0 94 89523 biggicr01 2007 1 HOU NL 141 517 68 130 31 3 95 89525 benitar01 2007 2 FLO NL 34 0 0 0 0 0 96 89526 benitar01 2007 1 SFN NL 19 0 0 0 0 0 97 89530 ausmubr01 2007 1 HOU NL 117 349 38 82 16 3 98 89533 aloumo01 2007 1 NYN NL 87 328 51 112 19 1 99 89534 alomasa02 2007 1 NYN NL 8 22 1 3 1 0 Wide DataFrames will be printed across multiple rows by default: pd.DataFrame(np.random.randn(3, 12)) 0 1 2 ... 9 10 11 0 -1.226825 0.769804 -1.281247 ... -1.110336 -0.619976 0.149748 1 -0.732339 0.687738 0.176444 ... 1.462696 -1.743161", "prev_chunk_id": "chunk_59", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_61", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "Console display#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "Console display#", "content": "-0.826591 2 -0.345352 1.314232 0.690579 ... 0.896171 -0.487602 -0.082240 [3 rows x 12 columns] You can change how much to print on a single row by setting the display.width option: pd.set_option(\"display.width\", 40) # default is 80 pd.DataFrame(np.random.randn(3, 12)) 0 1 2 ... 9 10 11 0 -2.182937 0.380396 0.084844 ... -0.023688 2.410179 1.450520 1 0.206053 -0.251905 -2.213588 ... -0.025747 -0.988387 0.094055 2 1.262731 1.289997 0.082423 ... -0.281461 0.030711 0.109121 [3 rows x 12 columns] You can adjust the max width of the individual columns by setting display.max_colwidth datafile = { .....: \"filename\": [\"filename_01\", \"filename_02\"], .....: \"path\": [ .....: \"media/user_name/storage/folder_01/filename_01\", .....: \"media/user_name/storage/folder_02/filename_02\", .....: ], .....: } .....: pd.set_option(\"display.max_colwidth\", 30) pd.DataFrame(datafile) filename path 0 filename_01 media/user_name/storage/fo... 1 filename_02 media/user_name/storage/fo... pd.set_option(\"display.max_colwidth\", 100) pd.DataFrame(datafile) filename path 0 filename_01 media/user_name/storage/folder_01/filename_01 1 filename_02 media/user_name/storage/folder_02/filename_02 You can also disable this feature via the expand_frame_repr option. This will print the table in one block.", "prev_chunk_id": "chunk_60", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_62", "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html", "title": "DataFrame column attribute access and IPython completion#", "page_title": "Intro to data structures — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame column attribute access and IPython completion#", "content": "DataFrame column attribute access and IPython completion# If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute: df = pd.DataFrame({\"foo1\": np.random.randn(5), \"foo2\": np.random.randn(5)}) df foo1 foo2 0 1.126203 0.781836 1 -0.977349 -1.071357 2 1.474071 0.441153 3 -0.064034 2.353925 4 -1.282782 0.583787 df.foo1 0 1.126203 1 -0.977349 2 1.474071 3 -0.064034 4 -1.282782 Name: foo1, dtype: float64 The columns are also connected to the IPython completion mechanism so they can be tab-completed: df.foo<TAB> # noqa: E225, E999 df.foo1 df.foo2", "prev_chunk_id": "chunk_61", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_63", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Essential basic functionality#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Essential basic functionality#", "content": "Essential basic functionality# Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let’s create some example objects like we did in the 10 minutes to pandas section: index = pd.date_range(\"1/1/2000\", periods=8) s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_64", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Head and tail#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Head and tail#", "content": "Head and tail# To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number. long_series = pd.Series(np.random.randn(1000)) long_series.head() 0 -1.157892 1 -1.344312 2 0.844885 3 1.075770 4 -0.109050 dtype: float64 long_series.tail(3) 997 -0.289388 998 -1.020544 999 0.589993 dtype: float64", "prev_chunk_id": "chunk_63", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_65", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Attributes and underlying data#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Attributes and underlying data#", "content": "Attributes and underlying data# pandas objects have a number of attributes enabling you to access the metadata - shape: gives the axis dimensions of the object, consistent with ndarray - Axis labelsSeries:index(only axis)DataFrame:index(rows) andcolumns Note, these attributes can be safely assigned to! df[:2] A B C 2000-01-01 -0.173215 0.119209 -1.044236 2000-01-02 -0.861849 -2.104569 -0.494929 df.columns = [x.lower() for x in df.columns] df a b c 2000-01-01 -0.173215 0.119209 -1.044236 2000-01-02 -0.861849 -2.104569 -0.494929 2000-01-03 1.071804 0.721555 -0.706771 2000-01-04 -1.039575 0.271860 -0.424972 2000-01-05 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 2000-01-07 0.524988 0.404705 0.577046 2000-01-08 -1.715002 -1.039268 -0.370647 pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy’s type system to add support for custom arrays (see dtypes). To get the actual data inside a Index or Series, use the .array property s.array <NumpyExtensionArray> [ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124, -1.1356323710171934, 1.2121120250208506] Length: 5, dtype: float64 s.index.array <NumpyExtensionArray> ['a', 'b', 'c', 'd', 'e'] Length: 5, dtype: object array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. See dtypes for more. If you know you need a NumPy array, use to_numpy() or numpy.asarray(). s.to_numpy() array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) np.asarray(s) array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more. to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider datetimes with timezones. NumPy doesn’t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations: - An", "prev_chunk_id": "chunk_64", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_66", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Attributes and underlying data#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Attributes and underlying data#", "content": "object-dtypenumpy.ndarraywithTimestampobjects, each with the correcttz - Adatetime64[ns]-dtypenumpy.ndarray, where the values have been converted to UTC and the timezone discarded Timezones may be preserved with dtype=object ser = pd.Series(pd.date_range(\"2000\", periods=2, tz=\"CET\")) ser.to_numpy(dtype=object) array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'), Timestamp('2000-01-02 00:00:00+0100', tz='CET')], dtype=object) Or thrown away with dtype='datetime64[ns]' ser.to_numpy(dtype=\"datetime64[ns]\") array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'], dtype='datetime64[ns]') Getting the “raw data” inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data: df.to_numpy() array([[-0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949], [ 1.0718, 0.7216, -0.7068], [-1.0396, 0.2719, -0.425 ], [ 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784], [ 0.525 , 0.4047, 0.577 ], [-1.715 , -1.0393, -0.3706]]) If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame’s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to. In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You’ll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks: - When your Series contains anextension type, it’s unclear whetherSeries.valuesreturns a NumPy array or the extension array.Series.arraywill always return anExtensionArray, and will never copy data.Series.to_numpy()will always return a NumPy array, potentially at the cost of copying / coercing values. - When your DataFrame contains a mixture of data types,DataFrame.valuesmay involve copying data and coercing values to a common dtype, a relatively expensive operation.DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.", "prev_chunk_id": "chunk_65", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_67", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Accelerated operations#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Accelerated operations#", "content": "Accelerated operations# pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries. These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans. Here is a sample (using 100 column x 100,000 row DataFrames): Operation | 0.11.0 (ms) | Prior Version (ms) | Ratio to Prior df1 > df2 | 13.32 | 125.35 | 0.1063 df1 * df2 | 21.71 | 36.63 | 0.5928 df1 + df2 | 22.04 | 36.50 | 0.6039 You are highly encouraged to install both libraries. See the section Recommended Dependencies for more installation info. These are both enabled to be used by default, you can control this by setting the options: pd.set_option(\"compute.use_bottleneck\", False) pd.set_option(\"compute.use_numexpr\", False)", "prev_chunk_id": "chunk_66", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_68", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Flexible binary operations#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Flexible binary operations#", "content": "Flexible binary operations# With binary operations between pandas data structures, there are two key points of interest: - Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects. - Missing data in computations. We will demonstrate how to manage these issues independently, though they can be handled simultaneously.", "prev_chunk_id": "chunk_67", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_69", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Matching / broadcasting behavior#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Matching / broadcasting behavior#", "content": "Matching / broadcasting behavior# DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), … for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword: df = pd.DataFrame( ....: { ....: \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]), ....: \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]), ....: \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]), ....: } ....: ) ....: df one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 row = df.iloc[1] column = df[\"two\"] df.sub(row, axis=\"columns\") one two three a 1.051928 -0.139606 NaN b 0.000000 0.000000 0.000000 c 0.352192 -0.433754 1.277825 d NaN -1.632779 -0.562782 df.sub(row, axis=1) one two three a 1.051928 -0.139606 NaN b 0.000000 0.000000 0.000000 c 0.352192 -0.433754 1.277825 d NaN -1.632779 -0.562782 df.sub(column, axis=\"index\") one two three a -0.377535 0.0 NaN b -1.569069 0.0 -1.962513 c -0.783123 0.0 -0.250933 d NaN 0.0 -0.892516 df.sub(column, axis=0) one two three a -0.377535 0.0 NaN b -1.569069 0.0 -1.962513 c -0.783123 0.0 -0.250933 d NaN 0.0 -0.892516 Furthermore you can align a level of a MultiIndexed DataFrame with a Series. dfmi = df.copy() dfmi.index = pd.MultiIndex.from_tuples( ....: [(1, \"a\"), (1, \"b\"), (1, \"c\"), (2, \"a\")], names=[\"first\", \"second\"] ....: ) ....: dfmi.sub(column, axis=0, level=\"second\") one two three first second 1 a -0.377535 0.000000 NaN b -1.569069 0.000000 -1.962513 c -0.783123 0.000000 -0.250933 2 a NaN -1.493173 -2.385688 Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example: s = pd.Series(np.arange(10)) s 0 0 1 1 2 2 3 3 4 4 5 5 6", "prev_chunk_id": "chunk_68", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_70", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Matching / broadcasting behavior#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Matching / broadcasting behavior#", "content": "6 7 7 8 8 9 9 dtype: int64 div, rem = divmod(s, 3) div 0 0 1 0 2 0 3 1 4 1 5 1 6 2 7 2 8 2 9 3 dtype: int64 rem 0 0 1 1 2 2 3 0 4 1 5 2 6 0 7 1 8 2 9 0 dtype: int64 idx = pd.Index(np.arange(10)) idx Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64') div, rem = divmod(idx, 3) div Index([0, 0, 0, 1, 1, 1, 2, 2, 2, 3], dtype='int64') rem Index([0, 1, 2, 0, 1, 2, 0, 1, 2, 0], dtype='int64') We can also do elementwise divmod(): div, rem = divmod(s, [2, 2, 3, 3, 4, 4, 5, 5, 6, 6]) div 0 0 1 0 2 0 3 1 4 1 5 1 6 1 7 1 8 1 9 1 dtype: int64 rem 0 0 1 1 2 2 3 0 4 0 5 1 6 1 7 2 8 2 9 3 dtype: int64", "prev_chunk_id": "chunk_69", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_71", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Missing data / operations with fill values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Missing data / operations with fill values#", "content": "Missing data / operations with fill values# In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish). df2 = df.copy() df2.loc[\"a\", \"three\"] = 1.0 df one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df2 one two three a 1.394981 1.772517 1.000000 b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df + df2 one two three a 2.789963 3.545034 NaN b 0.686107 3.824246 -0.100780 c 1.390491 2.956737 2.454870 d NaN 0.558688 -1.226343 df.add(df2, fill_value=0) one two three a 2.789963 3.545034 1.000000 b 0.686107 3.824246 -0.100780 c 1.390491 2.956737 2.454870 d NaN 0.558688 -1.226343", "prev_chunk_id": "chunk_70", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_72", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Flexible comparisons#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Flexible comparisons#", "content": "Flexible comparisons# Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above: df.gt(df2) one two three a False False False b False False False c False False False d False False False df2.ne(df) one two three a False False True b False False False c False False False d True False False These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations, see the section on Boolean indexing.", "prev_chunk_id": "chunk_71", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_73", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Boolean reductions#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Boolean reductions#", "content": "Boolean reductions# You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result. (df > 0).all() one False two True three False dtype: bool (df > 0).any() one True two True three True dtype: bool You can reduce to a final boolean value. (df > 0).any().any() True You can test if a pandas object is empty, via the empty property. df.empty False pd.DataFrame(columns=list(\"ABC\")).empty True", "prev_chunk_id": "chunk_72", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_74", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Comparing if objects are equivalent#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Comparing if objects are equivalent#", "content": "Comparing if objects are equivalent# Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using (df + df == df * 2).all(). But in fact, this expression is False: df + df == df * 2 one two three a True True False b True True True c True True True d False True True (df + df == df * 2).all() one False two True three False dtype: bool Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals: np.nan == np.nan False So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corresponding locations treated as equal. (df + df).equals(df * 2) True Note that the Series or DataFrame index needs to be in the same order for equality to be True: df1 = pd.DataFrame({\"col\": [\"foo\", 0, np.nan]}) df2 = pd.DataFrame({\"col\": [np.nan, 0, \"foo\"]}, index=[2, 1, 0]) df1.equals(df2) False df1.equals(df2.sort_index()) True", "prev_chunk_id": "chunk_73", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_75", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Comparing array-like objects#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Comparing array-like objects#", "content": "Comparing array-like objects# You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value: pd.Series([\"foo\", \"bar\", \"baz\"]) == \"foo\" 0 True 1 False 2 False dtype: bool pd.Index([\"foo\", \"bar\", \"baz\"]) == \"foo\" array([ True, False, False]) pandas also handles element-wise comparisons between different array-like objects of the same length: pd.Series([\"foo\", \"bar\", \"baz\"]) == pd.Index([\"foo\", \"bar\", \"qux\"]) 0 True 1 True 2 False dtype: bool pd.Series([\"foo\", \"bar\", \"baz\"]) == np.array([\"foo\", \"bar\", \"qux\"]) 0 True 1 True 2 False dtype: bool Trying to compare Index or Series objects of different lengths will raise a ValueError: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar']) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[69], line 1 ----> 1 pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar']) File ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other) 72 return NotImplemented 74 other = item_from_zerodim(other) ---> 76 return method(self, other) File ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__(self, other) 38 @unpack_zerodim_and_defer(\"__eq__\") 39 def __eq__(self, other): ---> 40 return self._cmp_method(other, operator.eq) File ~/work/pandas/pandas/pandas/core/series.py:6125, in Series._cmp_method(self, other, op) 6122 res_name = ops.get_op_result_name(self, other) 6124 if isinstance(other, Series) and not self._indexed_same(other): -> 6125 raise ValueError(\"Can only compare identically-labeled Series objects\") 6127 lvalues = self._values 6128 rvalues = extract_array(other, extract_numpy=True, extract_range=True) ValueError: Can only compare identically-labeled Series objects pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo']) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[70], line 1 ----> 1 pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo']) File ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other) 72 return NotImplemented 74 other = item_from_zerodim(other) ---> 76 return method(self, other) File ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__(self, other) 38 @unpack_zerodim_and_defer(\"__eq__\") 39 def __eq__(self, other): ---> 40 return self._cmp_method(other, operator.eq) File ~/work/pandas/pandas/pandas/core/series.py:6125, in Series._cmp_method(self, other, op) 6122 res_name = ops.get_op_result_name(self, other) 6124 if isinstance(other, Series) and not self._indexed_same(other): -> 6125 raise ValueError(\"Can only compare identically-labeled Series objects\") 6127 lvalues = self._values 6128 rvalues = extract_array(other, extract_numpy=True, extract_range=True) ValueError: Can only compare identically-labeled Series objects", "prev_chunk_id": "chunk_74", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_76", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Combining overlapping data sets#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Combining overlapping data sets#", "content": "Combining overlapping data sets# A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of “higher quality”. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate: df1 = pd.DataFrame( ....: {\"A\": [1.0, np.nan, 3.0, 5.0, np.nan], \"B\": [np.nan, 2.0, 3.0, np.nan, 6.0]} ....: ) ....: df2 = pd.DataFrame( ....: { ....: \"A\": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0], ....: \"B\": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0], ....: } ....: ) ....: df1 A B 0 1.0 NaN 1 NaN 2.0 2 3.0 3.0 3 5.0 NaN 4 NaN 6.0 df2 A B 0 5.0 NaN 1 2.0 NaN 2 4.0 3.0 3 NaN 4.0 4 3.0 6.0 5 7.0 8.0 df1.combine_first(df2) A B 0 1.0 NaN 1 2.0 2.0 2 3.0 3.0 3 5.0 4.0 4 3.0 6.0 5 7.0 8.0", "prev_chunk_id": "chunk_75", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_77", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "General DataFrame combine#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "General DataFrame combine#", "content": "General DataFrame combine# The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same). So, for instance, to reproduce combine_first() as above: def combiner(x, y): ....: return np.where(pd.isna(x), y, x) ....: df1.combine(df2, combiner) A B 0 1.0 NaN 1 2.0 2.0 2 3.0 3.0 3 5.0 4.0 4 3.0 6.0 5 7.0 8.0", "prev_chunk_id": "chunk_76", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_78", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Descriptive statistics#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Descriptive statistics#", "content": "Descriptive statistics# There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, …}, but the axis can be specified by name or integer: - Series: no axis argument needed - DataFrame: “index” (axis=0, default), “columns” (axis=1) For example: df one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df.mean(0) one 0.811094 two 1.360588 three 0.187958 dtype: float64 df.mean(1) a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 All such methods have a skipna option signaling whether to exclude missing data (True by default): df.sum(0, skipna=False) one NaN two 5.442353 three NaN dtype: float64 df.sum(axis=1, skipna=True) a 3.167498 b 2.204786 c 3.401050 d -0.333828 dtype: float64 Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely: ts_stand = (df - df.mean()) / df.std() ts_stand.std() one 1.0 two 1.0 three 1.0 dtype: float64 xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0) xs_stand.std(1) a 1.0 b 1.0 c 1.0 d 1.0 dtype: float64 Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter. df.cumsum() one two three a 1.394981 1.772517 NaN b 1.738035 3.684640 -0.050390 c 2.433281 5.163008 1.177045 d NaN 5.442353 0.563873 Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a", "prev_chunk_id": "chunk_77", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_79", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Descriptive statistics#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Descriptive statistics#", "content": "hierarchical index. Function | Description count | Number of non-NA observations sum | Sum of values mean | Mean of values median | Arithmetic median of values min | Minimum max | Maximum mode | Mode abs | Absolute Value prod | Product of values std | Bessel-corrected sample standard deviation var | Unbiased variance sem | Standard error of the mean skew | Sample skewness (3rd moment) kurt | Sample kurtosis (4th moment) quantile | Sample quantile (value at %) cumsum | Cumulative sum cumprod | Cumulative product cummax | Cumulative maximum cummin | Cumulative minimum Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default: np.mean(df[\"one\"]) 0.8110935116651192 np.mean(df[\"one\"].to_numpy()) nan Series.nunique() will return the number of unique non-NA values in a Series: series = pd.Series(np.random.randn(500)) series[20:500] = np.nan series[10:20] = 5 series.nunique() 11", "prev_chunk_id": "chunk_78", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_80", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Summarizing data: describe#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Summarizing data: describe#", "content": "Summarizing data: describe# There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course): series = pd.Series(np.random.randn(1000)) series[::2] = np.nan series.describe() count 500.000000 mean -0.021292 std 1.015906 min -2.683763 25% -0.699070 50% -0.069718 75% 0.714483 max 3.160915 dtype: float64 frame = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"]) frame.iloc[::2] = np.nan frame.describe() a b c d e count 500.000000 500.000000 500.000000 500.000000 500.000000 mean 0.033387 0.030045 -0.043719 -0.051686 0.005979 std 1.017152 0.978743 1.025270 1.015988 1.006695 min -3.000951 -2.637901 -3.303099 -3.159200 -3.188821 25% -0.647623 -0.576449 -0.712369 -0.691338 -0.691115 50% 0.047578 -0.021499 -0.023888 -0.032652 -0.025363 75% 0.729907 0.775880 0.618896 0.670047 0.649748 max 2.740139 2.752332 3.004229 2.728702 3.240991 You can select specific percentiles to include in the output: series.describe(percentiles=[0.05, 0.25, 0.75, 0.95]) count 500.000000 mean -0.021292 std 1.015906 min -2.683763 5% -1.645423 25% -0.699070 50% -0.069718 75% 0.714483 95% 1.711409 max 3.160915 dtype: float64 By default, the median is always included. For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values: s = pd.Series([\"a\", \"a\", \"b\", \"b\", \"a\", \"a\", np.nan, \"c\", \"d\", \"a\"]) s.describe() count 9 unique 4 top a freq 5 dtype: object Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns: frame = pd.DataFrame({\"a\": [\"Yes\", \"Yes\", \"No\", \"No\"], \"b\": range(4)}) frame.describe() b count 4.000000 mean 1.500000 std 1.290994 min 0.000000 25% 0.750000 50% 1.500000 75% 2.250000 max 3.000000 This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used: frame.describe(include=[\"object\"]) a count 4 unique 2 top Yes freq 2 frame.describe(include=[\"number\"]) b count 4.000000 mean 1.500000 std 1.290994 min", "prev_chunk_id": "chunk_79", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_81", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Summarizing data: describe#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Summarizing data: describe#", "content": "0.000000 25% 0.750000 50% 1.500000 75% 2.250000 max 3.000000 frame.describe(include=\"all\") a b count 4 4.000000 unique 2 NaN top Yes NaN freq 2 NaN mean NaN 1.500000 std NaN 1.290994 min NaN 0.000000 25% NaN 0.750000 50% NaN 1.500000 75% NaN 2.250000 max NaN 3.000000 That feature relies on select_dtypes. Refer to there for details about accepted inputs.", "prev_chunk_id": "chunk_80", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_82", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Index of min/max values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Index of min/max values#", "content": "Index of min/max values# The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values: s1 = pd.Series(np.random.randn(5)) s1 0 1.118076 1 -0.352051 2 -1.242883 3 -1.277155 4 -0.641184 dtype: float64 s1.idxmin(), s1.idxmax() (3, 0) df1 = pd.DataFrame(np.random.randn(5, 3), columns=[\"A\", \"B\", \"C\"]) df1 A B C 0 -0.327863 -0.946180 -0.137570 1 -0.186235 -0.257213 -0.486567 2 -0.507027 -0.871259 -0.111110 3 2.000339 -2.430505 0.089759 4 -0.321434 -0.033695 0.096271 df1.idxmin(axis=0) A 2 B 3 C 1 dtype: int64 df1.idxmax(axis=1) 0 C 1 A 2 C 3 A 4 C dtype: object When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index: df3 = pd.DataFrame([2, 1, 1, 3, np.nan], columns=[\"A\"], index=list(\"edcba\")) df3 A e 2.0 d 1.0 c 1.0 b 3.0 a NaN df3[\"A\"].idxmin() 'd'", "prev_chunk_id": "chunk_81", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_83", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Value counts (histogramming) / mode#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Value counts (histogramming) / mode#", "content": "Value counts (histogramming) / mode# The value_counts() Series method computes a histogram of a 1D array of values. It can also be used as a function on regular arrays: data = np.random.randint(0, 7, size=50) data array([6, 6, 2, 3, 5, 3, 2, 5, 4, 5, 4, 3, 4, 5, 0, 2, 0, 4, 2, 0, 3, 2, 2, 5, 6, 5, 3, 4, 6, 4, 3, 5, 6, 4, 3, 6, 2, 6, 6, 2, 3, 4, 2, 1, 6, 2, 6, 1, 5, 4]) s = pd.Series(data) s.value_counts() 6 10 2 10 4 9 3 8 5 8 0 3 1 2 Name: count, dtype: int64 The value_counts() method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the subset argument. data = {\"a\": [1, 2, 3, 4], \"b\": [\"x\", \"x\", \"y\", \"y\"]} frame = pd.DataFrame(data) frame.value_counts() a b 1 x 1 2 x 1 3 y 1 4 y 1 Name: count, dtype: int64 Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame: s5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7]) s5.mode() 0 3 1 7 dtype: int64 df5 = pd.DataFrame( .....: { .....: \"A\": np.random.randint(0, 7, size=50), .....: \"B\": np.random.randint(-10, 15, size=50), .....: } .....: ) .....: df5.mode() A B 0 1.0 -9 1 NaN 10 2 NaN 13", "prev_chunk_id": "chunk_82", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_84", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Discretization and quantiling#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Discretization and quantiling#", "content": "Discretization and quantiling# Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions: arr = np.random.randn(20) factor = pd.cut(arr, 4) factor [(-0.251, 0.464], (-0.968, -0.251], (0.464, 1.179], (-0.251, 0.464], (-0.968, -0.251], ..., (-0.251, 0.464], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251]] Length: 20 Categories (4, interval[float64, right]): [(-0.968, -0.251] < (-0.251, 0.464] < (0.464, 1.179] < (1.179, 1.893]] factor = pd.cut(arr, [-5, -1, 0, 1, 5]) factor [(0, 1], (-1, 0], (0, 1], (0, 1], (-1, 0], ..., (-1, 0], (-1, 0], (-1, 0], (-1, 0], (-1, 0]] Length: 20 Categories (4, interval[int64, right]): [(-5, -1] < (-1, 0] < (0, 1] < (1, 5]] qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so: arr = np.random.randn(30) factor = pd.qcut(arr, [0, 0.25, 0.5, 0.75, 1]) factor [(0.569, 1.184], (-2.278, -0.301], (-2.278, -0.301], (0.569, 1.184], (0.569, 1.184], ..., (-0.301, 0.569], (1.184, 2.346], (1.184, 2.346], (-0.301, 0.569], (-2.278, -0.301]] Length: 30 Categories (4, interval[float64, right]): [(-2.278, -0.301] < (-0.301, 0.569] < (0.569, 1.184] < (1.184, 2.346]] We can also pass infinite values to define the bins: arr = np.random.randn(20) factor = pd.cut(arr, [-np.inf, 0, np.inf]) factor [(-inf, 0.0], (0.0, inf], (0.0, inf], (-inf, 0.0], (-inf, 0.0], ..., (-inf, 0.0], (-inf, 0.0], (-inf, 0.0], (0.0, inf], (0.0, inf]] Length: 20 Categories (2, interval[float64, right]): [(-inf, 0.0] < (0.0, inf]]", "prev_chunk_id": "chunk_83", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_85", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Function application#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Function application#", "content": "Function application# To apply your own or another library’s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise. - Tablewise Function Application:pipe() - Row or Column-wise Function Application:apply() - Aggregation API:agg()andtransform() - Applying Elementwise Functions:map()", "prev_chunk_id": "chunk_84", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_86", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Tablewise function application#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Tablewise function application#", "content": "Tablewise function application# DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method. First some setup: def extract_city_name(df): .....: \"\"\" .....: Chicago, IL -> Chicago for city_name column .....: \"\"\" .....: df[\"city_name\"] = df[\"city_and_code\"].str.split(\",\").str.get(0) .....: return df .....: def add_country_name(df, country_name=None): .....: \"\"\" .....: Chicago -> Chicago-US for city_name column .....: \"\"\" .....: col = \"city_name\" .....: df[\"city_and_country\"] = df[col] + country_name .....: return df .....: df_p = pd.DataFrame({\"city_and_code\": [\"Chicago, IL\"]}) extract_city_name and add_country_name are functions taking and returning DataFrames. Now compare the following: add_country_name(extract_city_name(df_p), country_name=\"US\") city_and_code city_name city_and_country 0 Chicago, IL Chicago ChicagoUS Is equivalent to: df_p.pipe(extract_city_name).pipe(add_country_name, country_name=\"US\") city_and_code city_name city_and_country 0 Chicago, IL Chicago ChicagoUS pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library’s functions in method chains, alongside pandas’ methods. In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple. For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument, data. We pass in the function, keyword pair (sm.ols, 'data') to pipe: import statsmodels.formula.api as sm bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\") ( .....: bb.query(\"h > 0\") .....: .assign(ln_h=lambda df: np.log(df.h)) .....: .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\") .....: .fit() .....: .summary() .....: ) .....: Out[149]: <class 'statsmodels.iolib.summary.Summary'> \"\"\" OLS Regression Results ============================================================================== Dep. Variable: hr R-squared: 0.685 Model: OLS Adj. R-squared: 0.665 Method: Least Squares F-statistic: 34.28", "prev_chunk_id": "chunk_85", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_87", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Tablewise function application#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Tablewise function application#", "content": "Date: Tue, 22 Nov 2022 Prob (F-statistic): 3.48e-15 Time: 05:34:17 Log-Likelihood: -205.92 No. Observations: 68 AIC: 421.8 Df Residuals: 63 BIC: 432.9 Df Model: 4 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Intercept -8484.7720 4664.146 -1.819 0.074 -1.78e+04 835.780 C(lg)[T.NL] -2.2736 1.325 -1.716 0.091 -4.922 0.375 ln_h -1.3542 0.875 -1.547 0.127 -3.103 0.395 year 4.2277 2.324 1.819 0.074 -0.417 8.872 g 0.1841 0.029 6.258 0.000 0.125 0.243 ============================================================================== Omnibus: 10.875 Durbin-Watson: 1.999 Prob(Omnibus): 0.004 Jarque-Bera (JB): 17.298 Skew: 0.537 Prob(JB): 0.000175 Kurtosis: 5.225 Cond. No. 1.49e+07 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.49e+07. This might indicate that there are strong multicollinearity or other numerical problems. \"\"\" The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().", "prev_chunk_id": "chunk_86", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_88", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Row or column-wise function application#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Row or column-wise function application#", "content": "Row or column-wise function application# Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument: df.apply(lambda x: np.mean(x)) one 0.811094 two 1.360588 three 0.187958 dtype: float64 df.apply(lambda x: np.mean(x), axis=1) a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 df.apply(lambda x: x.max() - x.min()) one 1.051928 two 1.632779 three 1.840607 dtype: float64 df.apply(np.cumsum) one two three a 1.394981 1.772517 NaN b 1.738035 3.684640 -0.050390 c 2.433281 5.163008 1.177045 d NaN 5.442353 0.563873 df.apply(np.exp) one two three a 4.034899 5.885648 NaN b 1.409244 6.767440 0.950858 c 2.004201 4.385785 3.412466 d NaN 1.322262 0.541630 The apply() method will also dispatch on a string method name. df.apply(\"mean\") one 0.811094 two 1.360588 three 0.187958 dtype: float64 df.apply(\"mean\", axis=1) a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour: - If the applied function returns aSeries, the final output is aDataFrame. The columns match the index of theSeriesreturned by the applied function. - If the applied function returns any other type, the final output is aSeries. This default behaviour can be overridden using the result_type, which accepts three options: reduce, broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame. apply() combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred: tsdf = pd.DataFrame( .....: np.random.randn(1000, 3), .....: columns=[\"A\", \"B\", \"C\"], .....: index=pd.date_range(\"1/1/2000\", periods=1000), .....: ) .....: tsdf.apply(lambda x: x.idxmax()) A 2000-08-06 B 2001-01-18 C 2001-07-18 dtype: datetime64[ns] You may also pass additional arguments and keyword arguments to", "prev_chunk_id": "chunk_87", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_89", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Row or column-wise function application#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Row or column-wise function application#", "content": "the apply() method. def subtract_and_divide(x, sub, divide=1): .....: return (x - sub) / divide .....: df_udf = pd.DataFrame(np.ones((2, 2))) df_udf.apply(subtract_and_divide, args=(5,), divide=3) 0 1 0 -1.333333 -1.333333 1 -1.333333 -1.333333 Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row: tsdf = pd.DataFrame( .....: np.random.randn(10, 3), .....: columns=[\"A\", \"B\", \"C\"], .....: index=pd.date_range(\"1/1/2000\", periods=10), .....: ) .....: tsdf.iloc[3:7] = np.nan tsdf A B C 2000-01-01 -0.158131 -0.232466 0.321604 2000-01-02 -1.810340 -3.105758 0.433834 2000-01-03 -1.209847 -1.156793 -0.136794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 -0.653602 0.178875 1.008298 2000-01-09 1.007996 0.462824 0.254472 2000-01-10 0.307473 0.600337 1.643950 tsdf.apply(pd.Series.interpolate) A B C 2000-01-01 -0.158131 -0.232466 0.321604 2000-01-02 -1.810340 -3.105758 0.433834 2000-01-03 -1.209847 -1.156793 -0.136794 2000-01-04 -1.098598 -0.889659 0.092225 2000-01-05 -0.987349 -0.622526 0.321243 2000-01-06 -0.876100 -0.355392 0.550262 2000-01-07 -0.764851 -0.088259 0.779280 2000-01-08 -0.653602 0.178875 1.008298 2000-01-09 1.007996 0.462824 0.254472 2000-01-10 0.307473 0.600337 1.643950 Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.", "prev_chunk_id": "chunk_88", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_90", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Aggregation API#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aggregation API#", "content": "Aggregation API# The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg(). We will use a similar starting frame from above: tsdf = pd.DataFrame( .....: np.random.randn(10, 3), .....: columns=[\"A\", \"B\", \"C\"], .....: index=pd.date_range(\"1/1/2000\", periods=10), .....: ) .....: tsdf.iloc[3:7] = np.nan tsdf A B C 2000-01-01 1.257606 1.004194 0.167574 2000-01-02 -0.749892 0.288112 -0.757304 2000-01-03 -0.207550 -0.298599 0.116018 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.814347 -0.257623 0.869226 2000-01-09 -0.250663 -1.206601 0.896839 2000-01-10 2.169758 -1.333363 0.283157 Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output: tsdf.agg(lambda x: np.sum(x)) A 3.033606 B -1.803879 C 1.575510 dtype: float64 tsdf.agg(\"sum\") A 3.033606 B -1.803879 C 1.575510 dtype: float64 # these are equivalent to a ``.sum()`` because we are aggregating # on a single function tsdf.sum() A 3.033606 B -1.803879 C 1.575510 dtype: float64 Single aggregations on a Series this will return a scalar value: tsdf[\"A\"].agg(\"sum\") 3.033606102414146", "prev_chunk_id": "chunk_89", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_91", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Aggregating with multiple functions#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aggregating with multiple functions#", "content": "Aggregating with multiple functions# You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function. tsdf.agg([\"sum\"]) A B C sum 3.033606 -1.803879 1.57551 Multiple functions yield multiple rows: tsdf.agg([\"sum\", \"mean\"]) A B C sum 3.033606 -1.803879 1.575510 mean 0.505601 -0.300647 0.262585 On a Series, multiple functions return a Series, indexed by the function names: tsdf[\"A\"].agg([\"sum\", \"mean\"]) sum 3.033606 mean 0.505601 Name: A, dtype: float64 Passing a lambda function will yield a <lambda> named row: tsdf[\"A\"].agg([\"sum\", lambda x: x.mean()]) sum 3.033606 <lambda> 0.505601 Name: A, dtype: float64 Passing a named function will yield that name for the row: def mymean(x): .....: return x.mean() .....: tsdf[\"A\"].agg([\"sum\", mymean]) sum 3.033606 mymean 0.505601 Name: A, dtype: float64", "prev_chunk_id": "chunk_90", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_92", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Aggregating with a dict#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aggregating with a dict#", "content": "Aggregating with a dict# Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering. tsdf.agg({\"A\": \"mean\", \"B\": \"sum\"}) A 0.505601 B -1.803879 dtype: float64 Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN: tsdf.agg({\"A\": [\"mean\", \"min\"], \"B\": \"sum\"}) A B mean 0.505601 NaN min -0.749892 NaN sum NaN -1.803879", "prev_chunk_id": "chunk_91", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_93", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Custom describe#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Custom describe#", "content": "Custom describe# With .agg() it is possible to easily create a custom describe function, similar to the built in describe function. from functools import partial q_25 = partial(pd.Series.quantile, q=0.25) q_25.__name__ = \"25%\" q_75 = partial(pd.Series.quantile, q=0.75) q_75.__name__ = \"75%\" tsdf.agg([\"count\", \"mean\", \"std\", \"min\", q_25, \"median\", q_75, \"max\"]) A B C count 6.000000 6.000000 6.000000 mean 0.505601 -0.300647 0.262585 std 1.103362 0.887508 0.606860 min -0.749892 -1.333363 -0.757304 25% -0.239885 -0.979600 0.128907 median 0.303398 -0.278111 0.225365 75% 1.146791 0.151678 0.722709 max 2.169758 1.004194 0.896839", "prev_chunk_id": "chunk_92", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_94", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Transform API#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Transform API#", "content": "Transform API# The transform() method returns an object that is indexed the same (same size) as the original. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API. We create a frame similar to the one used in the above sections. tsdf = pd.DataFrame( .....: np.random.randn(10, 3), .....: columns=[\"A\", \"B\", \"C\"], .....: index=pd.date_range(\"1/1/2000\", periods=10), .....: ) .....: tsdf.iloc[3:7] = np.nan tsdf A B C 2000-01-01 -0.428759 -0.864890 -0.675341 2000-01-02 -0.168731 1.338144 -1.279321 2000-01-03 -1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 -1.240447 -0.201052 2000-01-09 -0.157795 0.791197 -1.144209 2000-01-10 -0.030876 0.371900 0.061932 Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function. tsdf.transform(np.abs) A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 tsdf.transform(\"abs\") A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 tsdf.transform(lambda x: x.abs()) A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 Here transform() received a single function; this is equivalent to a ufunc application. np.abs(tsdf) A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034", "prev_chunk_id": "chunk_93", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_95", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Transform API#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Transform API#", "content": "0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 Passing a single function to .transform() with a Series will yield a single Series in return. tsdf[\"A\"].transform(np.abs) 2000-01-01 0.428759 2000-01-02 0.168731 2000-01-03 1.621034 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 NaN 2000-01-07 NaN 2000-01-08 0.254374 2000-01-09 0.157795 2000-01-10 0.030876 Freq: D, Name: A, dtype: float64", "prev_chunk_id": "chunk_94", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_96", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Transform with multiple functions#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Transform with multiple functions#", "content": "Transform with multiple functions# Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions. tsdf.transform([np.abs, lambda x: x + 1]) A B C absolute <lambda> absolute <lambda> absolute <lambda> 2000-01-01 0.428759 0.571241 0.864890 0.135110 0.675341 0.324659 2000-01-02 0.168731 0.831269 1.338144 2.338144 1.279321 -0.279321 2000-01-03 1.621034 -0.621034 0.438107 1.438107 0.903794 1.903794 2000-01-04 NaN NaN NaN NaN NaN NaN 2000-01-05 NaN NaN NaN NaN NaN NaN 2000-01-06 NaN NaN NaN NaN NaN NaN 2000-01-07 NaN NaN NaN NaN NaN NaN 2000-01-08 0.254374 1.254374 1.240447 -0.240447 0.201052 0.798948 2000-01-09 0.157795 0.842205 0.791197 1.791197 1.144209 -0.144209 2000-01-10 0.030876 0.969124 0.371900 1.371900 0.061932 1.061932 Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions. tsdf[\"A\"].transform([np.abs, lambda x: x + 1]) absolute <lambda> 2000-01-01 0.428759 0.571241 2000-01-02 0.168731 0.831269 2000-01-03 1.621034 -0.621034 2000-01-04 NaN NaN 2000-01-05 NaN NaN 2000-01-06 NaN NaN 2000-01-07 NaN NaN 2000-01-08 0.254374 1.254374 2000-01-09 0.157795 0.842205 2000-01-10 0.030876 0.969124", "prev_chunk_id": "chunk_95", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_97", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Transforming with a dict#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Transforming with a dict#", "content": "Transforming with a dict# Passing a dict of functions will allow selective transforming per column. tsdf.transform({\"A\": np.abs, \"B\": lambda x: x + 1}) A B 2000-01-01 0.428759 0.135110 2000-01-02 0.168731 2.338144 2000-01-03 1.621034 1.438107 2000-01-04 NaN NaN 2000-01-05 NaN NaN 2000-01-06 NaN NaN 2000-01-07 NaN NaN 2000-01-08 0.254374 -0.240447 2000-01-09 0.157795 1.791197 2000-01-10 0.030876 1.371900 Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms. tsdf.transform({\"A\": np.abs, \"B\": [lambda x: x + 1, \"sqrt\"]}) A B absolute <lambda> sqrt 2000-01-01 0.428759 0.135110 NaN 2000-01-02 0.168731 2.338144 1.156782 2000-01-03 1.621034 1.438107 0.661897 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 -0.240447 NaN 2000-01-09 0.157795 1.791197 0.889493 2000-01-10 0.030876 1.371900 0.609836", "prev_chunk_id": "chunk_96", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_98", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Applying elementwise functions#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Applying elementwise functions#", "content": "Applying elementwise functions# Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods map() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example: df4 = df.copy() df4 one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 def f(x): .....: return len(str(x)) .....: df4[\"one\"].map(f) a 18 b 19 c 18 d 3 Name: one, dtype: int64 df4.map(f) one two three a 18 17 3 b 19 18 20 c 18 18 16 d 3 19 19 Series.map() has an additional feature; it can be used to easily “link” or “map” values defined by a secondary series. This is closely related to merging/joining functionality: s = pd.Series( .....: [\"six\", \"seven\", \"six\", \"seven\", \"six\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"] .....: ) .....: t = pd.Series({\"six\": 6.0, \"seven\": 7.0}) s a six b seven c six d seven e six dtype: object s.map(t) a 6.0 b 7.0 c 6.0 d 7.0 e 6.0 dtype: float64", "prev_chunk_id": "chunk_97", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_99", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Reindexing and altering labels#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Reindexing and altering labels#", "content": "Reindexing and altering labels# reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things: - Reorders the existing data to match a new set of labels - Inserts missing value (NA) markers in label locations where no data for that label existed - If specified,filldata for missing labels using logic (highly relevant to working with time series data) Here is a simple example: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) s a 1.695148 b 1.328614 c 1.234686 d -0.385845 e -1.326508 dtype: float64 s.reindex([\"e\", \"b\", \"f\", \"d\"]) e -1.326508 b 1.328614 f NaN d -0.385845 dtype: float64 Here, the f label was not contained in the Series and hence appears as NaN in the result. With a DataFrame, you can simultaneously reindex the index and columns: df one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df.reindex(index=[\"c\", \"f\", \"b\"], columns=[\"three\", \"two\", \"one\"]) three two one c 1.227435 1.478369 0.695246 f NaN NaN NaN b -0.050390 1.912123 0.343054 Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done: rs = s.reindex(df.index) rs a 1.695148 b 1.328614 c 1.234686 d -0.385845 dtype: float64 rs.index is df.index True This means that the reindexed Series’s index is the same Python object as the DataFrame’s index. DataFrame.reindex() also supports an “axis-style” calling convention, where you specify a single labels argument and the axis it applies to. df.reindex([\"c\", \"f\", \"b\"], axis=\"index\") one two three c 0.695246 1.478369 1.227435 f NaN NaN NaN", "prev_chunk_id": "chunk_98", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_100", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Reindexing and altering labels#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Reindexing and altering labels#", "content": "b 0.343054 1.912123 -0.050390 df.reindex([\"three\", \"two\", \"one\"], axis=\"columns\") three two one a NaN 1.772517 1.394981 b -0.050390 1.912123 0.343054 c 1.227435 1.478369 0.695246 d -0.613172 0.279344 NaN", "prev_chunk_id": "chunk_99", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_101", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Reindexing to align with another object#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Reindexing to align with another object#", "content": "Reindexing to align with another object# You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler: df2 = df.reindex([\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"]) df3 = df2 - df2.mean() df2 one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369 df3 one two a 0.583888 0.051514 b -0.468040 0.191120 c -0.115848 -0.242634 df.reindex_like(df2) one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369", "prev_chunk_id": "chunk_100", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_102", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Aligning objects with each other with align#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aligning objects with each other with align#", "content": "Aligning objects with each other with align# The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging): It returns a tuple with both of the reindexed Series: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) s1 = s[:4] s2 = s[1:] s1.align(s2) (a -0.186646 b -1.692424 c -0.303893 d -1.425662 e NaN dtype: float64, a NaN b -1.692424 c -0.303893 d -1.425662 e 1.114285 dtype: float64) s1.align(s2, join=\"inner\") (b -1.692424 c -0.303893 d -1.425662 dtype: float64, b -1.692424 c -0.303893 d -1.425662 dtype: float64) s1.align(s2, join=\"left\") (a -0.186646 b -1.692424 c -0.303893 d -1.425662 dtype: float64, a NaN b -1.692424 c -0.303893 d -1.425662 dtype: float64) For DataFrames, the join method will be applied to both the index and the columns by default: df.align(df2, join=\"inner\") ( one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369, one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369) You can also pass an axis option to only align on the specified axis: df.align(df2, join=\"inner\", axis=0) ( one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435, one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369) If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame’s index or columns using the axis argument: df.align(df2.iloc[0], axis=1) ( one three two a 1.394981 NaN 1.772517 b 0.343054 -0.050390 1.912123 c 0.695246 1.227435 1.478369 d NaN -0.613172 0.279344, one 1.394981 three NaN two 1.772517 Name: a, dtype: float64)", "prev_chunk_id": "chunk_101", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_103", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Filling while reindexing#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Filling while reindexing#", "content": "Filling while reindexing# reindex() takes an optional parameter method which is a filling method chosen from the following table: Method | Action pad / ffill | Fill values forward bfill / backfill | Fill values backward nearest | Fill from the nearest index value We illustrate these fill methods on a simple Series: rng = pd.date_range(\"1/3/2000\", periods=8) ts = pd.Series(np.random.randn(8), index=rng) ts2 = ts.iloc[[0, 3, 6]] ts 2000-01-03 0.183051 2000-01-04 0.400528 2000-01-05 -0.015083 2000-01-06 2.395489 2000-01-07 1.414806 2000-01-08 0.118428 2000-01-09 0.733639 2000-01-10 -0.936077 Freq: D, dtype: float64 ts2 2000-01-03 0.183051 2000-01-06 2.395489 2000-01-09 0.733639 Freq: 3D, dtype: float64 ts2.reindex(ts.index) 2000-01-03 0.183051 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 NaN 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 NaN Freq: D, dtype: float64 ts2.reindex(ts.index, method=\"ffill\") 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 0.183051 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 2.395489 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 ts2.reindex(ts.index, method=\"bfill\") 2000-01-03 0.183051 2000-01-04 2.395489 2000-01-05 2.395489 2000-01-06 2.395489 2000-01-07 0.733639 2000-01-08 0.733639 2000-01-09 0.733639 2000-01-10 NaN Freq: D, dtype: float64 ts2.reindex(ts.index, method=\"nearest\") 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 2.395489 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 0.733639 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 These methods require that the indexes are ordered increasing or decreasing. Note that the same result could have been achieved using ffill (except for method='nearest') or interpolate: ts2.reindex(ts.index).ffill() 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 0.183051 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 2.395489 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.", "prev_chunk_id": "chunk_102", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_104", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Limits on filling while reindexing#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Limits on filling while reindexing#", "content": "Limits on filling while reindexing# The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches: ts2.reindex(ts.index, method=\"ffill\", limit=1) 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 In contrast, tolerance specifies the maximum distance between the index and indexer values: ts2.reindex(ts.index, method=\"ffill\", tolerance=\"1 day\") 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.", "prev_chunk_id": "chunk_103", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_105", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Dropping labels from an axis#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Dropping labels from an axis#", "content": "Dropping labels from an axis# A method closely related to reindex is the drop() function. It removes a set of labels from an axis: df one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df.drop([\"a\", \"d\"], axis=0) one two three b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 df.drop([\"one\"], axis=1) two three a 1.772517 NaN b 1.912123 -0.050390 c 1.478369 1.227435 d 0.279344 -0.613172 Note that the following also works, but is a bit less obvious / clean: df.reindex(df.index.difference([\"a\", \"d\"])) one two three b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435", "prev_chunk_id": "chunk_104", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_106", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Renaming / mapping labels#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Renaming / mapping labels#", "content": "Renaming / mapping labels# The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function. s a -0.186646 b -1.692424 c -0.303893 d -1.425662 e 1.114285 dtype: float64 s.rename(str.upper) A -0.186646 B -1.692424 C -0.303893 D -1.425662 E 1.114285 dtype: float64 If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used: df.rename( .....: columns={\"one\": \"foo\", \"two\": \"bar\"}, .....: index={\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"}, .....: ) .....: foo bar three apple 1.394981 1.772517 NaN banana 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 durian NaN 0.279344 -0.613172 If the mapping doesn’t include a column/index label, it isn’t renamed. Note that extra labels in the mapping don’t throw an error. DataFrame.rename() also supports an “axis-style” calling convention, where you specify a single mapper and the axis to apply that mapping to. df.rename({\"one\": \"foo\", \"two\": \"bar\"}, axis=\"columns\") foo bar three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 df.rename({\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"}, axis=\"index\") one two three apple 1.394981 1.772517 NaN banana 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 durian NaN 0.279344 -0.613172 Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute. s.rename(\"scalar-name\") a -0.186646 b -1.692424 c -0.303893 d -1.425662 e 1.114285 Name: scalar-name, dtype: float64 The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels). df = pd.DataFrame( .....: {\"x\": [1, 2, 3, 4, 5, 6], \"y\": [10, 20, 30, 40, 50, 60]}, .....: index=pd.MultiIndex.from_product( .....: [[\"a\", \"b\", \"c\"], [1, 2]], names=[\"let\", \"num\"] .....: ), .....: ) .....: df x y let num a 1 1", "prev_chunk_id": "chunk_105", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_107", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Renaming / mapping labels#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Renaming / mapping labels#", "content": "10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60 df.rename_axis(index={\"let\": \"abc\"}) x y abc num a 1 1 10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60 df.rename_axis(index=str.upper) x y LET NUM a 1 1 10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60", "prev_chunk_id": "chunk_106", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_108", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Iteration#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Iteration#", "content": "Iteration# The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the “keys” of the objects. In short, basic iteration (for i in object) produces: - Series: values - DataFrame: column labels Thus, for example, iterating over a DataFrame gives you the column names: df = pd.DataFrame( .....: {\"col1\": np.random.randn(3), \"col2\": np.random.randn(3)}, index=[\"a\", \"b\", \"c\"] .....: ) .....: for col in df: .....: print(col) .....: col1 col2 pandas objects also have the dict-like items() method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods: - iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications. - itertuples(): Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster thaniterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.", "prev_chunk_id": "chunk_107", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_109", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "items#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "items#", "content": "items# Consistent with the dict-like interface, items() iterates through key-value pairs: - Series: (index, scalar value) pairs - DataFrame: (column, Series) pairs For example: for label, ser in df.items(): .....: print(label) .....: print(ser) .....: a 0 1 1 2 2 3 Name: a, dtype: int64 b 0 a 1 b 2 c Name: b, dtype: object", "prev_chunk_id": "chunk_108", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_110", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "iterrows#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "iterrows#", "content": "iterrows# iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row: for row_index, row in df.iterrows(): .....: print(row_index, row, sep=\"\\n\") .....: 0 a 1 b a Name: 0, dtype: object 1 a 2 b b Name: 1, dtype: object 2 a 3 b c Name: 2, dtype: object For instance, a contrived way to transpose the DataFrame would be: df2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}) print(df2) x y 0 1 4 1 2 5 2 3 6 print(df2.T) 0 1 2 x 1 2 3 y 4 5 6 df2_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()}) print(df2_t) 0 1 2 x 1 2 3 y 4 5 6", "prev_chunk_id": "chunk_109", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_111", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "itertuples#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "itertuples#", "content": "itertuples# The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row’s corresponding index value, while the remaining values are the row values. For instance: for row in df.itertuples(): .....: print(row) .....: Pandas(Index=0, a=1, b='a') Pandas(Index=1, a=2, b='b') Pandas(Index=2, a=3, b='c') This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows().", "prev_chunk_id": "chunk_110", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_112", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": ".dt accessor#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": ".dt accessor#", "content": ".dt accessor# Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a datetime/period like Series. This will return a Series, indexed like the existing Series. # datetime s = pd.Series(pd.date_range(\"20130101 09:10:12\", periods=4)) s 0 2013-01-01 09:10:12 1 2013-01-02 09:10:12 2 2013-01-03 09:10:12 3 2013-01-04 09:10:12 dtype: datetime64[ns] s.dt.hour 0 9 1 9 2 9 3 9 dtype: int32 s.dt.second 0 12 1 12 2 12 3 12 dtype: int32 s.dt.day 0 1 1 2 2 3 3 4 dtype: int32 This enables nice expressions like this: s[s.dt.day == 2] 1 2013-01-02 09:10:12 dtype: datetime64[ns] You can easily produces tz aware transformations: stz = s.dt.tz_localize(\"US/Eastern\") stz 0 2013-01-01 09:10:12-05:00 1 2013-01-02 09:10:12-05:00 2 2013-01-03 09:10:12-05:00 3 2013-01-04 09:10:12-05:00 dtype: datetime64[ns, US/Eastern] stz.dt.tz <DstTzInfo 'US/Eastern' LMT-1 day, 19:04:00 STD> You can also chain these types of operations: s.dt.tz_localize(\"UTC\").dt.tz_convert(\"US/Eastern\") 0 2013-01-01 04:10:12-05:00 1 2013-01-02 04:10:12-05:00 2 2013-01-03 04:10:12-05:00 3 2013-01-04 04:10:12-05:00 dtype: datetime64[ns, US/Eastern] You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime(). # DatetimeIndex s = pd.Series(pd.date_range(\"20130101\", periods=4)) s 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: datetime64[ns] s.dt.strftime(\"%Y/%m/%d\") 0 2013/01/01 1 2013/01/02 2 2013/01/03 3 2013/01/04 dtype: object # PeriodIndex s = pd.Series(pd.period_range(\"20130101\", periods=4)) s 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: period[D] s.dt.strftime(\"%Y/%m/%d\") 0 2013/01/01 1 2013/01/02 2 2013/01/03 3 2013/01/04 dtype: object The .dt accessor works for period and timedelta dtypes. # period s = pd.Series(pd.period_range(\"20130101\", periods=4, freq=\"D\")) s 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: period[D] s.dt.year 0 2013 1 2013 2 2013 3 2013 dtype: int64 s.dt.day 0 1 1 2 2 3 3 4 dtype: int64 # timedelta s = pd.Series(pd.timedelta_range(\"1 day 00:00:05\", periods=4, freq=\"s\")) s 0 1 days 00:00:05 1", "prev_chunk_id": "chunk_111", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_113", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": ".dt accessor#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": ".dt accessor#", "content": "1 days 00:00:06 2 1 days 00:00:07 3 1 days 00:00:08 dtype: timedelta64[ns] s.dt.days 0 1 1 1 2 1 3 1 dtype: int64 s.dt.seconds 0 5 1 6 2 7 3 8 dtype: int32 s.dt.components days hours minutes seconds milliseconds microseconds nanoseconds 0 1 0 0 5 0 0 0 1 1 0 0 6 0 0 0 2 1 0 0 7 0 0 0 3 1 0 0 8 0 0 0", "prev_chunk_id": "chunk_112", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_114", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Vectorized string methods#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Vectorized string methods#", "content": "Vectorized string methods# Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series’s str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example: Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expressions by default (and in some cases always uses them). Please see Vectorized String Methods for a complete description.", "prev_chunk_id": "chunk_113", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_115", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Sorting#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Sorting#", "content": "Sorting# pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.", "prev_chunk_id": "chunk_114", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_116", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "By index#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "By index#", "content": "By index# The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels. df = pd.DataFrame( .....: { .....: \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]), .....: \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]), .....: \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]), .....: } .....: ) .....: unsorted_df = df.reindex( .....: index=[\"a\", \"d\", \"c\", \"b\"], columns=[\"three\", \"two\", \"one\"] .....: ) .....: unsorted_df three two one a NaN -1.152244 0.562973 d -0.252916 -0.109597 NaN c 1.273388 -0.167123 0.640382 b -0.098217 0.009797 -1.299504 # DataFrame unsorted_df.sort_index() three two one a NaN -1.152244 0.562973 b -0.098217 0.009797 -1.299504 c 1.273388 -0.167123 0.640382 d -0.252916 -0.109597 NaN unsorted_df.sort_index(ascending=False) three two one d -0.252916 -0.109597 NaN c 1.273388 -0.167123 0.640382 b -0.098217 0.009797 -1.299504 a NaN -1.152244 0.562973 unsorted_df.sort_index(axis=1) one three two a 0.562973 NaN -1.152244 d NaN -0.252916 -0.109597 c 0.640382 1.273388 -0.167123 b -1.299504 -0.098217 0.009797 # Series unsorted_df[\"three\"].sort_index() a NaN b -0.098217 c 1.273388 d -0.252916 Name: three, dtype: float64 Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For MultiIndex objects, the key is applied per-level to the levels specified by level. s1 = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3], \"c\": [2, 3, 4]}).set_index( .....: list(\"ab\") .....: ) .....: s1 c a b B 1 2 a 2 3 C 3 4 s1.sort_index(level=\"a\") c a b B 1 2 C 3 4 a 2 3 s1.sort_index(level=\"a\", key=lambda idx: idx.str.lower()) c a b a 2 3 B 1 2 C 3 4 For information on key sorting by value, see value sorting.", "prev_chunk_id": "chunk_115", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_117", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "By values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "By values#", "content": "By values# The Series.sort_values() method is used to sort a Series by its values. The DataFrame.sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order. df1 = pd.DataFrame( .....: {\"one\": [2, 1, 1, 1], \"two\": [1, 3, 2, 4], \"three\": [5, 4, 3, 2]} .....: ) .....: df1.sort_values(by=\"two\") one two three 0 2 1 5 2 1 2 3 1 1 3 4 3 1 4 2 The by parameter can take a list of column names, e.g.: df1[[\"one\", \"two\", \"three\"]].sort_values(by=[\"one\", \"two\"]) one two three 2 1 2 3 1 1 3 4 3 1 4 2 0 2 1 5 These methods have special treatment of NA values via the na_position argument: s[2] = np.nan s.sort_values() 0 A 3 Aaba 1 B 4 Baca 6 CABA 8 cat 7 dog 2 <NA> 5 <NA> dtype: string s.sort_values(na_position=\"first\") 2 <NA> 5 <NA> 0 A 3 Aaba 1 B 4 Baca 6 CABA 8 cat 7 dog dtype: string Sorting also supports a key parameter that takes a callable function to apply to the values being sorted. s1 = pd.Series([\"B\", \"a\", \"C\"]) s1.sort_values() 0 B 2 C 1 a dtype: object s1.sort_values(key=lambda x: x.str.lower()) 1 a 0 B 2 C dtype: object key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g. df = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3]}) df.sort_values(by=\"a\") a b 0 B 1 2 C 3 1 a 2 df.sort_values(by=\"a\", key=lambda col: col.str.lower()) a b 1", "prev_chunk_id": "chunk_116", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_118", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "By values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "By values#", "content": "a 2 0 B 1 2 C 3 The name or type of each column can be used to apply different functions to different columns.", "prev_chunk_id": "chunk_117", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_119", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "By indexes and values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "By indexes and values#", "content": "By indexes and values# Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names. # Build MultiIndex idx = pd.MultiIndex.from_tuples( .....: [(\"a\", 1), (\"a\", 2), (\"a\", 2), (\"b\", 2), (\"b\", 1), (\"b\", 1)] .....: ) .....: idx.names = [\"first\", \"second\"] # Build DataFrame df_multi = pd.DataFrame({\"A\": np.arange(6, 0, -1)}, index=idx) df_multi A first second a 1 6 2 5 2 4 b 2 3 1 2 1 1 Sort by ‘second’ (index) and ‘A’ (column) df_multi.sort_values(by=[\"second\", \"A\"]) A first second b 1 1 1 2 a 1 6 b 2 3 a 2 4 2 5", "prev_chunk_id": "chunk_118", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_120", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "searchsorted#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "searchsorted#", "content": "searchsorted# Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted(). ser = pd.Series([1, 2, 3]) ser.searchsorted([0, 3]) array([0, 2]) ser.searchsorted([0, 4]) array([0, 3]) ser.searchsorted([1, 3], side=\"right\") array([1, 3]) ser.searchsorted([1, 3], side=\"left\") array([0, 2]) ser = pd.Series([3, 1, 2]) ser.searchsorted([0, 3], sorter=np.argsort(ser)) array([0, 2])", "prev_chunk_id": "chunk_119", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_121", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "smallest / largest values#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "smallest / largest values#", "content": "smallest / largest values# Series has the nsmallest() and nlargest() methods which return the smallest or largest \\(n\\) values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result. s = pd.Series(np.random.permutation(10)) s 0 2 1 0 2 3 3 7 4 1 5 5 6 9 7 6 8 8 9 4 dtype: int64 s.sort_values() 1 0 4 1 0 2 2 3 9 4 5 5 7 6 3 7 8 8 6 9 dtype: int64 s.nsmallest(3) 1 0 4 1 0 2 dtype: int64 s.nlargest(3) 6 9 8 8 3 7 dtype: int64 DataFrame also has the nlargest and nsmallest methods. df = pd.DataFrame( .....: { .....: \"a\": [-2, -1, 1, 10, 8, 11, -1], .....: \"b\": list(\"abdceff\"), .....: \"c\": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0], .....: } .....: ) .....: df.nlargest(3, \"a\") a b c 5 11 f 3.0 3 10 c 3.2 4 8 e NaN df.nlargest(5, [\"a\", \"c\"]) a b c 5 11 f 3.0 3 10 c 3.2 4 8 e NaN 2 1 d 4.0 6 -1 f 4.0 df.nsmallest(3, \"a\") a b c 0 -2 a 1.0 1 -1 b 2.0 6 -1 f 4.0 df.nsmallest(5, [\"a\", \"c\"]) a b c 0 -2 a 1.0 1 -1 b 2.0 6 -1 f 4.0 2 1 d 4.0 4 8 e NaN", "prev_chunk_id": "chunk_120", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_122", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Sorting by a MultiIndex column#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Sorting by a MultiIndex column#", "content": "Sorting by a MultiIndex column# You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by. df1.columns = pd.MultiIndex.from_tuples( .....: [(\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"three\")] .....: ) .....: df1.sort_values(by=(\"a\", \"two\")) a b one two three 0 2 1 5 2 1 2 3 1 1 3 4 3 1 4 2", "prev_chunk_id": "chunk_121", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_123", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Copying#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Copying#", "content": "Copying# The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are immutable) and returns a new object. Note that it is seldom necessary to copy objects. For example, there are only a handful of ways to alter a DataFrame in-place: - Inserting, deleting, or modifying a column. - Assigning to theindexorcolumnsattributes. - For homogeneous data, directly modifying the values via thevaluesattribute or advanced indexing. To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.", "prev_chunk_id": "chunk_122", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_124", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "dtypes#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "dtypes#", "content": "dtypes# For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float, int, bool, timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes). pandas and third-party libraries extend NumPy’s type system in a few places. This section describes the extensions pandas has made internally. See Extension types for how to write your own extension that works with pandas. See the ecosystem page for a list of third-party libraries that have implemented an extension. The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type. Kind of Data | Data Type | Scalar | Array | String Aliases tz-aware datetime | DatetimeTZDtype | Timestamp | arrays.DatetimeArray | 'datetime64[ns, <tz>]' Categorical | CategoricalDtype | (none) | Categorical | 'category' period (time spans) | PeriodDtype | Period | arrays.PeriodArray 'Period[<freq>]' | 'period[<freq>]', sparse | SparseDtype | (none) | arrays.SparseArray | 'Sparse', 'Sparse[int]', 'Sparse[float]' intervals | IntervalDtype | Interval | arrays.IntervalArray | 'interval', 'Interval', 'Interval[<numpy_dtype>]', 'Interval[datetime64[ns, <tz>]]', 'Interval[timedelta64[<freq>]]' nullable integer | Int64Dtype, … | (none) | arrays.IntegerArray | 'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64' nullable float | Float64Dtype, … | (none) | arrays.FloatingArray | 'Float32', 'Float64' Strings | StringDtype | str | arrays.StringArray | 'string' Boolean (with NA) | BooleanDtype | bool | arrays.BooleanArray | 'boolean' pandas has two ways to store strings. - objectdtype, which can hold any Python object, including strings. - StringDtype, which is dedicated to strings. Generally, we recommend using StringDtype. See Text data types for more. Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See", "prev_chunk_id": "chunk_123", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_125", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "dtypes#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "dtypes#", "content": "object conversion). A convenient dtypes attribute for DataFrame returns a Series with the data type of each column. dft = pd.DataFrame( .....: { .....: \"A\": np.random.rand(3), .....: \"B\": 1, .....: \"C\": \"foo\", .....: \"D\": pd.Timestamp(\"20010102\"), .....: \"E\": pd.Series([1.0] * 3).astype(\"float32\"), .....: \"F\": False, .....: \"G\": pd.Series([1] * 3, dtype=\"int8\"), .....: } .....: ) .....: dft A B C D E F G 0 0.035962 1 foo 2001-01-02 1.0 False 1 1 0.701379 1 foo 2001-01-02 1.0 False 1 2 0.281885 1 foo 2001-01-02 1.0 False 1 dft.dtypes A float64 B int64 C object D datetime64[s] E float32 F bool G int8 dtype: object On a Series object, use the dtype attribute. dft[\"A\"].dtype dtype('float64') If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types (object is the most general). # these ints are coerced to floats pd.Series([1, 2, 3, 4, 5, 6.0]) 0 1.0 1 2.0 2 3.0 3 4.0 4 5.0 5 6.0 dtype: float64 # string data forces an ``object`` dtype pd.Series([1, 2, 3, 6.0, \"foo\"]) 0 1 1 2 2 3 3 6.0 4 foo dtype: object The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes.value_counts(). dft.dtypes.value_counts() float64 1 int64 1 object 1 datetime64[s] 1 float32 1 bool 1 int8 1 Name: count, dtype: int64 Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray, or a passed Series), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste. df1 = pd.DataFrame(np.random.randn(8, 1), columns=[\"A\"], dtype=\"float32\") df1 A 0 0.224364 1 1.890546 2 0.182879 3 0.787847 4", "prev_chunk_id": "chunk_124", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_126", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "dtypes#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "dtypes#", "content": "-0.188449 5 0.667715 6 -0.011736 7 -0.399073 df1.dtypes A float32 dtype: object df2 = pd.DataFrame( .....: { .....: \"A\": pd.Series(np.random.randn(8), dtype=\"float16\"), .....: \"B\": pd.Series(np.random.randn(8)), .....: \"C\": pd.Series(np.random.randint(0, 255, size=8), dtype=\"uint8\"), # [0,255] (range of uint8) .....: } .....: ) .....: df2 A B C 0 0.823242 0.256090 26 1 1.607422 1.426469 86 2 -0.333740 -0.416203 46 3 -0.063477 1.139976 212 4 -1.014648 -1.193477 26 5 0.678711 0.096706 7 6 -0.040863 -1.956850 184 7 -0.357422 -0.714337 206 df2.dtypes A float16 B float64 C uint8 dtype: object", "prev_chunk_id": "chunk_125", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_127", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "defaults#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "defaults#", "content": "defaults# By default integer types are int64 and float types are float64, regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes. pd.DataFrame([1, 2], columns=[\"a\"]).dtypes a int64 dtype: object pd.DataFrame({\"a\": [1, 2]}).dtypes a int64 dtype: object pd.DataFrame({\"a\": 1}, index=list(range(2))).dtypes a int64 dtype: object Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform. frame = pd.DataFrame(np.array([1, 2]))", "prev_chunk_id": "chunk_126", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_128", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "upcasting#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "upcasting#", "content": "upcasting# Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float). df3 = df1.reindex_like(df2).fillna(value=0.0) + df2 df3 A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 -0.150862 -0.416203 46.0 3 0.724370 1.139976 212.0 4 -1.203098 -1.193477 26.0 5 1.346426 0.096706 7.0 6 -0.052599 -1.956850 184.0 7 -0.756495 -0.714337 206.0 df3.dtypes A float32 B float64 C float64 dtype: object DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting. df3.to_numpy().dtype dtype('float64')", "prev_chunk_id": "chunk_127", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_129", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "astype#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "astype#", "content": "astype# You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid. Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation. df3 A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 -0.150862 -0.416203 46.0 3 0.724370 1.139976 212.0 4 -1.203098 -1.193477 26.0 5 1.346426 0.096706 7.0 6 -0.052599 -1.956850 184.0 7 -0.756495 -0.714337 206.0 df3.dtypes A float32 B float64 C float64 dtype: object # conversion of dtypes df3.astype(\"float32\").dtypes A float32 B float32 C float32 dtype: object Convert a subset of columns to a specified type using astype(). dft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}) dft[[\"a\", \"b\"]] = dft[[\"a\", \"b\"]].astype(np.uint8) dft a b c 0 1 4 7 1 2 5 8 2 3 6 9 dft.dtypes a uint8 b uint8 c int64 dtype: object Convert certain columns to a specific dtype by passing a dict to astype(). dft1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}) dft1 = dft1.astype({\"a\": np.bool_, \"c\": np.float64}) dft1 a b c 0 True 4 7.0 1 False 5 8.0 2 True 6 9.0 dft1.dtypes a bool b int64 c float64 dtype: object", "prev_chunk_id": "chunk_128", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_130", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "object conversion#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "object conversion#", "content": "object conversion# pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type. Because the data was transposed the original inference stored all columns as object, which infer_objects will correct. The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type: - to_numeric()(conversion to numeric dtypes)In [388]:m=[\"1.1\",2,3]In [389]:pd.to_numeric(m)Out[389]:array([1.1, 2. , 3. ]) - to_datetime()(conversion to datetime objects)In [390]:importdatetimeIn [391]:m=[\"2016-07-09\",datetime.datetime(2016,3,2)]In [392]:pd.to_datetime(m)Out[392]:DatetimeIndex(['2016-07-09', '2016-03-02'], dtype='datetime64[ns]', freq=None) - to_timedelta()(conversion to timedelta objects)In [393]:m=[\"5us\",pd.Timedelta(\"1day\")]In [394]:pd.to_timedelta(m)Out[394]:TimedeltaIndex(['0 days 00:00:00.000005', '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None) To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise', meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce', these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing: import datetime m = [\"apple\", datetime.datetime(2016, 3, 2)] pd.to_datetime(m, errors=\"coerce\") DatetimeIndex(['NaT', '2016-03-02'], dtype='datetime64[ns]', freq=None) m = [\"apple\", 2, 3] pd.to_numeric(m, errors=\"coerce\") array([nan, 2., 3.]) m = [\"apple\", pd.Timedelta(\"1day\")] pd.to_timedelta(m, errors=\"coerce\") TimedeltaIndex([NaT, '1 days'], dtype='timedelta64[ns]', freq=None) In addition to object conversion, to_numeric() provides another argument downcast, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory: m = [\"1\", 2, 3] pd.to_numeric(m, downcast=\"integer\") #", "prev_chunk_id": "chunk_129", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_131", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "object conversion#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "object conversion#", "content": "smallest signed int dtype array([1, 2, 3], dtype=int8) pd.to_numeric(m, downcast=\"signed\") # same as 'integer' array([1, 2, 3], dtype=int8) pd.to_numeric(m, downcast=\"unsigned\") # smallest unsigned int dtype array([1, 2, 3], dtype=uint8) pd.to_numeric(m, downcast=\"float\") # smallest float dtype array([1., 2., 3.], dtype=float32) As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with apply(), we can “apply” the function over each column efficiently: import datetime df = pd.DataFrame([[\"2016-07-09\", datetime.datetime(2016, 3, 2)]] * 2, dtype=\"O\") df 0 1 0 2016-07-09 2016-03-02 00:00:00 1 2016-07-09 2016-03-02 00:00:00 df.apply(pd.to_datetime) 0 1 0 2016-07-09 2016-03-02 1 2016-07-09 2016-03-02 df = pd.DataFrame([[\"1.1\", 2, 3]] * 2, dtype=\"O\") df 0 1 2 0 1.1 2 3 1 1.1 2 3 df.apply(pd.to_numeric) 0 1 2 0 1.1 2 3 1 1.1 2 3 df = pd.DataFrame([[\"5us\", pd.Timedelta(\"1day\")]] * 2, dtype=\"O\") df 0 1 0 5us 1 days 00:00:00 1 5us 1 days 00:00:00 df.apply(pd.to_timedelta) 0 1 0 0 days 00:00:00.000005 1 days 1 0 days 00:00:00.000005 1 days", "prev_chunk_id": "chunk_130", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_132", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "gotchas#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "gotchas#", "content": "gotchas# Performing selection operations on integer type data can easily upcast the data to floating. The dtype of the input data will be preserved in cases where nans are not introduced. See also Support for integer NA. dfi = df3.astype(\"int32\") dfi[\"E\"] = 1 dfi A B C E 0 1 0 26 1 1 3 1 86 1 2 0 0 46 1 3 0 1 212 1 4 -1 -1 26 1 5 1 0 7 1 6 0 -1 184 1 7 0 0 206 1 dfi.dtypes A int32 B int32 C int32 E int64 dtype: object casted = dfi[dfi > 0] casted A B C E 0 1.0 NaN 26 1 1 3.0 1.0 86 1 2 NaN NaN 46 1 3 NaN 1.0 212 1 4 NaN NaN 26 1 5 1.0 NaN 7 1 6 NaN NaN 184 1 7 NaN NaN 206 1 casted.dtypes A float64 B float64 C int32 E int64 dtype: object While float dtypes are unchanged. dfa = df3.copy() dfa[\"A\"] = dfa[\"A\"].astype(\"float32\") dfa.dtypes A float32 B float64 C float64 dtype: object casted = dfa[df2 > 0] casted A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 NaN NaN 46.0 3 NaN 1.139976 212.0 4 NaN NaN 26.0 5 1.346426 0.096706 7.0 6 NaN NaN 184.0 7 NaN NaN 206.0 casted.dtypes A float32 B float64 C float64 dtype: object", "prev_chunk_id": "chunk_131", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_133", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Selecting columns based on dtype#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Selecting columns based on dtype#", "content": "Selecting columns based on dtype# The select_dtypes() method implements subsetting of columns based on their dtype. First, let’s create a DataFrame with a slew of different dtypes: df = pd.DataFrame( .....: { .....: \"string\": list(\"abc\"), .....: \"int64\": list(range(1, 4)), .....: \"uint8\": np.arange(3, 6).astype(\"u1\"), .....: \"float64\": np.arange(4.0, 7.0), .....: \"bool1\": [True, False, True], .....: \"bool2\": [False, True, False], .....: \"dates\": pd.date_range(\"now\", periods=3), .....: \"category\": pd.Series(list(\"ABC\")).astype(\"category\"), .....: } .....: ) .....: df[\"tdeltas\"] = df.dates.diff() df[\"uint64\"] = np.arange(3, 6).astype(\"u8\") df[\"other_dates\"] = pd.date_range(\"20130101\", periods=3) df[\"tz_aware_dates\"] = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\") df string int64 uint8 ... uint64 other_dates tz_aware_dates 0 a 1 3 ... 3 2013-01-01 2013-01-01 00:00:00-05:00 1 b 2 4 ... 4 2013-01-02 2013-01-02 00:00:00-05:00 2 c 3 5 ... 5 2013-01-03 2013-01-03 00:00:00-05:00 [3 rows x 12 columns] And the dtypes: df.dtypes string object int64 int64 uint8 uint8 float64 float64 bool1 bool bool2 bool dates datetime64[ns] category category tdeltas timedelta64[ns] uint64 uint64 other_dates datetime64[ns] tz_aware_dates datetime64[ns, US/Eastern] dtype: object select_dtypes() has two parameters include and exclude that allow you to say “give me the columns with these dtypes” (include) and/or “give the columns without these dtypes” (exclude). For example, to select bool columns: df.select_dtypes(include=[bool]) bool1 bool2 0 True False 1 False True 2 True False You can also pass the name of a dtype in the NumPy dtype hierarchy: df.select_dtypes(include=[\"bool\"]) bool1 bool2 0 True False 1 False True 2 True False select_dtypes() also works with generic dtypes as well. For example, to select all numeric and boolean columns while excluding unsigned integers: df.select_dtypes(include=[\"number\", \"bool\"], exclude=[\"unsignedinteger\"]) int64 float64 bool1 bool2 tdeltas 0 1 4.0 True False NaT 1 2 5.0 False True 1 days 2 3 6.0 True False 1 days To select string columns you must use the object dtype: df.select_dtypes(include=[\"object\"]) string 0 a 1 b 2 c To see all the child", "prev_chunk_id": "chunk_132", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_134", "url": "https://pandas.pydata.org/docs/user_guide/basics.html", "title": "Selecting columns based on dtype#", "page_title": "Essential basic functionality — pandas 2.3.1 documentation", "breadcrumbs": "Selecting columns based on dtype#", "content": "dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes: def subdtypes(dtype): .....: subs = dtype.__subclasses__() .....: if not subs: .....: return dtype .....: return [dtype, [subdtypes(dt) for dt in subs]] .....: All NumPy dtypes are subclasses of numpy.generic: subdtypes(np.generic) [numpy.generic, [[numpy.number, [[numpy.integer, [[numpy.signedinteger, [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.longlong, numpy.timedelta64]], [numpy.unsignedinteger, [numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, numpy.ulonglong]]]], [numpy.inexact, [[numpy.floating, [numpy.float16, numpy.float32, numpy.float64, numpy.longdouble]], [numpy.complexfloating, [numpy.complex64, numpy.complex128, numpy.clongdouble]]]]]], [numpy.flexible, [[numpy.character, [numpy.bytes_, numpy.str_]], [numpy.void, [numpy.record]]]], numpy.bool_, numpy.datetime64, numpy.object_]]", "prev_chunk_id": "chunk_133", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_135", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "IO tools (text, CSV, HDF5, …)#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "IO tools (text, CSV, HDF5, …)#", "content": "IO tools (text, CSV, HDF5, …)# The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv() that generally return a pandas object. The corresponding writer functions are object methods that are accessed like DataFrame.to_csv(). Below is a table containing available readers and writers. Format Type | Data Description | Reader | Writer text | CSV | read_csv | to_csv text | Fixed-Width Text File | read_fwf | NA text | JSON | read_json | to_json text | HTML | read_html | to_html text | LaTeX | Styler.to_latex | NA text | XML | read_xml | to_xml text | Local clipboard | read_clipboard | to_clipboard binary | MS Excel | read_excel | to_excel binary | OpenDocument | read_excel | NA binary | HDF5 Format | read_hdf | to_hdf binary | Feather Format | read_feather | to_feather binary | Parquet Format | read_parquet | to_parquet binary | ORC Format | read_orc | to_orc binary | Stata | read_stata | to_stata binary | SAS | read_sas | NA binary | SPSS | read_spss | NA binary | Python Pickle Format | read_pickle | to_pickle SQL | SQL | read_sql | to_sql SQL | Google BigQuery;:ref:read_gbq<io.bigquery>;:ref:to_gbq<io.bigquery> | | Here is an informal performance comparison for some of these IO methods.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_136", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "CSV & text files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "CSV & text files#", "content": "CSV & text files# The workhorse function for reading text files (a.k.a. flat files) is read_csv(). See the cookbook for some advanced strategies.", "prev_chunk_id": "chunk_135", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_137", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parsing options#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parsing options#", "content": "Parsing options# read_csv() accepts the following common arguments:", "prev_chunk_id": "chunk_136", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_138", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying column data types#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying column data types#", "content": "Specifying column data types# You can indicate the data type for the whole DataFrame or individual columns: import numpy as np data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\" print(data) a,b,c,d 1,2,3,4 5,6,7,8 9,10,11 df = pd.read_csv(StringIO(data), dtype=object) df a b c d 0 1 2 3 4 1 5 6 7 8 2 9 10 11 NaN df[\"a\"][0] '1' df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"}) df.dtypes a int64 b object c float64 d Int64 dtype: object Fortunately, pandas offers more than one way to ensure that your column(s) contain only one dtype. If you’re unfamiliar with these concepts, you can see here to learn more about dtypes, and here to learn more about object conversion in pandas. For instance, you can use the converters argument of read_csv(): data = \"col_1\\n1\\n2\\n'A'\\n4.22\" df = pd.read_csv(StringIO(data), converters={\"col_1\": str}) df col_1 0 1 1 2 2 'A' 3 4.22 df[\"col_1\"].apply(type).value_counts() col_1 <class 'str'> 4 Name: count, dtype: int64 Or you can use the to_numeric() function to coerce the dtypes after reading in the data, df2 = pd.read_csv(StringIO(data)) df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\") df2 col_1 0 1.00 1 2.00 2 NaN 3 4.22 df2[\"col_1\"].apply(type).value_counts() col_1 <class 'float'> 4 Name: count, dtype: int64 which will convert all valid parsing to floats, leaving the invalid parsing as NaN. Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to NaN out the data anomalies, then to_numeric() is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the converters argument of read_csv() would certainly be worth trying. Setting dtype_backend=\"numpy_nullable\" will result in nullable dtypes for every column. data = \"\"\"a,b,c,d,e,f,g,h,i,j ....: 1,2.5,True,a,,,,,12-31-2019, ....: 3,4.5,False,b,6,7.5,True,a,12-31-2019, ....: \"\"\" ....: df = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"]) df a b", "prev_chunk_id": "chunk_137", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_139", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying column data types#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying column data types#", "content": "c d e f g h i j 0 1 2.5 True a <NA> <NA> <NA> <NA> 2019-12-31 <NA> 1 3 4.5 False b 6 7.5 True a 2019-12-31 <NA> df.dtypes a Int64 b Float64 c boolean d string[python] e Int64 f Float64 g boolean h string[python] i datetime64[ns] j Int64 dtype: object", "prev_chunk_id": "chunk_138", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_140", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying categorical dtype#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying categorical dtype#", "content": "Specifying categorical dtype# Categorical columns can be parsed directly by specifying dtype='category' or dtype=CategoricalDtype(categories, ordered). data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\" pd.read_csv(StringIO(data)) col1 col2 col3 0 a b 1 1 a b 2 2 c d 3 pd.read_csv(StringIO(data)).dtypes col1 object col2 object col3 int64 dtype: object pd.read_csv(StringIO(data), dtype=\"category\").dtypes col1 category col2 category col3 category dtype: object Individual columns can be parsed as a Categorical using a dict specification: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes col1 category col2 object col3 int64 dtype: object Specifying dtype='category' will result in an unordered Categorical whose categories are the unique values observed in the data. For more control on the categories and order, create a CategoricalDtype ahead of time, and pass that for that column’s dtype. from pandas.api.types import CategoricalDtype dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True) pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes col1 category col2 object col3 int64 dtype: object When using dtype=CategoricalDtype, “unexpected” values outside of dtype.categories are treated as missing values. dtype = CategoricalDtype([\"a\", \"b\", \"d\"]) # No 'c' pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1 0 a 1 a 2 NaN Name: col1, dtype: category Categories (3, object): ['a', 'b', 'd'] This matches the behavior of Categorical.set_categories().", "prev_chunk_id": "chunk_139", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_141", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Handling column names#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Handling column names#", "content": "Handling column names# A file may or may not have a header row. pandas assumes the first row should be used as the column names: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\" print(data) a,b,c 1,2,3 4,5,6 7,8,9 pd.read_csv(StringIO(data)) a b c 0 1 2 3 1 4 5 6 2 7 8 9 By specifying the names argument in conjunction with header you can indicate other names to use and whether or not to throw away the header row (if any): print(data) a,b,c 1,2,3 4,5,6 7,8,9 pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0) foo bar baz 0 1 2 3 1 4 5 6 2 7 8 9 pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None) foo bar baz 0 a b c 1 1 2 3 2 4 5 6 3 7 8 9 If the header is in a row other than the first, pass the row number to header. This will skip the preceding rows: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\" pd.read_csv(StringIO(data), header=1) a b c 0 1 2 3 1 4 5 6 2 7 8 9", "prev_chunk_id": "chunk_140", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_142", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Duplicate names parsing#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate names parsing#", "content": "Duplicate names parsing# If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data: data = \"a,b,a\\n0,1,2\\n3,4,5\" pd.read_csv(StringIO(data)) a b a.1 0 0 1 2 1 3 4 5 There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become ‘X’, ‘X.1’, …, ‘X.N’.", "prev_chunk_id": "chunk_141", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_143", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Filtering columns (usecols)#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Filtering columns (usecols)#", "content": "Filtering columns (usecols)# The usecols argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\" pd.read_csv(StringIO(data)) a b c d 0 1 2 3 foo 1 4 5 6 bar 2 7 8 9 baz pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"]) b d 0 2 foo 1 5 bar 2 8 baz pd.read_csv(StringIO(data), usecols=[0, 2, 3]) a c d 0 1 3 foo 1 4 6 bar 2 7 9 baz pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"]) a c 0 1 3 1 4 6 2 7 9 The usecols argument can also be used to specify which columns not to use in the final result: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"]) b d 0 2 foo 1 5 bar 2 8 baz In this case, the callable is specifying that we exclude the “a” and “c” columns from the output.", "prev_chunk_id": "chunk_142", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_144", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Ignoring line comments and empty lines#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Ignoring line comments and empty lines#", "content": "Ignoring line comments and empty lines# If the comment parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well. data = \"\\na,b,c\\n \\n# commented line\\n1,2,3\\n\\n4,5,6\" print(data) a,b,c # commented line 1,2,3 4,5,6 pd.read_csv(StringIO(data), comment=\"#\") a b c 0 1 2 3 1 4 5 6 If skip_blank_lines=False, then read_csv will not ignore blank lines: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\" pd.read_csv(StringIO(data), skip_blank_lines=False) a b c 0 NaN NaN NaN 1 1.0 2.0 3.0 2 NaN NaN NaN 3 NaN NaN NaN 4 4.0 5.0 6.0 data = ( ....: \"# empty\\n\" ....: \"# second empty line\\n\" ....: \"# third emptyline\\n\" ....: \"X,Y,Z\\n\" ....: \"1,2,3\\n\" ....: \"A,B,C\\n\" ....: \"1,2.,4.\\n\" ....: \"5.,NaN,10.0\\n\" ....: ) ....: print(data) # empty # second empty line # third emptyline X,Y,Z 1,2,3 A,B,C 1,2.,4. 5.,NaN,10.0 pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1) A B C 0 1.0 2.0 4.0 1 5.0 NaN 10.0", "prev_chunk_id": "chunk_143", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_145", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Comments#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Comments#", "content": "Comments# Sometimes comments or meta data may be included in a file: data = ( ....: \"ID,level,category\\n\" ....: \"Patient1,123000,x # really unpleasant\\n\" ....: \"Patient2,23000,y # wouldn't take his medicine\\n\" ....: \"Patient3,1234018,z # awesome\" ....: ) ....: with open(\"tmp.csv\", \"w\") as fh: ....: fh.write(data) ....: print(open(\"tmp.csv\").read()) ID,level,category Patient1,123000,x # really unpleasant Patient2,23000,y # wouldn't take his medicine Patient3,1234018,z # awesome By default, the parser includes the comments in the output: df = pd.read_csv(\"tmp.csv\") df ID level category 0 Patient1 123000 x # really unpleasant 1 Patient2 23000 y # wouldn't take his medicine 2 Patient3 1234018 z # awesome We can suppress the comments using the comment keyword: df = pd.read_csv(\"tmp.csv\", comment=\"#\") df ID level category 0 Patient1 123000 x 1 Patient2 23000 y 2 Patient3 1234018 z", "prev_chunk_id": "chunk_144", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_146", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Dealing with Unicode data#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Dealing with Unicode data#", "content": "Dealing with Unicode data# The encoding argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result: from io import BytesIO data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\" data = data.decode(\"utf8\").encode(\"latin-1\") df = pd.read_csv(BytesIO(data), encoding=\"latin-1\") df word length 0 Träumen 7 1 Grüße 5 df[\"word\"][1] 'Grüße' Some formats which encode all characters as multiple bytes, like UTF-16, won’t parse correctly at all without specifying the encoding. Full list of Python standard encodings.", "prev_chunk_id": "chunk_145", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_147", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Index columns and trailing delimiters#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Index columns and trailing delimiters#", "content": "Index columns and trailing delimiters# If a file has one more column of data than the number of column names, the first column will be used as the DataFrame’s row names: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\" pd.read_csv(StringIO(data)) a b c 4 apple bat 5.7 8 orange cow 10.0 data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\" pd.read_csv(StringIO(data), index_col=0) a b c index 4 apple bat 5.7 8 orange cow 10.0 Ordinarily, you can achieve this behavior using the index_col option. There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\" print(data) a,b,c 4,apple,bat, 8,orange,cow, pd.read_csv(StringIO(data)) a b c 4 apple bat NaN 8 orange cow NaN pd.read_csv(StringIO(data), index_col=False) a b c 0 4 apple bat 1 8 orange cow If a subset of data is being parsed using the usecols option, the index_col specification is based on that subset, not the original data. data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\" print(data) a,b,c 4,apple,bat, 8,orange,cow, pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"]) b c 4 bat NaN 8 cow NaN pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0) b c 4 bat NaN 8 cow NaN", "prev_chunk_id": "chunk_146", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_148", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying date columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying date columns#", "content": "Specifying date columns# To better facilitate working with datetime data, read_csv() uses the keyword arguments parse_dates and date_format to allow users to specify a variety of columns and date/time formats to turn the input text data into datetime objects. The simplest case is to just pass in parse_dates=True: with open(\"foo.csv\", mode=\"w\") as f: .....: f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\") .....: # Use a column as an index, and parse it as dates. df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True) df A B C date 2009-01-01 a 1 2 2009-01-02 b 3 4 2009-01-03 c 4 5 # These are Python datetime objects df.index DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None) It is often the case that we may want to store date and time data separately, or store various date fields separately. the parse_dates keyword can be used to specify a combination of columns to parse the dates and/or times from. You can specify a list of column lists to parse_dates, the resulting date columns will be prepended to the output (so as to not affect the existing column order) and the new column names will be the concatenation of the component column names: data = ( .....: \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\" .....: \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\" .....: \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\" .....: \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\" .....: \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\" .....: \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\" .....: ) .....: with open(\"tmp.csv\", \"w\") as fh: .....: fh.write(data) .....: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]]) df 1_2 1_3 0 4 0 1999-01-27 19:00:00 1999-01-27 18:56:00 KORD 0.81 1 1999-01-27 20:00:00 1999-01-27 19:56:00 KORD 0.01 2 1999-01-27 21:00:00 1999-01-27 20:56:00 KORD -0.59 3 1999-01-27 21:00:00 1999-01-27 21:18:00 KORD -0.99 4 1999-01-27 22:00:00 1999-01-27 21:56:00 KORD -0.59 5 1999-01-27 23:00:00 1999-01-27 22:56:00 KORD -0.59 By default the parser removes the component date columns, but you can choose to", "prev_chunk_id": "chunk_147", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_149", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying date columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying date columns#", "content": "retain them via the keep_date_col keyword: df = pd.read_csv( .....: \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True .....: ) .....: df 1_2 1_3 0 ... 2 3 4 0 1999-01-27 19:00:00 1999-01-27 18:56:00 KORD ... 19:00:00 18:56:00 0.81 1 1999-01-27 20:00:00 1999-01-27 19:56:00 KORD ... 20:00:00 19:56:00 0.01 2 1999-01-27 21:00:00 1999-01-27 20:56:00 KORD ... 21:00:00 20:56:00 -0.59 3 1999-01-27 21:00:00 1999-01-27 21:18:00 KORD ... 21:00:00 21:18:00 -0.99 4 1999-01-27 22:00:00 1999-01-27 21:56:00 KORD ... 22:00:00 21:56:00 -0.59 5 1999-01-27 23:00:00 1999-01-27 22:56:00 KORD ... 23:00:00 22:56:00 -0.59 [6 rows x 7 columns] Note that if you wish to combine multiple columns into a single date column, a nested list must be used. In other words, parse_dates=[1, 2] indicates that the second and third columns should each be parsed as separate date columns while parse_dates=[[1, 2]] means the two columns should be parsed into a single column. You can also use a dict to specify custom name columns: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]} df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec) df nominal actual 0 4 0 1999-01-27 19:00:00 1999-01-27 18:56:00 KORD 0.81 1 1999-01-27 20:00:00 1999-01-27 19:56:00 KORD 0.01 2 1999-01-27 21:00:00 1999-01-27 20:56:00 KORD -0.59 3 1999-01-27 21:00:00 1999-01-27 21:18:00 KORD -0.99 4 1999-01-27 22:00:00 1999-01-27 21:56:00 KORD -0.59 5 1999-01-27 23:00:00 1999-01-27 22:56:00 KORD -0.59 It is important to remember that if multiple text columns are to be parsed into a single date column, then a new column is prepended to the data. The index_col specification is based off of this new set of columns rather than the original data columns: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]} df = pd.read_csv( .....: \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0 .....: ) # index is the nominal column .....: df actual 0 4 nominal 1999-01-27 19:00:00 1999-01-27 18:56:00 KORD 0.81", "prev_chunk_id": "chunk_148", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_150", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying date columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying date columns#", "content": "1999-01-27 20:00:00 1999-01-27 19:56:00 KORD 0.01 1999-01-27 21:00:00 1999-01-27 20:56:00 KORD -0.59 1999-01-27 21:00:00 1999-01-27 21:18:00 KORD -0.99 1999-01-27 22:00:00 1999-01-27 21:56:00 KORD -0.59 1999-01-27 23:00:00 1999-01-27 22:56:00 KORD -0.59", "prev_chunk_id": "chunk_149", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_151", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Date parsing functions#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Date parsing functions#", "content": "Date parsing functions# Finally, the parser allows you to specify a custom date_format. Performance-wise, you should try these methods of parsing dates in order: - If you know the format, usedate_format, e.g.:date_format=\"%d/%m/%Y\"ordate_format={column_name:\"%d/%m/%Y\"}. - If you different formats for different columns, or want to pass any extra options (such asutc) toto_datetime, then you should read in your data asobjectdtype, and then useto_datetime.", "prev_chunk_id": "chunk_150", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_152", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parsing a CSV with mixed timezones#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parsing a CSV with mixed timezones#", "content": "Parsing a CSV with mixed timezones# pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with parse_dates. To parse the mixed-timezone values as a datetime column, read in as object dtype and then call to_datetime() with utc=True. content = \"\"\"\\ .....: a .....: 2000-01-01T00:00:00+05:00 .....: 2000-01-01T00:00:00+06:00\"\"\" .....: df = pd.read_csv(StringIO(content)) df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True) df[\"a\"] 0 1999-12-31 19:00:00+00:00 1 1999-12-31 18:00:00+00:00 Name: a, dtype: datetime64[ns, UTC]", "prev_chunk_id": "chunk_151", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_153", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Inferring datetime format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Inferring datetime format#", "content": "Inferring datetime format# Here are some examples of datetime strings that can be guessed (all representing December 30th, 2011 at 00:00:00): - “20111230” - “2011/12/30” - “20111230 00:00:00” - “12/30/2011 00:00:00” - “30/Dec/2011 00:00:00” - “30/December/2011 00:00:00” Note that format inference is sensitive to dayfirst. With dayfirst=True, it will guess “01/12/2011” to be December 1st. With dayfirst=False (default) it will guess “01/12/2011” to be January 12th. If you try to parse a column of date strings, pandas will attempt to guess the format from the first non-NaN element, and will then parse the rest of the column with that format. If pandas fails to guess the format (for example if your first string is '01 December US/Pacific 2000'), then a warning will be raised and each row will be parsed individually by dateutil.parser.parse. The safest way to parse dates is to explicitly set format=. df = pd.read_csv( .....: \"foo.csv\", .....: index_col=0, .....: parse_dates=True, .....: ) .....: df A B C date 2009-01-01 a 1 2 2009-01-02 b 3 4 2009-01-03 c 4 5 In the case that you have mixed datetime formats within the same column, you can pass format='mixed' data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\") df = pd.read_csv(data) df['date'] = pd.to_datetime(df['date'], format='mixed') df date 0 2000-01-12 1 2000-01-13 or, if your datetime formats are all ISO8601 (possibly not identically-formatted): data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\") df = pd.read_csv(data) df['date'] = pd.to_datetime(df['date'], format='ISO8601') df date 0 2020-01-01 00:00:00 1 2020-01-01 03:00:00", "prev_chunk_id": "chunk_152", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_154", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "International date formats#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "International date formats#", "content": "International date formats# While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a dayfirst keyword is provided: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\" print(data) date,value,cat 1/6/2000,5,a 2/6/2000,10,b 3/6/2000,15,c with open(\"tmp.csv\", \"w\") as fh: .....: fh.write(data) .....: pd.read_csv(\"tmp.csv\", parse_dates=[0]) date value cat 0 2000-01-06 5 a 1 2000-02-06 10 b 2 2000-03-06 15 c pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0]) date value cat 0 2000-06-01 5 a 1 2000-06-02 10 b 2 2000-06-03 15 c", "prev_chunk_id": "chunk_153", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_155", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing CSVs to binary file objects#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing CSVs to binary file objects#", "content": "Writing CSVs to binary file objects# df.to_csv(..., mode=\"wb\") allows writing a CSV to a file object opened binary mode. In most cases, it is not necessary to specify mode as Pandas will auto-detect whether the file object is opened in text or binary mode. import io data = pd.DataFrame([0, 1, 2]) buffer = io.BytesIO() data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\")", "prev_chunk_id": "chunk_154", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_156", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying method for floating-point conversion#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying method for floating-point conversion#", "content": "Specifying method for floating-point conversion# The parameter float_precision can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example: val = \"0.3066101993807095471566981359501369297504425048828125\" data = \"a,b,c\\n1,2,{0}\".format(val) abs( .....: pd.read_csv( .....: StringIO(data), .....: engine=\"c\", .....: float_precision=None, .....: )[\"c\"][0] - float(val) .....: ) .....: 5.551115123125783e-17 abs( .....: pd.read_csv( .....: StringIO(data), .....: engine=\"c\", .....: float_precision=\"high\", .....: )[\"c\"][0] - float(val) .....: ) .....: 5.551115123125783e-17 abs( .....: pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0] .....: - float(val) .....: ) .....: 0.0", "prev_chunk_id": "chunk_155", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_157", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Thousand separators#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Thousand separators#", "content": "Thousand separators# For large numbers that have been written with a thousands separator, you can set the thousands keyword to a string of length 1 so that integers will be parsed correctly: By default, numbers with a thousands separator will be parsed as strings: data = ( .....: \"ID|level|category\\n\" .....: \"Patient1|123,000|x\\n\" .....: \"Patient2|23,000|y\\n\" .....: \"Patient3|1,234,018|z\" .....: ) .....: with open(\"tmp.csv\", \"w\") as fh: .....: fh.write(data) .....: df = pd.read_csv(\"tmp.csv\", sep=\"|\") df ID level category 0 Patient1 123,000 x 1 Patient2 23,000 y 2 Patient3 1,234,018 z df.level.dtype dtype('O') The thousands keyword allows integers to be parsed correctly: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\") df ID level category 0 Patient1 123000 x 1 Patient2 23000 y 2 Patient3 1234018 z df.level.dtype dtype('int64')", "prev_chunk_id": "chunk_156", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_158", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "NA values#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "NA values#", "content": "NA values# To control which values are parsed as missing values (which are signified by NaN), specify a string in na_values. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a float, like 5.0 or an integer like 5), the corresponding equivalent values will also imply a missing value (in this case effectively [5.0, 5] are recognized as NaN). To completely override the default values that are recognized as missing, specify keep_default_na=False. The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', 'None', '']. Let us consider some examples: pd.read_csv(\"path_to_file.csv\", na_values=[5]) In the example above 5 and 5.0 will be recognized as NaN, in addition to the defaults. A string will first be interpreted as a numerical 5, then as a NaN. pd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"]) Above, only an empty field will be recognized as NaN. pd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"]) Above, both NA and 0 as strings are NaN. pd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"]) The default values, in addition to the string \"Nope\" are recognized as NaN.", "prev_chunk_id": "chunk_157", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_159", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Infinity#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Infinity#", "content": "Infinity# inf like values will be parsed as np.inf (positive infinity), and -inf as -np.inf (negative infinity). These will ignore the case of the value, meaning Inf, will also be parsed as np.inf.", "prev_chunk_id": "chunk_158", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_160", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Boolean values#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Boolean values#", "content": "Boolean values# The common values True, False, TRUE, and FALSE are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the true_values and false_values options as follows: data = \"a,b,c\\n1,Yes,2\\n3,No,4\" print(data) a,b,c 1,Yes,2 3,No,4 pd.read_csv(StringIO(data)) a b c 0 1 Yes 2 1 3 No 4 pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"]) a b c 0 1 True 2 1 3 False 4", "prev_chunk_id": "chunk_159", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_161", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Handling “bad” lines#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Handling “bad” lines#", "content": "Handling “bad” lines# Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\" pd.read_csv(StringIO(data)) --------------------------------------------------------------------------- ParserError Traceback (most recent call last) Cell In[161], line 1 ----> 1 pd.read_csv(StringIO(data)) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend) 1013 kwds_defaults = _refine_defaults_read( 1014 dialect, 1015 delimiter, (...) 1022 dtype_backend=dtype_backend, 1023 ) 1024 kwds.update(kwds_defaults) -> 1026 return _read(filepath_or_buffer, kwds) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:626, in _read(filepath_or_buffer, kwds) 623 return parser 625 with parser: --> 626 return parser.read(nrows) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:1923, in TextFileReader.read(self, nrows) 1916 nrows = validate_integer(\"nrows\", nrows) 1917 try: 1918 # error: \"ParserBase\" has no attribute \"read\" 1919 ( 1920 index, 1921 columns, 1922 col_dict, -> 1923 ) = self._engine.read( # type: ignore[attr-defined] 1924 nrows 1925 ) 1926 except Exception: 1927 self.close() File ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows) 232 try: 233 if self.low_memory: --> 234 chunks = self._reader.read_low_memory(nrows) 235 # destructive to chunks 236 data = _concatenate_chunks(chunks) File ~/work/pandas/pandas/pandas/_libs/parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory() File ~/work/pandas/pandas/pandas/_libs/parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows() File ~/work/pandas/pandas/pandas/_libs/parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows() File ~/work/pandas/pandas/pandas/_libs/parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status() File ~/work/pandas/pandas/pandas/_libs/parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error() ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4 You can elect to skip bad lines: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\" pd.read_csv(StringIO(data), on_bad_lines=\"skip\") a b c 0 1 2 3 1 8 9 10 Or pass a callable function to handle the bad line if engine=\"python\". The bad line will be a list of strings that was split by the sep: external_list", "prev_chunk_id": "chunk_160", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_162", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Handling “bad” lines#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Handling “bad” lines#", "content": "= [] def bad_lines_func(line): .....: external_list.append(line) .....: return line[-3:] .....: external_list [] You can also use the usecols parameter to eliminate extraneous column data that appear in some lines but not others: pd.read_csv(StringIO(data), usecols=[0, 1, 2]) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[171], line 1 ----> 1 pd.read_csv(StringIO(data), usecols=[0, 1, 2]) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend) 1013 kwds_defaults = _refine_defaults_read( 1014 dialect, 1015 delimiter, (...) 1022 dtype_backend=dtype_backend, 1023 ) 1024 kwds.update(kwds_defaults) -> 1026 return _read(filepath_or_buffer, kwds) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds) 617 _validate_names(kwds.get(\"names\", None)) 619 # Create the parser. --> 620 parser = TextFileReader(filepath_or_buffer, **kwds) 622 if chunksize or iterator: 623 return parser File ~/work/pandas/pandas/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds) 1617 self.options[\"has_index_names\"] = kwds[\"has_index_names\"] 1619 self.handles: IOHandles | None = None -> 1620 self._engine = self._make_engine(f, self.engine) File ~/work/pandas/pandas/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine) 1895 raise ValueError(msg) 1897 try: -> 1898 return mapping[engine](f, **self.options) 1899 except Exception: 1900 if self.handles is not None: File ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:155, in CParserWrapper.__init__(self, src, **kwds) 152 # error: Cannot determine type of 'names' 153 if len(self.names) < len(usecols): # type: ignore[has-type] 154 # error: Cannot determine type of 'names' --> 155 self._validate_usecols_names( 156 usecols, 157 self.names, # type: ignore[has-type] 158 ) 160 # error: Cannot determine type of 'names' 161 self._validate_parse_dates_presence(self.names) # type: ignore[has-type] File ~/work/pandas/pandas/pandas/io/parsers/base_parser.py:988, in ParserBase._validate_usecols_names(self, usecols, names) 986 missing = [c for c in usecols if c not in names] 987 if len(missing) > 0: --> 988 raise ValueError( 989 f\"Usecols do not match columns, columns expected but not found: \" 990 f\"{missing}\" 991 )", "prev_chunk_id": "chunk_161", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_163", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Handling “bad” lines#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Handling “bad” lines#", "content": "993 return usecols ValueError: Usecols do not match columns, columns expected but not found: [0, 1, 2] In case you want to keep all data including the lines with too many fields, you can specify a sufficient number of names. This ensures that lines with not enough fields are filled with NaN. pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd']) a b c d 0 name type NaN NaN 1 name a a is of type a NaN NaN 2 name b b is of type b\" NaN NaN", "prev_chunk_id": "chunk_162", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_164", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Dialect#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Dialect#", "content": "Dialect# The dialect keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a csv.Dialect instance. Suppose you had data with unenclosed quotes: data = \"label1,label2,label3\\n\" 'index1,\"a,c,e\\n' \"index2,b,d,f\" print(data) label1,label2,label3 index1,\"a,c,e index2,b,d,f By default, read_csv uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote. We can get around this using dialect: import csv dia = csv.excel() dia.quoting = csv.QUOTE_NONE pd.read_csv(StringIO(data), dialect=dia) label1 label2 label3 index1 \"a c e index2 b d f All of the dialect options can be specified separately by keyword arguments: data = \"a,b,c~1,2,3~4,5,6\" pd.read_csv(StringIO(data), lineterminator=\"~\") a b c 0 1 2 3 1 4 5 6 Another common dialect option is skipinitialspace, to skip any whitespace after a delimiter: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\" print(data) a, b, c 1, 2, 3 4, 5, 6 pd.read_csv(StringIO(data), skipinitialspace=True) a b c 0 1 2 3 1 4 5 6 The parsers make every attempt to “do the right thing” and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.", "prev_chunk_id": "chunk_163", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_165", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Quoting and Escape Characters#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Quoting and Escape Characters#", "content": "Quoting and Escape Characters# Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the escapechar option: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5' print(data) a,b \"hello, \\\"Bob\\\", nice to see you\",5 pd.read_csv(StringIO(data), escapechar=\"\\\\\") a b 0 hello, \"Bob\", nice to see you 5", "prev_chunk_id": "chunk_164", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_166", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Files with fixed width columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Files with fixed width columns#", "content": "Files with fixed width columns# While read_csv() reads delimited data, the read_fwf() function works with data files that have known and fixed column widths. The function parameters to read_fwf are largely the same as read_csv with two extra parameters, and a different usage of the delimiter parameter: - colspecs: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’ can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer. - widths: A list of field widths which can be used instead of ‘colspecs’ if the intervals are contiguous. - delimiter: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler character of the fields if it is not spaces (e.g., ‘~’). Consider a typical fixed-width data file: data1 = ( .....: \"id8141 360.242940 149.910199 11950.7\\n\" .....: \"id1594 444.953632 166.985655 11788.4\\n\" .....: \"id1849 364.136849 183.628767 11806.2\\n\" .....: \"id1230 413.836124 184.375703 11916.8\\n\" .....: \"id1948 502.953953 173.237159 12468.3\" .....: ) .....: with open(\"bar.csv\", \"w\") as f: .....: f.write(data1) .....: In order to parse this file into a DataFrame, we simply need to supply the column specifications to the read_fwf function along with the file name: # Column specifications are a list of half-intervals colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)] df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0) df 1 2 3 0 id8141 360.242940 149.910199 11950.7 id1594 444.953632 166.985655 11788.4 id1849 364.136849 183.628767 11806.2 id1230 413.836124 184.375703 11916.8 id1948 502.953953 173.237159 12468.3 Note how the parser automatically picks column names X.<column number> when header=None argument is specified. Alternatively, you can supply just the column widths for contiguous columns: # Widths", "prev_chunk_id": "chunk_165", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_167", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Files with fixed width columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Files with fixed width columns#", "content": "are a list of integers widths = [6, 14, 13, 10] df = pd.read_fwf(\"bar.csv\", widths=widths, header=None) df 0 1 2 3 0 id8141 360.242940 149.910199 11950.7 1 id1594 444.953632 166.985655 11788.4 2 id1849 364.136849 183.628767 11806.2 3 id1230 413.836124 184.375703 11916.8 4 id1948 502.953953 173.237159 12468.3 The parser will take care of extra white spaces around the columns so it’s ok to have extra separation between the columns in the file. By default, read_fwf will try to infer the file’s colspecs by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided delimiter (default delimiter is whitespace). df = pd.read_fwf(\"bar.csv\", header=None, index_col=0) df 1 2 3 0 id8141 360.242940 149.910199 11950.7 id1594 444.953632 166.985655 11788.4 id1849 364.136849 183.628767 11806.2 id1230 413.836124 184.375703 11916.8 id1948 502.953953 173.237159 12468.3 read_fwf supports the dtype parameter for specifying the types of parsed columns to be different from the inferred type. pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes 1 float64 2 float64 3 float64 dtype: object pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes 0 object 1 float64 2 object 3 float64 dtype: object", "prev_chunk_id": "chunk_166", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_168", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Files with an “implicit” index column#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Files with an “implicit” index column#", "content": "Files with an “implicit” index column# Consider a file with one less entry in the header than the number of data column: data = \"A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\" print(data) A,B,C 20090101,a,1,2 20090102,b,3,4 20090103,c,4,5 with open(\"foo.csv\", \"w\") as f: .....: f.write(data) .....: In this special case, read_csv assumes that the first column is to be used as the index of the DataFrame: pd.read_csv(\"foo.csv\") A B C 20090101 a 1 2 20090102 b 3 4 20090103 c 4 5 Note that the dates weren’t automatically parsed. In that case you would need to do as before: df = pd.read_csv(\"foo.csv\", parse_dates=True) df.index DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_167", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_169", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading an index with a MultiIndex#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading an index with a MultiIndex#", "content": "Reading an index with a MultiIndex# Suppose you have data indexed by two columns: data = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5' print(data) year,indiv,zit,xit 1977,\"A\",1.2,.6 1977,\"B\",1.5,.5 with open(\"mindex_ex.csv\", mode=\"w\") as f: .....: f.write(data) .....: The index_col argument to read_csv can take a list of column numbers to turn multiple columns into a MultiIndex for the index of the returned object: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1]) df zit xit year indiv 1977 A 1.2 0.6 B 1.5 0.5 df.loc[1977] zit xit indiv A 1.2 0.6 B 1.5 0.5", "prev_chunk_id": "chunk_168", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_170", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading columns with a MultiIndex#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading columns with a MultiIndex#", "content": "Reading columns with a MultiIndex# By specifying list of row locations for the header argument, you can read in a MultiIndex for the columns. Specifying non-consecutive rows will skip the intervening rows. mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\")) mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\")) df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col) df.to_csv(\"mi.csv\") print(open(\"mi.csv\").read()) c,,1,2 d,,a,b a,b,, 1,a,1.0,1.0 2,b,1.0,1.0 3,c,1.0,1.0 4,d,1.0,1.0 pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1]) c 1 2 d a b a Unnamed: 2_level_2 Unnamed: 3_level_2 1 1.0 1.0 2 b 1.0 1.0 3 c 1.0 1.0 4 d 1.0 1.0 read_csv is also able to interpret a more common format of multi-columns indices. data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\" print(data) ,a,a,a,b,c,c ,q,r,s,t,u,v one,1,2,3,4,5,6 two,7,8,9,10,11,12 with open(\"mi2.csv\", \"w\") as fh: .....: fh.write(data) .....: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0) a b c q r s t u v one 1 2 3 4 5 6 two 7 8 9 10 11 12", "prev_chunk_id": "chunk_169", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_171", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Automatically “sniffing” the delimiter#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Automatically “sniffing” the delimiter#", "content": "Automatically “sniffing” the delimiter# read_csv is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the csv.Sniffer class of the csv module. For this, you have to specify sep=None. df = pd.DataFrame(np.random.randn(10, 4)) df.to_csv(\"tmp2.csv\", sep=\":\", index=False) pd.read_csv(\"tmp2.csv\", sep=None, engine=\"python\") 0 1 2 3 0 0.469112 -0.282863 -1.509059 -1.135632 1 1.212112 -0.173215 0.119209 -1.044236 2 -0.861849 -2.104569 -0.494929 1.071804 3 0.721555 -0.706771 -1.039575 0.271860 4 -0.424972 0.567020 0.276232 -1.087401 5 -0.673690 0.113648 -1.478427 0.524988 6 0.404705 0.577046 -1.715002 -1.039268 7 -0.370647 -1.157892 -1.344312 0.844885 8 1.075770 -0.109050 1.643563 -1.469388 9 0.357021 -0.674600 -1.776904 -0.968914", "prev_chunk_id": "chunk_170", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_172", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading multiple files to create a single DataFrame#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading multiple files to create a single DataFrame#", "content": "Reading multiple files to create a single DataFrame# It’s best to use concat() to combine multiple files. See the cookbook for an example.", "prev_chunk_id": "chunk_171", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_173", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Iterating through files chunk by chunk#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Iterating through files chunk by chunk#", "content": "Iterating through files chunk by chunk# Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following: df = pd.DataFrame(np.random.randn(10, 4)) df.to_csv(\"tmp.csv\", index=False) table = pd.read_csv(\"tmp.csv\") table 0 1 2 3 0 -1.294524 0.413738 0.276662 -0.472035 1 -0.013960 -0.362543 -0.006154 -0.923061 2 0.895717 0.805244 -1.206412 2.565646 3 1.431256 1.340309 -1.170299 -0.226169 4 0.410835 0.813850 0.132003 -0.827317 5 -0.076467 -1.187678 1.130127 -1.436737 6 -1.413681 1.607920 1.024180 0.569605 7 0.875906 -2.211372 0.974466 -2.006747 8 -0.410001 -0.078638 0.545952 -1.219217 9 -1.226825 0.769804 -1.281247 -0.727707 By specifying a chunksize to read_csv, the return value will be an iterable object of type TextFileReader: with pd.read_csv(\"tmp.csv\", chunksize=4) as reader: .....: print(reader) .....: for chunk in reader: .....: print(chunk) .....: <pandas.io.parsers.readers.TextFileReader object at 0x7f10516496c0> 0 1 2 3 0 -1.294524 0.413738 0.276662 -0.472035 1 -0.013960 -0.362543 -0.006154 -0.923061 2 0.895717 0.805244 -1.206412 2.565646 3 1.431256 1.340309 -1.170299 -0.226169 0 1 2 3 4 0.410835 0.813850 0.132003 -0.827317 5 -0.076467 -1.187678 1.130127 -1.436737 6 -1.413681 1.607920 1.024180 0.569605 7 0.875906 -2.211372 0.974466 -2.006747 0 1 2 3 8 -0.410001 -0.078638 0.545952 -1.219217 9 -1.226825 0.769804 -1.281247 -0.727707 Specifying iterator=True will also return the TextFileReader object: with pd.read_csv(\"tmp.csv\", iterator=True) as reader: .....: print(reader.get_chunk(5)) .....: 0 1 2 3 0 -1.294524 0.413738 0.276662 -0.472035 1 -0.013960 -0.362543 -0.006154 -0.923061 2 0.895717 0.805244 -1.206412 2.565646 3 1.431256 1.340309 -1.170299 -0.226169 4 0.410835 0.813850 0.132003 -0.827317", "prev_chunk_id": "chunk_172", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_174", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying the parser engine#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying the parser engine#", "content": "Specifying the parser engine# Pandas currently supports three engines, the C engine, the python engine, and an experimental pyarrow engine (requires the pyarrow package). In general, the pyarrow engine is fastest on larger workloads and is equivalent in speed to the C engine on most other workloads. The python engine tends to be slower than the pyarrow and C engines on most workloads. However, the pyarrow engine is much less robust than the C engine, which lacks a few features compared to the Python engine. Where possible, pandas uses the C parser (specified as engine='c'), but it may fall back to Python if C-unsupported options are specified. Currently, options unsupported by the C and pyarrow engines include: - sepother than a single character (e.g. regex separators) - skipfooter - sep=Nonewithdelim_whitespace=False Specifying any of the above options will produce a ParserWarning unless the python engine is selected explicitly using engine='python'. Options that are unsupported by the pyarrow engine which are not covered by the list above include: - float_precision - chunksize - comment - nrows - thousands - memory_map - dialect - on_bad_lines - delim_whitespace - quoting - lineterminator - converters - decimal - iterator - dayfirst - infer_datetime_format - verbose - skipinitialspace - low_memory Specifying these options with engine='pyarrow' will raise a ValueError.", "prev_chunk_id": "chunk_173", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_175", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading/writing remote files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading/writing remote files#", "content": "Reading/writing remote files# You can pass in a URL to read or write remote files to many of pandas’ IO functions - the following example shows reading a CSV file: df = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\") A custom header can be sent alongside HTTP(s) requests by passing a dictionary of header key value mappings to the storage_options keyword argument as shown below: headers = {\"User-Agent\": \"pandas\"} df = pd.read_csv( \"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\", storage_options=headers ) All URLs which are not local files or HTTP(s) are handled by fsspec, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require additional packages to be installed, for example S3 URLs require the s3fs library: df = pd.read_json(\"s3://pandas-test/adatafile.json\") When dealing with remote storage systems, you might need extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the S3Fs documentation. The same is true for several of the storage backends, and you should follow the links at fsimpl1 for implementations built into fsspec and fsimpl2 for those not included in the main fsspec distribution. You can also pass parameters directly to the backend driver. Since fsspec does not utilize the AWS_S3_HOST environment variable, we can directly define a dictionary containing the endpoint_url and pass the object into the storage option parameter: storage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}} df = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options) More sample configurations and documentation can be found at S3Fs documentation. If you do not have S3 credentials, you can still access public data by specifying an anonymous connection, such as pd.read_csv( \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\" \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\", storage_options={\"anon\": True}, ) fsspec also allows complex URLs, for accessing data in compressed archives, local caching of files, and", "prev_chunk_id": "chunk_174", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_176", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading/writing remote files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading/writing remote files#", "content": "more. To locally cache the above example, you would modify the call to pd.read_csv( \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\" \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\", storage_options={\"s3\": {\"anon\": True}}, ) where we specify that the “anon” parameter is meant for the “s3” part of the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.", "prev_chunk_id": "chunk_175", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_177", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing to CSV format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing to CSV format#", "content": "Writing to CSV format# The Series and DataFrame objects have an instance method to_csv which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required. - path_or_buf: A string path to the file to write or a file object. If a file object it must be opened withnewline='' - sep: Field delimiter for the output file (default “,”) - na_rep: A string representation of a missing value (default ‘’) - float_format: Format string for floating point numbers - columns: Columns to write (default None) - header: Whether to write out the column names (default True) - index: whether to write row (index) names (default True) - index_label: Column label(s) for index column(s) if desired. If None (default), andheaderandindexare True, then the index names are used. (A sequence should be given if theDataFrameuses MultiIndex). - mode: Python write mode, default ‘w’ - encoding: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior to 3 - lineterminator: Character sequence denoting line end (defaultos.linesep) - quoting: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set afloat_formatthen floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric - quotechar: Character used to quote fields (default ‘”’) - doublequote: Control quoting ofquotecharin fields (default True) - escapechar: Character used to escapesepandquotecharwhen appropriate (default None) - chunksize: Number of rows to write at a time - date_format: Format string for datetime objects", "prev_chunk_id": "chunk_176", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_178", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing a formatted string#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing a formatted string#", "content": "Writing a formatted string# The DataFrame object has an instance method to_string which allows control over the string representation of the object. All arguments are optional: - bufdefault None, for example a StringIO object - columnsdefault None, which columns to write - col_spacedefault None, minimum width of each column. - na_repdefaultNaN, representation of NA value - formattersdefault None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string - float_formatdefault None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in theDataFrame. - sparsifydefault True, set to False for aDataFramewith a hierarchical index to print every MultiIndex key at each row. - index_namesdefault True, will print the names of the indices - indexdefault True, will print the index (ie, row labels) - headerdefault True, will print the column labels - justifydefaultleft, will print column headers left- or right-justified The Series object also has a to_string method, but with only the buf, na_rep, float_format arguments. There is also a length argument which, if set to True, will additionally output the length of the Series.", "prev_chunk_id": "chunk_177", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_179", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "JSON#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "JSON#", "content": "JSON# Read and write JSON format files and strings.", "prev_chunk_id": "chunk_178", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_180", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing JSON#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing JSON#", "content": "Writing JSON# A Series or DataFrame can be converted to a valid JSON string. Use to_json with optional parameters: - path_or_buf: the pathname or buffer to write the output. This can beNonein which case a JSON string is returned. - orient:Series:default isindexallowed values are {split,records,index}DataFrame:default iscolumnsallowed values are {split,records,index,columns,values,table}The format of the JSON stringsplitdict like {index -> [index]; columns -> [columns]; data -> [values]}recordslist like [{column -> value}; … ]indexdict like {index -> {column -> value}}columnsdict like {column -> {index -> value}}valuesjust the values arraytableadhering to the JSONTable Schema - date_format: string, type of date conversion, ‘epoch’ for timestamp, ‘iso’ for ISO8601. - double_precision: The number of decimal places to use when encoding floating point values, default 10. - force_ascii: force encoded string to be ASCII, default True. - date_unit: The time unit to encode to, governs timestamp and ISO8601 precision. One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds respectively. Default ‘ms’. - default_handler: The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object. - lines: Ifrecordsorient, then will write each record per line as json. - mode: string, writer mode when writing to path. ‘w’ for write, ‘a’ for append. Default ‘w’ Note NaN’s, NaT’s and None will be converted to null and datetime objects will be converted based on the date_format and date_unit parameters. dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\")) json = dfj.to_json() json '{\"A\":{\"0\":-0.1213062281,\"1\":0.6957746499,\"2\":0.9597255933,\"3\":-0.6199759194,\"4\":-0.7323393705},\"B\":{\"0\":-0.0978826728,\"1\":0.3417343559,\"2\":-1.1103361029,\"3\":0.1497483186,\"4\":0.6877383895}}'", "prev_chunk_id": "chunk_179", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_181", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Orient options#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Orient options#", "content": "Orient options# There are a number of different options for the format of the resulting JSON file / string. Consider the following DataFrame and Series: dfjo = pd.DataFrame( .....: dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)), .....: columns=list(\"ABC\"), .....: index=list(\"xyz\"), .....: ) .....: dfjo A B C x 1 4 7 y 2 5 8 z 3 6 9 sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\") sjo x 15 y 16 z 17 Name: D, dtype: int64 Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index: dfjo.to_json(orient=\"columns\") '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}' # Not available for Series Index oriented (the default for Series) similar to column oriented but the index labels are now primary: dfjo.to_json(orient=\"index\") '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}' sjo.to_json(orient=\"index\") '{\"x\":15,\"y\":16,\"z\":17}' Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js: dfjo.to_json(orient=\"records\") '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]' sjo.to_json(orient=\"records\") '[15,16,17]' Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included: dfjo.to_json(orient=\"values\") '[[1,4,7],[2,5,8],[3,6,9]]' # Not available for Series Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series: dfjo.to_json(orient=\"split\") '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}' sjo.to_json(orient=\"split\") '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}' Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.", "prev_chunk_id": "chunk_180", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_182", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Date handling#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Date handling#", "content": "Date handling# Writing in ISO date format: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\")) dfd[\"date\"] = pd.Timestamp(\"20130101\") dfd = dfd.sort_index(axis=1, ascending=False) json = dfd.to_json(date_format=\"iso\") json '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' Writing in ISO date format, with microseconds: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\") json '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000\",\"1\":\"2013-01-01T00:00:00.000000\",\"2\":\"2013-01-01T00:00:00.000000\",\"3\":\"2013-01-01T00:00:00.000000\",\"4\":\"2013-01-01T00:00:00.000000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' Epoch timestamps, in seconds: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\") json '{\"date\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' Writing to a file, with a date index and a date column: dfj2 = dfj.copy() dfj2[\"date\"] = pd.Timestamp(\"20130101\") dfj2[\"ints\"] = list(range(5)) dfj2[\"bools\"] = True dfj2.index = pd.date_range(\"20130101\", periods=5) dfj2.to_json(\"test.json\") with open(\"test.json\") as fh: .....: print(fh.read()) .....: {\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}}", "prev_chunk_id": "chunk_181", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_183", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Fallback behavior#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Fallback behavior#", "content": "Fallback behavior# If the JSON serializer cannot handle the container contents directly it will fall back in the following manner: - if the dtype is unsupported (e.g.np.complex_) then thedefault_handler, if provided, will be called for each value, otherwise an exception is raised. - if an object is unsupported it will attempt the following:check if the object has defined atoDictmethod and call it. AtoDictmethod should return adictwhich will then be JSON serialized.invoke thedefault_handlerif one was provided.convert the object to adictby traversing its contents. However this will often fail with anOverflowErroror give unexpected results. In general the best approach for unsupported objects or dtypes is to provide a default_handler. For example: >>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json() # raises RuntimeError: Unhandled numpy dtype 15 can be dealt with by specifying a simple default_handler: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str) '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}'", "prev_chunk_id": "chunk_182", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_184", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading JSON#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading JSON#", "content": "Reading JSON# Reading a JSON string to pandas object can take a number of parameters. The parser will try to parse a DataFrame if typ is not supplied or is None. To explicitly force Series parsing, pass typ=series - filepath_or_buffer: aVALIDJSON string or file handle / StringIO. The string could be a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json - typ: type of object to recover (series or frame), default ‘frame’ - orient:Series :default isindexallowed values are {split,records,index}DataFramedefault iscolumnsallowed values are {split,records,index,columns,values,table}The format of the JSON stringsplitdict like {index -> [index]; columns -> [columns]; data -> [values]}recordslist like [{column -> value} …]indexdict like {index -> {column -> value}}columnsdict like {column -> {index -> value}}valuesjust the values arraytableadhering to the JSONTable Schema - dtype: if True, infer dtypes, if a dict of column to dtype, then use those, ifFalse, then don’t infer dtypes at all, default is True, apply only to the data. - convert_axes: boolean, try to convert the axes to the proper dtypes, default isTrue - convert_dates: a list of columns to parse for dates; IfTrue, then try to parse date-like columns, default isTrue. - keep_default_dates: boolean, defaultTrue. If parsing dates, then parse the default date-like columns. - precise_float: boolean, defaultFalse. Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality. - date_unit: string, the timestamp unit to detect if converting dates. Default None. By default the timestamp precision will be detected, if this is not desired then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds, milliseconds, microseconds or nanoseconds respectively. - lines: reads file as one json", "prev_chunk_id": "chunk_183", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_185", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading JSON#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading JSON#", "content": "object per line. - encoding: The encoding to use to decode py3 bytes. - chunksize: when used in combination withlines=True, return apandas.api.typing.JsonReaderwhich reads inchunksizelines per iteration. - engine: Either\"ujson\", the built-in JSON parser, or\"pyarrow\"which dispatches to pyarrow’spyarrow.json.read_json. The\"pyarrow\"is only available whenlines=True The parser will raise one of ValueError/TypeError/AssertionError if the JSON is not parseable. If a non-default orient was used when encoding to JSON be sure to pass the same option here so that decoding produces sensible results, see Orient Options for an overview.", "prev_chunk_id": "chunk_184", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_186", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Data conversion#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Data conversion#", "content": "Data conversion# The default of convert_axes=True, dtype=True, and convert_dates=True will try to parse the axes, and all of the data into appropriate types, including dates. If you need to override specific dtypes, pass a dict to dtype. convert_axes should only be set to False if you need to preserve string-like numbers (e.g. ‘1’, ‘2’) in an axes. Reading from a JSON string: from io import StringIO pd.read_json(StringIO(json)) date B A 0 1 0.403310 0.176444 1 1 0.301624 -0.154951 2 1 -1.369849 -2.179861 3 1 1.462696 -0.954208 4 1 -0.826591 -1.743161 Reading from a file: pd.read_json(\"test.json\") A B date ints bools 2013-01-01 -0.121306 -0.097883 1356 0 True 2013-01-02 0.695775 0.341734 1356 1 True 2013-01-03 0.959726 -1.110336 1356 2 True 2013-01-04 -0.619976 0.149748 1356 3 True 2013-01-05 -0.732339 0.687738 1356 4 True Don’t convert any data (but still convert axes and dates): pd.read_json(\"test.json\", dtype=object).dtypes A object B object date object ints object bools object dtype: object Specify dtypes for conversion: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes A float32 B float64 date int64 ints int64 bools int8 dtype: object Preserve string indices: from io import StringIO si = pd.DataFrame( .....: np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)] .....: ) .....: si 0 1 2 3 0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 si.index Index(['0', '1', '2', '3'], dtype='object') si.columns Index([0, 1, 2, 3], dtype='int64') json = si.to_json() sij = pd.read_json(StringIO(json), convert_axes=False) sij 0 1 2 3 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 3 0 0 0 0 sij.index Index(['0', '1', '2', '3'], dtype='object') sij.columns Index(['0', '1', '2', '3'], dtype='object') Dates written in nanoseconds need to be read back in nanoseconds: from io import StringIO json = dfj2.to_json(date_unit=\"ns\") # Try to parse", "prev_chunk_id": "chunk_185", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_187", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Data conversion#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Data conversion#", "content": "timestamps as milliseconds -> Won't Work dfju = pd.read_json(StringIO(json), date_unit=\"ms\") dfju A B date ints bools 1356998400000000000 -0.121306 -0.097883 1356998400 0 True 1357084800000000000 0.695775 0.341734 1356998400 1 True 1357171200000000000 0.959726 -1.110336 1356998400 2 True 1357257600000000000 -0.619976 0.149748 1356998400 3 True 1357344000000000000 -0.732339 0.687738 1356998400 4 True # Let pandas detect the correct precision dfju = pd.read_json(StringIO(json)) dfju A B date ints bools 2013-01-01 -0.121306 -0.097883 2013-01-01 0 True 2013-01-02 0.695775 0.341734 2013-01-01 1 True 2013-01-03 0.959726 -1.110336 2013-01-01 2 True 2013-01-04 -0.619976 0.149748 2013-01-01 3 True 2013-01-05 -0.732339 0.687738 2013-01-01 4 True # Or specify that all timestamps are in nanoseconds dfju = pd.read_json(StringIO(json), date_unit=\"ns\") dfju A B date ints bools 2013-01-01 -0.121306 -0.097883 1356998400 0 True 2013-01-02 0.695775 0.341734 1356998400 1 True 2013-01-03 0.959726 -1.110336 1356998400 2 True 2013-01-04 -0.619976 0.149748 1356998400 3 True 2013-01-05 -0.732339 0.687738 1356998400 4 True By setting the dtype_backend argument you can control the default dtypes used for the resulting DataFrame. data = ( .....: '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},' .....: '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},' .....: '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}' .....: ) .....: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\") df a b c d e f g h i j 0 1 2.5 True a <NA> <NA> <NA> <NA> 12-31-2019 None 1 3 4.5 False b 6 7.5 True a 12-31-2019 None df.dtypes a int64[pyarrow] b double[pyarrow] c bool[pyarrow] d string[pyarrow] e int64[pyarrow] f double[pyarrow] g bool[pyarrow] h string[pyarrow] i string[pyarrow] j null[pyarrow] dtype: object", "prev_chunk_id": "chunk_186", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_188", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Normalization#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Normalization#", "content": "Normalization# pandas provides a utility function to take a dict or list of dicts and normalize this semi-structured data into a flat table. data = [ .....: {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}}, .....: {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}}, .....: {\"id\": 2, \"name\": \"Faye Raker\"}, .....: ] .....: pd.json_normalize(data) id name.first name.last name.given name.family name 0 1.0 Coleen Volk NaN NaN NaN 1 NaN NaN NaN Mark Regner NaN 2 2.0 NaN NaN NaN NaN Faye Raker data = [ .....: { .....: \"state\": \"Florida\", .....: \"shortname\": \"FL\", .....: \"info\": {\"governor\": \"Rick Scott\"}, .....: \"county\": [ .....: {\"name\": \"Dade\", \"population\": 12345}, .....: {\"name\": \"Broward\", \"population\": 40000}, .....: {\"name\": \"Palm Beach\", \"population\": 60000}, .....: ], .....: }, .....: { .....: \"state\": \"Ohio\", .....: \"shortname\": \"OH\", .....: \"info\": {\"governor\": \"John Kasich\"}, .....: \"county\": [ .....: {\"name\": \"Summit\", \"population\": 1234}, .....: {\"name\": \"Cuyahoga\", \"population\": 1337}, .....: ], .....: }, .....: ] .....: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]]) name population state shortname info.governor 0 Dade 12345 Florida FL Rick Scott 1 Broward 40000 Florida FL Rick Scott 2 Palm Beach 60000 Florida FL Rick Scott 3 Summit 1234 Ohio OH John Kasich 4 Cuyahoga 1337 Ohio OH John Kasich The max_level parameter provides more control over which level to end normalization. With max_level=1 the following snippet normalizes until 1st nesting level of the provided dict. data = [ .....: { .....: \"CreatedBy\": {\"Name\": \"User001\"}, .....: \"Lookup\": { .....: \"TextField\": \"Some text\", .....: \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"}, .....: }, .....: \"Image\": {\"a\": \"b\"}, .....: } .....: ] .....: pd.json_normalize(data, max_level=1) CreatedBy.Name Lookup.TextField Lookup.UserField Image.a 0 User001 Some text {'Id': 'ID001', 'Name': 'Name001'} b", "prev_chunk_id": "chunk_187", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_189", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Line delimited json#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Line delimited json#", "content": "Line delimited json# pandas is able to read and write line-delimited json files that are common in data processing pipelines using Hadoop or Spark. For line-delimited json files, pandas can also return an iterator which reads in chunksize lines at a time. This can be useful for large files or to read from a stream. from io import StringIO jsonl = \"\"\" .....: {\"a\": 1, \"b\": 2} .....: {\"a\": 3, \"b\": 4} .....: \"\"\" .....: df = pd.read_json(StringIO(jsonl), lines=True) df a b 0 1 2 1 3 4 df.to_json(orient=\"records\", lines=True) '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n' # reader is an iterator that returns ``chunksize`` lines each iteration with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader: .....: reader .....: for chunk in reader: .....: print(chunk) .....: Empty DataFrame Columns: [] Index: [] a b 0 1 2 a b 1 3 4 Line-limited json can also be read using the pyarrow reader by specifying engine=\"pyarrow\". from io import BytesIO df = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\") df a b 0 1 2 1 3 4", "prev_chunk_id": "chunk_188", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_190", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Table schema#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Table schema#", "content": "Table schema# Table Schema is a spec for describing tabular datasets as a JSON object. The JSON includes information on the field names, types, and other attributes. You can use the orient table to build a JSON string with two fields, schema and data. df = pd.DataFrame( .....: { .....: \"A\": [1, 2, 3], .....: \"B\": [\"a\", \"b\", \"c\"], .....: \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3), .....: }, .....: index=pd.Index(range(3), name=\"idx\"), .....: ) .....: df A B C idx 0 1 a 2016-01-01 1 2 b 2016-01-02 2 3 c 2016-01-03 df.to_json(orient=\"table\", date_format=\"iso\") '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}' The schema field contains the fields key, which itself contains a list of column name to type pairs, including the Index or MultiIndex (see below for a list of types). The schema field also contains a primaryKey field if the (Multi)index is unique. The second field, data, contains the serialized data with the records orient. The index is included, and any datetimes are ISO 8601 formatted, as required by the Table Schema spec. The full list of types supported are described in the Table Schema spec. This table shows the mapping from pandas types: pandas type | Table Schema type int64 | integer float64 | number bool | boolean datetime64[ns] | datetime timedelta64[ns] | duration categorical | any object | str A few notes on the generated table schema: - Theschemaobject contains apandas_versionfield. This contains the version of pandas’ dialect of the schema, and will be incremented with each revision. - All dates are converted to UTC when serializing. Even timezone naive values, which are treated as UTC with an offset of 0.In [307]:frompandas.io.jsonimportbuild_table_schemaIn [308]:s=pd.Series(pd.date_range(\"2016\",periods=4))In [309]:build_table_schema(s)Out[309]:{'fields': [{'name': 'index', 'type': 'integer'},{'name': 'values', 'type': 'datetime'}],'primaryKey': ['index'],'pandas_version': '1.4.0'} - datetimes with a timezone (before serializing), include an additional fieldtzwith the time zone name (e.g.'US/Central').In [310]:s_tz=pd.Series(pd.date_range(\"2016\",periods=12,tz=\"US/Central\"))In [311]:build_table_schema(s_tz)Out[311]:{'fields': [{'name': 'index', 'type': 'integer'},{'name': 'values',", "prev_chunk_id": "chunk_189", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_191", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Table schema#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Table schema#", "content": "'type': 'datetime', 'tz': 'US/Central'}],'primaryKey': ['index'],'pandas_version': '1.4.0'} - Periods are converted to timestamps before serialization, and so have the same behavior of being converted to UTC. In addition, periods will contain and additional fieldfreqwith the period’s frequency, e.g.'A-DEC'.In [312]:s_per=pd.Series(1,index=pd.period_range(\"2016\",freq=\"Y-DEC\",periods=4))In [313]:build_table_schema(s_per)Out[313]:{'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'YE-DEC'},{'name': 'values', 'type': 'integer'}],'primaryKey': ['index'],'pandas_version': '1.4.0'} - Categoricals use theanytype and anenumconstraint listing the set of possible values. Additionally, anorderedfield is included:In [314]:s_cat=pd.Series(pd.Categorical([\"a\",\"b\",\"a\"]))In [315]:build_table_schema(s_cat)Out[315]:{'fields': [{'name': 'index', 'type': 'integer'},{'name': 'values','type': 'any','constraints': {'enum': ['a', 'b']},'ordered': False}],'primaryKey': ['index'],'pandas_version': '1.4.0'} - AprimaryKeyfield, containing an array of labels, is includedif the index is unique:In [316]:s_dupe=pd.Series([1,2],index=[1,1])In [317]:build_table_schema(s_dupe)Out[317]:{'fields': [{'name': 'index', 'type': 'integer'},{'name': 'values', 'type': 'integer'}],'pandas_version': '1.4.0'} - TheprimaryKeybehavior is the same with MultiIndexes, but in this case theprimaryKeyis an array:In [318]:s_multi=pd.Series(1,index=pd.MultiIndex.from_product([(\"a\",\"b\"),(0,1)]))In [319]:build_table_schema(s_multi)Out[319]:{'fields': [{'name': 'level_0', 'type': 'string'},{'name': 'level_1', 'type': 'integer'},{'name': 'values', 'type': 'integer'}],'primaryKey': FrozenList(['level_0', 'level_1']),'pandas_version': '1.4.0'} - The default naming roughly follows these rules:For series, theobject.nameis used. If that’s none, then the name isvaluesForDataFrames, the stringified version of the column name is usedForIndex(notMultiIndex),index.nameis used, with a fallback toindexif that is None.ForMultiIndex,mi.namesis used. If any level has no name, thenlevel_<i>is used. read_json also accepts orient='table' as an argument. This allows for the preservation of metadata such as dtypes and index names in a round-trippable manner. df = pd.DataFrame( .....: { .....: \"foo\": [1, 2, 3, 4], .....: \"bar\": [\"a\", \"b\", \"c\", \"d\"], .....: \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4), .....: \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]), .....: }, .....: index=pd.Index(range(4), name=\"idx\"), .....: ) .....: df foo bar baz qux idx 0 1 a 2018-01-01 a 1 2 b 2018-01-02 b 2 3 c 2018-01-03 c 3 4 d 2018-01-04 c df.dtypes foo int64 bar object baz datetime64[ns] qux category dtype: object df.to_json(\"test.json\", orient=\"table\") new_df = pd.read_json(\"test.json\", orient=\"table\") new_df foo bar baz qux idx 0 1 a 2018-01-01 a 1 2 b 2018-01-02 b 2 3 c 2018-01-03 c", "prev_chunk_id": "chunk_190", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_192", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Table schema#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Table schema#", "content": "3 4 d 2018-01-04 c new_df.dtypes foo int64 bar object baz datetime64[ns] qux category dtype: object Please note that the literal string ‘index’ as the name of an Index is not round-trippable, nor are any names beginning with 'level_' within a MultiIndex. These are used by default in DataFrame.to_json() to indicate missing values and the subsequent read cannot distinguish the intent. df.index.name = \"index\" df.to_json(\"test.json\", orient=\"table\") new_df = pd.read_json(\"test.json\", orient=\"table\") print(new_df.index.name) None When using orient='table' along with user-defined ExtensionArray, the generated schema will contain an additional extDtype key in the respective fields element. This extra key is not standard but does enable JSON roundtrips for extension types (e.g. read_json(df.to_json(orient=\"table\"), orient=\"table\")). The extDtype key carries the name of the extension, if you have properly registered the ExtensionDtype, pandas will use said name to perform a lookup into the registry and re-convert the serialized data into your custom dtype.", "prev_chunk_id": "chunk_191", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_193", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading HTML content#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading HTML content#", "content": "Reading HTML content# The top-level read_html() function can accept an HTML string/file/URL and will parse HTML tables into list of pandas DataFrames. Let’s look at a few examples. Read a URL with no options: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\" pd.read_html(url) Out[321]: [ Bank NameBank CityCity StateSt ... Acquiring InstitutionAI Closing DateClosing FundFund 0 Almena State Bank Almena KS ... Equity Bank October 23, 2020 10538 1 First City Bank of Florida Fort Walton Beach FL ... United Fidelity Bank, fsb October 16, 2020 10537 2 The First State Bank Barboursville WV ... MVB Bank, Inc. April 3, 2020 10536 3 Ericson State Bank Ericson NE ... Farmers and Merchants Bank February 14, 2020 10535 4 City National Bank of New Jersey Newark NJ ... Industrial Bank November 1, 2019 10534 .. ... ... ... ... ... ... ... 558 Superior Bank, FSB Hinsdale IL ... Superior Federal, FSB July 27, 2001 6004 559 Malta National Bank Malta OH ... North Valley Bank May 3, 2001 4648 560 First Alliance Bank & Trust Co. Manchester NH ... Southern New Hampshire Bank & Trust February 2, 2001 4647 561 National State Bank of Metropolis Metropolis IL ... Banterra Bank of Marion December 14, 2000 4646 562 Bank of Honolulu Honolulu HI ... Bank of the Orient October 13, 2000 4645 [563 rows x 7 columns]] Read a URL while passing headers alongside the HTTP request: url = 'https://www.sump.org/notes/request/' # HTTP request reflector pd.read_html(url) Out[323]: [ 0 1 0 Remote Socket: 51.15.105.256:51760 1 Protocol Version: HTTP/1.1 2 Request Method: GET 3 Request URI: /notes/request/ 4 Request Query: NaN, 0 Accept-Encoding: identity 1 Host: www.sump.org 2 User-Agent: Python-urllib/3.8 3 Connection: close] headers = { 'User-Agent':'Mozilla Firefox v14.0', 'Accept':'application/json', 'Connection':'keep-alive', 'Auth':'Bearer 2*/f3+fe68df*4' } pd.read_html(url, storage_options=headers) Out[340]: [ 0 1 0 Remote Socket: 51.15.105.256:51760 1 Protocol Version: HTTP/1.1", "prev_chunk_id": "chunk_192", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_194", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading HTML content#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading HTML content#", "content": "2 Request Method: GET 3 Request URI: /notes/request/ 4 Request Query: NaN, 0 User-Agent: Mozilla Firefox v14.0 1 AcceptEncoding: gzip, deflate, br 2 Accept: application/json 3 Connection: keep-alive 4 Auth: Bearer 2*/f3+fe68df*4] Read in the content of the file from the above URL and pass it to read_html as a string: html_str = \"\"\" .....: <table> .....: <tr> .....: <th>A</th> .....: <th colspan=\"1\">B</th> .....: <th rowspan=\"1\">C</th> .....: </tr> .....: <tr> .....: <td>a</td> .....: <td>b</td> .....: <td>c</td> .....: </tr> .....: </table> .....: \"\"\" .....: with open(\"tmp.html\", \"w\") as f: .....: f.write(html_str) .....: df = pd.read_html(\"tmp.html\") df[0] A B C 0 a b c You can even pass in an instance of StringIO if you so desire: dfs = pd.read_html(StringIO(html_str)) dfs[0] A B C 0 a b c Read a URL and match a table that contains specific text: match = \"Metcalf Bank\" df_list = pd.read_html(url, match=match) Specify a header row (by default <th> or <td> elements located within a <thead> are used to form the column index, if multiple rows are contained within <thead> then a MultiIndex is created); if specified, the header row is taken from the data minus the parsed header elements (<th> elements). dfs = pd.read_html(url, header=0) Specify an index column: dfs = pd.read_html(url, index_col=0) Specify a number of rows to skip: dfs = pd.read_html(url, skiprows=0) Specify a number of rows to skip using a list (range works as well): dfs = pd.read_html(url, skiprows=range(2)) Specify an HTML attribute: dfs1 = pd.read_html(url, attrs={\"id\": \"table\"}) dfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"}) print(np.array_equal(dfs1[0], dfs2[0])) # Should be True Specify values that should be converted to NaN: dfs = pd.read_html(url, na_values=[\"No Acquirer\"]) Specify whether to keep the default set of NaN values: dfs = pd.read_html(url, keep_default_na=False) Specify converters for columns. This is useful for numerical text data that has leading zeros. By default", "prev_chunk_id": "chunk_193", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_195", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading HTML content#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading HTML content#", "content": "columns that are numerical are cast to numeric types and the leading zeros are lost. To avoid this, we can convert these columns to strings. url_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\" dfs = pd.read_html( url_mcc, match=\"Telekom Albania\", header=0, converters={\"MNC\": str}, ) Use some combination of the above: dfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0) Read in pandas to_html output (with some loss of floating point precision): df = pd.DataFrame(np.random.randn(2, 2)) s = df.to_html(float_format=\"{0:.40g}\".format) dfin = pd.read_html(s, index_col=0) The lxml backend will raise an error on a failed parse if that is the only parser you provide. If you only have a single parser you can provide just a string, but it is considered good practice to pass a list with one string if, for example, the function expects a sequence of strings. You may use: dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"]) Or you could pass flavor='lxml' without a list: dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\") However, if you have bs4 and html5lib installed and pass None or ['lxml', 'bs4'] then the parse will most likely succeed. Note that as soon as a parse succeeds, the function will return. dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"]) Links can be extracted from cells along with the text using extract_links=\"all\". html_table = \"\"\" .....: <table> .....: <tr> .....: <th>GitHub</th> .....: </tr> .....: <tr> .....: <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td> .....: </tr> .....: </table> .....: \"\"\" .....: df = pd.read_html( .....: StringIO(html_table), .....: extract_links=\"all\" .....: )[0] .....: df (GitHub, None) 0 (pandas, https://github.com/pandas-dev/pandas) df[(\"GitHub\", None)] 0 (pandas, https://github.com/pandas-dev/pandas) Name: (GitHub, None), dtype: object df[(\"GitHub\", None)].str[1] 0 https://github.com/pandas-dev/pandas Name: (GitHub, None), dtype: object", "prev_chunk_id": "chunk_194", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_196", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing to HTML files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing to HTML files#", "content": "Writing to HTML files# DataFrame objects have an instance method to_html which renders the contents of the DataFrame as an HTML table. The function arguments are as in the method to_string described above. from IPython.display import display, HTML df = pd.DataFrame(np.random.randn(2, 2)) df 0 1 0 -0.345352 1.314232 1 0.690579 0.995761 html = df.to_html() print(html) # raw html <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>-0.345352</td> <td>1.314232</td> </tr> <tr> <th>1</th> <td>0.690579</td> <td>0.995761</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> The columns argument will limit the columns shown: html = df.to_html(columns=[0]) print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>0</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>-0.345352</td> </tr> <tr> <th>1</th> <td>0.690579</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> float_format takes a Python callable to control the precision of floating point values: html = df.to_html(float_format=\"{0:.10f}\".format) print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>-0.3453521949</td> <td>1.3142323796</td> </tr> <tr> <th>1</th> <td>0.6905793352</td> <td>0.9957609037</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> bold_rows will make the row labels bold by default, but you can turn that off: html = df.to_html(bold_rows=False) print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>-0.345352</td> <td>1.314232</td> </tr> <tr> <td>1</td> <td>0.690579</td> <td>0.995761</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> The classes argument provides the ability to give the resulting HTML table CSS classes. Note that these classes are appended to the existing 'dataframe' class. print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"])) <table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>0</th> <th>1</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>-0.345352</td> <td>1.314232</td> </tr> <tr> <th>1</th> <td>0.690579</td> <td>0.995761</td> </tr> </tbody> </table> The render_links argument provides the ability to add hyperlinks to cells that contain URLs. url_df = pd.DataFrame( .....: { .....: \"name\": [\"Python\", \"pandas\"], .....: \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"], .....: } .....: )", "prev_chunk_id": "chunk_195", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_197", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing to HTML files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing to HTML files#", "content": ".....: html = url_df.to_html(render_links=True) print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>name</th> <th>url</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>Python</td> <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td> </tr> <tr> <th>1</th> <td>pandas</td> <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> Finally, the escape argument allows you to control whether the “<”, “>” and “&” characters escaped in the resulting HTML (by default it is True). So to get the HTML without escaped characters pass escape=False df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)}) Escaped: html = df.to_html() print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>a</th> <th>b</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>&amp;</td> <td>2.396780</td> </tr> <tr> <th>1</th> <td>&lt;</td> <td>0.014871</td> </tr> <tr> <th>2</th> <td>&gt;</td> <td>3.357427</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object> Not escaped: html = df.to_html(escape=False) print(html) <table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>a</th> <th>b</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>&</td> <td>2.396780</td> </tr> <tr> <th>1</th> <td><</td> <td>0.014871</td> </tr> <tr> <th>2</th> <td>></td> <td>3.357427</td> </tr> </tbody> </table> display(HTML(html)) <IPython.core.display.HTML object>", "prev_chunk_id": "chunk_196", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_198", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "HTML Table Parsing Gotchas#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "HTML Table Parsing Gotchas#", "content": "HTML Table Parsing Gotchas# There are some versioning issues surrounding the libraries that are used to parse HTML tables in the top-level pandas io function read_html. Issues with lxml - Benefitslxmlis very fast.lxmlrequires Cython to install correctly. - Drawbackslxmldoesnotmake any guarantees about the results of its parseunlessit is givenstrictly valid markup.In light of the above, we have chosen to allow you, the user, to use thelxmlbackend, butthis backend will usehtml5libiflxmlfails to parseIt is thereforehighly recommendedthat you install bothBeautifulSoup4andhtml5lib, so that you will still get a valid result (provided everything else is valid) even iflxmlfails. Issues with BeautifulSoup4 using lxml as a backend - The above issues hold here as well sinceBeautifulSoup4is essentially just a wrapper around a parser backend. Issues with BeautifulSoup4 using html5lib as a backend - Benefitshtml5libis far more lenient thanlxmland consequently deals withreal-life markupin a much saner way rather than just, e.g., dropping an element without notifying you.html5libgenerates valid HTML5 markup from invalid markup automatically. This is extremely important for parsing HTML tables, since it guarantees a valid document. However, that does NOT mean that it is “correct”, since the process of fixing markup does not have a single definition.html5libis pure Python and requires no additional build steps beyond its own installation. - DrawbacksThe biggest drawback to usinghtml5libis that it is slow as molasses. However consider the fact that many tables on the web are not big enough for the parsing algorithm runtime to matter. It is more likely that the bottleneck will be in the process of reading the raw text from the URL over the web, i.e., IO (input-output). For very large tables, this might not be true.", "prev_chunk_id": "chunk_197", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_199", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "LaTeX#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "LaTeX#", "content": "LaTeX# Currently there are no methods to read from LaTeX, only output methods.", "prev_chunk_id": "chunk_198", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_200", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing to LaTeX files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing to LaTeX files#", "content": "Writing to LaTeX files# Review the documentation for Styler.to_latex, which gives examples of conditional styling and explains the operation of its keyword arguments. For simple application the following pattern is sufficient. df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"]) print(df.style.to_latex()) \\begin{tabular}{lrr} & c & d \\\\ a & 1 & 2 \\\\ b & 3 & 4 \\\\ \\end{tabular} To format values before output, chain the Styler.format method. print(df.style.format(\"€ {}\").to_latex()) \\begin{tabular}{lrr} & c & d \\\\ a & € 1 & € 2 \\\\ b & € 3 & € 4 \\\\ \\end{tabular}", "prev_chunk_id": "chunk_199", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_201", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading XML#", "content": "Reading XML# The top-level read_xml() function can accept an XML string/file/URL and will parse nodes and attributes into a pandas DataFrame. Let’s look at a few examples. Read an XML string: from io import StringIO xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?> .....: <bookstore> .....: <book category=\"cooking\"> .....: <title lang=\"en\">Everyday Italian</title> .....: <author>Giada De Laurentiis</author> .....: <year>2005</year> .....: <price>30.00</price> .....: </book> .....: <book category=\"children\"> .....: <title lang=\"en\">Harry Potter</title> .....: <author>J K. Rowling</author> .....: <year>2005</year> .....: <price>29.99</price> .....: </book> .....: <book category=\"web\"> .....: <title lang=\"en\">Learning XML</title> .....: <author>Erik T. Ray</author> .....: <year>2003</year> .....: <price>39.95</price> .....: </book> .....: </bookstore>\"\"\" .....: df = pd.read_xml(StringIO(xml)) df category title author year price 0 cooking Everyday Italian Giada De Laurentiis 2005 30.00 1 children Harry Potter J K. Rowling 2005 29.99 2 web Learning XML Erik T. Ray 2003 39.95 Read a URL with no options: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\") df category title author year price cover 0 cooking Everyday Italian Giada De Laurentiis 2005 30.00 None 1 children Harry Potter J K. Rowling 2005 29.99 None 2 web XQuery Kick Start Vaidyanathan Nagarajan 2003 49.99 None 3 web Learning XML Erik T. Ray 2003 39.95 paperback Read in the content of the “books.xml” file and pass it to read_xml as a string: file_path = \"books.xml\" with open(file_path, \"w\") as f: .....: f.write(xml) .....: with open(file_path, \"r\") as f: .....: df = pd.read_xml(StringIO(f.read())) .....: df category title author year price 0 cooking Everyday Italian Giada De Laurentiis 2005 30.00 1 children Harry Potter J K. Rowling 2005 29.99 2 web Learning XML Erik T. Ray 2003 39.95 Read in the content of the “books.xml” as instance of StringIO or BytesIO and pass it to read_xml: with open(file_path, \"r\") as f: .....: sio = StringIO(f.read()) .....: df = pd.read_xml(sio) df category title author year price 0 cooking Everyday Italian Giada", "prev_chunk_id": "chunk_200", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_202", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading XML#", "content": "De Laurentiis 2005 30.00 1 children Harry Potter J K. Rowling 2005 29.99 2 web Learning XML Erik T. Ray 2003 39.95 with open(file_path, \"rb\") as f: .....: bio = BytesIO(f.read()) .....: df = pd.read_xml(bio) df category title author year price 0 cooking Everyday Italian Giada De Laurentiis 2005 30.00 1 children Harry Potter J K. Rowling 2005 29.99 2 web Learning XML Erik T. Ray 2003 39.95 Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing Biomedical and Life Science Jorurnals: df = pd.read_xml( .....: \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\", .....: xpath=\".//journal-meta\", .....: ) .....: df journal-id journal-title issn publisher 0 Cardiovasc Ultrasound Cardiovascular Ultrasound 1476-7120 NaN With lxml as default parser, you access the full-featured XML library that extends Python’s ElementTree API. One powerful tool is ability to query nodes selectively or conditionally with more expressive XPath: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\") df category title author year price 0 cooking Everyday Italian Giada De Laurentiis 2005 30.00 1 children Harry Potter J K. Rowling 2005 29.99 Specify only elements or only attributes to parse: df = pd.read_xml(file_path, elems_only=True) df title author year price 0 Everyday Italian Giada De Laurentiis 2005 30.00 1 Harry Potter J K. Rowling 2005 29.99 2 Learning XML Erik T. Ray 2003 39.95 df = pd.read_xml(file_path, attrs_only=True) df category 0 cooking 1 children 2 web XML documents can have namespaces with prefixes and default namespaces without prefixes both of which are denoted with a special attribute xmlns. In order to parse by node under a namespace context, xpath must reference a prefix. For example, below XML contains a namespace with prefix, doc, and URI at https://example.com. In order to parse doc:row nodes, namespaces must be used. xml = \"\"\"<?xml version='1.0' encoding='utf-8'?> .....: <doc:data xmlns:doc=\"https://example.com\"> .....: <doc:row> .....: <doc:shape>square</doc:shape> .....: <doc:degrees>360</doc:degrees> .....: <doc:sides>4.0</doc:sides> .....:", "prev_chunk_id": "chunk_201", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_203", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading XML#", "content": "</doc:row> .....: <doc:row> .....: <doc:shape>circle</doc:shape> .....: <doc:degrees>360</doc:degrees> .....: <doc:sides/> .....: </doc:row> .....: <doc:row> .....: <doc:shape>triangle</doc:shape> .....: <doc:degrees>180</doc:degrees> .....: <doc:sides>3.0</doc:sides> .....: </doc:row> .....: </doc:data>\"\"\" .....: df = pd.read_xml(StringIO(xml), .....: xpath=\"//doc:row\", .....: namespaces={\"doc\": \"https://example.com\"}) .....: df shape degrees sides 0 square 360 4.0 1 circle 360 NaN 2 triangle 180 3.0 Similarly, an XML document can have a default namespace without prefix. Failing to assign a temporary prefix will return no nodes and raise a ValueError. But assigning any temporary name to correct URI allows parsing by nodes. xml = \"\"\"<?xml version='1.0' encoding='utf-8'?> .....: <data xmlns=\"https://example.com\"> .....: <row> .....: <shape>square</shape> .....: <degrees>360</degrees> .....: <sides>4.0</sides> .....: </row> .....: <row> .....: <shape>circle</shape> .....: <degrees>360</degrees> .....: <sides/> .....: </row> .....: <row> .....: <shape>triangle</shape> .....: <degrees>180</degrees> .....: <sides>3.0</sides> .....: </row> .....: </data>\"\"\" .....: df = pd.read_xml(StringIO(xml), .....: xpath=\"//pandas:row\", .....: namespaces={\"pandas\": \"https://example.com\"}) .....: df shape degrees sides 0 square 360 4.0 1 circle 360 NaN 2 triangle 180 3.0 However, if XPath does not reference node names such as default, /*, then namespaces is not required. With lxml as parser, you can flatten nested XML documents with an XSLT script which also can be string/file/URL types. As background, XSLT is a special-purpose language written in a special XML file that can transform original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT processor. For example, consider this somewhat nested structure of Chicago “L” Rides where station and rides elements encapsulate data in their own sections. With below XSLT, lxml can transform original nested document into a flatter output (as shown below for demonstration) for easier parse into DataFrame: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?> .....: <response> .....: <row> .....: <station id=\"40850\" name=\"Library\"/> .....: <month>2020-09-01T00:00:00</month> .....: <rides> .....: <avg_weekday_rides>864.2</avg_weekday_rides> .....: <avg_saturday_rides>534</avg_saturday_rides> .....: <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides> .....: </rides> .....: </row> .....: <row> .....: <station id=\"41700\" name=\"Washington/Wabash\"/> .....:", "prev_chunk_id": "chunk_202", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_204", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading XML#", "content": "<month>2020-09-01T00:00:00</month> .....: <rides> .....: <avg_weekday_rides>2707.4</avg_weekday_rides> .....: <avg_saturday_rides>1909.8</avg_saturday_rides> .....: <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides> .....: </rides> .....: </row> .....: <row> .....: <station id=\"40380\" name=\"Clark/Lake\"/> .....: <month>2020-09-01T00:00:00</month> .....: <rides> .....: <avg_weekday_rides>2949.6</avg_weekday_rides> .....: <avg_saturday_rides>1657</avg_saturday_rides> .....: <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides> .....: </rides> .....: </row> .....: </response>\"\"\" .....: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> .....: <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/> .....: <xsl:strip-space elements=\"*\"/> .....: <xsl:template match=\"/response\"> .....: <xsl:copy> .....: <xsl:apply-templates select=\"row\"/> .....: </xsl:copy> .....: </xsl:template> .....: <xsl:template match=\"row\"> .....: <xsl:copy> .....: <station_id><xsl:value-of select=\"station/@id\"/></station_id> .....: <station_name><xsl:value-of select=\"station/@name\"/></station_name> .....: <xsl:copy-of select=\"month|rides/*\"/> .....: </xsl:copy> .....: </xsl:template> .....: </xsl:stylesheet>\"\"\" .....: output = \"\"\"<?xml version='1.0' encoding='utf-8'?> .....: <response> .....: <row> .....: <station_id>40850</station_id> .....: <station_name>Library</station_name> .....: <month>2020-09-01T00:00:00</month> .....: <avg_weekday_rides>864.2</avg_weekday_rides> .....: <avg_saturday_rides>534</avg_saturday_rides> .....: <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides> .....: </row> .....: <row> .....: <station_id>41700</station_id> .....: <station_name>Washington/Wabash</station_name> .....: <month>2020-09-01T00:00:00</month> .....: <avg_weekday_rides>2707.4</avg_weekday_rides> .....: <avg_saturday_rides>1909.8</avg_saturday_rides> .....: <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides> .....: </row> .....: <row> .....: <station_id>40380</station_id> .....: <station_name>Clark/Lake</station_name> .....: <month>2020-09-01T00:00:00</month> .....: <avg_weekday_rides>2949.6</avg_weekday_rides> .....: <avg_saturday_rides>1657</avg_saturday_rides> .....: <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides> .....: </row> .....: </response>\"\"\" .....: df = pd.read_xml(StringIO(xml), stylesheet=xsl) df station_id station_name ... avg_saturday_rides avg_sunday_holiday_rides 0 40850 Library ... 534.0 417.2 1 41700 Washington/Wabash ... 1909.8 1438.6 2 40380 Clark/Lake ... 1657.0 1453.8 [3 rows x 6 columns] For very large XML files that can range in hundreds of megabytes to gigabytes, pandas.read_xml() supports parsing such sizeable files using lxml’s iterparse and etree’s iterparse which are memory-efficient methods to iterate through an XML tree and extract specific elements and attributes. without holding entire tree in memory. To use this feature, you must pass a physical XML file path into read_xml and use the iterparse argument. Files should not be compressed or point to online sources but stored on local disk. Also, iterparse should be a dictionary where the key is the repeating nodes in document (which become the rows) and the value is a list of any element or attribute that is a descendant (i.e., child, grandchild) of repeating node. Since XPath is not used", "prev_chunk_id": "chunk_203", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_205", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading XML#", "content": "in this method, descendants do not need to share same relationship with one another. Below shows example of reading in Wikipedia’s very large (12 GB+) latest article data dump. df = pd.read_xml( ... \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\", ... iterparse = {\"page\": [\"title\", \"ns\", \"id\"]} ... ) ... df Out[2]: title ns id 0 Gettysburg Address 0 21450 1 Main Page 0 42950 2 Declaration by United Nations 0 8435 3 Constitution of the United States of America 0 8435 4 Declaration of Independence (Israel) 0 17858 ... ... ... ... 3578760 Page:Black cat 1897 07 v2 n10.pdf/17 104 219649 3578761 Page:Black cat 1897 07 v2 n10.pdf/43 104 219649 3578762 Page:Black cat 1897 07 v2 n10.pdf/44 104 219649 3578763 The History of Tom Jones, a Foundling/Book IX 0 12084291 3578764 Page:Shakespeare of Stratford (1926) Yale.djvu/91 104 21450 [3578765 rows x 3 columns]", "prev_chunk_id": "chunk_204", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_206", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing XML#", "content": "Writing XML# DataFrame objects have an instance method to_xml which renders the contents of the DataFrame as an XML document. Let’s look at a few examples. Write an XML without options: geom_df = pd.DataFrame( .....: { .....: \"shape\": [\"square\", \"circle\", \"triangle\"], .....: \"degrees\": [360, 360, 180], .....: \"sides\": [4, np.nan, 3], .....: } .....: ) .....: print(geom_df.to_xml()) <?xml version='1.0' encoding='utf-8'?> <data> <row> <index>0</index> <shape>square</shape> <degrees>360</degrees> <sides>4.0</sides> </row> <row> <index>1</index> <shape>circle</shape> <degrees>360</degrees> <sides/> </row> <row> <index>2</index> <shape>triangle</shape> <degrees>180</degrees> <sides>3.0</sides> </row> </data> Write an XML with new root and row name: print(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\")) <?xml version='1.0' encoding='utf-8'?> <geometry> <objects> <index>0</index> <shape>square</shape> <degrees>360</degrees> <sides>4.0</sides> </objects> <objects> <index>1</index> <shape>circle</shape> <degrees>360</degrees> <sides/> </objects> <objects> <index>2</index> <shape>triangle</shape> <degrees>180</degrees> <sides>3.0</sides> </objects> </geometry> Write an attribute-centric XML: print(geom_df.to_xml(attr_cols=geom_df.columns.tolist())) <?xml version='1.0' encoding='utf-8'?> <data> <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/> <row index=\"1\" shape=\"circle\" degrees=\"360\"/> <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/> </data> Write a mix of elements and attributes: print( .....: geom_df.to_xml( .....: index=False, .....: attr_cols=['shape'], .....: elem_cols=['degrees', 'sides']) .....: ) .....: <?xml version='1.0' encoding='utf-8'?> <data> <row shape=\"square\"> <degrees>360</degrees> <sides>4.0</sides> </row> <row shape=\"circle\"> <degrees>360</degrees> <sides/> </row> <row shape=\"triangle\"> <degrees>180</degrees> <sides>3.0</sides> </row> </data> Any DataFrames with hierarchical columns will be flattened for XML element names with levels delimited by underscores: ext_geom_df = pd.DataFrame( .....: { .....: \"type\": [\"polygon\", \"other\", \"polygon\"], .....: \"shape\": [\"square\", \"circle\", \"triangle\"], .....: \"degrees\": [360, 360, 180], .....: \"sides\": [4, np.nan, 3], .....: } .....: ) .....: pvt_df = ext_geom_df.pivot_table(index='shape', .....: columns='type', .....: values=['degrees', 'sides'], .....: aggfunc='sum') .....: pvt_df degrees sides type other polygon other polygon shape circle 360.0 NaN 0.0 NaN square NaN 360.0 NaN 4.0 triangle NaN 180.0 NaN 3.0 print(pvt_df.to_xml()) <?xml version='1.0' encoding='utf-8'?> <data> <row> <shape>circle</shape> <degrees_other>360.0</degrees_other> <degrees_polygon/> <sides_other>0.0</sides_other> <sides_polygon/> </row> <row> <shape>square</shape> <degrees_other/> <degrees_polygon>360.0</degrees_polygon> <sides_other/> <sides_polygon>4.0</sides_polygon> </row> <row> <shape>triangle</shape> <degrees_other/> <degrees_polygon>180.0</degrees_polygon> <sides_other/> <sides_polygon>3.0</sides_polygon> </row> </data> Write an XML with default namespace: print(geom_df.to_xml(namespaces={\"\": \"https://example.com\"})) <?xml version='1.0' encoding='utf-8'?> <data xmlns=\"https://example.com\">", "prev_chunk_id": "chunk_205", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_207", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing XML#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing XML#", "content": "<row> <index>0</index> <shape>square</shape> <degrees>360</degrees> <sides>4.0</sides> </row> <row> <index>1</index> <shape>circle</shape> <degrees>360</degrees> <sides/> </row> <row> <index>2</index> <shape>triangle</shape> <degrees>180</degrees> <sides>3.0</sides> </row> </data> Write an XML with namespace prefix: print( .....: geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"}, .....: prefix=\"doc\") .....: ) .....: <?xml version='1.0' encoding='utf-8'?> <doc:data xmlns:doc=\"https://example.com\"> <doc:row> <doc:index>0</doc:index> <doc:shape>square</doc:shape> <doc:degrees>360</doc:degrees> <doc:sides>4.0</doc:sides> </doc:row> <doc:row> <doc:index>1</doc:index> <doc:shape>circle</doc:shape> <doc:degrees>360</doc:degrees> <doc:sides/> </doc:row> <doc:row> <doc:index>2</doc:index> <doc:shape>triangle</doc:shape> <doc:degrees>180</doc:degrees> <doc:sides>3.0</doc:sides> </doc:row> </doc:data> Write an XML without declaration or pretty print: print( .....: geom_df.to_xml(xml_declaration=False, .....: pretty_print=False) .....: ) .....: <data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data> Write an XML and transform with stylesheet: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> .....: <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/> .....: <xsl:strip-space elements=\"*\"/> .....: <xsl:template match=\"/data\"> .....: <geometry> .....: <xsl:apply-templates select=\"row\"/> .....: </geometry> .....: </xsl:template> .....: <xsl:template match=\"row\"> .....: <object index=\"{index}\"> .....: <xsl:if test=\"shape!='circle'\"> .....: <xsl:attribute name=\"type\">polygon</xsl:attribute> .....: </xsl:if> .....: <xsl:copy-of select=\"shape\"/> .....: <property> .....: <xsl:copy-of select=\"degrees|sides\"/> .....: </property> .....: </object> .....: </xsl:template> .....: </xsl:stylesheet>\"\"\" .....: print(geom_df.to_xml(stylesheet=xsl)) <?xml version=\"1.0\"?> <geometry> <object index=\"0\" type=\"polygon\"> <shape>square</shape> <property> <degrees>360</degrees> <sides>4.0</sides> </property> </object> <object index=\"1\"> <shape>circle</shape> <property> <degrees>360</degrees> <sides/> </property> </object> <object index=\"2\" type=\"polygon\"> <shape>triangle</shape> <property> <degrees>180</degrees> <sides>3.0</sides> </property> </object> </geometry>", "prev_chunk_id": "chunk_206", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_208", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "XML Final Notes#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "XML Final Notes#", "content": "XML Final Notes# - All XML documents adhere toW3C specifications. Bothetreeandlxmlparsers will fail to parse any markup document that is not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document unless it follows XHTML specs. However, other popular markup types including KML, XAML, RSS, MusicML, MathML are compliantXML schemas. - For above reason, if your application builds XML prior to pandas operations, use appropriate DOM libraries likeetreeandlxmlto build the necessary document and not by string concatenation or regex adjustments. Always remember XML is aspecialtext file with markup rules. - With very large XML files (several hundred MBs to GBs), XPath and XSLT can become memory-intensive operations. Be sure to have enough available RAM for reading and writing to large XML files (roughly about 5 times the size of text). - Because XSLT is a programming language, use it with caution since such scripts can pose a security risk in your environment and can run large or infinite recursive operations. Always test scripts on small fragments before full run. - Theetreeparser supports all functionality of bothread_xmlandto_xmlexcept for complex XPath and any XSLT. Though limited in features,etreeis still a reliable and capable parser and tree builder. Its performance may traillxmlto a certain degree for larger files but relatively unnoticeable on small to medium size files.", "prev_chunk_id": "chunk_207", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_209", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Excel files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Excel files#", "content": "Excel files# The read_excel() method can read Excel 2007+ (.xlsx) files using the openpyxl Python module. Excel 2003 (.xls) files can be read using xlrd. Binary Excel (.xlsb) files can be read using pyxlsb. All formats can be read using calamine engine. The to_excel() instance method is used for saving a DataFrame to Excel. Generally the semantics are similar to working with csv data. See the cookbook for some advanced strategies.", "prev_chunk_id": "chunk_208", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_210", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading Excel files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading Excel files#", "content": "Reading Excel files# In the most basic use-case, read_excel takes a path to an Excel file, and the sheet_name indicating which sheet to parse. When using the engine_kwargs parameter, pandas will pass these arguments to the engine. For this, it is important to know which function pandas is using internally. - For the engine openpyxl, pandas is usingopenpyxl.load_workbook()to read in (.xlsx) and (.xlsm) files. - For the engine xlrd, pandas is usingxlrd.open_workbook()to read in (.xls) files. - For the engine pyxlsb, pandas is usingpyxlsb.open_workbook()to read in (.xlsb) files. - For the engine odf, pandas is usingodf.opendocument.load()to read in (.ods) files. - For the engine calamine, pandas is usingpython_calamine.load_workbook()to read in (.xlsx), (.xlsm), (.xls), (.xlsb), (.ods) files. # Returns a DataFrame pd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\")", "prev_chunk_id": "chunk_209", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_211", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "ExcelFile class#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "ExcelFile class#", "content": "ExcelFile class# To facilitate working with multiple sheets from the same file, the ExcelFile class can be used to wrap the file and can be passed into read_excel There will be a performance benefit for reading multiple sheets as the file is read into memory only once. xlsx = pd.ExcelFile(\"path_to_file.xls\") df = pd.read_excel(xlsx, \"Sheet1\") The ExcelFile class can also be used as a context manager. with pd.ExcelFile(\"path_to_file.xls\") as xls: df1 = pd.read_excel(xls, \"Sheet1\") df2 = pd.read_excel(xls, \"Sheet2\") The sheet_names property will generate a list of the sheet names in the file. The primary use-case for an ExcelFile is parsing multiple sheets with different parameters: data = {} # For when Sheet1's format differs from Sheet2 with pd.ExcelFile(\"path_to_file.xls\") as xls: data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"]) data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1) Note that if the same parsing parameters are used for all sheets, a list of sheet names can simply be passed to read_excel with no loss in performance. # using the ExcelFile class data = {} with pd.ExcelFile(\"path_to_file.xls\") as xls: data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"]) data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"]) # equivalent using the read_excel function data = pd.read_excel( \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"] ) ExcelFile can also be called with a xlrd.book.Book object as a parameter. This allows the user to control how the excel file is read. For example, sheets can be loaded on demand by calling xlrd.open_workbook() with on_demand=True. import xlrd xlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True) with pd.ExcelFile(xlrd_book) as xls: df1 = pd.read_excel(xls, \"Sheet1\") df2 = pd.read_excel(xls, \"Sheet2\")", "prev_chunk_id": "chunk_210", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_212", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Specifying sheets#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Specifying sheets#", "content": "Specifying sheets# - The argumentssheet_nameallows specifying the sheet or sheets to read. - The default value forsheet_nameis 0, indicating to read the first sheet - Pass a string to refer to the name of a particular sheet in the workbook. - Pass an integer to refer to the index of a sheet. Indices follow Python convention, beginning at 0. - Pass a list of either strings or integers, to return a dictionary of specified sheets. - Pass aNoneto return a dictionary of all available sheets. # Returns a DataFrame pd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) Using the sheet index: # Returns a DataFrame pd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"]) Using all default values: # Returns a DataFrame pd.read_excel(\"path_to_file.xls\") Using None to get all sheets: # Returns a dictionary of DataFrames pd.read_excel(\"path_to_file.xls\", sheet_name=None) Using a list to get multiple sheets: # Returns the 1st and 4th sheet, as a dictionary of DataFrames. pd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3]) read_excel can read more than one sheet, by setting sheet_name to either a list of sheet names, a list of sheet positions, or None to read all sheets. Sheets can be specified by sheet index or sheet name, using an integer or string, respectively.", "prev_chunk_id": "chunk_211", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_213", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading a MultiIndex#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading a MultiIndex#", "content": "Reading a MultiIndex# read_excel can read a MultiIndex index, by passing a list of columns to index_col and a MultiIndex column by passing a list of rows to header. If either the index or columns have serialized level names those will be read in as well by specifying the rows/columns that make up the levels. For example, to read in a MultiIndex index without names: df = pd.DataFrame( .....: {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]}, .....: index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]), .....: ) .....: df.to_excel(\"path_to_file.xlsx\") df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1]) df a b a c 1 5 d 2 6 b c 3 7 d 4 8 If the index has level names, they will parsed as well, using the same parameters. df.index = df.index.set_names([\"lvl1\", \"lvl2\"]) df.to_excel(\"path_to_file.xlsx\") df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1]) df a b lvl1 lvl2 a c 1 5 d 2 6 b c 3 7 d 4 8 If the source file has both MultiIndex index and columns, lists specifying each should be passed to index_col and header: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"]) df.to_excel(\"path_to_file.xlsx\") df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1]) df c1 a c2 b d lvl1 lvl2 a c 1 5 d 2 6 b c 3 7 d 4 8 Missing values in columns specified in index_col will be forward filled to allow roundtripping with to_excel for merged_cells=True. To avoid forward filling the missing values use set_index after reading the data instead of index_col.", "prev_chunk_id": "chunk_212", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_214", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parsing specific columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parsing specific columns#", "content": "Parsing specific columns# It is often the case that users will insert columns to do temporary computations in Excel and you may not want to read in those columns. read_excel takes a usecols keyword to allow you to specify a subset of columns to parse. You can specify a comma-delimited set of Excel columns and ranges as a string: pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\") If usecols is a list of integers, then it is assumed to be the file column indices to be parsed. pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3]) Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. If usecols is a list of strings, it is assumed that each string corresponds to a column name provided either by the user in names or inferred from the document header row(s). Those strings define which columns will be parsed: pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"]) Element order is ignored, so usecols=['baz', 'joe'] is the same as ['joe', 'baz']. If usecols is callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha())", "prev_chunk_id": "chunk_213", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_215", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parsing dates#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parsing dates#", "content": "Parsing dates# Datetime-like values are normally automatically converted to the appropriate dtype when reading the excel file. But if you have a column of strings that look like dates (but are not actually formatted as dates in excel), you can use the parse_dates keyword to parse those strings to datetimes: pd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"])", "prev_chunk_id": "chunk_214", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_216", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Cell converters#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Cell converters#", "content": "Cell converters# It is possible to transform the contents of Excel cells via the converters option. For instance, to convert a column to boolean: pd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool}) This options handles missing values and treats exceptions in the converters as missing data. Transformations are applied cell by cell rather than to the column as a whole, so the array dtype is not guaranteed. For instance, a column of integers with missing values cannot be transformed to an array with integer dtype, because NaN is strictly a float. You can manually mask missing data to recover integer dtype: def cfun(x): return int(x) if x else -1 pd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun})", "prev_chunk_id": "chunk_215", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_217", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Dtype specifications#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Dtype specifications#", "content": "Dtype specifications# As an alternative to converters, the type for an entire column can be specified using the dtype keyword, which takes a dictionary mapping column names to types. To interpret data with no type inference, use the type str or object. pd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str})", "prev_chunk_id": "chunk_216", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_218", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing Excel files to disk#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing Excel files to disk#", "content": "Writing Excel files to disk# To write a DataFrame object to a sheet of an Excel file, you can use the to_excel instance method. The arguments are largely the same as to_csv described above, the first argument being the name of the excel file, and the optional second argument the name of the sheet to which the DataFrame should be written. For example: df.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") Files with a .xlsx extension will be written using xlsxwriter (if available) or openpyxl. The DataFrame will be written in a way that tries to mimic the REPL output. The index_label will be placed in the second row instead of the first. You can place it in the first row by setting the merge_cells option in to_excel() to False: df.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False) In order to write separate DataFrames to separate sheets in a single Excel file, one can pass an ExcelWriter. with pd.ExcelWriter(\"path_to_file.xlsx\") as writer: df1.to_excel(writer, sheet_name=\"Sheet1\") df2.to_excel(writer, sheet_name=\"Sheet2\") When using the engine_kwargs parameter, pandas will pass these arguments to the engine. For this, it is important to know which function pandas is using internally. - For the engine openpyxl, pandas is usingopenpyxl.Workbook()to create a new sheet andopenpyxl.load_workbook()to append data to an existing sheet. The openpyxl engine writes to (.xlsx) and (.xlsm) files. - For the engine xlsxwriter, pandas is usingxlsxwriter.Workbook()to write to (.xlsx) files. - For the engine odf, pandas is usingodf.opendocument.OpenDocumentSpreadsheet()to write to (.ods) files.", "prev_chunk_id": "chunk_217", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_219", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing Excel files to memory#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing Excel files to memory#", "content": "Writing Excel files to memory# pandas supports writing Excel files to buffer-like objects such as StringIO or BytesIO using ExcelWriter. from io import BytesIO bio = BytesIO() # By setting the 'engine' in the ExcelWriter constructor. writer = pd.ExcelWriter(bio, engine=\"xlsxwriter\") df.to_excel(writer, sheet_name=\"Sheet1\") # Save the workbook writer.save() # Seek to the beginning and read to copy the workbook to a variable in memory bio.seek(0) workbook = bio.read()", "prev_chunk_id": "chunk_218", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_220", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Excel writer engines#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Excel writer engines#", "content": "Excel writer engines# pandas chooses an Excel writer via two methods: - theenginekeyword argument - the filename extension (via the default specified in config options) By default, pandas uses the XlsxWriter for .xlsx, openpyxl for .xlsm. If you have multiple engines installed, you can set the default engine through setting the config options io.excel.xlsx.writer and io.excel.xls.writer. pandas will fall back on openpyxl for .xlsx files if Xlsxwriter is not available. To specify which writer you want to use, you can pass an engine keyword argument to to_excel and to ExcelWriter. The built-in engines are: - openpyxl: version 2.4 or higher is required - xlsxwriter # By setting the 'engine' in the DataFrame 'to_excel()' methods. df.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\") # By setting the 'engine' in the ExcelWriter constructor. writer = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\") # Or via pandas configuration. from pandas import options # noqa: E402 options.io.excel.xlsx.writer = \"xlsxwriter\" df.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")", "prev_chunk_id": "chunk_219", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_221", "url": "https://pandas.pydata.org/docs/user_guide/pyarrow.html", "title": "PyArrow Functionality#", "page_title": "PyArrow Functionality — pandas 2.3.1 documentation", "breadcrumbs": "PyArrow Functionality#", "content": "PyArrow Functionality# pandas can utilize PyArrow to extend functionality and improve the performance of various APIs. This includes: - More extensivedata typescompared to NumPy - Missing data support (NA) for all data types - Performant IO reader integration - Facilitate interoperability with other dataframe libraries based on the Apache Arrow specification (e.g. polars, cuDF) To use this functionality, please ensure you have installed the minimum supported PyArrow version.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_222", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Style and formatting#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Style and formatting#", "content": "Style and formatting# The look and feel of Excel worksheets created from pandas can be modified using the following parameters on the DataFrame’s to_excel method. - float_format: Format string for floating point numbers (defaultNone). - freeze_panes: A tuple of two integers representing the bottommost row and rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will freeze the first row and first column (defaultNone). Using the Xlsxwriter engine provides many options for controlling the format of an Excel worksheet created with the to_excel method. Excellent examples can be found in the Xlsxwriter documentation here: https://xlsxwriter.readthedocs.io/working_with_pandas.html", "prev_chunk_id": "chunk_220", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_223", "url": "https://pandas.pydata.org/docs/user_guide/pyarrow.html", "title": "Data Structure Integration#", "page_title": "PyArrow Functionality — pandas 2.3.1 documentation", "breadcrumbs": "Data Structure Integration#", "content": "Data Structure Integration# A Series, Index, or the columns of a DataFrame can be directly backed by a pyarrow.ChunkedArray which is similar to a NumPy array. To construct these from the main pandas data structures, you can pass in a string of the type followed by [pyarrow], e.g. \"int64[pyarrow]\"\" into the dtype parameter ser = pd.Series([-1.5, 0.2, None], dtype=\"float32[pyarrow]\") ser 0 -1.5 1 0.2 2 <NA> dtype: float[pyarrow] idx = pd.Index([True, None], dtype=\"bool[pyarrow]\") idx Index([True, <NA>], dtype='bool[pyarrow]') df = pd.DataFrame([[1, 2], [3, 4]], dtype=\"uint64[pyarrow]\") df 0 1 0 1 2 1 3 4 For PyArrow types that accept parameters, you can pass in a PyArrow type with those parameters into ArrowDtype to use in the dtype parameter. import pyarrow as pa list_str_type = pa.list_(pa.string()) ser = pd.Series([[\"hello\"], [\"there\"]], dtype=pd.ArrowDtype(list_str_type)) ser 0 ['hello'] 1 ['there'] dtype: list<item: string>[pyarrow] from datetime import time idx = pd.Index([time(12, 30), None], dtype=pd.ArrowDtype(pa.time64(\"us\"))) idx Index([12:30:00, <NA>], dtype='time64[us][pyarrow]') from decimal import Decimal decimal_type = pd.ArrowDtype(pa.decimal128(3, scale=2)) data = [[Decimal(\"3.19\"), None], [None, Decimal(\"-1.23\")]] df = pd.DataFrame(data, dtype=decimal_type) df 0 1 0 3.19 <NA> 1 <NA> -1.23 If you already have an pyarrow.Array or pyarrow.ChunkedArray, you can pass it into arrays.ArrowExtensionArray to construct the associated Series, Index or DataFrame object. pa_array = pa.array( ....: [{\"1\": \"2\"}, {\"10\": \"20\"}, None], ....: type=pa.map_(pa.string(), pa.string()), ....: ) ....: ser = pd.Series(pd.arrays.ArrowExtensionArray(pa_array)) ser 0 [('1', '2')] 1 [('10', '20')] 2 <NA> dtype: map<string, string>[pyarrow] To retrieve a pyarrow pyarrow.ChunkedArray from a Series or Index, you can call the pyarrow array constructor on the Series or Index. ser = pd.Series([1, 2, None], dtype=\"uint8[pyarrow]\") pa.array(ser) <pyarrow.lib.UInt8Array object at 0x7f1087e61780> [ 1, 2, null ] idx = pd.Index(ser) pa.array(idx) <pyarrow.lib.UInt8Array object at 0x7f1087e3a380> [ 1, 2, null ] To convert a pyarrow.Table to a DataFrame, you can call the pyarrow.Table.to_pandas() method with types_mapper=pd.ArrowDtype. table = pa.table([pa.array([1,", "prev_chunk_id": "chunk_221", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_224", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "OpenDocument Spreadsheets#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "OpenDocument Spreadsheets#", "content": "OpenDocument Spreadsheets# The io methods for Excel files also support reading and writing OpenDocument spreadsheets using the odfpy module. The semantics and features for reading and writing OpenDocument spreadsheets match what can be done for Excel files using engine='odf'. The optional dependency ‘odfpy’ needs to be installed. The read_excel() method can read OpenDocument spreadsheets # Returns a DataFrame pd.read_excel(\"path_to_file.ods\", engine=\"odf\") Similarly, the to_excel() method can write OpenDocument spreadsheets # Writes DataFrame to a .ods file df.to_excel(\"path_to_file.ods\", engine=\"odf\")", "prev_chunk_id": "chunk_222", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_225", "url": "https://pandas.pydata.org/docs/user_guide/pyarrow.html", "title": "Data Structure Integration#", "page_title": "PyArrow Functionality — pandas 2.3.1 documentation", "breadcrumbs": "Data Structure Integration#", "content": "2, 3], type=pa.int64())], names=[\"a\"]) df = table.to_pandas(types_mapper=pd.ArrowDtype) df a 0 1 1 2 2 3 df.dtypes a int64[pyarrow] dtype: object", "prev_chunk_id": "chunk_223", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_226", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Binary Excel (.xlsb) files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Binary Excel (.xlsb) files#", "content": "Binary Excel (.xlsb) files# The read_excel() method can also read binary Excel files using the pyxlsb module. The semantics and features for reading binary Excel files mostly match what can be done for Excel files using engine='pyxlsb'. pyxlsb does not recognize datetime types in files and will return floats instead (you can use calamine if you need recognize datetime types). # Returns a DataFrame pd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\")", "prev_chunk_id": "chunk_224", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_227", "url": "https://pandas.pydata.org/docs/user_guide/pyarrow.html", "title": "Operations#", "page_title": "PyArrow Functionality — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# PyArrow data structure integration is implemented through pandas’ ExtensionArray interface; therefore, supported functionality exists where this interface is integrated within the pandas API. Additionally, this functionality is accelerated with PyArrow compute functions where available. This includes: - Numeric aggregations - Numeric arithmetic - Numeric rounding - Logical and comparison functions - String functionality - Datetime functionality The following are just some examples of operations that are accelerated by native PyArrow compute functions. import pyarrow as pa ser = pd.Series([-1.545, 0.211, None], dtype=\"float32[pyarrow]\") ser.mean() -0.6669999808073044 ser + ser 0 -3.09 1 0.422 2 <NA> dtype: float[pyarrow] ser > (ser + 1) 0 False 1 False 2 <NA> dtype: bool[pyarrow] ser.dropna() 0 -1.545 1 0.211 dtype: float[pyarrow] ser.isna() 0 False 1 False 2 True dtype: bool ser.fillna(0) 0 -1.545 1 0.211 2 0.0 dtype: float[pyarrow] ser_str = pd.Series([\"a\", \"b\", None], dtype=pd.ArrowDtype(pa.string())) ser_str.str.startswith(\"a\") 0 True 1 False 2 <NA> dtype: bool[pyarrow] from datetime import datetime pa_type = pd.ArrowDtype(pa.timestamp(\"ns\")) ser_dt = pd.Series([datetime(2022, 1, 1), None], dtype=pa_type) ser_dt.dt.strftime(\"%Y-%m\") 0 2022-01 1 <NA> dtype: string[pyarrow]", "prev_chunk_id": "chunk_225", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_228", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Calamine (Excel and ODS files)#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Calamine (Excel and ODS files)#", "content": "Calamine (Excel and ODS files)# The read_excel() method can read Excel file (.xlsx, .xlsm, .xls, .xlsb) and OpenDocument spreadsheets (.ods) using the python-calamine module. This module is a binding for Rust library calamine and is faster than other engines in most cases. The optional dependency ‘python-calamine’ needs to be installed. # Returns a DataFrame pd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\")", "prev_chunk_id": "chunk_226", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_229", "url": "https://pandas.pydata.org/docs/user_guide/pyarrow.html", "title": "I/O Reading#", "page_title": "PyArrow Functionality — pandas 2.3.1 documentation", "breadcrumbs": "I/O Reading#", "content": "I/O Reading# PyArrow also provides IO reading functionality that has been integrated into several pandas IO readers. The following functions provide an engine keyword that can dispatch to PyArrow to accelerate reading from an IO source. - read_csv() - read_json() - read_orc() - read_feather() import io data = io.StringIO(\"\"\"a,b,c ....: 1,2.5,True ....: 3,4.5,False ....: \"\"\") ....: df = pd.read_csv(data, engine=\"pyarrow\") df a b c 0 1 2.5 True 1 3 4.5 False By default, these functions and all other IO reader functions return NumPy-backed data. These readers can return PyArrow-backed data by specifying the parameter dtype_backend=\"pyarrow\". A reader does not need to set engine=\"pyarrow\" to necessarily return PyArrow-backed data. import io data = io.StringIO(\"\"\"a,b,c,d,e,f,g,h,i ....: 1,2.5,True,a,,,,, ....: 3,4.5,False,b,6,7.5,True,a, ....: \"\"\") ....: df_pyarrow = pd.read_csv(data, dtype_backend=\"pyarrow\") df_pyarrow.dtypes a int64[pyarrow] b double[pyarrow] c bool[pyarrow] d string[pyarrow] e int64[pyarrow] f double[pyarrow] g bool[pyarrow] h string[pyarrow] i null[pyarrow] dtype: object Several non-IO reader functions can also use the dtype_backend argument to return PyArrow-backed data including: - to_numeric() - DataFrame.convert_dtypes() - Series.convert_dtypes()", "prev_chunk_id": "chunk_227", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_230", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Clipboard#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Clipboard#", "content": "Clipboard# A handy way to grab data is to use the read_clipboard() method, which takes the contents of the clipboard buffer and passes them to the read_csv method. For instance, you can copy the following text to the clipboard (CTRL-C on many operating systems): A B C x 1 4 p y 2 5 q z 3 6 r And then import the data directly to a DataFrame by calling: >>> clipdf = pd.read_clipboard() >>> clipdf A B C x 1 4 p y 2 5 q z 3 6 r The to_clipboard method can be used to write the contents of a DataFrame to the clipboard. Following which you can paste the clipboard contents into other applications (CTRL-V on many operating systems). Here we illustrate writing a DataFrame into clipboard and reading it back. >>> df = pd.DataFrame( ... {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"] ... ) >>> df A B C x 1 4 p y 2 5 q z 3 6 r >>> df.to_clipboard() >>> pd.read_clipboard() A B C x 1 4 p y 2 5 q z 3 6 r We can see that we got the same content back, which we had earlier written to the clipboard.", "prev_chunk_id": "chunk_228", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_231", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Pickling#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Pickling#", "content": "Pickling# All pandas objects are equipped with to_pickle methods which use Python’s cPickle module to save data structures to disk using the pickle format. df c1 a c2 b d lvl1 lvl2 a c 1 5 d 2 6 b c 3 7 d 4 8 df.to_pickle(\"foo.pkl\") The read_pickle function in the pandas namespace can be used to load any pickled pandas object (or any other pickled object) from file: pd.read_pickle(\"foo.pkl\") c1 a c2 b d lvl1 lvl2 a c 1 5 d 2 6 b c 3 7 d 4 8", "prev_chunk_id": "chunk_230", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_232", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Compressed pickle files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Compressed pickle files#", "content": "Compressed pickle files# read_pickle(), DataFrame.to_pickle() and Series.to_pickle() can read and write compressed pickle files. The compression types of gzip, bz2, xz, zstd are supported for reading and writing. The zip file format only supports reading and must contain only one data file to be read. The compression type can be an explicit parameter or be inferred from the file extension. If ‘infer’, then use gzip, bz2, zip, xz, zstd if filename ends in '.gz', '.bz2', '.zip', '.xz', or '.zst', respectively. The compression parameter can also be a dict in order to pass options to the compression protocol. It must have a 'method' key set to the name of the compression protocol, which must be one of {'zip', 'gzip', 'bz2', 'xz', 'zstd'}. All other key-value pairs are passed to the underlying compression library. df = pd.DataFrame( .....: { .....: \"A\": np.random.randn(1000), .....: \"B\": \"foo\", .....: \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"), .....: } .....: ) .....: df A B C 0 -0.317441 foo 2013-01-01 00:00:00 1 -1.236269 foo 2013-01-01 00:00:01 2 0.896171 foo 2013-01-01 00:00:02 3 -0.487602 foo 2013-01-01 00:00:03 4 -0.082240 foo 2013-01-01 00:00:04 .. ... ... ... 995 -0.171092 foo 2013-01-01 00:16:35 996 1.786173 foo 2013-01-01 00:16:36 997 -0.575189 foo 2013-01-01 00:16:37 998 0.820750 foo 2013-01-01 00:16:38 999 -1.256530 foo 2013-01-01 00:16:39 [1000 rows x 3 columns] Using an explicit compression type: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\") rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\") rt A B C 0 -0.317441 foo 2013-01-01 00:00:00 1 -1.236269 foo 2013-01-01 00:00:01 2 0.896171 foo 2013-01-01 00:00:02 3 -0.487602 foo 2013-01-01 00:00:03 4 -0.082240 foo 2013-01-01 00:00:04 .. ... ... ... 995 -0.171092 foo 2013-01-01 00:16:35 996 1.786173 foo 2013-01-01 00:16:36 997 -0.575189 foo 2013-01-01 00:16:37 998 0.820750 foo 2013-01-01 00:16:38 999 -1.256530 foo 2013-01-01 00:16:39 [1000 rows x 3 columns] Inferring compression type from the extension: df.to_pickle(\"data.pkl.xz\", compression=\"infer\") rt =", "prev_chunk_id": "chunk_231", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_233", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Compressed pickle files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Compressed pickle files#", "content": "pd.read_pickle(\"data.pkl.xz\", compression=\"infer\") rt A B C 0 -0.317441 foo 2013-01-01 00:00:00 1 -1.236269 foo 2013-01-01 00:00:01 2 0.896171 foo 2013-01-01 00:00:02 3 -0.487602 foo 2013-01-01 00:00:03 4 -0.082240 foo 2013-01-01 00:00:04 .. ... ... ... 995 -0.171092 foo 2013-01-01 00:16:35 996 1.786173 foo 2013-01-01 00:16:36 997 -0.575189 foo 2013-01-01 00:16:37 998 0.820750 foo 2013-01-01 00:16:38 999 -1.256530 foo 2013-01-01 00:16:39 [1000 rows x 3 columns] The default is to ‘infer’: df.to_pickle(\"data.pkl.gz\") rt = pd.read_pickle(\"data.pkl.gz\") rt A B C 0 -0.317441 foo 2013-01-01 00:00:00 1 -1.236269 foo 2013-01-01 00:00:01 2 0.896171 foo 2013-01-01 00:00:02 3 -0.487602 foo 2013-01-01 00:00:03 4 -0.082240 foo 2013-01-01 00:00:04 .. ... ... ... 995 -0.171092 foo 2013-01-01 00:16:35 996 1.786173 foo 2013-01-01 00:16:36 997 -0.575189 foo 2013-01-01 00:16:37 998 0.820750 foo 2013-01-01 00:16:38 999 -1.256530 foo 2013-01-01 00:16:39 [1000 rows x 3 columns] df[\"A\"].to_pickle(\"s1.pkl.bz2\") rt = pd.read_pickle(\"s1.pkl.bz2\") rt 0 -0.317441 1 -1.236269 2 0.896171 3 -0.487602 4 -0.082240 ... 995 -0.171092 996 1.786173 997 -0.575189 998 0.820750 999 -1.256530 Name: A, Length: 1000, dtype: float64 Passing options to the compression protocol in order to speed up compression: df.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1})", "prev_chunk_id": "chunk_232", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_234", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "msgpack#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "msgpack#", "content": "msgpack# pandas support for msgpack has been removed in version 1.0.0. It is recommended to use pickle instead. Alternatively, you can also the Arrow IPC serialization format for on-the-wire transmission of pandas objects. For documentation on pyarrow, see here.", "prev_chunk_id": "chunk_233", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_235", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "HDF5 (PyTables)#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "HDF5 (PyTables)#", "content": "HDF5 (PyTables)# HDFStore is a dict-like object which reads and writes pandas using the high performance HDF5 format using the excellent PyTables library. See the cookbook for some advanced strategies store = pd.HDFStore(\"store.h5\") print(store) <class 'pandas.io.pytables.HDFStore'> File path: store.h5 Objects can be written to the file just like adding key-value pairs to a dict: index = pd.date_range(\"1/1/2000\", periods=8) s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"]) # store.put('s', s) is an equivalent method store[\"s\"] = s store[\"df\"] = df store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 In a current or later Python session, you can retrieve stored objects: # store.get('df') is an equivalent method store[\"df\"] A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517 # dotted (attribute) access provides get as well store.df A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517 Deletion of the object specified by the key: # store.remove('df') is an equivalent method del store[\"df\"] store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 Closing a Store and using a context manager: store.close() store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 store.is_open False # Working with, and automatically closing the store using a context manager with pd.HDFStore(\"store.h5\") as store: .....: store.keys() .....:", "prev_chunk_id": "chunk_234", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_236", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Read/write API#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Read/write API#", "content": "Read/write API# HDFStore supports a top-level API using read_hdf for reading and to_hdf for writing, similar to how read_csv and to_csv work. df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))}) df_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True) pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"]) A B 3 3 3 4 4 4 HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting dropna=True. df_with_missing = pd.DataFrame( .....: { .....: \"col1\": [0, np.nan, 2], .....: \"col2\": [1, np.nan, np.nan], .....: } .....: ) .....: df_with_missing col1 col2 0 0.0 1.0 1 NaN NaN 2 2.0 NaN df_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\") pd.read_hdf(\"file.h5\", \"df_with_missing\") col1 col2 0 0.0 1.0 1 NaN NaN 2 2.0 NaN df_with_missing.to_hdf( .....: \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True .....: ) .....: pd.read_hdf(\"file.h5\", \"df_with_missing\") col1 col2 0 0.0 1.0 2 2.0 NaN", "prev_chunk_id": "chunk_235", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_237", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Fixed format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Fixed format#", "content": "Fixed format# The examples above show storing using put, which write the HDF5 to PyTables in a fixed array format, called the fixed format. These types of stores are not appendable once written (though you can simply remove them and rewrite). Nor are they queryable; they must be retrieved in their entirety. They also do not support dataframes with non-unique column names. The fixed format stores offer very fast writing and slightly faster reading than table stores. This format is specified by default when using put or to_hdf or by format='fixed' or format='f'.", "prev_chunk_id": "chunk_236", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_238", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Table format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Table format#", "content": "Table format# HDFStore supports another PyTables format on disk, the table format. Conceptually a table is shaped very much like a DataFrame, with rows and columns. A table may be appended to in the same or other sessions. In addition, delete and query type operations are supported. This format is specified by format='table' or format='t' to append or put or to_hdf. This format can be set as an option as well pd.set_option('io.hdf.default_format','table') to enable put/append/to_hdf to by default store in the table format. store = pd.HDFStore(\"store.h5\") df1 = df[0:4] df2 = df[4:] # append data (creates a table automatically) store.append(\"df\", df1) store.append(\"df\", df2) store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 # select the entire object store.select(\"df\") A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517 # the type of stored data store.root.df._v_attrs.pandas_type 'frame_table'", "prev_chunk_id": "chunk_237", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_239", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Hierarchical keys#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Hierarchical keys#", "content": "Hierarchical keys# Keys to a store can be specified as a string. These can be in a hierarchical path-name like format (e.g. foo/bar/bah), which will generate a hierarchy of sub-stores (or Groups in PyTables parlance). Keys can be specified without the leading ‘/’ and are always absolute (e.g. ‘foo’ refers to ‘/foo’). Removal operations can remove everything in the sub-store and below, so be careful. store.put(\"foo/bar/bah\", df) store.append(\"food/orange\", df) store.append(\"food/apple\", df) store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 # a list of keys are returned store.keys() ['/df', '/food/apple', '/food/orange', '/foo/bar/bah'] # remove all nodes under this level store.remove(\"food\") store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 You can walk through the group hierarchy using the walk method which will yield a tuple for each group key along with the relative keys of its contents. for (path, subgroups, subkeys) in store.walk(): .....: for subgroup in subgroups: .....: print(\"GROUP: {}/{}\".format(path, subgroup)) .....: for subkey in subkeys: .....: key = \"/\".join([path, subkey]) .....: print(\"KEY: {}\".format(key)) .....: print(store.get(key)) .....: GROUP: /foo KEY: /df A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517 GROUP: /foo/bar KEY: /foo/bar/bah A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517", "prev_chunk_id": "chunk_238", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_240", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Storing mixed types in a table#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Storing mixed types in a table#", "content": "Storing mixed types in a table# Storing mixed-dtype data is supported. Strings are stored as a fixed-width using the maximum size of the appended column. Subsequent attempts at appending longer strings will raise a ValueError. Passing min_itemsize={`values`: size} as a parameter to append will set a larger minimum for the string columns. Storing floats, strings, ints, bools, datetime64 are currently supported. For string columns, passing nan_rep = 'nan' to append will change the default nan representation on disk (which converts to/from np.nan), this defaults to nan. df_mixed = pd.DataFrame( .....: { .....: \"A\": np.random.randn(8), .....: \"B\": np.random.randn(8), .....: \"C\": np.array(np.random.randn(8), dtype=\"float32\"), .....: \"string\": \"string\", .....: \"int\": 1, .....: \"bool\": True, .....: \"datetime64\": pd.Timestamp(\"20010102\"), .....: }, .....: index=list(range(8)), .....: ) .....: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50}) df_mixed1 = store.select(\"df_mixed\") df_mixed1 A B C ... int bool datetime64 0 0.013747 -1.166078 -1.292080 ... 1 True 1970-01-01 00:00:00.978393600 1 -0.712009 0.247572 1.526911 ... 1 True 1970-01-01 00:00:00.978393600 2 -0.645096 1.687406 0.288504 ... 1 True 1970-01-01 00:00:00.978393600 3 NaN NaN 0.097771 ... 1 True NaT 4 NaN NaN 1.536408 ... 1 True NaT 5 -0.023202 0.043702 0.926790 ... 1 True 1970-01-01 00:00:00.978393600 6 2.359782 0.088224 -0.676448 ... 1 True 1970-01-01 00:00:00.978393600 7 -0.143428 -0.813360 -0.179724 ... 1 True 1970-01-01 00:00:00.978393600 [8 rows x 7 columns] df_mixed1.dtypes.value_counts() float64 2 float32 1 object 1 int64 1 bool 1 datetime64[ns] 1 Name: count, dtype: int64 # we have provided a minimum string column size store.root.df_mixed.table /df_mixed/table (Table(8,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1), \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2), \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3), \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4), \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5), \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)} byteorder := 'little' chunkshape := (689,) autoindex := True colindexes := { \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}", "prev_chunk_id": "chunk_239", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_241", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Storing MultiIndex DataFrames#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Storing MultiIndex DataFrames#", "content": "Storing MultiIndex DataFrames# Storing MultiIndex DataFrames as tables is very similar to storing/selecting from homogeneous index DataFrames. index = pd.MultiIndex( .....: levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]], .....: codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]], .....: names=[\"foo\", \"bar\"], .....: ) .....: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"]) df_mi A B C foo bar foo one -1.303456 -0.642994 -0.649456 two 1.012694 0.414147 1.950460 three 1.094544 -0.802899 -0.583343 bar one 0.410395 0.618321 0.560398 two 1.434027 -0.033270 0.343197 baz two -1.646063 -0.695847 -0.429156 three -0.244688 -1.428229 -0.138691 qux one 1.866184 -1.446617 0.036660 two -1.660522 0.929553 -1.298649 three 3.565769 0.682402 1.041927 store.append(\"df_mi\", df_mi) store.select(\"df_mi\") A B C foo bar foo one -1.303456 -0.642994 -0.649456 two 1.012694 0.414147 1.950460 three 1.094544 -0.802899 -0.583343 bar one 0.410395 0.618321 0.560398 two 1.434027 -0.033270 0.343197 baz two -1.646063 -0.695847 -0.429156 three -0.244688 -1.428229 -0.138691 qux one 1.866184 -1.446617 0.036660 two -1.660522 0.929553 -1.298649 three 3.565769 0.682402 1.041927 # the levels are automatically included as data columns store.select(\"df_mi\", \"foo=bar\") A B C foo bar bar one 0.410395 0.618321 0.560398 two 1.434027 -0.033270 0.343197", "prev_chunk_id": "chunk_240", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_242", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Querying a table#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Querying a table#", "content": "Querying a table# select and delete operations have an optional criterion that can be specified to select/delete only a subset of the data. This allows one to have a very large on-disk table and retrieve only a portion of the data. A query is specified using the Term class under the hood, as a boolean expression. - indexandcolumnsare supported indexers ofDataFrames. - ifdata_columnsare specified, these can be used as additional indexers. - level name in a MultiIndex, with default namelevel_0,level_1, … if not provided. Valid comparison operators are: =, ==, !=, >, >=, <, <= Valid boolean expressions are combined with: - |: or - &: and - (and): for grouping These rules are similar to how boolean expressions are used in pandas for indexing. The following are valid expressions: - 'index>=date' - \"columns=['A','D']\" - \"columnsin['A','D']\" - 'columns=A' - 'columns==A' - \"~(columns=['A','B'])\" - 'index>df.index[3]&string=\"bar\"' - '(index>df.index[3]&index<=df.index[6])|string=\"bar\"' - \"ts>=Timestamp('2012-02-01')\" - \"major_axis>=20130101\" The indexers are on the left-hand side of the sub-expression: columns, major_axis, ts The right-hand side of the sub-expression (after a comparison operator) can be: - functions that will be evaluated, e.g.Timestamp('2012-02-01') - strings, e.g.\"bar\" - date-like, e.g.20130101, or\"20130101\" - lists, e.g.\"['A','B']\" - variables that are defined in the local names space, e.g.date Here are some examples: dfq = pd.DataFrame( .....: np.random.randn(10, 4), .....: columns=list(\"ABCD\"), .....: index=pd.date_range(\"20130101\", periods=10), .....: ) .....: store.append(\"dfq\", dfq, format=\"table\", data_columns=True) Use boolean expressions, with in-line function evaluation. store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\") A B 2013-01-05 -0.830545 -0.457071 2013-01-06 0.431186 1.049421 2013-01-07 0.617509 -0.811230 2013-01-08 0.947422 -0.671233 2013-01-09 -0.183798 -1.211230 2013-01-10 0.361428 0.887304 Use inline column reference. store.select(\"dfq\", where=\"A>0 or C>0\") A B C D 2013-01-02 0.658179 0.362814 -0.917897 0.010165 2013-01-03 0.905122 1.848731 -1.184241 0.932053 2013-01-05 -0.830545 -0.457071 1.565581 1.148032 2013-01-06 0.431186 1.049421 0.383309 0.595013 2013-01-07 0.617509 -0.811230 -2.088563 -1.393500 2013-01-08 0.947422 -0.671233 -0.847097 -1.187785 2013-01-10", "prev_chunk_id": "chunk_241", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_243", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Querying a table#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Querying a table#", "content": "0.361428 0.887304 0.266457 -0.399641 The columns keyword can be supplied to select a list of columns to be returned, this is equivalent to passing a 'columns=list_of_columns_to_filter': store.select(\"df\", \"columns=['A', 'B']\") A B 2000-01-01 0.858644 -0.851236 2000-01-02 -0.080372 -1.268121 2000-01-03 0.816983 1.965656 2000-01-04 0.712795 -0.062433 2000-01-05 -0.298721 -1.988045 2000-01-06 1.103675 1.382242 2000-01-07 -0.729161 -0.142928 2000-01-08 -1.005977 0.465222 start and stop parameters can be specified to limit the total search space. These are in terms of the total number of rows in a table.", "prev_chunk_id": "chunk_242", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_244", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Query timedelta64[ns]#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Query timedelta64[ns]#", "content": "Query timedelta64[ns]# You can store and query using the timedelta64[ns] type. Terms can be specified in the format: <float>(<unit>), where float may be signed (and fractional), and unit can be D,s,ms,us,ns for the timedelta. Here’s an example: from datetime import timedelta dftd = pd.DataFrame( .....: { .....: \"A\": pd.Timestamp(\"20130101\"), .....: \"B\": [ .....: pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10) .....: for i in range(10) .....: ], .....: } .....: ) .....: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"] dftd A B C 0 2013-01-01 2013-01-01 00:00:10 -1 days +23:59:50 1 2013-01-01 2013-01-02 00:00:10 -2 days +23:59:50 2 2013-01-01 2013-01-03 00:00:10 -3 days +23:59:50 3 2013-01-01 2013-01-04 00:00:10 -4 days +23:59:50 4 2013-01-01 2013-01-05 00:00:10 -5 days +23:59:50 5 2013-01-01 2013-01-06 00:00:10 -6 days +23:59:50 6 2013-01-01 2013-01-07 00:00:10 -7 days +23:59:50 7 2013-01-01 2013-01-08 00:00:10 -8 days +23:59:50 8 2013-01-01 2013-01-09 00:00:10 -9 days +23:59:50 9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50 store.append(\"dftd\", dftd, data_columns=True) store.select(\"dftd\", \"C<'-3.5D'\") A B C 4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10 -5 days +23:59:50 5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10 -6 days +23:59:50 6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10 -7 days +23:59:50 7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10 -8 days +23:59:50 8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10 -9 days +23:59:50 9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50", "prev_chunk_id": "chunk_243", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_245", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Query MultiIndex#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Query MultiIndex#", "content": "Query MultiIndex# Selecting from a MultiIndex can be achieved by using the name of the level. df_mi.index.names FrozenList(['foo', 'bar']) store.select(\"df_mi\", \"foo=baz and bar=two\") A B C foo bar baz two -1.646063 -0.695847 -0.429156 If the MultiIndex levels names are None, the levels are automatically made available via the level_n keyword with n the level of the MultiIndex you want to select from. index = pd.MultiIndex( .....: levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]], .....: codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]], .....: ) .....: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"]) df_mi_2 A B C foo one -0.219582 1.186860 -1.437189 two 0.053768 1.872644 -1.469813 three -0.564201 0.876341 0.407749 bar one -0.232583 0.179812 0.922152 two -1.820952 -0.641360 2.133239 baz two -0.941248 -0.136307 -1.271305 three -0.099774 -0.061438 -0.845172 qux one 0.465793 0.756995 -0.541690 two -0.802241 0.877657 -2.553831 three 0.094899 -2.319519 0.293601 store.append(\"df_mi_2\", df_mi_2) # the levels are automatically included as data columns with keyword level_n store.select(\"df_mi_2\", \"level_0=foo and level_1=two\") A B C foo two 0.053768 1.872644 -1.469813", "prev_chunk_id": "chunk_244", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_246", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Indexing#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Indexing#", "content": "Indexing# You can create/modify an index for a table with create_table_index after data is already in the table (after and append/put operation). Creating a table index is highly encouraged. This will speed your queries a great deal when you use a select with the indexed dimension as the where. # we have automagically already created an index (in the first section) i = store.root.df.table.cols.index.index i.optlevel, i.kind (6, 'medium') # change an index by passing new parameters store.create_table_index(\"df\", optlevel=9, kind=\"full\") i = store.root.df.table.cols.index.index i.optlevel, i.kind (9, 'full') Oftentimes when appending large amounts of data to a store, it is useful to turn off index creation for each append, then recreate at the end. df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\")) df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\")) st = pd.HDFStore(\"appends.h5\", mode=\"w\") st.append(\"df\", df_1, data_columns=[\"B\"], index=False) st.append(\"df\", df_2, data_columns=[\"B\"], index=False) st.get_storer(\"df\").table /df/table (Table(20,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1), \"B\": Float64Col(shape=(), dflt=0.0, pos=2)} byteorder := 'little' chunkshape := (2730,) Then create the index when finished appending. st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\") st.get_storer(\"df\").table /df/table (Table(20,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1), \"B\": Float64Col(shape=(), dflt=0.0, pos=2)} byteorder := 'little' chunkshape := (2730,) autoindex := True colindexes := { \"B\": Index(9, fullshuffle, zlib(1)).is_csi=True} st.close() See here for how to create a completely-sorted-index (CSI) on an existing store.", "prev_chunk_id": "chunk_245", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_247", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Query via data columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Query via data columns#", "content": "Query via data columns# You can designate (and index) certain columns that you want to be able to perform queries (other than the indexable columns, which you can always query). For instance say you want to perform this common operation, on-disk, and return just the frame that matches this query. You can specify data_columns = True to force all columns to be data_columns. df_dc = df.copy() df_dc[\"string\"] = \"foo\" df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\" df_dc[\"string2\"] = \"cool\" df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0 df_dc A B C string string2 2000-01-01 0.858644 -0.851236 1.058006 foo cool 2000-01-02 -0.080372 1.000000 1.000000 foo cool 2000-01-03 0.816983 1.000000 1.000000 foo cool 2000-01-04 0.712795 -0.062433 0.736755 foo cool 2000-01-05 -0.298721 -1.988045 1.475308 NaN cool 2000-01-06 1.103675 1.382242 -0.650762 NaN cool 2000-01-07 -0.729161 -0.142928 -1.063038 foo cool 2000-01-08 -1.005977 0.465222 -0.094517 bar cool # on-disk operations store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"]) store.select(\"df_dc\", where=\"B > 0\") A B C string string2 2000-01-02 -0.080372 1.000000 1.000000 foo cool 2000-01-03 0.816983 1.000000 1.000000 foo cool 2000-01-06 1.103675 1.382242 -0.650762 NaN cool 2000-01-08 -1.005977 0.465222 -0.094517 bar cool # getting creative store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\") A B C string string2 2000-01-02 -0.080372 1.0 1.0 foo cool 2000-01-03 0.816983 1.0 1.0 foo cool # this is in-memory version of this type of selection df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")] A B C string string2 2000-01-02 -0.080372 1.0 1.0 foo cool 2000-01-03 0.816983 1.0 1.0 foo cool # we have automagically created this index and the B/C/string/string2 # columns are stored separately as ``PyTables`` columns store.root.df_dc.table /df_dc/table (Table(8,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1), \"B\": Float64Col(shape=(), dflt=0.0, pos=2), \"C\": Float64Col(shape=(), dflt=0.0, pos=3), \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4), \"string2\": StringCol(itemsize=4, shape=(),", "prev_chunk_id": "chunk_246", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_248", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Query via data columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Query via data columns#", "content": "dflt=b'', pos=5)} byteorder := 'little' chunkshape := (1680,) autoindex := True colindexes := { \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False, \"B\": Index(6, mediumshuffle, zlib(1)).is_csi=False, \"C\": Index(6, mediumshuffle, zlib(1)).is_csi=False, \"string\": Index(6, mediumshuffle, zlib(1)).is_csi=False, \"string2\": Index(6, mediumshuffle, zlib(1)).is_csi=False} There is some performance degradation by making lots of columns into data columns, so it is up to the user to designate these. In addition, you cannot change data columns (nor indexables) after the first append/put operation (Of course you can simply read in the data and create a new table!).", "prev_chunk_id": "chunk_247", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_249", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Iterator#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Iterator#", "content": "Iterator# You can pass iterator=True or chunksize=number_in_a_chunk to select and select_as_multiple to return an iterator on the results. The default is 50,000 rows returned in a chunk. for df in store.select(\"df\", chunksize=3): .....: print(df) .....: A B C 2000-01-01 0.858644 -0.851236 1.058006 2000-01-02 -0.080372 -1.268121 1.561967 2000-01-03 0.816983 1.965656 -1.169408 A B C 2000-01-04 0.712795 -0.062433 0.736755 2000-01-05 -0.298721 -1.988045 1.475308 2000-01-06 1.103675 1.382242 -0.650762 A B C 2000-01-07 -0.729161 -0.142928 -1.063038 2000-01-08 -1.005977 0.465222 -0.094517 Note, that the chunksize keyword applies to the source rows. So if you are doing a query, then the chunksize will subdivide the total rows in the table and the query applied, returning an iterator on potentially unequal sized chunks. Here is a recipe for generating a query and using it to create equal sized return chunks. dfeq = pd.DataFrame({\"number\": np.arange(1, 11)}) dfeq number 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 store.append(\"dfeq\", dfeq, data_columns=[\"number\"]) def chunks(l, n): .....: return [l[i: i + n] for i in range(0, len(l), n)] .....: evens = [2, 4, 6, 8, 10] coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\") for c in chunks(coordinates, 2): .....: print(store.select(\"dfeq\", where=c)) .....: number 1 2 3 4 number 5 6 7 8 number 9 10", "prev_chunk_id": "chunk_248", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_250", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Select a single column#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Select a single column#", "content": "Select a single column# To retrieve a single indexable or data column, use the method select_column. This will, for example, enable you to get the index very quickly. These return a Series of the result, indexed by the row number. These do not currently accept the where selector. store.select_column(\"df_dc\", \"index\") 0 2000-01-01 1 2000-01-02 2 2000-01-03 3 2000-01-04 4 2000-01-05 5 2000-01-06 6 2000-01-07 7 2000-01-08 Name: index, dtype: datetime64[ns] store.select_column(\"df_dc\", \"string\") 0 foo 1 foo 2 foo 3 foo 4 NaN 5 NaN 6 foo 7 bar Name: string, dtype: object", "prev_chunk_id": "chunk_249", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_251", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Selecting coordinates#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Selecting coordinates#", "content": "Selecting coordinates# Sometimes you want to get the coordinates (a.k.a the index locations) of your query. This returns an Index of the resulting locations. These coordinates can also be passed to subsequent where operations. df_coord = pd.DataFrame( .....: np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000) .....: ) .....: store.append(\"df_coord\", df_coord) c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\") c Index([732, 733, 734, 735, 736, 737, 738, 739, 740, 741, ... 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], dtype='int64', length=268) store.select(\"df_coord\", where=c) 0 1 2002-01-02 0.007717 1.168386 2002-01-03 0.759328 -0.638934 2002-01-04 -1.154018 -0.324071 2002-01-05 -0.804551 -1.280593 2002-01-06 -0.047208 1.260503 ... ... ... 2002-09-22 -1.139583 0.344316 2002-09-23 -0.760643 -1.306704 2002-09-24 0.059018 1.775482 2002-09-25 1.242255 -0.055457 2002-09-26 0.410317 2.194489 [268 rows x 2 columns]", "prev_chunk_id": "chunk_250", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_252", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Selecting using a where mask#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Selecting using a where mask#", "content": "Selecting using a where mask# Sometime your query can involve creating a list of rows to select. Usually this mask would be a resulting index from an indexing operation. This example selects the months of a datetimeindex which are 5. df_mask = pd.DataFrame( .....: np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000) .....: ) .....: store.append(\"df_mask\", df_mask) c = store.select_column(\"df_mask\", \"index\") where = c[pd.DatetimeIndex(c).month == 5].index store.select(\"df_mask\", where=where) 0 1 2000-05-01 1.479511 0.516433 2000-05-02 -0.334984 -1.493537 2000-05-03 0.900321 0.049695 2000-05-04 0.614266 -1.077151 2000-05-05 0.233881 0.493246 ... ... ... 2002-05-27 0.294122 0.457407 2002-05-28 -1.102535 1.215650 2002-05-29 -0.432911 0.753606 2002-05-30 -1.105212 2.311877 2002-05-31 2.567296 2.610691 [93 rows x 2 columns]", "prev_chunk_id": "chunk_251", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_253", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Storer object#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Storer object#", "content": "Storer object# If you want to inspect the stored object, retrieve via get_storer. You could use this programmatically to say get the number of rows in an object. store.get_storer(\"df_dc\").nrows 8", "prev_chunk_id": "chunk_252", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_254", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Multiple table queries#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Multiple table queries#", "content": "Multiple table queries# The methods append_to_multiple and select_as_multiple can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables with an index matching the selector table’s index. You can then perform a very fast query on the selector table, yet get lots of data back. This method is similar to having a very wide table, but enables more efficient queries. The append_to_multiple method splits a given single DataFrame into multiple tables according to d, a dictionary that maps the table names to a list of ‘columns’ you want in that table. If None is used in place of a list, that table will have the remaining unspecified columns of the given DataFrame. The argument selector defines which table is the selector table (which you can make queries from). The argument dropna will drop rows from the input DataFrame to ensure tables are synchronized. This means that if a row for one of the tables being written to is entirely np.nan, that row will be dropped from all tables. If dropna is False, THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES. Remember that entirely np.Nan rows are not written to the HDFStore, so if you choose to call dropna=False, some tables may have more rows than others, and therefore select_as_multiple may not work or it may return unexpected results. df_mt = pd.DataFrame( .....: np.random.randn(8, 6), .....: index=pd.date_range(\"1/1/2000\", periods=8), .....: columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], .....: ) .....: df_mt[\"foo\"] = \"bar\" df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan # you can also create the tables individually store.append_to_multiple( .....: {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\" .....: ) .....: store <class 'pandas.io.pytables.HDFStore'> File path: store.h5 # individual tables", "prev_chunk_id": "chunk_253", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_255", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Multiple table queries#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Multiple table queries#", "content": "were created store.select(\"df1_mt\") A B 2000-01-01 0.162291 -0.430489 2000-01-02 NaN NaN 2000-01-03 0.429207 -1.099274 2000-01-04 1.869081 -1.466039 2000-01-05 0.092130 -1.726280 2000-01-06 0.266901 -0.036854 2000-01-07 -0.517871 -0.990317 2000-01-08 -0.231342 0.557402 store.select(\"df2_mt\") C D E F foo 2000-01-01 -2.502042 0.668149 0.460708 1.834518 bar 2000-01-02 0.130441 -0.608465 0.439872 0.506364 bar 2000-01-03 -1.069546 1.236277 0.116634 -1.772519 bar 2000-01-04 0.137462 0.313939 0.748471 -0.943009 bar 2000-01-05 0.836517 2.049798 0.562167 0.189952 bar 2000-01-06 1.112750 -0.151596 1.503311 0.939470 bar 2000-01-07 -0.294348 0.335844 -0.794159 1.495614 bar 2000-01-08 0.860312 -0.538674 -0.541986 -1.759606 bar # as a multiple store.select_as_multiple( .....: [\"df1_mt\", \"df2_mt\"], .....: where=[\"A>0\", \"B>0\"], .....: selector=\"df1_mt\", .....: ) .....: Empty DataFrame Columns: [A, B, C, D, E, F, foo] Index: []", "prev_chunk_id": "chunk_254", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_256", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Delete from a table#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Delete from a table#", "content": "Delete from a table# You can delete from a table selectively by specifying a where. In deleting rows, it is important to understand the PyTables deletes rows by erasing the rows, then moving the following data. Thus deleting can potentially be a very expensive operation depending on the orientation of your data. To get optimal performance, it’s worthwhile to have the dimension you are deleting be the first of the indexables. Data is ordered (on the disk) in terms of the indexables. Here’s a simple use case. You store panel-type data, with dates in the major_axis and ids in the minor_axis. The data is then interleaved like this: - date_1id_1id_2.id_n - date_2id_1.id_n It should be clear that a delete operation on the major_axis will be fairly quick, as one chunk is removed, then the following data moved. On the other hand a delete operation on the minor_axis will be very expensive. In this case it would almost certainly be faster to rewrite the table using a where that selects all but the missing data.", "prev_chunk_id": "chunk_255", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_257", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Compression#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Compression#", "content": "Compression# PyTables allows the stored data to be compressed. This applies to all kinds of stores, not just tables. Two parameters are used to control compression: complevel and complib. - complevelspecifies if and how hard data is to be compressed.complevel=0andcomplevel=Nonedisables compression and0<complevel<10enables compression. - complibspecifies which compression library to use. If nothing is specified the default libraryzlibis used. A compression library usually optimizes for either good compression rates or speed and the results will depend on the type of data. Which type of compression to choose depends on your specific needs and data. The list of supported compression libraries:zlib: The default compression library. A classic in terms of compression, achieves good compression rates but is somewhat slow.lzo: Fast compression and decompression.bzip2: Good compression rates.blosc: Fast compression and decompression.Support for alternative blosc compressors:blosc:blosclzThis is the default compressor forbloscblosc:lz4: A compact, very popular and fast compressor.blosc:lz4hc: A tweaked version of LZ4, produces better compression ratios at the expense of speed.blosc:snappy: A popular compressor used in many places.blosc:zlib: A classic; somewhat slower than the previous ones, but achieving better compression ratios.blosc:zstd: An extremely well balanced codec; it provides the best compression ratios among the others above, and at reasonably fast speed.Ifcomplibis defined as something other than the listed libraries aValueErrorexception is issued. Enable compression for all objects within the file: store_compressed = pd.HDFStore( \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\" ) Or on-the-fly compression (this only applies to tables) in stores where compression is not enabled: store.append(\"df\", df, complib=\"zlib\", complevel=5)", "prev_chunk_id": "chunk_256", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_258", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "ptrepack#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "ptrepack#", "content": "ptrepack# PyTables offers better write performance when tables are compressed after they are written, as opposed to turning on compression at the very beginning. You can use the supplied PyTables utility ptrepack. In addition, ptrepack can change compression levels after the fact. ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5 Furthermore ptrepack in.h5 out.h5 will repack the file to allow you to reuse previously deleted space. Alternatively, one can simply remove the file and write again, or use the copy method.", "prev_chunk_id": "chunk_257", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_259", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Caveats#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Caveats#", "content": "Caveats# - If you use locks to manage write access between multiple processes, you may want to usefsync()before releasing write locks. For convenience you can usestore.flush(fsync=True)to do this for you. - Once atableis created columns (DataFrame) are fixed; only exactly the same columns can be appended - Be aware that timezones (e.g.,pytz.timezone('US/Eastern')) are not necessarily equal across timezone versions. So if data is localized to a specific timezone in the HDFStore using one version of a timezone library and that data is updated with another version, the data will be converted to UTC since these timezones are not considered equal. Either use the same version of timezone library or usetz_convertwith the updated timezone definition.", "prev_chunk_id": "chunk_258", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_260", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "DataTypes#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "DataTypes#", "content": "DataTypes# HDFStore will map an object dtype to the PyTables underlying dtype. This means the following types are known to work: Type | Represents missing values floating : float64, float32, float16 | np.nan integer : int64, int32, int8, uint64,uint32, uint8 | boolean | datetime64[ns] | NaT timedelta64[ns] | NaT categorical : see the section below | object : strings | np.nan unicode columns are not supported, and WILL FAIL.", "prev_chunk_id": "chunk_259", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_261", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Categorical data#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Categorical data#", "content": "Categorical data# You can write data that contains category dtypes to a HDFStore. Queries work the same as if it was an object array. However, the category dtyped data is stored in a more efficient manner. dfcat = pd.DataFrame( .....: {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)} .....: ) .....: dfcat A B 0 a -1.520478 1 a -1.069391 2 b -0.551981 3 b 0.452407 4 c 0.409257 5 d 0.301911 6 b -0.640843 7 a -2.253022 dfcat.dtypes A category B float64 dtype: object cstore = pd.HDFStore(\"cats.h5\", mode=\"w\") cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"]) result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\") result A B 2 b -0.551981 3 b 0.452407 4 c 0.409257 6 b -0.640843 result.dtypes A category B float64 dtype: object", "prev_chunk_id": "chunk_260", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_262", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "String columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "String columns#", "content": "String columns# min_itemsize The underlying implementation of HDFStore uses a fixed column width (itemsize) for string columns. A string column itemsize is calculated as the maximum of the length of data (for that column) that is passed to the HDFStore, in the first append. Subsequent appends, may introduce a string for a column larger than the column can hold, an Exception will be raised (otherwise you could have a silent truncation of these columns, leading to loss of information). In the future we may relax this and allow a user-specified truncation to occur. Pass min_itemsize on the first table creation to a-priori specify the minimum length of a particular string column. min_itemsize can be an integer, or a dict mapping a column name to an integer. You can pass values as a key to allow all indexables or data_columns to have this min_itemsize. Passing a min_itemsize dict will cause all passed columns to be created as data_columns automatically. dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5))) dfs A B 0 foo bar 1 foo bar 2 foo bar 3 foo bar 4 foo bar # A and B have a size of 30 store.append(\"dfs\", dfs, min_itemsize=30) store.get_storer(\"dfs\").table /dfs/table (Table(5,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)} byteorder := 'little' chunkshape := (963,) autoindex := True colindexes := { \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False} # A is created as a data_column with a size of 30 # B is size is calculated store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30}) store.get_storer(\"dfs2\").table /dfs2/table (Table(5,)) '' description := { \"index\": Int64Col(shape=(), dflt=0, pos=0), \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1), \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)} byteorder := 'little' chunkshape := (1598,) autoindex := True colindexes := { \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False, \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False} nan_rep String columns will serialize a np.nan (a missing value)", "prev_chunk_id": "chunk_261", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_263", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "String columns#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "String columns#", "content": "with the nan_rep string representation. This defaults to the string value nan. You could inadvertently turn an actual nan value into a missing value. dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]}) dfss A 0 foo 1 bar 2 nan store.append(\"dfss\", dfss) store.select(\"dfss\") A 0 foo 1 bar 2 NaN # here you need to specify a different nan rep store.append(\"dfss2\", dfss, nan_rep=\"_nan_\") store.select(\"dfss2\") A 0 foo 1 bar 2 nan", "prev_chunk_id": "chunk_262", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_264", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Performance#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Performance#", "content": "Performance# - tablesformat come with a writing performance penalty as compared tofixedstores. The benefit is the ability to append/delete and query (potentially very large amounts of data). Write times are generally longer as compared with regular stores. Query times can be quite fast, especially on an indexed axis. - You can passchunksize=<int>toappend, specifying the write chunksize (default is 50000). This will significantly lower your memory usage on writing. - You can passexpectedrows=<int>to the firstappend, to set the TOTAL number of rows thatPyTableswill expect. This will optimize read/write performance. - Duplicate rows can be written to tables, but are filtered out in selection (with the last items being selected; thus a table is unique on major, minor pairs) - APerformanceWarningwill be raised if you are attempting to store types that will be pickled by PyTables (rather than stored as endemic types). SeeHerefor more information and some solutions.", "prev_chunk_id": "chunk_263", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_265", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Feather#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Feather#", "content": "Feather# Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Feather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz. Several caveats: - The format will NOT write anIndex, orMultiIndexfor theDataFrameand will raise an error if a non-default one is provided. You can.reset_index()to store the index or.reset_index(drop=True)to ignore it. - Duplicate column names and non-string columns names are not supported - Actual Python objects in object dtype columns are not supported. These will raise a helpful error message on an attempt at serialization. See the Full Documentation. df = pd.DataFrame( .....: { .....: \"a\": list(\"abc\"), .....: \"b\": list(range(1, 4)), .....: \"c\": np.arange(3, 6).astype(\"u1\"), .....: \"d\": np.arange(4.0, 7.0, dtype=\"float64\"), .....: \"e\": [True, False, True], .....: \"f\": pd.Categorical(list(\"abc\")), .....: \"g\": pd.date_range(\"20130101\", periods=3), .....: \"h\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"), .....: \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"), .....: } .....: ) .....: df a b c ... g h i 0 a 1 3 ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000 1 b 2 4 ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001 2 c 3 5 ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002 [3 rows x 9 columns] df.dtypes a object b int64 c uint8 d float64 e bool f category g datetime64[ns] h datetime64[ns, US/Eastern] i datetime64[ns] dtype: object Write to a feather file. df.to_feather(\"example.feather\") Read from a feather file. result = pd.read_feather(\"example.feather\") result a b c ... g h i 0 a 1 3 ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000 1 b 2 4 ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001 2 c 3 5 ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002 [3 rows x 9 columns] # we preserve dtypes result.dtypes a object b", "prev_chunk_id": "chunk_264", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_266", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Feather#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Feather#", "content": "int64 c uint8 d float64 e bool f category g datetime64[ns] h datetime64[ns, US/Eastern] i datetime64[ns] dtype: object", "prev_chunk_id": "chunk_265", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_267", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parquet#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parquet#", "content": "Parquet# Apache Parquet provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques to shrink the file size as much as possible while still maintaining good read performance. Parquet is designed to faithfully serialize and de-serialize DataFrame s, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz. Several caveats. - Duplicate column names and non-string columns names are not supported. - Thepyarrowengine always writes the index to the output, butfastparquetonly writes non-default indexes. This extra column can cause problems for non-pandas consumers that are not expecting it. You can force including or omitting indexes with theindexargument, regardless of the underlying engine. - Index level names, if specified, must be strings. - In thepyarrowengine, categorical dtypes for non-string types can be serialized to parquet, but will de-serialize as their primitive dtype. - Thepyarrowengine preserves theorderedflag of categorical dtypes with string types.fastparquetdoes not preserve theorderedflag. - Non supported types includeIntervaland actual Python object types. These will raise a helpful error message on an attempt at serialization.Periodtype is supported with pyarrow >= 0.16.0. - Thepyarrowengine preserves extension data types such as the nullable integer and string data type (requiring pyarrow >= 0.16.0, and requiring the extension type to implement the needed protocols, see theextension types documentation). You can specify an engine to direct the serialization. This can be one of pyarrow, or fastparquet, or auto. If the engine is NOT specified, then the pd.options.io.parquet.engine option is checked; if this is also auto, then pyarrow is tried, and falling back to fastparquet. See the documentation for pyarrow and fastparquet. df = pd.DataFrame( .....: { .....: \"a\": list(\"abc\"), .....: \"b\": list(range(1, 4)),", "prev_chunk_id": "chunk_266", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_268", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Parquet#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Parquet#", "content": ".....: \"c\": np.arange(3, 6).astype(\"u1\"), .....: \"d\": np.arange(4.0, 7.0, dtype=\"float64\"), .....: \"e\": [True, False, True], .....: \"f\": pd.date_range(\"20130101\", periods=3), .....: \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"), .....: \"h\": pd.Categorical(list(\"abc\")), .....: \"i\": pd.Categorical(list(\"abc\"), ordered=True), .....: } .....: ) .....: df a b c d e f g h i 0 a 1 3 4.0 True 2013-01-01 2013-01-01 00:00:00-05:00 a a 1 b 2 4 5.0 False 2013-01-02 2013-01-02 00:00:00-05:00 b b 2 c 3 5 6.0 True 2013-01-03 2013-01-03 00:00:00-05:00 c c df.dtypes a object b int64 c uint8 d float64 e bool f datetime64[ns] g datetime64[ns, US/Eastern] h category i category dtype: object Write to a parquet file. df.to_parquet(\"example_pa.parquet\", engine=\"pyarrow\") df.to_parquet(\"example_fp.parquet\", engine=\"fastparquet\") Read from a parquet file. result = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\") result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\") result.dtypes a object b int64 c uint8 d float64 e bool f datetime64[ns] g datetime64[ns, US/Eastern] h category i category dtype: object By setting the dtype_backend argument you can control the default dtypes used for the resulting DataFrame. result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\") result.dtypes a string[pyarrow] b int64[pyarrow] c uint8[pyarrow] d double[pyarrow] e bool[pyarrow] f timestamp[ns][pyarrow] g timestamp[ns, tz=US/Eastern][pyarrow] h dictionary<values=string, indices=int8, ordere... i dictionary<values=string, indices=int8, ordere... dtype: object Read only certain columns of a parquet file. result = pd.read_parquet( .....: \"example_fp.parquet\", .....: engine=\"fastparquet\", .....: columns=[\"a\", \"b\"], .....: ) .....: result = pd.read_parquet( .....: \"example_pa.parquet\", .....: engine=\"pyarrow\", .....: columns=[\"a\", \"b\"], .....: ) .....: result.dtypes a object b int64 dtype: object", "prev_chunk_id": "chunk_267", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_269", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Handling indexes#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Handling indexes#", "content": "Handling indexes# Serializing a DataFrame to parquet may include the implicit index as one or more columns in the output file. Thus, this code: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}) df.to_parquet(\"test.parquet\", engine=\"pyarrow\") creates a parquet file with three columns if you use pyarrow for serialization: a, b, and __index_level_0__. If you’re using fastparquet, the index may or may not be written to the file. This unexpected extra column causes some databases like Amazon Redshift to reject the file, because that column doesn’t exist in the target table. If you want to omit a dataframe’s indexes when writing, pass index=False to to_parquet(): df.to_parquet(\"test.parquet\", index=False) This creates a parquet file with just the two expected columns, a and b. If your DataFrame has a custom index, you won’t get it back when you load this file into a DataFrame. Passing index=True will always write the index, even if that’s not the underlying engine’s default behavior.", "prev_chunk_id": "chunk_268", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_270", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Partitioning Parquet files#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Partitioning Parquet files#", "content": "Partitioning Parquet files# Parquet supports partitioning of data based on the values of one or more columns. df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]}) df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None) The path specifies the parent directory to which data will be saved. The partition_cols are the column names by which the dataset will be partitioned. Columns are partitioned in the order they are given. The partition splits are determined by the unique values in the partition columns. The above example creates a partitioned dataset that may look like: test ├── a=0 │ ├── 0bac803e32dc42ae83fddfd029cbdebc.parquet │ └── ... └── a=1 ├── e6ab24a4f45147b49b54a662f0c412a3.parquet └── ...", "prev_chunk_id": "chunk_269", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_271", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "ORC#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "ORC#", "content": "ORC# Similar to the parquet format, the ORC Format is a binary columnar serialization for data frames. It is designed to make reading data frames efficient. pandas provides both the reader and the writer for the ORC format, read_orc() and to_orc(). This requires the pyarrow library. df = pd.DataFrame( .....: { .....: \"a\": list(\"abc\"), .....: \"b\": list(range(1, 4)), .....: \"c\": np.arange(4.0, 7.0, dtype=\"float64\"), .....: \"d\": [True, False, True], .....: \"e\": pd.date_range(\"20130101\", periods=3), .....: } .....: ) .....: df a b c d e 0 a 1 4.0 True 2013-01-01 1 b 2 5.0 False 2013-01-02 2 c 3 6.0 True 2013-01-03 df.dtypes a object b int64 c float64 d bool e datetime64[ns] dtype: object Write to an orc file. df.to_orc(\"example_pa.orc\", engine=\"pyarrow\") Read from an orc file. result = pd.read_orc(\"example_pa.orc\") result.dtypes a object b int64 c float64 d bool e datetime64[ns] dtype: object Read only certain columns of an orc file. result = pd.read_orc( .....: \"example_pa.orc\", .....: columns=[\"a\", \"b\"], .....: ) .....: result.dtypes a object b int64 dtype: object", "prev_chunk_id": "chunk_270", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_272", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SQL queries#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SQL queries#", "content": "SQL queries# The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. Where available, users may first want to opt for Apache Arrow ADBC drivers. These drivers should provide the best performance, null handling, and type detection. For a full list of ADBC drivers and their development status, see the ADBC Driver Implementation Status documentation. Where an ADBC driver is not available or may be missing functionality, users should opt for installing SQLAlchemy alongside their database driver library. Examples of such drivers are psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in Python’s standard library by default. You can find an overview of supported drivers for each SQL dialect in the SQLAlchemy docs. If SQLAlchemy is not installed, you can use a sqlite3.Connection in place of a SQLAlchemy engine, connection, or URI string. See also some cookbook examples for some advanced strategies. The key functions are: read_sql_table(table_name, con[, schema, ...]) | Read SQL database table into a DataFrame. read_sql_query(sql, con[, index_col, ...]) | Read SQL query into a DataFrame. read_sql(sql, con[, index_col, ...]) | Read SQL query or database table into a DataFrame. DataFrame.to_sql(name, con, *[, schema, ...]) | Write records stored in a DataFrame to a SQL database. In the following example, we use the SQlite SQL database engine. You can use a temporary SQLite database where data are stored in “memory”. To connect using an ADBC driver you will want to install the adbc_driver_sqlite using your package manager. Once installed, you can use the DBAPI interface provided by the ADBC driver to connect to your database. import adbc_driver_sqlite.dbapi as sqlite_dbapi # Create the connection with sqlite_dbapi.connect(\"sqlite:///:memory:\") as conn: df = pd.read_sql_table(\"data\", conn) To connect with SQLAlchemy you use the create_engine() function to create", "prev_chunk_id": "chunk_271", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_273", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SQL queries#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SQL queries#", "content": "an engine object from database URI. You only need to create the engine once per database you are connecting to. For more information on create_engine() and the URI formatting, see the examples below and the SQLAlchemy documentation from sqlalchemy import create_engine # Create your engine. engine = create_engine(\"sqlite:///:memory:\") If you want to manage your own connections you can pass one of those instead. The example below opens a connection to the database using a Python context manager that automatically closes the connection after the block has completed. See the SQLAlchemy docs for an explanation of how the database connection is handled. with engine.connect() as conn, conn.begin(): data = pd.read_sql_table(\"data\", conn)", "prev_chunk_id": "chunk_272", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_274", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing DataFrames#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing DataFrames#", "content": "Writing DataFrames# Assuming the following data is in a DataFrame data, we can insert it into the database using to_sql(). id | Date | Col_1 | Col_2 | Col_3 26 | 2012-10-18 | X | 25.7 | True 42 | 2012-10-19 | Y | -12.4 | False 63 | 2012-10-20 | Z | 5.73 | True import datetime c = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"] d = [ .....: (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True), .....: (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False), .....: (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True), .....: ] .....: data = pd.DataFrame(d, columns=c) data id Date Col_1 Col_2 Col_3 0 26 2010-10-18 X 27.50 True 1 42 2010-10-19 Y -12.50 False 2 63 2010-10-20 Z 5.73 True data.to_sql(\"data\", con=engine) 3 With some databases, writing large DataFrames can result in errors due to packet size limitations being exceeded. This can be avoided by setting the chunksize parameter when calling to_sql. For example, the following writes data to the database in batches of 1000 rows at a time: data.to_sql(\"data_chunked\", con=engine, chunksize=1000) 3", "prev_chunk_id": "chunk_273", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_275", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SQL data types#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SQL data types#", "content": "SQL data types# Ensuring consistent data type management across SQL databases is challenging. Not every SQL database offers the same types, and even when they do the implementation of a given type can vary in ways that have subtle effects on how types can be preserved. For the best odds at preserving database types users are advised to use ADBC drivers when available. The Arrow type system offers a wider array of types that more closely match database types than the historical pandas/NumPy type system. To illustrate, note this (non-exhaustive) listing of types available in different databases and pandas backends: numpy/pandas | arrow | postgres | sqlite int16/Int16 | int16 | SMALLINT | INTEGER int32/Int32 | int32 | INTEGER | INTEGER int64/Int64 | int64 | BIGINT | INTEGER float32 | float32 | REAL | REAL float64 | float64 | DOUBLE PRECISION | REAL object | string | TEXT | TEXT bool | bool_ | BOOLEAN | datetime64[ns] | timestamp(us) | TIMESTAMP | datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ | | date32 | DATE | | month_day_nano_interval | INTERVAL | | binary | BINARY | BLOB | decimal128 | DECIMAL [1] | | list | ARRAY [1] | | struct | COMPOSITE TYPE[1] | Footnotes If you are interested in preserving database types as best as possible throughout the lifecycle of your DataFrame, users are encouraged to leverage the dtype_backend=\"pyarrow\" argument of read_sql() # for roundtripping with pg_dbapi.connect(uri) as conn: df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\") This will prevent your data from being converted to the traditional pandas/NumPy type system, which often converts SQL types in ways that make them impossible to round-trip. In case an ADBC driver is not available, to_sql() will try to map your data to an appropriate SQL data type based on the dtype of the data. When you", "prev_chunk_id": "chunk_274", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_276", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SQL data types#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SQL data types#", "content": "have columns of dtype object, pandas will try to infer the data type. You can always override the default type by specifying the desired SQL type of any of the columns by using the dtype argument. This argument needs a dictionary mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback mode). For example, specifying to use the sqlalchemy String type instead of the default Text type for string columns: from sqlalchemy.types import String data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String}) 3", "prev_chunk_id": "chunk_275", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_277", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Datetime data types#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Datetime data types#", "content": "Datetime data types# Using ADBC or SQLAlchemy, to_sql() is capable of writing datetime data that is timezone naive or timezone aware. However, the resulting data stored in the database ultimately depends on the supported data type for datetime data of the database system being used. The following table lists supported data types for datetime data for some common databases. Other database dialects may have different data types for datetime data. Database | SQL Datetime Types | Timezone Support SQLite | TEXT | No MySQL | TIMESTAMP or DATETIME | No PostgreSQL | TIMESTAMP or TIMESTAMP WITH TIME ZONE | Yes When writing timezone aware data to databases that do not support timezones, the data will be written as timezone naive timestamps that are in local time with respect to the timezone. read_sql_table() is also capable of reading datetime data that is timezone aware or naive. When reading TIMESTAMP WITH TIME ZONE types, pandas will convert the data to UTC.", "prev_chunk_id": "chunk_276", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_278", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Insertion method#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Insertion method#", "content": "Insertion method# The parameter method controls the SQL insertion clause used. Possible values are: - None: Uses standard SQLINSERTclause (one per row). - 'multi': Pass multiple values in a singleINSERTclause. It uses aspecialSQL syntax not supported by all backends. This usually provides better performance for analytic databases likePrestoandRedshift, but has worse performance for traditional SQL backend if the table contains many columns. For more information check the SQLAlchemydocumentation. - callable with signature(pd_table,conn,keys,data_iter): This can be used to implement a more performant insertion method based on specific backend dialect features. Example of a callable using PostgreSQL COPY clause: # Alternative to_sql() *method* for DBs that support COPY FROM import csv from io import StringIO def psql_insert_copy(table, conn, keys, data_iter): \"\"\" Execute SQL statement inserting data Parameters ---------- table : pandas.io.sql.SQLTable conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection keys : list of str Column names data_iter : Iterable that iterates the values to be inserted \"\"\" # gets a DBAPI connection that can provide a cursor dbapi_conn = conn.connection with dbapi_conn.cursor() as cur: s_buf = StringIO() writer = csv.writer(s_buf) writer.writerows(data_iter) s_buf.seek(0) columns = ', '.join(['\"{}\"'.format(k) for k in keys]) if table.schema: table_name = '{}.{}'.format(table.schema, table.name) else: table_name = table.name sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format( table_name, columns) cur.copy_expert(sql=sql, file=s_buf)", "prev_chunk_id": "chunk_277", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_279", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading tables#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading tables#", "content": "Reading tables# read_sql_table() will read a database table given the table name and optionally a subset of columns to read. pd.read_sql_table(\"data\", engine) index id Date Col_1 Col_2 Col_3 0 0 26 2010-10-18 X 27.50 True 1 1 42 2010-10-19 Y -12.50 False 2 2 63 2010-10-20 Z 5.73 True You can also specify the name of the column as the DataFrame index, and specify a subset of columns to be read. pd.read_sql_table(\"data\", engine, index_col=\"id\") index Date Col_1 Col_2 Col_3 id 26 0 2010-10-18 X 27.50 True 42 1 2010-10-19 Y -12.50 False 63 2 2010-10-20 Z 5.73 True pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"]) Col_1 Col_2 0 X 27.50 1 Y -12.50 2 Z 5.73 And you can explicitly force columns to be parsed as dates: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"]) index id Date Col_1 Col_2 Col_3 0 0 26 2010-10-18 X 27.50 True 1 1 42 2010-10-19 Y -12.50 False 2 2 63 2010-10-20 Z 5.73 True If needed you can explicitly specify a format string, or a dict of arguments to pass to pandas.to_datetime(): pd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"}) pd.read_sql_table( \"data\", engine, parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}}, ) You can check if a table exists using has_table()", "prev_chunk_id": "chunk_278", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_280", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Schema support#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Schema support#", "content": "Schema support# Reading from and writing to different schema’s is supported through the schema keyword in the read_sql_table() and to_sql() functions. Note however that this depends on the database flavor (sqlite does not have schema’s). For example: df.to_sql(name=\"table\", con=engine, schema=\"other_schema\") pd.read_sql_table(\"table\", engine, schema=\"other_schema\")", "prev_chunk_id": "chunk_279", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_281", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Querying#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Querying#", "content": "Querying# You can query using raw SQL in the read_sql_query() function. In this case you must use the SQL variant appropriate for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which are database-agnostic. pd.read_sql_query(\"SELECT * FROM data\", engine) index id Date Col_1 Col_2 Col_3 0 0 26 2010-10-18 00:00:00.000000 X 27.50 1 1 1 42 2010-10-19 00:00:00.000000 Y -12.50 0 2 2 63 2010-10-20 00:00:00.000000 Z 5.73 1 Of course, you can specify a more “complex” query. pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine) id Col_1 Col_2 0 42 Y -12.5 The read_sql_query() function supports a chunksize argument. Specifying this will return an iterator through chunks of the query result: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\")) df.to_sql(name=\"data_chunks\", con=engine, index=False) 20 for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5): .....: print(chunk) .....: a b c 0 -0.395347 -0.822726 -0.363777 1 1.676124 -0.908102 -1.391346 2 -1.094269 0.278380 1.205899 3 1.503443 0.932171 -0.709459 4 -0.645944 -1.351389 0.132023 a b c 0 0.210427 0.192202 0.661949 1 1.690629 -1.046044 0.618697 2 -0.013863 1.314289 1.951611 3 -1.485026 0.304662 1.194757 4 -0.446717 0.528496 -0.657575 a b c 0 -0.876654 0.336252 0.172668 1 0.337684 -0.411202 -0.828394 2 -0.244413 1.094948 0.087183 3 1.125934 -1.480095 1.205944 4 -0.451849 0.452214 -2.208192 a b c 0 -2.061019 0.044184 -0.017118 1 1.248959 -0.675595 -1.908296 2 -0.125934 1.491974 0.648726 3 0.391214 0.438609 1.634248 4 1.208707 -1.535740 1.620399", "prev_chunk_id": "chunk_280", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_282", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Engine connection examples#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Engine connection examples#", "content": "Engine connection examples# To connect with SQLAlchemy you use the create_engine() function to create an engine object from database URI. You only need to create the engine once per database you are connecting to. from sqlalchemy import create_engine engine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\") engine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\") engine = create_engine(\"oracle://scott:[email protected]:1521/sidname\") engine = create_engine(\"mssql+pyodbc://mydsn\") # sqlite://<nohostname>/<path> # where <path> is relative: engine = create_engine(\"sqlite:///foo.db\") # or absolute, starting with a slash: engine = create_engine(\"sqlite:////absolute/path/to/foo.db\") For more information see the examples the SQLAlchemy documentation", "prev_chunk_id": "chunk_281", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_283", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Advanced SQLAlchemy queries#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Advanced SQLAlchemy queries#", "content": "Advanced SQLAlchemy queries# You can use SQLAlchemy constructs to describe your query. Use sqlalchemy.text() to specify query parameters in a backend-neutral way import sqlalchemy as sa pd.read_sql( .....: sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"} .....: ) .....: index id Date Col_1 Col_2 Col_3 0 0 26 2010-10-18 00:00:00.000000 X 27.5 1 If you have an SQLAlchemy description of your database you can express where conditions using SQLAlchemy expressions metadata = sa.MetaData() data_table = sa.Table( .....: \"data\", .....: metadata, .....: sa.Column(\"index\", sa.Integer), .....: sa.Column(\"Date\", sa.DateTime), .....: sa.Column(\"Col_1\", sa.String), .....: sa.Column(\"Col_2\", sa.Float), .....: sa.Column(\"Col_3\", sa.Boolean), .....: ) .....: pd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine) Empty DataFrame Columns: [index, Date, Col_1, Col_2, Col_3] Index: [] You can combine SQLAlchemy expressions with parameters passed to read_sql() using sqlalchemy.bindparam() import datetime as dt expr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\")) pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)}) index Date Col_1 Col_2 Col_3 0 1 2010-10-19 Y -12.50 False 1 2 2010-10-20 Z 5.73 True", "prev_chunk_id": "chunk_282", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_284", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Sqlite fallback#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Sqlite fallback#", "content": "Sqlite fallback# The use of sqlite is supported without using SQLAlchemy. This mode requires a Python database adapter which respect the Python DB-API. You can create connections like so: import sqlite3 con = sqlite3.connect(\":memory:\") And then issue the following queries: data.to_sql(\"data\", con) pd.read_sql_query(\"SELECT * FROM data\", con)", "prev_chunk_id": "chunk_283", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_285", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Google BigQuery#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Google BigQuery#", "content": "Google BigQuery# The pandas-gbq package provides functionality to read/write from Google BigQuery. pandas integrates with this external package. if pandas-gbq is installed, you can use the pandas methods pd.read_gbq and DataFrame.to_gbq, which will call the respective functions from pandas-gbq. Full documentation can be found here.", "prev_chunk_id": "chunk_284", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_286", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Writing to stata format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Writing to stata format#", "content": "Writing to stata format# The method DataFrame.to_stata() will write a DataFrame into a .dta file. The format version of this file is always 115 (Stata 12). df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\")) df.to_stata(\"stata.dta\") Stata data files have limited data type support; only strings with 244 or fewer characters, int8, int16, int32, float32 and float64 can be stored in .dta files. Additionally, Stata reserves certain values to represent missing data. Exporting a non-missing value that is outside of the permitted range in Stata for a particular data type will retype the variable to the next larger size. For example, int8 values are restricted to lie between -127 and 100 in Stata, and so variables with values above 100 will trigger a conversion to int16. nan values in floating points data types are stored as the basic missing data type (. in Stata). The Stata writer gracefully handles other data types including int64, bool, uint8, uint16, uint32 by casting to the smallest supported type that can represent the data. For example, data with a type of uint8 will be cast to int8 if all values are less than 100 (the upper bound for non-missing int8 data in Stata), or, if values are outside of this range, the variable is cast to int16.", "prev_chunk_id": "chunk_285", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_287", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Reading from Stata format#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Reading from Stata format#", "content": "Reading from Stata format# The top-level function read_stata will read a dta file and return either a DataFrame or a pandas.api.typing.StataReader that can be used to read the file incrementally. pd.read_stata(\"stata.dta\") index A B 0 0 -0.165614 0.490482 1 1 -0.637829 0.067091 2 2 -0.242577 1.348038 3 3 0.647699 -0.644937 4 4 0.625771 0.918376 5 5 0.401781 -1.488919 6 6 -0.981845 -0.046882 7 7 -0.306796 0.877025 8 8 -0.336606 0.624747 9 9 -1.582600 0.806340 Specifying a chunksize yields a pandas.api.typing.StataReader instance that can be used to read chunksize lines from the file at a time. The StataReader object can be used as an iterator. with pd.read_stata(\"stata.dta\", chunksize=3) as reader: .....: for df in reader: .....: print(df.shape) .....: (3, 3) (3, 3) (3, 3) (1, 3) For more fine-grained control, use iterator=True and specify chunksize with each call to read(). with pd.read_stata(\"stata.dta\", iterator=True) as reader: .....: chunk1 = reader.read(5) .....: chunk2 = reader.read(5) .....: Currently the index is retrieved as a column. The parameter convert_categoricals indicates whether value labels should be read and used to create a Categorical variable from them. Value labels can also be retrieved by the function value_labels, which requires read() to be called before use. The parameter convert_missing indicates whether missing value representations in Stata should be preserved. If False (the default), missing values are represented as np.nan. If True, missing values are represented using StataMissingValue objects, and columns containing missing values will have object data type.", "prev_chunk_id": "chunk_286", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_288", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Categorical data#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Categorical data#", "content": "Categorical data# Categorical data can be exported to Stata data files as value labeled data. The exported data consists of the underlying category codes as integer data values and the categories as value labels. Stata does not have an explicit equivalent to a Categorical and information about whether the variable is ordered is lost when exporting. Labeled data can similarly be imported from Stata data files as Categorical variables using the keyword argument convert_categoricals (True by default). The keyword argument order_categoricals (True by default) determines whether imported Categorical variables are ordered.", "prev_chunk_id": "chunk_287", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_289", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SAS formats#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SAS formats#", "content": "SAS formats# The top-level function read_sas() can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat) format files. SAS files only contain two value types: ASCII text and floating point values (usually 8 bytes but sometimes truncated). For xport files, there is no automatic type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format codes may allow date variables to be automatically converted to dates. By default the whole file is read and returned as a DataFrame. Specify a chunksize or use iterator=True to obtain reader objects (XportReader or SAS7BDATReader) for incrementally reading the file. The reader objects also have attributes that contain additional information about the file and its variables. Read a SAS7BDAT file: df = pd.read_sas(\"sas_data.sas7bdat\") Obtain an iterator and read an XPORT file 100,000 lines at a time: def do_something(chunk): pass with pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr: for chunk in rdr: do_something(chunk) The specification for the xport file format is available from the SAS web site. No official documentation is available for the SAS7BDAT format.", "prev_chunk_id": "chunk_288", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_290", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "SPSS formats#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "SPSS formats#", "content": "SPSS formats# The top-level function read_spss() can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav) format files. SPSS files contain column names. By default the whole file is read, categorical columns are converted into pd.Categorical, and a DataFrame with all columns is returned. Specify the usecols parameter to obtain a subset of columns. Specify convert_categoricals=False to avoid converting categorical columns into pd.Categorical. Read an SPSS file: df = pd.read_spss(\"spss_data.sav\") Extract a subset of columns contained in usecols from an SPSS file and avoid converting categorical columns into pd.Categorical: df = pd.read_spss( \"spss_data.sav\", usecols=[\"foo\", \"bar\"], convert_categoricals=False, ) More information about the SAV and ZSAV file formats is available here.", "prev_chunk_id": "chunk_289", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_291", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Other file formats#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Other file formats#", "content": "Other file formats# pandas itself only supports IO with a limited set of file formats that map cleanly to its tabular data model. For reading and writing other file formats into and from pandas, we recommend these packages from the broader community.", "prev_chunk_id": "chunk_290", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_292", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "netCDF#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "netCDF#", "content": "netCDF# xarray provides data structures inspired by the pandas DataFrame for working with multi-dimensional datasets, with a focus on the netCDF file format and easy conversion to and from pandas.", "prev_chunk_id": "chunk_291", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_293", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Performance considerations#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Performance considerations#", "content": "Performance considerations# This is an informal comparison of various IO methods, using pandas 0.24.2. Timings are machine dependent and small differences should be ignored. sz = 1000000 df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz}) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 2 columns): A 1000000 non-null float64 B 1000000 non-null int64 dtypes: float64(1), int64(1) memory usage: 15.3 MB The following test functions will be used below to compare the performance of several IO methods: import numpy as np import os sz = 1000000 df = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz}) sz = 1000000 np.random.seed(42) df = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz}) def test_sql_write(df): if os.path.exists(\"test.sql\"): os.remove(\"test.sql\") sql_db = sqlite3.connect(\"test.sql\") df.to_sql(name=\"test_table\", con=sql_db) sql_db.close() def test_sql_read(): sql_db = sqlite3.connect(\"test.sql\") pd.read_sql_query(\"select * from test_table\", sql_db) sql_db.close() def test_hdf_fixed_write(df): df.to_hdf(\"test_fixed.hdf\", key=\"test\", mode=\"w\") def test_hdf_fixed_read(): pd.read_hdf(\"test_fixed.hdf\", \"test\") def test_hdf_fixed_write_compress(df): df.to_hdf(\"test_fixed_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\") def test_hdf_fixed_read_compress(): pd.read_hdf(\"test_fixed_compress.hdf\", \"test\") def test_hdf_table_write(df): df.to_hdf(\"test_table.hdf\", key=\"test\", mode=\"w\", format=\"table\") def test_hdf_table_read(): pd.read_hdf(\"test_table.hdf\", \"test\") def test_hdf_table_write_compress(df): df.to_hdf( \"test_table_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\", format=\"table\" ) def test_hdf_table_read_compress(): pd.read_hdf(\"test_table_compress.hdf\", \"test\") def test_csv_write(df): df.to_csv(\"test.csv\", mode=\"w\") def test_csv_read(): pd.read_csv(\"test.csv\", index_col=0) def test_feather_write(df): df.to_feather(\"test.feather\") def test_feather_read(): pd.read_feather(\"test.feather\") def test_pickle_write(df): df.to_pickle(\"test.pkl\") def test_pickle_read(): pd.read_pickle(\"test.pkl\") def test_pickle_write_compress(df): df.to_pickle(\"test.pkl.compress\", compression=\"xz\") def test_pickle_read_compress(): pd.read_pickle(\"test.pkl.compress\", compression=\"xz\") def test_parquet_write(df): df.to_parquet(\"test.parquet\") def test_parquet_read(): pd.read_parquet(\"test.parquet\") When writing, the top three functions in terms of speed are test_feather_write, test_hdf_fixed_write and test_hdf_fixed_write_compress. %timeit test_sql_write(df) 3.29 s ± 43.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_hdf_fixed_write(df) 19.4 ms ± 560 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_hdf_fixed_write_compress(df) 19.6 ms ± 308 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_hdf_table_write(df) 449 ms ± 5.61 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_hdf_table_write_compress(df)", "prev_chunk_id": "chunk_292", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_294", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Performance considerations#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Performance considerations#", "content": "448 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_csv_write(df) 3.66 s ± 26.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_feather_write(df) 9.75 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit test_pickle_write(df) 30.1 ms ± 229 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_pickle_write_compress(df) 4.29 s ± 15.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_parquet_write(df) 67.6 ms ± 706 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) When reading, the top three functions in terms of speed are test_feather_read, test_pickle_read and test_hdf_fixed_read. %timeit test_sql_read() 1.77 s ± 17.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_hdf_fixed_read() 19.4 ms ± 436 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_hdf_fixed_read_compress() 19.5 ms ± 222 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_hdf_table_read() 38.6 ms ± 857 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_hdf_table_read_compress() 38.8 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit test_csv_read() 452 ms ± 9.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_feather_read() 12.4 ms ± 99.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit test_pickle_read() 18.4 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit test_pickle_read_compress() 915 ms ± 7.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit test_parquet_read()", "prev_chunk_id": "chunk_293", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_295", "url": "https://pandas.pydata.org/docs/user_guide/io.html", "title": "Performance considerations#", "page_title": "IO tools (text, CSV, HDF5, …) — pandas 2.3.1 documentation", "breadcrumbs": "Performance considerations#", "content": "24.4 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) The files test.pkl.compress, test.parquet and test.feather took the least space on disk (in bytes). 29519500 Oct 10 06:45 test.csv 16000248 Oct 10 06:45 test.feather 8281983 Oct 10 06:49 test.parquet 16000857 Oct 10 06:47 test.pkl 7552144 Oct 10 06:48 test.pkl.compress 34816000 Oct 10 06:42 test.sql 24009288 Oct 10 06:43 test_fixed.hdf 24009288 Oct 10 06:43 test_fixed_compress.hdf 24458940 Oct 10 06:44 test_table.hdf 24458940 Oct 10 06:44 test_table_compress.hdf", "prev_chunk_id": "chunk_294", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_296", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Indexing and selecting data#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Indexing and selecting data#", "content": "Indexing and selecting data# The axis labeling information in pandas objects serves many purposes: - Identifies data (i.e. providesmetadata) using known indicators, important for analysis, visualization, and interactive console display. - Enables automatic and explicit data alignment. - Allows intuitive getting and setting of subsets of the data set. In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area. See the MultiIndex / Advanced Indexing for MultiIndex and more advanced indexing documentation. See the cookbook for some advanced strategies.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_297", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Different choices for indexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Different choices for indexing#", "content": "Different choices for indexing# Object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing. - .locis primarily label based, but may also be used with a boolean array..locwill raiseKeyErrorwhen the items are not found. Allowed inputs are:A single label, e.g.5or'a'(Note that5is interpreted as alabelof the index. This use isnotan integer position along the index.).A list or array of labels['a','b','c'].A slice object with labels'a':'f'(Note that contrary to usual Python slices,boththe start and the stop are included, when present in the index! SeeSlicing with labelsandEndpoints are inclusive.)A boolean array (anyNAvalues will be treated asFalse).Acallablefunction with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).A tuple of row (and column) indices whose elements are one of the above inputs.See more atSelection by Label. - .ilocis primarily integer position based (from0tolength-1of the axis), but may also be used with a boolean array..ilocwill raiseIndexErrorif a requested indexer is out-of-bounds, exceptsliceindexers which allow out-of-bounds indexing. (this conforms with Python/NumPyslicesemantics). Allowed inputs are:An integer e.g.5.A list or array of integers[4,3,0].A slice object with ints1:7.A boolean array (anyNAvalues will be treated asFalse).Acallablefunction with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).A tuple of row (and column) indices whose elements are one of the above inputs.See more atSelection by Position,Advanced IndexingandAdvanced Hierarchical. - .loc,.iloc, and also[]indexing can accept acallableas indexer. See more atSelection By Callable.NoteDestructuring tuple keys into row (and column) indexes occursbeforecallables are applied, so you cannot return a tuple from a callable to index both rows and columns. Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as", "prev_chunk_id": "chunk_296", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_298", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Different choices for indexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Different choices for indexing#", "content": "well). Any of the axes accessors may be the null slice :. Axes left out of the specification are assumed to be :, e.g. p.loc['a'] is equivalent to p.loc['a', :]. ser = pd.Series(range(5), index=list(\"abcde\")) ser.loc[[\"a\", \"c\", \"e\"]] a 0 c 2 e 4 dtype: int64 df = pd.DataFrame(np.arange(25).reshape(5, 5), index=list(\"abcde\"), columns=list(\"abcde\")) df.loc[[\"a\", \"c\", \"e\"], [\"b\", \"d\"]] b d a 1 3 c 11 13 e 21 23", "prev_chunk_id": "chunk_297", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_299", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Basics#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Basics#", "content": "Basics# As mentioned when introducing the data structures in the last section, the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with []: Object Type | Selection | Return Value Type Series | series[label] | scalar value DataFrame | frame[colname] | Series corresponding to colname Here we construct a simple time series data set to use for illustrating the indexing functionality: dates = pd.date_range('1/1/2000', periods=8) df = pd.DataFrame(np.random.randn(8, 4), ...: index=dates, columns=['A', 'B', 'C', 'D']) ...: df A B C D 2000-01-01 0.469112 -0.282863 -1.509059 -1.135632 2000-01-02 1.212112 -0.173215 0.119209 -1.044236 2000-01-03 -0.861849 -2.104569 -0.494929 1.071804 2000-01-04 0.721555 -0.706771 -1.039575 0.271860 2000-01-05 -0.424972 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 0.524988 2000-01-07 0.404705 0.577046 -1.715002 -1.039268 2000-01-08 -0.370647 -1.157892 -1.344312 0.844885 Thus, as per above, we have the most basic indexing using []: s = df['A'] s[dates[5]] -0.6736897080883706 You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner: df A B C D 2000-01-01 0.469112 -0.282863 -1.509059 -1.135632 2000-01-02 1.212112 -0.173215 0.119209 -1.044236 2000-01-03 -0.861849 -2.104569 -0.494929 1.071804 2000-01-04 0.721555 -0.706771 -1.039575 0.271860 2000-01-05 -0.424972 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 0.524988 2000-01-07 0.404705 0.577046 -1.715002 -1.039268 2000-01-08 -0.370647 -1.157892 -1.344312 0.844885 df[['B', 'A']] = df[['A', 'B']] df A B C D 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 2000-01-04 -0.706771 0.721555 -1.039575 0.271860 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 You may find this", "prev_chunk_id": "chunk_298", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_300", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Basics#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Basics#", "content": "useful for applying a transform (in-place) to a subset of the columns.", "prev_chunk_id": "chunk_299", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_301", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Attribute access#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Attribute access#", "content": "Attribute access# You may access an index on a Series or column on a DataFrame directly as an attribute: sa = pd.Series([1, 2, 3], index=list('abc')) dfa = df.copy() sa.b 2 dfa.A 2000-01-01 -0.282863 2000-01-02 -0.173215 2000-01-03 -2.104569 2000-01-04 -0.706771 2000-01-05 0.567020 2000-01-06 0.113648 2000-01-07 0.577046 2000-01-08 -1.157892 Freq: D, Name: A, dtype: float64 sa.a = 5 sa a 5 b 2 c 3 dtype: int64 dfa.A = list(range(len(dfa.index))) # ok if A already exists dfa A B C D 2000-01-01 0 0.469112 -1.509059 -1.135632 2000-01-02 1 1.212112 0.119209 -1.044236 2000-01-03 2 -0.861849 -0.494929 1.071804 2000-01-04 3 0.721555 -1.039575 0.271860 2000-01-05 4 -0.424972 0.276232 -1.087401 2000-01-06 5 -0.673690 -1.478427 0.524988 2000-01-07 6 0.404705 -1.715002 -1.039268 2000-01-08 7 -0.370647 -1.344312 0.844885 dfa['A'] = list(range(len(dfa.index))) # use this form to create a new column dfa A B C D 2000-01-01 0 0.469112 -1.509059 -1.135632 2000-01-02 1 1.212112 0.119209 -1.044236 2000-01-03 2 -0.861849 -0.494929 1.071804 2000-01-04 3 0.721555 -1.039575 0.271860 2000-01-05 4 -0.424972 0.276232 -1.087401 2000-01-06 5 -0.673690 -1.478427 0.524988 2000-01-07 6 0.404705 -1.715002 -1.039268 2000-01-08 7 -0.370647 -1.344312 0.844885 If you are using the IPython environment, you may also use tab-completion to see these accessible attributes. You can also assign a dict to a row of a DataFrame: x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]}) x.iloc[1] = {'x': 9, 'y': 99} x x y 0 1 3 1 9 99 2 3 5 You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column and will this raise a UserWarning: df_new = pd.DataFrame({'one': [1., 2., 3.]}) df_new.two = [4, 5, 6] df_new one 0 1.0 1 2.0 2", "prev_chunk_id": "chunk_300", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_302", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Attribute access#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Attribute access#", "content": "3.0", "prev_chunk_id": "chunk_301", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_303", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Slicing ranges#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Slicing ranges#", "content": "Slicing ranges# The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator. With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels: s[:5] 2000-01-01 0.469112 2000-01-02 1.212112 2000-01-03 -0.861849 2000-01-04 0.721555 2000-01-05 -0.424972 Freq: D, Name: A, dtype: float64 s[::2] 2000-01-01 0.469112 2000-01-03 -0.861849 2000-01-05 -0.424972 2000-01-07 0.404705 Freq: 2D, Name: A, dtype: float64 s[::-1] 2000-01-08 -0.370647 2000-01-07 0.404705 2000-01-06 -0.673690 2000-01-05 -0.424972 2000-01-04 0.721555 2000-01-03 -0.861849 2000-01-02 1.212112 2000-01-01 0.469112 Freq: -1D, Name: A, dtype: float64 Note that setting works as well: s2 = s.copy() s2[:5] = 0 s2 2000-01-01 0.000000 2000-01-02 0.000000 2000-01-03 0.000000 2000-01-04 0.000000 2000-01-05 0.000000 2000-01-06 -0.673690 2000-01-07 0.404705 2000-01-08 -0.370647 Freq: D, Name: A, dtype: float64 With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation. df[:3] A B C D 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 df[::-1] A B C D 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 2000-01-04 -0.706771 0.721555 -1.039575 0.271860 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632", "prev_chunk_id": "chunk_302", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_304", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by label#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by label#", "content": "Selection by label# pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The .loc attribute is the primary access method. The following are valid inputs: - A single label, e.g.5or'a'(Note that5is interpreted as alabelof the index. This use isnotan integer position along the index.). - A list or array of labels['a','b','c']. - A slice object with labels'a':'f'(Note that contrary to usual Python slices,boththe start and the stop are included, when present in the index! SeeSlicing with labels. - A boolean array. - Acallable, seeSelection By Callable. s1 = pd.Series(np.random.randn(6), index=list('abcdef')) s1 a 1.431256 b 1.340309 c -1.170299 d -0.226169 e 0.410835 f 0.813850 dtype: float64 s1.loc['c':] c -1.170299 d -0.226169 e 0.410835 f 0.813850 dtype: float64 s1.loc['b'] 1.3403088497993827 Note that setting works as well: s1.loc['c':] = 0 s1 a 1.431256 b 1.340309 c 0.000000 d 0.000000 e 0.000000 f 0.000000 dtype: float64 With a DataFrame: df1 = pd.DataFrame(np.random.randn(6, 4), ....: index=list('abcdef'), ....: columns=list('ABCD')) ....: df1 A B C D a 0.132003 -0.827317 -0.076467 -1.187678 b 1.130127 -1.436737 -1.413681 1.607920 c 1.024180 0.569605 0.875906 -2.211372 d 0.974466 -2.006747 -0.410001 -0.078638 e 0.545952 -1.219217 -1.226825 0.769804 f -1.281247 -0.727707 -0.121306 -0.097883 df1.loc[['a', 'b', 'd'], :] A B C D a 0.132003 -0.827317 -0.076467 -1.187678 b 1.130127 -1.436737 -1.413681 1.607920 d 0.974466 -2.006747 -0.410001 -0.078638 Accessing via label slices: df1.loc['d':, 'A':'C'] A B C d 0.974466 -2.006747 -0.410001 e 0.545952 -1.219217 -1.226825 f -1.281247 -0.727707 -0.121306 For getting a cross section using a label (equivalent", "prev_chunk_id": "chunk_303", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_305", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by label#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by label#", "content": "to df.xs('a')): df1.loc['a'] A 0.132003 B -0.827317 C -0.076467 D -1.187678 Name: a, dtype: float64 For getting values with a boolean array: df1.loc['a'] > 0 A True B False C False D False Name: a, dtype: bool df1.loc[:, df1.loc['a'] > 0] A a 0.132003 b 1.130127 c 1.024180 d 0.974466 e 0.545952 f -1.281247 NA values in a boolean array propagate as False: mask = pd.array([True, False, True, False, pd.NA, False], dtype=\"boolean\") mask <BooleanArray> [True, False, True, False, <NA>, False] Length: 6, dtype: boolean df1[mask] A B C D a 0.132003 -0.827317 -0.076467 -1.187678 c 1.024180 0.569605 0.875906 -2.211372 For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` df1.loc['a', 'A'] 0.13200317033032932", "prev_chunk_id": "chunk_304", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_306", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Slicing with labels#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Slicing with labels#", "content": "Slicing with labels# When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4]) s.loc[3:5] 3 b 2 c 5 d dtype: object If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: s.sort_index() 0 a 2 c 3 b 4 e 5 d dtype: object s.sort_index().loc[1:6] 2 c 3 b 4 e 5 d dtype: object However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError. For the rationale behind this behavior, see Endpoints are inclusive. s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2]) s.loc[3:5] 3 b 2 c 5 d dtype: object Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError. For more information about duplicate labels, see Duplicate Labels.", "prev_chunk_id": "chunk_305", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_307", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by position#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by position#", "content": "Selection by position# pandas provides a suite of methods in order to get purely integer based indexing. The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included, while the upper bound is excluded. Trying to use a non-integer, even a valid label will raise an IndexError. The .iloc attribute is the primary access method. The following are valid inputs: - An integer e.g.5. - A list or array of integers[4,3,0]. - A slice object with ints1:7. - A boolean array. - Acallable, seeSelection By Callable. - A tuple of row (and column) indexes, whose elements are one of the above types. s1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2))) s1 0 0.695775 2 0.341734 4 0.959726 6 -1.110336 8 -0.619976 dtype: float64 s1.iloc[:3] 0 0.695775 2 0.341734 4 0.959726 dtype: float64 s1.iloc[3] -1.110336102891167 Note that setting works as well: s1.iloc[:3] = 0 s1 0 0.000000 2 0.000000 4 0.000000 6 -1.110336 8 -0.619976 dtype: float64 With a DataFrame: df1 = pd.DataFrame(np.random.randn(6, 4), ....: index=list(range(0, 12, 2)), ....: columns=list(range(0, 8, 2))) ....: df1 0 2 4 6 0 0.149748 -0.732339 0.687738 0.176444 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 6 -0.826591 -0.345352 1.314232 0.690579 8 0.995761 2.396780 0.014871 3.357427 10 -0.317441 -1.236269 0.896171 -0.487602 Select via integer slicing: df1.iloc[:3] 0 2 4 6 0 0.149748 -0.732339 0.687738 0.176444 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 df1.iloc[1:5, 2:4] 4 6 2 0.301624 -2.179861 4 1.462696 -1.743161 6 1.314232 0.690579 8 0.014871 3.357427 Select via integer list: df1.iloc[[1, 3, 5], [1, 3]] 2 6 2 -0.154951 -2.179861 6 -0.345352 0.690579 10 -1.236269 -0.487602 df1.iloc[1:3, :] 0 2 4 6 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 df1.iloc[:, 1:3] 2 4 0 -0.732339 0.687738 2 -0.154951 0.301624 4", "prev_chunk_id": "chunk_306", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_308", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by position#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by position#", "content": "-0.954208 1.462696 6 -0.345352 1.314232 8 2.396780 0.014871 10 -1.236269 0.896171 # this is also equivalent to ``df1.iat[1,1]`` df1.iloc[1, 1] -0.1549507744249032 For getting a cross section using an integer position (equiv to df.xs(1)): df1.iloc[1] 0 0.403310 2 -0.154951 4 0.301624 6 -2.179861 Name: 2, dtype: float64 Out of range slice indexes are handled gracefully just as in Python/NumPy. # these are allowed in Python/NumPy. x = list('abcdef') x ['a', 'b', 'c', 'd', 'e', 'f'] x[4:10] ['e', 'f'] x[8:10] [] s = pd.Series(x) s 0 a 1 b 2 c 3 d 4 e 5 f dtype: object s.iloc[4:10] 4 e 5 f dtype: object s.iloc[8:10] Series([], dtype: object) Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned). dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB')) dfl A B 0 -0.082240 -2.182937 1 0.380396 0.084844 2 0.432390 1.519970 3 -0.493662 0.600178 4 0.274230 0.132885 dfl.iloc[:, 2:3] Empty DataFrame Columns: [] Index: [0, 1, 2, 3, 4] dfl.iloc[:, 1:3] B 0 -2.182937 1 0.084844 2 1.519970 3 0.600178 4 0.132885 dfl.iloc[4:6] A B 4 0.27423 0.132885 A single indexer that is out of bounds will raise an IndexError. A list of indexers where any element is out of bounds will raise an IndexError. dfl.iloc[[4, 5, 6]] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexing.py:1714, in _iLocIndexer._get_list_axis(self, key, axis) 1713 try: -> 1714 return self.obj._take_with_is_copy(key, axis=axis) 1715 except IndexError as err: 1716 # re-raise with different error message, e.g. test_getitem_ndarray_3d File ~/work/pandas/pandas/pandas/core/generic.py:4172, in NDFrame._take_with_is_copy(self, indices, axis) 4163 \"\"\" 4164 Internal version of the `take` method that sets the `_is_copy` 4165 attribute to keep track of the parent dataframe (using in indexing (...) 4170 See the docstring of `take` for full explanation of the parameters. 4171 \"\"\" -> 4172 result = self.take(indices=indices, axis=axis) 4173", "prev_chunk_id": "chunk_307", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_309", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by position#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by position#", "content": "# Maybe set copy if we didn't actually change the index. File ~/work/pandas/pandas/pandas/core/generic.py:4152, in NDFrame.take(self, indices, axis, **kwargs) 4148 indices = np.arange( 4149 indices.start, indices.stop, indices.step, dtype=np.intp 4150 ) -> 4152 new_data = self._mgr.take( 4153 indices, 4154 axis=self._get_block_manager_axis(axis), 4155 verify=True, 4156 ) 4157 return self._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__( 4158 self, method=\"take\" 4159 ) File ~/work/pandas/pandas/pandas/core/internals/managers.py:891, in BaseBlockManager.take(self, indexer, axis, verify) 890 n = self.shape[axis] --> 891 indexer = maybe_convert_indices(indexer, n, verify=verify) 893 new_labels = self.axes[axis].take(indexer) File ~/work/pandas/pandas/pandas/core/indexers/utils.py:282, in maybe_convert_indices(indices, n, verify) 281 if mask.any(): --> 282 raise IndexError(\"indices are out-of-bounds\") 283 return indices IndexError: indices are out-of-bounds The above exception was the direct cause of the following exception: IndexError Traceback (most recent call last) Cell In[100], line 1 ----> 1 dfl.iloc[[4, 5, 6]] File ~/work/pandas/pandas/pandas/core/indexing.py:1191, in _LocationIndexer.__getitem__(self, key) 1189 maybe_callable = com.apply_if_callable(key, self.obj) 1190 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable) -> 1191 return self._getitem_axis(maybe_callable, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1743, in _iLocIndexer._getitem_axis(self, key, axis) 1741 # a list of integers 1742 elif is_list_like_indexer(key): -> 1743 return self._get_list_axis(key, axis=axis) 1745 # a single integer 1746 else: 1747 key = item_from_zerodim(key) File ~/work/pandas/pandas/pandas/core/indexing.py:1717, in _iLocIndexer._get_list_axis(self, key, axis) 1714 return self.obj._take_with_is_copy(key, axis=axis) 1715 except IndexError as err: 1716 # re-raise with different error message, e.g. test_getitem_ndarray_3d -> 1717 raise IndexError(\"positional indexers are out-of-bounds\") from err IndexError: positional indexers are out-of-bounds dfl.iloc[:, 4] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) Cell In[101], line 1 ----> 1 dfl.iloc[:, 4] File ~/work/pandas/pandas/pandas/core/indexing.py:1184, in _LocationIndexer.__getitem__(self, key) 1182 if self._is_scalar_access(key): 1183 return self.obj._get_value(*key, takeable=self._takeable) -> 1184 return self._getitem_tuple(key) 1185 else: 1186 # we by definition only have the 0th axis 1187 axis = self.axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1690, in _iLocIndexer._getitem_tuple(self, tup) 1689 def _getitem_tuple(self, tup: tuple): -> 1690 tup = self._validate_tuple_indexer(tup) 1691 with suppress(IndexingError): 1692 return self._getitem_lowerdim(tup) File ~/work/pandas/pandas/pandas/core/indexing.py:966, in _LocationIndexer._validate_tuple_indexer(self, key) 964 for i, k in enumerate(key): 965 try: --> 966 self._validate_key(k, i) 967", "prev_chunk_id": "chunk_308", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_310", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by position#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by position#", "content": "except ValueError as err: 968 raise ValueError( 969 \"Location based indexing can only have \" 970 f\"[{self._valid_types}] types\" 971 ) from err File ~/work/pandas/pandas/pandas/core/indexing.py:1592, in _iLocIndexer._validate_key(self, key, axis) 1590 return 1591 elif is_integer(key): -> 1592 self._validate_integer(key, axis) 1593 elif isinstance(key, tuple): 1594 # a tuple should already have been caught by this point 1595 # so don't treat a tuple as a valid indexer 1596 raise IndexingError(\"Too many indexers\") File ~/work/pandas/pandas/pandas/core/indexing.py:1685, in _iLocIndexer._validate_integer(self, key, axis) 1683 len_axis = len(self.obj._get_axis(axis)) 1684 if key >= len_axis or key < -len_axis: -> 1685 raise IndexError(\"single positional indexer is out-of-bounds\") IndexError: single positional indexer is out-of-bounds", "prev_chunk_id": "chunk_309", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_311", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by callable#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by callable#", "content": "Selection by callable# .loc, .iloc, and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing. df1 = pd.DataFrame(np.random.randn(6, 4), .....: index=list('abcdef'), .....: columns=list('ABCD')) .....: df1 A B C D a -0.023688 2.410179 1.450520 0.206053 b -0.251905 -2.213588 1.063327 1.266143 c 0.299368 -0.863838 0.408204 -1.048089 d -0.025747 -0.988387 0.094055 1.262731 e 1.289997 0.082423 -0.055758 0.536580 f -0.489682 0.369374 -0.034571 -2.484478 df1.loc[lambda df: df['A'] > 0, :] A B C D c 0.299368 -0.863838 0.408204 -1.048089 e 1.289997 0.082423 -0.055758 0.536580 df1.loc[:, lambda df: ['A', 'B']] A B a -0.023688 2.410179 b -0.251905 -2.213588 c 0.299368 -0.863838 d -0.025747 -0.988387 e 1.289997 0.082423 f -0.489682 0.369374 df1.iloc[:, lambda df: [0, 1]] A B a -0.023688 2.410179 b -0.251905 -2.213588 c 0.299368 -0.863838 d -0.025747 -0.988387 e 1.289997 0.082423 f -0.489682 0.369374 df1[lambda df: df.columns[0]] a -0.023688 b -0.251905 c 0.299368 d -0.025747 e 1.289997 f -0.489682 Name: A, dtype: float64 You can use callable indexing in Series. df1['A'].loc[lambda s: s > 0] c 0.299368 e 1.289997 Name: A, dtype: float64 Using these methods / indexers, you can chain data selection operations without using a temporary variable. bb = pd.read_csv('data/baseball.csv', index_col='id') (bb.groupby(['year', 'team']).sum(numeric_only=True) .....: .loc[lambda df: df['r'] > 100]) .....: stint g ab r h X2b ... so ibb hbp sh sf gidp year team ... 2007 CIN 6 379 745 101 203 35 ... 127.0 14.0 1.0 1.0 15.0 18.0 DET 5 301 1062 162 283 54 ... 176.0 3.0 10.0 4.0 8.0 28.0 HOU 4 311 926 109 218 47 ... 212.0 3.0 9.0 16.0 6.0 17.0 LAN 11 413 1021 153 293 61 ... 141.0 8.0 9.0 3.0 8.0 29.0 NYN 13 622 1854 240 509 101 ... 310.0 24.0", "prev_chunk_id": "chunk_310", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_312", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selection by callable#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selection by callable#", "content": "23.0 18.0 15.0 48.0 SFN 5 482 1305 198 337 67 ... 188.0 51.0 8.0 16.0 6.0 41.0 TEX 2 198 729 115 200 40 ... 140.0 4.0 5.0 2.0 8.0 16.0 TOR 4 459 1408 187 378 96 ... 265.0 16.0 12.0 4.0 16.0 38.0 [8 rows x 18 columns]", "prev_chunk_id": "chunk_311", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_313", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Combining positional and label-based indexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Combining positional and label-based indexing#", "content": "Combining positional and label-based indexing# If you wish to get the 0th and the 2nd elements from the index in the ‘A’ column, you can do: dfd = pd.DataFrame({'A': [1, 2, 3], .....: 'B': [4, 5, 6]}, .....: index=list('abc')) .....: dfd A B a 1 4 b 2 5 c 3 6 dfd.loc[dfd.index[[0, 2]], 'A'] a 1 c 3 Name: A, dtype: int64 This can also be expressed using .iloc, by explicitly getting locations on the indexers, and using positional indexing to select things. dfd.iloc[[0, 2], dfd.columns.get_loc('A')] a 1 c 3 Name: A, dtype: int64 For getting multiple indexers, using .get_indexer: dfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])] A B a 1 4 c 3 6", "prev_chunk_id": "chunk_312", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_314", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Reindexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Reindexing#", "content": "Reindexing# The idiomatic way to achieve selecting potentially not-found elements is via .reindex(). See also the section on reindexing. s = pd.Series([1, 2, 3]) s.reindex([1, 2, 3]) 1 2.0 2 3.0 3 NaN dtype: float64 Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection. labels = [1, 2, 3] s.loc[s.index.intersection(labels)] 1 2 2 3 dtype: int64 Having a duplicated index will raise for a .reindex(): s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c']) labels = ['c', 'd'] s.reindex(labels) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[122], line 1 ----> 1 s.reindex(labels) File ~/work/pandas/pandas/pandas/core/series.py:5164, in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance) 5147 @doc( 5148 NDFrame.reindex, # type: ignore[has-type] 5149 klass=_shared_doc_kwargs[\"klass\"], (...) 5162 tolerance=None, 5163 ) -> Series: -> 5164 return super().reindex( 5165 index=index, 5166 method=method, 5167 copy=copy, 5168 level=level, 5169 fill_value=fill_value, 5170 limit=limit, 5171 tolerance=tolerance, 5172 ) File ~/work/pandas/pandas/pandas/core/generic.py:5629, in NDFrame.reindex(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance) 5626 return self._reindex_multi(axes, copy, fill_value) 5628 # perform the reindex on the axes -> 5629 return self._reindex_axes( 5630 axes, level, limit, tolerance, method, fill_value, copy 5631 ).__finalize__(self, method=\"reindex\") File ~/work/pandas/pandas/pandas/core/generic.py:5652, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy) 5649 continue 5651 ax = self._get_axis(a) -> 5652 new_index, indexer = ax.reindex( 5653 labels, level=level, limit=limit, tolerance=tolerance, method=method 5654 ) 5656 axis = self._get_axis_number(a) 5657 obj = obj._reindex_with_indexers( 5658 {axis: [new_index, indexer]}, 5659 fill_value=fill_value, 5660 copy=copy, 5661 allow_dups=False, 5662 ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:4436, in Index.reindex(self, target, method, level, limit, tolerance) 4433 raise ValueError(\"cannot handle a non-unique multi-index!\") 4434 elif not self.is_unique: 4435 # GH#42568 -> 4436 raise ValueError(\"cannot reindex on an axis with duplicate labels\") 4437 else: 4438 indexer, _ = self.get_indexer_non_unique(target) ValueError: cannot reindex on an axis with duplicate labels Generally, you", "prev_chunk_id": "chunk_313", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_315", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Reindexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Reindexing#", "content": "can intersect the desired labels with the current axis, and then reindex. s.loc[s.index.intersection(labels)].reindex(labels) c 3.0 d NaN dtype: float64 However, this would still raise if your resulting index is duplicated. labels = ['a', 'd'] s.loc[s.index.intersection(labels)].reindex(labels) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[125], line 1 ----> 1 s.loc[s.index.intersection(labels)].reindex(labels) File ~/work/pandas/pandas/pandas/core/series.py:5164, in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance) 5147 @doc( 5148 NDFrame.reindex, # type: ignore[has-type] 5149 klass=_shared_doc_kwargs[\"klass\"], (...) 5162 tolerance=None, 5163 ) -> Series: -> 5164 return super().reindex( 5165 index=index, 5166 method=method, 5167 copy=copy, 5168 level=level, 5169 fill_value=fill_value, 5170 limit=limit, 5171 tolerance=tolerance, 5172 ) File ~/work/pandas/pandas/pandas/core/generic.py:5629, in NDFrame.reindex(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance) 5626 return self._reindex_multi(axes, copy, fill_value) 5628 # perform the reindex on the axes -> 5629 return self._reindex_axes( 5630 axes, level, limit, tolerance, method, fill_value, copy 5631 ).__finalize__(self, method=\"reindex\") File ~/work/pandas/pandas/pandas/core/generic.py:5652, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy) 5649 continue 5651 ax = self._get_axis(a) -> 5652 new_index, indexer = ax.reindex( 5653 labels, level=level, limit=limit, tolerance=tolerance, method=method 5654 ) 5656 axis = self._get_axis_number(a) 5657 obj = obj._reindex_with_indexers( 5658 {axis: [new_index, indexer]}, 5659 fill_value=fill_value, 5660 copy=copy, 5661 allow_dups=False, 5662 ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:4436, in Index.reindex(self, target, method, level, limit, tolerance) 4433 raise ValueError(\"cannot handle a non-unique multi-index!\") 4434 elif not self.is_unique: 4435 # GH#42568 -> 4436 raise ValueError(\"cannot reindex on an axis with duplicate labels\") 4437 else: 4438 indexer, _ = self.get_indexer_non_unique(target) ValueError: cannot reindex on an axis with duplicate labels", "prev_chunk_id": "chunk_314", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_316", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selecting random samples#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selecting random samples#", "content": "Selecting random samples# A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. s = pd.Series([0, 1, 2, 3, 4, 5]) # When no arguments are passed, returns 1 row. s.sample() 4 4 dtype: int64 # One may specify either a number of rows: s.sample(n=3) 0 0 4 4 1 1 dtype: int64 # Or a fraction of the rows: s.sample(frac=0.5) 5 5 3 3 1 1 dtype: int64 By default, sample will return each row at most once, but one can also sample with replacement using the replace option: s = pd.Series([0, 1, 2, 3, 4, 5]) # Without replacement (default): s.sample(n=6, replace=False) 0 0 1 1 5 5 3 3 2 2 4 4 dtype: int64 # With replacement: s.sample(n=6, replace=True) 0 0 4 4 3 3 2 2 4 4 4 4 dtype: int64 By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: s = pd.Series([0, 1, 2, 3, 4, 5]) example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4] s.sample(n=3, weights=example_weights) 5 5 4 4 3 3 dtype: int64 # Weights will be re-normalized automatically example_weights2 = [0.5, 0, 0, 0, 0, 0] s.sample(n=1,", "prev_chunk_id": "chunk_315", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_317", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Selecting random samples#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Selecting random samples#", "content": "weights=example_weights2) 0 0 dtype: int64 When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. df2 = pd.DataFrame({'col1': [9, 8, 7, 6], .....: 'weight_column': [0.5, 0.4, 0.1, 0]}) .....: df2.sample(n=3, weights='weight_column') col1 weight_column 1 8 0.4 0 9 0.5 2 7 0.1 sample also allows users to sample columns instead of rows using the axis argument. df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]}) df3.sample(n=1, axis=1) col1 0 1 1 2 2 3 Finally, one can also set a seed for sample’s random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object. df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]}) # With a given seed, the sample will always draw the same rows. df4.sample(n=2, random_state=2) col1 col2 2 3 4 1 2 3 df4.sample(n=2, random_state=2) col1 col2 2 3 4 1 2 3", "prev_chunk_id": "chunk_316", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_318", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Setting with enlargement#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Setting with enlargement#", "content": "Setting with enlargement# The .loc/[] operations can perform enlargement when setting a non-existent key for that axis. In the Series case this is effectively an appending operation. se = pd.Series([1, 2, 3]) se 0 1 1 2 2 3 dtype: int64 se[5] = 5. se 0 1.0 1 2.0 2 3.0 5 5.0 dtype: float64 A DataFrame can be enlarged on either axis via .loc. dfi = pd.DataFrame(np.arange(6).reshape(3, 2), .....: columns=['A', 'B']) .....: dfi A B 0 0 1 1 2 3 2 4 5 dfi.loc[:, 'C'] = dfi.loc[:, 'A'] dfi A B C 0 0 1 0 1 2 3 2 2 4 5 4 This is like an append operation on the DataFrame. dfi.loc[3] = 5 dfi A B C 0 0 1 0 1 2 3 2 2 4 5 4 3 5 5 5", "prev_chunk_id": "chunk_317", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_319", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Fast scalar value getting and setting#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Fast scalar value getting and setting#", "content": "Fast scalar value getting and setting# Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you’re asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures. Similarly to loc, at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc s.iat[5] 5 df.at[dates[5], 'A'] 0.1136484096888855 df.iat[3, 0] -0.7067711336300845 You can also set using these same indexers. df.at[dates[5], 'E'] = 7 df.iat[3, 0] = 7 at may enlarge the object in-place as above if the indexer is missing. df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7 df A B C D E 0 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 NaN NaN 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 NaN NaN 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 NaN NaN 2000-01-04 7.000000 0.721555 -1.039575 0.271860 NaN NaN 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 NaN NaN 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 7.0 NaN 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 NaN NaN 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 NaN NaN 2000-01-09 NaN NaN NaN NaN NaN 7.0", "prev_chunk_id": "chunk_318", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_320", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Boolean indexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Boolean indexing#", "content": "Boolean indexing# Another common operation is the use of boolean vectors to filter the data. The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3, while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3). Using a boolean vector to index a Series works exactly as in a NumPy ndarray: s = pd.Series(range(-3, 4)) s 0 -3 1 -2 2 -1 3 0 4 1 5 2 6 3 dtype: int64 s[s > 0] 4 1 5 2 6 3 dtype: int64 s[(s < -1) | (s > 0.5)] 0 -3 1 -2 4 1 5 2 6 3 dtype: int64 s[~(s < 0)] 3 0 4 1 5 2 6 3 dtype: int64 You may select rows from a DataFrame using a boolean vector the same length as the DataFrame’s index (for example, something derived from one of the columns of the DataFrame): df[df['A'] > 0] A B C D E 0 2000-01-04 7.000000 0.721555 -1.039575 0.271860 NaN NaN 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 NaN NaN 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 7.0 NaN 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 NaN NaN List comprehensions and the map method of Series can also be used to produce more complex criteria: df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], .....: 'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'], .....: 'c': np.random.randn(7)}) .....: # only want 'two' or 'three' criterion = df2['a'].map(lambda x: x.startswith('t')) df2[criterion] a b c 2 two y 0.041290 3 three x 0.361719 4 two y -0.238075 # equivalent but slower df2[[x.startswith('t') for x in df2['a']]] a b c 2 two", "prev_chunk_id": "chunk_319", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_321", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Boolean indexing#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Boolean indexing#", "content": "y 0.041290 3 three x 0.361719 4 two y -0.238075 # Multiple criteria df2[criterion & (df2['b'] == 'x')] a b c 3 three x 0.361719 With the choice methods Selection by Label, Selection by Position, and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions. df2.loc[criterion & (df2['b'] == 'x'), 'b':'c'] b c 3 x 0.361719", "prev_chunk_id": "chunk_320", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_322", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Indexing with isin#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with isin#", "content": "Indexing with isin# Consider the isin() method of Series, which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want: s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64') s 4 0 3 1 2 2 1 3 0 4 dtype: int64 s.isin([2, 4, 6]) 4 False 3 False 2 True 1 False 0 True dtype: bool s[s.isin([2, 4, 6])] 2 2 0 4 dtype: int64 The same method is available for Index objects and is useful for the cases when you don’t know which of the sought labels are in fact present: s[s.index.isin([2, 4, 6])] 4 0 2 2 dtype: int64 # compare it to the following s.reindex([2, 4, 6]) 2 2.0 4 0.0 6 NaN dtype: float64 In addition to that, MultiIndex allows selecting a separate level to use in the membership check: s_mi = pd.Series(np.arange(6), .....: index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']])) .....: s_mi 0 a 0 b 1 c 2 1 a 3 b 4 c 5 dtype: int64 s_mi.iloc[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])] 0 c 2 1 a 3 dtype: int64 s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)] 0 a 0 c 2 1 a 3 c 5 dtype: int64 DataFrame also has an isin() method. When calling isin, pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values. df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'], .....: 'ids2': ['a', 'n', 'c', 'n']}) .....: values = ['a', 'b', 1, 3] df.isin(values) vals ids ids2 0 True True True 1 False True False 2 True False", "prev_chunk_id": "chunk_321", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_323", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Indexing with isin#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with isin#", "content": "False 3 False False False Oftentimes you’ll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for. values = {'ids': ['a', 'b'], 'vals': [1, 3]} df.isin(values) vals ids ids2 0 True True False 1 False True False 2 True False False 3 False False False To return the DataFrame of booleans where the values are not in the original DataFrame, use the ~ operator: values = {'ids': ['a', 'b'], 'vals': [1, 3]} ~df.isin(values) vals ids ids2 0 False False True 1 True False True 2 False True True 3 True True True Combine DataFrame’s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion: values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]} row_mask = df.isin(values).all(1) df[row_mask] vals ids ids2 0 1 a a", "prev_chunk_id": "chunk_322", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_324", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The where() Method and Masking#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The where() Method and Masking#", "content": "The where() Method and Masking# Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the where method in Series and DataFrame. To return only the selected rows: s[s > 0] 3 1 2 2 1 3 0 4 dtype: int64 To return a Series of the same shape as the original: s.where(s > 0) 4 NaN 3 1.0 2 2.0 1 3.0 0 4.0 dtype: float64 Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation. The code below is equivalent to df.where(df < 0). dates = pd.date_range('1/1/2000', periods=8) df = pd.DataFrame(np.random.randn(8, 4), .....: index=dates, columns=['A', 'B', 'C', 'D']) .....: df[df < 0] A B C D 2000-01-01 -2.104139 -1.309525 NaN NaN 2000-01-02 -0.352480 NaN -1.192319 NaN 2000-01-03 -0.864883 NaN -0.227870 NaN 2000-01-04 NaN -1.222082 NaN -1.233203 2000-01-05 NaN -0.605656 -1.169184 NaN 2000-01-06 NaN -0.948458 NaN -0.684718 2000-01-07 -2.670153 -0.114722 NaN -0.048048 2000-01-08 NaN NaN -0.048788 -0.808838 In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy. df.where(df < 0, -df) A B C D 2000-01-01 -2.104139 -1.309525 -0.485855 -0.245166 2000-01-02 -0.352480 -0.390389 -1.192319 -1.655824 2000-01-03 -0.864883 -0.299674 -0.227870 -0.281059 2000-01-04 -0.846958 -1.222082 -0.600705 -1.233203 2000-01-05 -0.669692 -0.605656 -1.169184 -0.342416 2000-01-06 -0.868584 -0.948458 -2.297780 -0.684718 2000-01-07 -2.670153 -0.114722 -0.168904 -0.048048 2000-01-08 -0.801196 -1.392071 -0.048788 -0.808838 You may wish to set values based on some boolean criteria. This can be done intuitively like so: s2 = s.copy() s2[s2 < 0] = 0 s2 4 0 3 1 2 2 1 3 0 4 dtype: int64 df2 = df.copy() df2[df2 < 0] = 0", "prev_chunk_id": "chunk_323", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_325", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The where() Method and Masking#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The where() Method and Masking#", "content": "df2 A B C D 2000-01-01 0.000000 0.000000 0.485855 0.245166 2000-01-02 0.000000 0.390389 0.000000 1.655824 2000-01-03 0.000000 0.299674 0.000000 0.281059 2000-01-04 0.846958 0.000000 0.600705 0.000000 2000-01-05 0.669692 0.000000 0.000000 0.342416 2000-01-06 0.868584 0.000000 2.297780 0.000000 2000-01-07 0.000000 0.000000 0.168904 0.000000 2000-01-08 0.801196 1.392071 0.000000 0.000000 where returns a modified copy of the data. Alignment Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels). df2 = df.copy() df2[df2[1:4] > 0] = 3 df2 A B C D 2000-01-01 -2.104139 -1.309525 0.485855 0.245166 2000-01-02 -0.352480 3.000000 -1.192319 3.000000 2000-01-03 -0.864883 3.000000 -0.227870 3.000000 2000-01-04 3.000000 -1.222082 3.000000 -1.233203 2000-01-05 0.669692 -0.605656 -1.169184 0.342416 2000-01-06 0.868584 -0.948458 2.297780 -0.684718 2000-01-07 -2.670153 -0.114722 0.168904 -0.048048 2000-01-08 0.801196 1.392071 -0.048788 -0.808838 Where can also accept axis and level parameters to align the input when performing the where. df2 = df.copy() df2.where(df2 > 0, df2['A'], axis='index') A B C D 2000-01-01 -2.104139 -2.104139 0.485855 0.245166 2000-01-02 -0.352480 0.390389 -0.352480 1.655824 2000-01-03 -0.864883 0.299674 -0.864883 0.281059 2000-01-04 0.846958 0.846958 0.600705 0.846958 2000-01-05 0.669692 0.669692 0.669692 0.342416 2000-01-06 0.868584 0.868584 2.297780 0.868584 2000-01-07 -2.670153 -2.670153 0.168904 -2.670153 2000-01-08 0.801196 1.392071 0.801196 0.801196 This is equivalent to (but faster than) the following. df2 = df.copy() df.apply(lambda x, y: x.where(x > 0, y), y=df['A']) A B C D 2000-01-01 -2.104139 -2.104139 0.485855 0.245166 2000-01-02 -0.352480 0.390389 -0.352480 1.655824 2000-01-03 -0.864883 0.299674 -0.864883 0.281059 2000-01-04 0.846958 0.846958 0.600705 0.846958 2000-01-05 0.669692 0.669692 0.669692 0.342416 2000-01-06 0.868584 0.868584 2.297780 0.868584 2000-01-07 -2.670153 -2.670153 0.168904 -2.670153 2000-01-08 0.801196 1.392071 0.801196 0.801196 where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid", "prev_chunk_id": "chunk_324", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_326", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The where() Method and Masking#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The where() Method and Masking#", "content": "output as condition and other argument. df3 = pd.DataFrame({'A': [1, 2, 3], .....: 'B': [4, 5, 6], .....: 'C': [7, 8, 9]}) .....: df3.where(lambda x: x > 4, lambda x: x + 10) A B C 0 11 14 7 1 12 5 8 2 13 6 9", "prev_chunk_id": "chunk_325", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_327", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Mask#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Mask#", "content": "Mask# mask() is the inverse boolean operation of where. s.mask(s >= 0) 4 NaN 3 NaN 2 NaN 1 NaN 0 NaN dtype: float64 df.mask(df >= 0) A B C D 2000-01-01 -2.104139 -1.309525 NaN NaN 2000-01-02 -0.352480 NaN -1.192319 NaN 2000-01-03 -0.864883 NaN -0.227870 NaN 2000-01-04 NaN -1.222082 NaN -1.233203 2000-01-05 NaN -0.605656 -1.169184 NaN 2000-01-06 NaN -0.948458 NaN -0.684718 2000-01-07 -2.670153 -0.114722 NaN -0.048048 2000-01-08 NaN NaN -0.048788 -0.808838", "prev_chunk_id": "chunk_326", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_328", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Setting with enlargement conditionally using numpy()#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Setting with enlargement conditionally using numpy()#", "content": "Setting with enlargement conditionally using numpy()# An alternative to where() is to use numpy.where(). Combined with setting a new column, you can use it to enlarge a DataFrame where the values are determined conditionally. Consider you have two choices to choose from in the following DataFrame. And you want to set a new column color to ‘green’ when the second column has ‘Z’. You can do the following: df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')}) df['color'] = np.where(df['col2'] == 'Z', 'green', 'red') df col1 col2 color 0 A Z green 1 B Z green 2 B X red 3 C Y red If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following. conditions = [ .....: (df['col2'] == 'Z') & (df['col1'] == 'A'), .....: (df['col2'] == 'Z') & (df['col1'] == 'B'), .....: (df['col1'] == 'B') .....: ] .....: choices = ['yellow', 'blue', 'purple'] df['color'] = np.select(conditions, choices, default='black') df col1 col2 color 0 A Z yellow 1 B Z blue 2 B X purple 3 C Y black", "prev_chunk_id": "chunk_327", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_329", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The query() Method#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The query() Method#", "content": "The query() Method# DataFrame objects have a query() method that allows selection using an expression. You can get the value of the frame where column b has values between the values of columns a and c. For example: n = 10 df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc')) df a b c 0 0.438921 0.118680 0.863670 1 0.138138 0.577363 0.686602 2 0.595307 0.564592 0.520630 3 0.913052 0.926075 0.616184 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 6 0.792342 0.216974 0.564056 7 0.397890 0.454131 0.915716 8 0.074315 0.437913 0.019794 9 0.559209 0.502065 0.026437 # pure python df[(df['a'] < df['b']) & (df['b'] < df['c'])] a b c 1 0.138138 0.577363 0.686602 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 7 0.397890 0.454131 0.915716 # query df.query('(a < b) & (b < c)') a b c 1 0.138138 0.577363 0.686602 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 7 0.397890 0.454131 0.915716 Do the same thing but fall back on a named index if there is no column with the name a. df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc')) df.index.name = 'a' df b c a 0 0 4 1 0 1 2 3 4 3 4 3 4 1 4 5 0 3 6 0 1 7 3 4 8 2 3 9 1 1 df.query('a < b and b < c') b c a 2 3 4 If instead you don’t want to or cannot name your index, you can use the name index in your query expression: df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc')) df b c 0 3 1 1 3 0 2 5 6 3 5 2 4 7 4 5 0 1 6 2 5 7 0 1 8 6 0 9 7 9 df.query('index < b < c') b c 2 5 6", "prev_chunk_id": "chunk_328", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_330", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "MultiIndex query() Syntax#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "MultiIndex query() Syntax#", "content": "MultiIndex query() Syntax# You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame: n = 10 colors = np.random.choice(['red', 'green'], size=n) foods = np.random.choice(['eggs', 'ham'], size=n) colors array(['red', 'red', 'red', 'green', 'green', 'green', 'green', 'green', 'green', 'green'], dtype='<U5') foods array(['ham', 'ham', 'eggs', 'eggs', 'eggs', 'ham', 'ham', 'eggs', 'eggs', 'eggs'], dtype='<U4') index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food']) df = pd.DataFrame(np.random.randn(n, 2), index=index) df 0 1 color food red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 green eggs -0.748199 1.318931 eggs -2.029766 0.792652 ham 0.461007 -0.542749 ham -0.305384 -0.479195 eggs 0.095031 -0.270099 eggs -0.707140 -0.773882 eggs 0.229453 0.304418 df.query('color == \"red\"') 0 1 color food red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 If the levels of the MultiIndex are unnamed, you can refer to them using special names: df.index.names = [None, None] df 0 1 red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 green eggs -0.748199 1.318931 eggs -2.029766 0.792652 ham 0.461007 -0.542749 ham -0.305384 -0.479195 eggs 0.095031 -0.270099 eggs -0.707140 -0.773882 eggs 0.229453 0.304418 df.query('ilevel_0 == \"red\"') 0 1 red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 The convention is ilevel_0, which means “index level 0” for the 0th level of the index.", "prev_chunk_id": "chunk_329", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_331", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "query() Use Cases#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "query() Use Cases#", "content": "query() Use Cases# A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc')) df a b c 0 0.224283 0.736107 0.139168 1 0.302827 0.657803 0.713897 2 0.611185 0.136624 0.984960 3 0.195246 0.123436 0.627712 4 0.618673 0.371660 0.047902 5 0.480088 0.062993 0.185760 6 0.568018 0.483467 0.445289 7 0.309040 0.274580 0.587101 8 0.258993 0.477769 0.370255 9 0.550459 0.840870 0.304611 df2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns) df2 a b c 0 0.357579 0.229800 0.596001 1 0.309059 0.957923 0.965663 2 0.123102 0.336914 0.318616 3 0.526506 0.323321 0.860813 4 0.518736 0.486514 0.384724 5 0.190804 0.505723 0.614533 6 0.891939 0.623977 0.676639 7 0.480559 0.378528 0.460858 8 0.420223 0.136404 0.141295 9 0.732206 0.419540 0.604675 10 0.604466 0.848974 0.896165 11 0.589168 0.920046 0.732716 expr = '0.0 <= a <= c <= 0.5' map(lambda frame: frame.query(expr), [df, df2]) <map at 0x7f1056b33400>", "prev_chunk_id": "chunk_330", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_332", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "query() Python versus pandas Syntax Comparison#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "query() Python versus pandas Syntax Comparison#", "content": "query() Python versus pandas Syntax Comparison# Full numpy-like syntax: df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc')) df a b c 0 7 8 9 1 1 0 7 2 2 7 2 3 6 2 2 4 2 6 3 5 3 8 2 6 1 7 2 7 5 1 5 8 9 8 0 9 1 5 0 df.query('(a < b) & (b < c)') a b c 0 7 8 9 df[(df['a'] < df['b']) & (df['b'] < df['c'])] a b c 0 7 8 9 Slightly nicer by removing the parentheses (comparison operators bind tighter than & and |): df.query('a < b & b < c') a b c 0 7 8 9 Use English instead of symbols: df.query('a < b and b < c') a b c 0 7 8 9 Pretty close to how you might write it on paper: df.query('a < b < c') a b c 0 7 8 9", "prev_chunk_id": "chunk_331", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_333", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "MultiIndex / advanced indexing#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "MultiIndex / advanced indexing#", "content": "MultiIndex / advanced indexing# This section covers indexing with a MultiIndex and other advanced indexing features. See the Indexing and Selecting Data for general indexing documentation. See the cookbook for some advanced strategies.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_334", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The in and not in operators#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The in and not in operators#", "content": "The in and not in operators# query() also supports special use of Python’s in and not in comparison operators, providing a succinct syntax for calling the isin method of a Series or DataFrame. # get all rows where columns \"a\" and \"b\" have overlapping values df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'), .....: 'c': np.random.randint(5, size=12), .....: 'd': np.random.randint(9, size=12)}) .....: df a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 df.query('a in b') a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 # How you'd do it in pure Python df[df['a'].isin(df['b'])] a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 df.query('a not in b') a b c d 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 # pure Python df[~df['a'].isin(df['b'])] a b c d 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 You can combine this with other expressions for very succinct queries: # rows where cols a and b have", "prev_chunk_id": "chunk_332", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_335", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Hierarchical indexing (MultiIndex)#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Hierarchical indexing (MultiIndex)#", "content": "Hierarchical indexing (MultiIndex)# Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d). In this section, we will show what exactly we mean by “hierarchical” indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing group by and pivoting and reshaping data, we’ll show non-trivial applications to illustrate how it aids in structuring data for analysis. See the cookbook for some advanced strategies.", "prev_chunk_id": "chunk_333", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_336", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "The in and not in operators#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "The in and not in operators#", "content": "overlapping values # and col c's values are less than col d's df.query('a in b and c < d') a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 4 c b 3 6 5 c b 0 2 # pure Python df[df['b'].isin(df['a']) & (df['c'] < df['d'])] a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 4 c b 3 6 5 c b 0 2 10 f c 0 6 11 f c 1 2", "prev_chunk_id": "chunk_334", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_337", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Creating a MultiIndex (hierarchical index) object#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Creating a MultiIndex (hierarchical index) object#", "content": "Creating a MultiIndex (hierarchical index) object# The MultiIndex object is the hierarchical analogue of the standard Index object which typically stores the axis labels in pandas objects. You can think of MultiIndex as an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays()), an array of tuples (using MultiIndex.from_tuples()), a crossed set of iterables (using MultiIndex.from_product()), or a DataFrame (using MultiIndex.from_frame()). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes. arrays = [ ...: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ...: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ...: ] ...: tuples = list(zip(*arrays)) tuples [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')] index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"]) index MultiIndex([('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], names=['first', 'second']) s = pd.Series(np.random.randn(8), index=index) s first second bar one 0.469112 two -0.282863 baz one -1.509059 two -1.135632 foo one 1.212112 two -0.173215 qux one 0.119209 two -1.044236 dtype: float64 When you want every pairing of the elements in two iterables, it can be easier to use the MultiIndex.from_product() method: iterables = [[\"bar\", \"baz\", \"foo\", \"qux\"], [\"one\", \"two\"]] pd.MultiIndex.from_product(iterables, names=[\"first\", \"second\"]) MultiIndex([('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], names=['first', 'second']) You can also construct a MultiIndex from a DataFrame directly, using the method MultiIndex.from_frame(). This is a complementary method to MultiIndex.to_frame(). df = pd.DataFrame( ....: [[\"bar\", \"one\"], [\"bar\", \"two\"], [\"foo\", \"one\"], [\"foo\", \"two\"]], ....: columns=[\"first\", \"second\"], ....: ) ....: pd.MultiIndex.from_frame(df) MultiIndex([('bar', 'one'), ('bar', 'two'), ('foo', 'one'), ('foo', 'two')], names=['first', 'second']) As a convenience, you can pass", "prev_chunk_id": "chunk_335", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_338", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Special use of the == operator with list objects#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Special use of the == operator with list objects#", "content": "Special use of the == operator with list objects# Comparing a list of values to a column using ==/!= works similarly to in/not in. df.query('b == [\"a\", \"b\", \"c\"]') a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 # pure Python df[df['b'].isin([\"a\", \"b\", \"c\"])] a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 df.query('c == [1, 2]') a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2 df.query('c != [1, 2]') a b c d 1 a a 4 7 4 c b 3 6 5 c b 0 2 6 d b 3 3 8 e c 4 3 10 f c 0 6 # using in/not in df.query('[1, 2] in c') a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2 df.query('[1, 2] not in c') a b c d 1 a a 4 7 4 c b 3 6", "prev_chunk_id": "chunk_336", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_339", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Creating a MultiIndex (hierarchical index) object#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Creating a MultiIndex (hierarchical index) object#", "content": "a list of arrays directly into Series or DataFrame to construct a MultiIndex automatically: arrays = [ ....: np.array([\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"]), ....: np.array([\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"]), ....: ] ....: s = pd.Series(np.random.randn(8), index=arrays) s bar one -0.861849 two -2.104569 baz one -0.494929 two 1.071804 foo one 0.721555 two -0.706771 qux one -1.039575 two 0.271860 dtype: float64 df = pd.DataFrame(np.random.randn(8, 4), index=arrays) df 0 1 2 3 bar one -0.424972 0.567020 0.276232 -1.087401 two -0.673690 0.113648 -1.478427 0.524988 baz one 0.404705 0.577046 -1.715002 -1.039268 two -0.370647 -1.157892 -1.344312 0.844885 foo one 1.075770 -0.109050 1.643563 -1.469388 two 0.357021 -0.674600 -1.776904 -0.968914 qux one -1.294524 0.413738 0.276662 -0.472035 two -0.013960 -0.362543 -0.006154 -0.923061 All of the MultiIndex constructors accept a names argument which stores string names for the levels themselves. If no names are provided, None will be assigned: df.index.names FrozenList([None, None]) This index can back any axis of a pandas object, and the number of levels of the index is up to you: df = pd.DataFrame(np.random.randn(3, 8), index=[\"A\", \"B\", \"C\"], columns=index) df first bar baz ... foo qux second one two one ... two one two A 0.895717 0.805244 -1.206412 ... 1.340309 -1.170299 -0.226169 B 0.410835 0.813850 0.132003 ... -1.187678 1.130127 -1.436737 C -1.413681 1.607920 1.024180 ... -2.211372 0.974466 -2.006747 [3 rows x 8 columns] pd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6]) first bar baz foo second one two one two one two first second bar one -0.410001 -0.078638 0.545952 -1.219217 -1.226825 0.769804 two -1.281247 -0.727707 -0.121306 -0.097883 0.695775 0.341734 baz one 0.959726 -1.110336 -0.619976 0.149748 -0.732339 0.687738 two 0.176444 0.403310 -0.154951 0.301624 -2.179861 -1.369849 foo one -0.954208 1.462696 -1.743161 -0.826591 -0.345352 1.314232 two 0.690579 0.995761 2.396780 0.014871 3.357427 -0.317441 We’ve “sparsified” the higher levels of the indexes to make the console output a bit easier", "prev_chunk_id": "chunk_337", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_340", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Special use of the == operator with list objects#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Special use of the == operator with list objects#", "content": "5 c b 0 2 6 d b 3 3 8 e c 4 3 10 f c 0 6 # pure Python df[df['c'].isin([1, 2])] a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2", "prev_chunk_id": "chunk_338", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_341", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Creating a MultiIndex (hierarchical index) object#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Creating a MultiIndex (hierarchical index) object#", "content": "on the eyes. Note that how the index is displayed can be controlled using the multi_sparse option in pandas.set_options(): with pd.option_context(\"display.multi_sparse\", False): ....: df ....: It’s worth keeping in mind that there’s nothing preventing you from using tuples as atomic labels on an axis: pd.Series(np.random.randn(8), index=tuples) (bar, one) -1.236269 (bar, two) 0.896171 (baz, one) -0.487602 (baz, two) -0.082240 (foo, one) -2.182937 (foo, two) 0.380396 (qux, one) 0.084844 (qux, two) 0.432390 dtype: float64 The reason that the MultiIndex matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a MultiIndex explicitly yourself. However, when loading data from a file, you may wish to generate your own MultiIndex when preparing the data set.", "prev_chunk_id": "chunk_339", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_342", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Boolean operators#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Boolean operators#", "content": "Boolean operators# You can negate boolean expressions with the word not or the ~ operator. df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc')) df['bools'] = np.random.rand(len(df)) > 0.5 df.query('~bools') a b c bools 2 0.697753 0.212799 0.329209 False 7 0.275396 0.691034 0.826619 False 8 0.190649 0.558748 0.262467 False df.query('not bools') a b c bools 2 0.697753 0.212799 0.329209 False 7 0.275396 0.691034 0.826619 False 8 0.190649 0.558748 0.262467 False df.query('not bools') == df[~df['bools']] a b c bools 2 True True True True 7 True True True True 8 True True True True Of course, expressions can be arbitrarily complex too: # short query syntax shorter = df.query('a < b < c and (not bools) or bools > 2') # equivalent in pure Python longer = df[(df['a'] < df['b']) .....: & (df['b'] < df['c']) .....: & (~df['bools']) .....: | (df['bools'] > 2)] .....: shorter a b c bools 7 0.275396 0.691034 0.826619 False longer a b c bools 7 0.275396 0.691034 0.826619 False shorter == longer a b c bools 7 True True True True", "prev_chunk_id": "chunk_340", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_343", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Reconstructing the level labels#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Reconstructing the level labels#", "content": "Reconstructing the level labels# The method get_level_values() will return a vector of the labels for each location at a particular level: index.get_level_values(0) Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='first') index.get_level_values(\"second\") Index(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'], dtype='object', name='second')", "prev_chunk_id": "chunk_341", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_344", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Performance of query()#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Performance of query()#", "content": "Performance of query()# DataFrame.query() using numexpr is slightly faster than Python for large frames. You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 100,000 rows. This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn(). df = pd.DataFrame(np.random.randn(8, 4), .....: index=dates, columns=['A', 'B', 'C', 'D']) .....: df2 = df.copy()", "prev_chunk_id": "chunk_342", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_345", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Basic indexing on axis with MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Basic indexing on axis with MultiIndex#", "content": "Basic indexing on axis with MultiIndex# One of the important features of hierarchical indexing is that you can select data by a “partial” label identifying a subgroup in the data. Partial selection “drops” levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame: df[\"bar\"] second one two A 0.895717 0.805244 B 0.410835 0.813850 C -1.413681 1.607920 df[\"bar\", \"one\"] A 0.895717 B 0.410835 C -1.413681 Name: (bar, one), dtype: float64 df[\"bar\"][\"one\"] A 0.895717 B 0.410835 C -1.413681 Name: one, dtype: float64 s[\"qux\"] one -1.039575 two 0.271860 dtype: float64 See Cross-section with hierarchical index for how to select on a deeper level.", "prev_chunk_id": "chunk_343", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_346", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Duplicate data#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate data#", "content": "Duplicate data# If you want to identify and remove duplicate rows in a DataFrame, there are two methods that will help: duplicated and drop_duplicates. Each takes as an argument the columns to use to identify duplicated rows. - duplicatedreturns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated. - drop_duplicatesremoves duplicate rows. By default, the first observed row of a duplicate set is considered unique, but each method has a keep parameter to specify targets to be kept. - keep='first'(default): mark / drop duplicates except for the first occurrence. - keep='last': mark / drop duplicates except for the last occurrence. - keep=False: mark / drop all duplicates. df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'], .....: 'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'], .....: 'c': np.random.randn(7)}) .....: df2 a b c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329 df2.duplicated('a') 0 False 1 True 2 False 3 True 4 True 5 False 6 False dtype: bool df2.duplicated('a', keep='last') 0 True 1 False 2 True 3 True 4 False 5 False 6 False dtype: bool df2.duplicated('a', keep=False) 0 True 1 True 2 True 3 True 4 True 5 False 6 False dtype: bool df2.drop_duplicates('a') a b c 0 one x -1.067137 2 two x -0.211056 5 three x -1.964475 6 four x 1.298329 df2.drop_duplicates('a', keep='last') a b c 1 one y 0.309500 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329 df2.drop_duplicates('a', keep=False) a b c 5 three x -1.964475 6 four x 1.298329 Also, you can pass a list of columns to identify duplications. df2.duplicated(['a', 'b']) 0 False 1 False 2 False 3 False 4", "prev_chunk_id": "chunk_344", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_347", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Defined levels#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Defined levels#", "content": "Defined levels# The MultiIndex keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example: df.columns.levels # original MultiIndex FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']]) df[[\"foo\",\"qux\"]].columns.levels # sliced FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']]) This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the get_level_values() method. df[[\"foo\", \"qux\"]].columns.to_numpy() array([('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], dtype=object) # for a specific level df[[\"foo\", \"qux\"]].columns.get_level_values(0) Index(['foo', 'foo', 'qux', 'qux'], dtype='object', name='first') To reconstruct the MultiIndex with only the used levels, the remove_unused_levels() method may be used. new_mi = df[[\"foo\", \"qux\"]].columns.remove_unused_levels() new_mi.levels FrozenList([['foo', 'qux'], ['one', 'two']])", "prev_chunk_id": "chunk_345", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_348", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Duplicate data#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate data#", "content": "True 5 False 6 False dtype: bool df2.drop_duplicates(['a', 'b']) a b c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 5 three x -1.964475 6 four x 1.298329 To drop duplicates by index value, use Index.duplicated then perform slicing. The same set of options are available for the keep parameter. df3 = pd.DataFrame({'a': np.arange(6), .....: 'b': np.random.randn(6)}, .....: index=['a', 'a', 'b', 'c', 'b', 'a']) .....: df3 a b a 0 1.440455 a 1 2.456086 b 2 1.038402 c 3 -0.894409 b 4 0.683536 a 5 3.082764 df3.index.duplicated() array([False, True, False, False, True, True]) df3[~df3.index.duplicated()] a b a 0 1.440455 b 2 1.038402 c 3 -0.894409 df3[~df3.index.duplicated(keep='last')] a b c 3 -0.894409 b 4 0.683536 a 5 3.082764 df3[~df3.index.duplicated(keep=False)] a b c 3 -0.894409", "prev_chunk_id": "chunk_346", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_349", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Data alignment and using reindex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Data alignment and using reindex#", "content": "Data alignment and using reindex# Operations between differently-indexed objects having MultiIndex on the axes will work as you expect; data alignment will work the same as an Index of tuples: s + s[:-2] bar one -1.723698 two -4.209138 baz one -0.989859 two 2.143608 foo one 1.443110 two -1.413542 qux one NaN two NaN dtype: float64 s + s[::2] bar one -1.723698 two NaN baz one -0.989859 two NaN foo one 1.443110 two NaN qux one -2.079150 two NaN dtype: float64 The reindex() method of Series/DataFrames can be called with another MultiIndex, or even a list or array of tuples: s.reindex(index[:3]) first second bar one -0.861849 two -2.104569 baz one -0.494929 dtype: float64 s.reindex([(\"foo\", \"two\"), (\"bar\", \"one\"), (\"qux\", \"one\"), (\"baz\", \"one\")]) foo two -0.706771 bar one -0.861849 qux one -1.039575 baz one -0.494929 dtype: float64", "prev_chunk_id": "chunk_347", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_350", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Dictionary-like get() method#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Dictionary-like get() method#", "content": "Dictionary-like get() method# Each of Series or DataFrame have a get method which can return a default value. s = pd.Series([1, 2, 3], index=['a', 'b', 'c']) s.get('a') # equivalent to s['a'] 1 s.get('x', default=-1) -1", "prev_chunk_id": "chunk_348", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_351", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Advanced indexing with hierarchical index#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Advanced indexing with hierarchical index#", "content": "Advanced indexing with hierarchical index# Syntactically integrating MultiIndex in advanced indexing with .loc is a bit challenging, but we’ve made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect: df = df.T df A B C first second bar one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 two -0.226169 -1.436737 -2.006747 df.loc[(\"bar\", \"two\")] A 0.805244 B 0.813850 C 1.607920 Name: (bar, two), dtype: float64 Note that df.loc['bar', 'two'] would also work in this example, but this shorthand notation can lead to ambiguity in general. If you also want to index a specific column with .loc, you must use a tuple like this: df.loc[(\"bar\", \"two\"), \"A\"] 0.8052440253863785 You don’t have to specify all levels of the MultiIndex by passing only the first elements of the tuple. For example, you can use “partial” indexing to get all elements with bar in the first level as follows: df.loc[\"bar\"] A B C second one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example). “Partial” slicing also works quite nicely. df.loc[\"baz\":\"foo\"] A B C first second baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 You can slice with a ‘range’ of values, by providing a slice of tuples. df.loc[(\"baz\", \"two\"):(\"qux\", \"one\")] A B C first second baz two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 df.loc[(\"baz\", \"two\"):\"foo\"] A B C first second baz two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two", "prev_chunk_id": "chunk_349", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_352", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Looking up values by index/column labels#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Looking up values by index/column labels#", "content": "Looking up values by index/column labels# Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by pandas.factorize and NumPy indexing. For instance: df = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"], .....: 'A': [80, 23, np.nan, 22], .....: 'B': [80, 55, 76, 67]}) .....: df col A B 0 A 80.0 80 1 A 23.0 55 2 B NaN 76 3 B 22.0 67 idx, cols = pd.factorize(df['col']) df.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx] array([80., 23., 76., 67.]) Formerly this could be achieved with the dedicated DataFrame.lookup method which was deprecated in version 1.2.0 and removed in version 2.0.0.", "prev_chunk_id": "chunk_350", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_353", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Advanced indexing with hierarchical index#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Advanced indexing with hierarchical index#", "content": "1.340309 -1.187678 -2.211372 Passing a list of labels or tuples works similar to reindexing: df.loc[[(\"bar\", \"two\"), (\"qux\", \"one\")]] A B C first second bar two 0.805244 0.813850 1.607920 qux one -1.170299 1.130127 0.974466 Importantly, a list of tuples indexes several complete MultiIndex keys, whereas a tuple of lists refer to several values within a level: s = pd.Series( ....: [1, 2, 3, 4, 5, 6], ....: index=pd.MultiIndex.from_product([[\"A\", \"B\"], [\"c\", \"d\", \"e\"]]), ....: ) ....: s.loc[[(\"A\", \"c\"), (\"B\", \"d\")]] # list of tuples A c 1 B d 5 dtype: int64 s.loc[([\"A\", \"B\"], [\"c\", \"d\"])] # tuple of lists A c 1 d 2 B c 4 d 5 dtype: int64", "prev_chunk_id": "chunk_351", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_354", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Index objects#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Index objects#", "content": "Index objects# The pandas Index class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. Index also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an Index directly is to pass a list or other sequence to Index: index = pd.Index(['e', 'd', 'a', 'b']) index Index(['e', 'd', 'a', 'b'], dtype='object') 'd' in index True or using numbers: index = pd.Index([1, 5, 12]) index Index([1, 5, 12], dtype='int64') 5 in index True If no dtype is given, Index tries to infer the dtype from the data. It is also possible to give an explicit dtype when instantiating an Index: index = pd.Index(['e', 'd', 'a', 'b'], dtype=\"string\") index Index(['e', 'd', 'a', 'b'], dtype='string') index = pd.Index([1, 5, 12], dtype=\"int8\") index Index([1, 5, 12], dtype='int8') index = pd.Index([1, 5, 12], dtype=\"float32\") index Index([1.0, 5.0, 12.0], dtype='float32') You can also pass a name to be stored in the index: index = pd.Index(['e', 'd', 'a', 'b'], name='something') index.name 'something' The name, if set, will be shown in the console display: index = pd.Index(list(range(5)), name='rows') columns = pd.Index(['A', 'B', 'C'], name='cols') df = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns) df cols A B C rows 0 1.295989 -1.051694 1.340429 1 -2.366110 0.428241 0.387275 2 0.433306 0.929548 0.278094 3 2.154730 -0.315628 0.264223 4 1.126818 1.132290 -0.353310 df['A'] rows 0 1.295989 1 -2.366110 2 0.433306 3 2.154730 4 1.126818 Name: A, dtype: float64", "prev_chunk_id": "chunk_352", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_355", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Using slicers#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Using slicers#", "content": "Using slicers# You can slice a MultiIndex by providing multiple indexers. You can provide any of the selectors as if you are indexing by label, see Selection by Label, including slices, lists of labels, labels, and boolean indexers. You can use slice(None) to select all the contents of that level. You do not need to specify all the deeper levels, they will be implied as slice(None). As usual, both sides of the slicers are included as this is label indexing. def mklbl(prefix, n): ....: return [\"%s%s\" % (prefix, i) for i in range(n)] ....: miindex = pd.MultiIndex.from_product( ....: [mklbl(\"A\", 4), mklbl(\"B\", 2), mklbl(\"C\", 4), mklbl(\"D\", 2)] ....: ) ....: micolumns = pd.MultiIndex.from_tuples( ....: [(\"a\", \"foo\"), (\"a\", \"bar\"), (\"b\", \"foo\"), (\"b\", \"bah\")], names=[\"lvl0\", \"lvl1\"] ....: ) ....: dfmi = ( ....: pd.DataFrame( ....: np.arange(len(miindex) * len(micolumns)).reshape( ....: (len(miindex), len(micolumns)) ....: ), ....: index=miindex, ....: columns=micolumns, ....: ) ....: .sort_index() ....: .sort_index(axis=1) ....: ) ....: dfmi lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 9 8 11 10 D1 13 12 15 14 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 237 236 239 238 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 249 248 251 250 D1 253 252 255 254 [64 rows x 4 columns] Basic MultiIndex slicing using slices, lists, and labels. dfmi.loc[(slice(\"A1\", \"A3\"), slice(None), [\"C1\", \"C3\"]), :] lvl0 a b lvl1 bar foo bah foo A1 B0 C1 D0 73 72 75 74 D1 77 76 79 78 C3 D0 89 88 91 90 D1 93 92 95 94 B1 C1 D0 105 104 107 106 ... ... ... ... ... A3 B0 C3 D1 221 220 223 222 B1 C1", "prev_chunk_id": "chunk_353", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_356", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Setting metadata#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Setting metadata#", "content": "Setting metadata# Indexes are “mostly immutable”, but it is possible to set and change their name attribute. You can use the rename, set_names to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. ind = pd.Index([1, 2, 3]) ind.rename(\"apple\") Index([1, 2, 3], dtype='int64', name='apple') ind Index([1, 2, 3], dtype='int64') ind = ind.set_names([\"apple\"]) ind.name = \"bob\" ind Index([1, 2, 3], dtype='int64', name='bob') set_names, set_levels, and set_codes also take an optional level argument index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second']) index MultiIndex([(0, 'one'), (0, 'two'), (1, 'one'), (1, 'two'), (2, 'one'), (2, 'two')], names=['first', 'second']) index.levels[1] Index(['one', 'two'], dtype='object', name='second') index.set_levels([\"a\", \"b\"], level=1) MultiIndex([(0, 'a'), (0, 'b'), (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')], names=['first', 'second'])", "prev_chunk_id": "chunk_354", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_357", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Using slicers#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Using slicers#", "content": "D0 233 232 235 234 D1 237 236 239 238 C3 D0 249 248 251 250 D1 253 252 255 254 [24 rows x 4 columns] You can use pandas.IndexSlice to facilitate a more natural syntax using :, rather than using slice(None). idx = pd.IndexSlice dfmi.loc[idx[:, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]] lvl0 a b lvl1 foo foo A0 B0 C1 D0 8 10 D1 12 14 C3 D0 24 26 D1 28 30 B1 C1 D0 40 42 ... ... ... A3 B0 C3 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254 [32 rows x 2 columns] It is possible to perform quite complicated selections using this method on multiple axes at the same time. dfmi.loc[\"A1\", (slice(None), \"foo\")] lvl0 a b lvl1 foo foo B0 C0 D0 64 66 D1 68 70 C1 D0 72 74 D1 76 78 C2 D0 80 82 ... ... ... B1 C1 D1 108 110 C2 D0 112 114 D1 116 118 C3 D0 120 122 D1 124 126 [16 rows x 2 columns] dfmi.loc[idx[:, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]] lvl0 a b lvl1 foo foo A0 B0 C1 D0 8 10 D1 12 14 C3 D0 24 26 D1 28 30 B1 C1 D0 40 42 ... ... ... A3 B0 C3 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254 [32 rows x 2 columns] Using a boolean indexer you can provide selection related to the values. mask = dfmi[(\"a\", \"foo\")] > 200 dfmi.loc[idx[mask, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]] lvl0 a b lvl1 foo foo A3 B0 C1 D1 204 206 C3 D0 216 218 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254", "prev_chunk_id": "chunk_355", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_358", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Set operations on Index objects#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Set operations on Index objects#", "content": "Set operations on Index objects# The two main operations are union and intersection. Difference is provided via the .difference() method. a = pd.Index(['c', 'b', 'a']) b = pd.Index(['c', 'e', 'd']) a.difference(b) Index(['a', 'b'], dtype='object') Also available is the symmetric_difference operation, which returns elements that appear in either idx1 or idx2, but not in both. This is equivalent to the Index created by idx1.difference(idx2).union(idx2.difference(idx1)), with duplicates dropped. idx1 = pd.Index([1, 2, 3, 4]) idx2 = pd.Index([2, 3, 4, 5]) idx1.symmetric_difference(idx2) Index([1, 5], dtype='int64') When performing Index.union() between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float idx1 = pd.Index([0, 1, 2]) idx2 = pd.Index([0.5, 1.5]) idx1.union(idx2) Index([0.0, 0.5, 1.0, 1.5, 2.0], dtype='float64')", "prev_chunk_id": "chunk_356", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_359", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Using slicers#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Using slicers#", "content": "You can also specify the axis argument to .loc to interpret the passed slicers on a single axis. dfmi.loc(axis=0)[:, :, [\"C1\", \"C3\"]] lvl0 a b lvl1 bar foo bah foo A0 B0 C1 D0 9 8 11 10 D1 13 12 15 14 C3 D0 25 24 27 26 D1 29 28 31 30 B1 C1 D0 41 40 43 42 ... ... ... ... ... A3 B0 C3 D1 221 220 223 222 B1 C1 D0 233 232 235 234 D1 237 236 239 238 C3 D0 249 248 251 250 D1 253 252 255 254 [32 rows x 4 columns] Furthermore, you can set the values using the following methods. df2 = dfmi.copy() df2.loc(axis=0)[:, :, [\"C1\", \"C3\"]] = -10 df2 lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 -10 -10 -10 -10 D1 -10 -10 -10 -10 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 -10 -10 -10 -10 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 -10 -10 -10 -10 D1 -10 -10 -10 -10 [64 rows x 4 columns] You can use a right-hand-side of an alignable object as well. df2 = dfmi.copy() df2.loc[idx[:, :, [\"C1\", \"C3\"]], :] = df2 * 1000 df2 lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 9000 8000 11000 10000 D1 13000 12000 15000 14000 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 237000 236000 239000 238000 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 249000 248000 251000 250000 D1 253000 252000 255000 254000 [64 rows x", "prev_chunk_id": "chunk_357", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_360", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Missing values#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Missing values#", "content": "Missing values# Index.fillna fills missing values with specified scalar value. idx1 = pd.Index([1, np.nan, 3, 4]) idx1 Index([1.0, nan, 3.0, 4.0], dtype='float64') idx1.fillna(2) Index([1.0, 2.0, 3.0, 4.0], dtype='float64') idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'), .....: pd.NaT, .....: pd.Timestamp('2011-01-03')]) .....: idx2 DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03'], dtype='datetime64[ns]', freq=None) idx2.fillna(pd.Timestamp('2011-01-02')) DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_358", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_361", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Using slicers#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Using slicers#", "content": "4 columns]", "prev_chunk_id": "chunk_359", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_362", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Set / reset index#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Set / reset index#", "content": "Set / reset index# Occasionally you will load or create a data set into a DataFrame and want to add an index after you’ve already done so. There are a couple of different ways.", "prev_chunk_id": "chunk_360", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_363", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Cross-section#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Cross-section#", "content": "Cross-section# The xs() method of DataFrame additionally takes a level argument to make selecting data at a particular level of a MultiIndex easier. df A B C first second bar one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 two -0.226169 -1.436737 -2.006747 df.xs(\"one\", level=\"second\") A B C first bar 0.895717 0.410835 -1.413681 baz -1.206412 0.132003 1.024180 foo 1.431256 -0.076467 0.875906 qux -1.170299 1.130127 0.974466 # using the slicers df.loc[(slice(None), \"one\"), :] A B C first second bar one 0.895717 0.410835 -1.413681 baz one -1.206412 0.132003 1.024180 foo one 1.431256 -0.076467 0.875906 qux one -1.170299 1.130127 0.974466 You can also select on the columns with xs, by providing the axis argument. df = df.T df.xs(\"one\", level=\"second\", axis=1) first bar baz foo qux A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 # using the slicers df.loc[:, (slice(None), \"one\")] first bar baz foo qux second one one one one A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 xs also allows selection with multiple keys. df.xs((\"one\", \"bar\"), level=(\"second\", \"first\"), axis=1) first bar second one A 0.895717 B 0.410835 C -1.413681 # using the slicers df.loc[:, (\"bar\", \"one\")] A 0.895717 B 0.410835 C -1.413681 Name: (bar, one), dtype: float64 You can pass drop_level=False to xs to retain the level that was selected. df.xs(\"one\", level=\"second\", axis=1, drop_level=False) first bar baz foo qux second one one one one A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 Compare the above with the result using drop_level=True (the default value). df.xs(\"one\", level=\"second\", axis=1, drop_level=True) first bar baz foo qux A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835", "prev_chunk_id": "chunk_361", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_364", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Set an index#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Set an index#", "content": "Set an index# DataFrame has a set_index() method which takes a column name (for a regular Index) or a list of column names (for a MultiIndex). To create a new, re-indexed DataFrame: data = pd.DataFrame({'a': ['bar', 'bar', 'foo', 'foo'], .....: 'b': ['one', 'two', 'one', 'two'], .....: 'c': ['z', 'y', 'x', 'w'], .....: 'd': [1., 2., 3, 4]}) .....: data a b c d 0 bar one z 1.0 1 bar two y 2.0 2 foo one x 3.0 3 foo two w 4.0 indexed1 = data.set_index('c') indexed1 a b d c z bar one 1.0 y bar two 2.0 x foo one 3.0 w foo two 4.0 indexed2 = data.set_index(['a', 'b']) indexed2 c d a b bar one z 1.0 two y 2.0 foo one x 3.0 two w 4.0 The append keyword option allow you to keep the existing index and append the given columns to a MultiIndex: frame = data.set_index('c', drop=False) frame = frame.set_index(['a', 'b'], append=True) frame c d c a b z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0 Other options in set_index allow you not drop the index columns. data.set_index('c', drop=False) a b c d c z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0", "prev_chunk_id": "chunk_362", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_365", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Cross-section#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Cross-section#", "content": "0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466", "prev_chunk_id": "chunk_363", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_366", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Reset the index#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Reset the index#", "content": "Reset the index# As a convenience, there is a new function on DataFrame called reset_index() which transfers the index values into the DataFrame’s columns and sets a simple integer index. This is the inverse operation of set_index(). data a b c d 0 bar one z 1.0 1 bar two y 2.0 2 foo one x 3.0 3 foo two w 4.0 data.reset_index() index a b c d 0 0 bar one z 1.0 1 1 bar two y 2.0 2 2 foo one x 3.0 3 3 foo two w 4.0 The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the names attribute. You can use the level keyword to remove only a portion of the index: frame c d c a b z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0 frame.reset_index(level=1) a c d c b z one bar z 1.0 y two bar y 2.0 x one foo x 3.0 w two foo w 4.0 reset_index takes an optional parameter drop which if true simply discards the index, instead of putting index values in the DataFrame’s columns.", "prev_chunk_id": "chunk_364", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_367", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Advanced reindexing and alignment#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Advanced reindexing and alignment#", "content": "Advanced reindexing and alignment# Using the parameter level in the reindex() and align() methods of pandas objects is useful to broadcast values across a level. For instance: midx = pd.MultiIndex( ....: levels=[[\"zero\", \"one\"], [\"x\", \"y\"]], codes=[[1, 1, 0, 0], [1, 0, 1, 0]] ....: ) ....: df = pd.DataFrame(np.random.randn(4, 2), index=midx) df 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 df2 = df.groupby(level=0).mean() df2 0 1 one 1.060074 -0.109716 zero 1.271532 0.713416 df2.reindex(df.index, level=0) 0 1 one y 1.060074 -0.109716 x 1.060074 -0.109716 zero y 1.271532 0.713416 x 1.271532 0.713416 # aligning df_aligned, df2_aligned = df.align(df2, level=0) df_aligned 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 df2_aligned 0 1 one y 1.060074 -0.109716 x 1.060074 -0.109716 zero y 1.271532 0.713416 x 1.271532 0.713416", "prev_chunk_id": "chunk_365", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_368", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Adding an ad hoc index#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Adding an ad hoc index#", "content": "Adding an ad hoc index# You can assign a custom index to the index attribute: df_idx = pd.DataFrame(range(4)) df_idx.index = pd.Index([10, 20, 30, 40], name=\"a\") df_idx 0 a 10 0 20 1 30 2 40 3", "prev_chunk_id": "chunk_366", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_369", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Swapping levels with swaplevel#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Swapping levels with swaplevel#", "content": "Swapping levels with swaplevel# The swaplevel() method can switch the order of two levels: df[:5] 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 df[:5].swaplevel(0, 1, axis=0) 0 1 y one 1.519970 -0.493662 x one 0.600178 0.274230 y zero 0.132885 -0.023688 x zero 2.410179 1.450520", "prev_chunk_id": "chunk_367", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_370", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Returning a view versus a copy#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Returning a view versus a copy#", "content": "Returning a view versus a copy# When setting values in a pandas object, care must be taken to avoid what is called chained indexing. Here is an example. dfmi = pd.DataFrame([list('abcd'), .....: list('efgh'), .....: list('ijkl'), .....: list('mnop')], .....: columns=pd.MultiIndex.from_product([['one', 'two'], .....: ['first', 'second']])) .....: dfmi one two first second first second 0 a b c d 1 e f g h 2 i j k l 3 m n o p Compare these two access methods: dfmi['one']['second'] 0 b 1 f 2 j 3 n Name: second, dtype: object dfmi.loc[:, ('one', 'second')] 0 b 1 f 2 j 3 n Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 (.loc) is much preferred over method 1 (chained []). dfmi['one'] selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation dfmi_with_one['second'] selects the series indexed by 'second'. This is indicated by the variable dfmi_with_one because pandas sees these operations as separate events. e.g. separate calls to __getitem__, so it has to treat them as linear operations, they happen one after another. Contrast this to df.loc[:,('one','second')] which passes a nested tuple of (slice(None),('one','second')) to a single call to __getitem__. This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired.", "prev_chunk_id": "chunk_368", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_371", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Reordering levels with reorder_levels#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Reordering levels with reorder_levels#", "content": "Reordering levels with reorder_levels# The reorder_levels() method generalizes the swaplevel method, allowing you to permute the hierarchical index levels in one step: df[:5].reorder_levels([1, 0], axis=0) 0 1 y one 1.519970 -0.493662 x one 0.600178 0.274230 y zero 0.132885 -0.023688 x zero 2.410179 1.450520", "prev_chunk_id": "chunk_369", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_372", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Why does assignment fail when using chained indexing?#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Why does assignment fail when using chained indexing?#", "content": "Why does assignment fail when using chained indexing?# The problem in the previous section is just a performance issue. What’s up with the SettingWithCopy warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: dfmi.loc[:, ('one', 'second')] = value # becomes dfmi.loc.__setitem__((slice(None), ('one', 'second')), value) But this code is handled differently: dfmi['one']['second'] = value # becomes dfmi.__getitem__('one').__setitem__('second', value) See that __getitem__ in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the __setitem__ will modify dfmi or a temporary object that gets thrown out immediately afterward. That’s what SettingWithCopy is warning you about! Sometimes a SettingWithCopy warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that SettingWithCopy is designed to catch! pandas is probably trying to warn you that you’ve done this: def do_something(df): foo = df[['bar', 'baz']] # Is foo a view? A copy? Nobody knows! # ... many lines here ... # We don't know whether this will modify df or not! foo['quux'] = value return foo Yikes!", "prev_chunk_id": "chunk_370", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_373", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Renaming names of an Index or MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Renaming names of an Index or MultiIndex#", "content": "Renaming names of an Index or MultiIndex# The rename() method is used to rename the labels of a MultiIndex, and is typically used to rename the columns of a DataFrame. The columns argument of rename allows a dictionary to be specified that includes only the columns you wish to rename. df.rename(columns={0: \"col0\", 1: \"col1\"}) col0 col1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 This method can also be used to rename specific labels of the main index of the DataFrame. df.rename(index={\"one\": \"two\", \"y\": \"z\"}) 0 1 two z 1.519970 -0.493662 x 0.600178 0.274230 zero z 0.132885 -0.023688 x 2.410179 1.450520 The rename_axis() method is used to rename the name of a Index or MultiIndex. In particular, the names of the levels of a MultiIndex can be specified, which is useful if reset_index() is later used to move the values from the MultiIndex to a column. df.rename_axis(index=[\"abc\", \"def\"]) 0 1 abc def one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 Note that the columns of a DataFrame are an index, so that using rename_axis with the columns argument will change the name of that index. df.rename_axis(columns=\"Cols\").columns RangeIndex(start=0, stop=2, step=1, name='Cols') Both rename and rename_axis support specifying a dictionary, Series or a mapping function to map labels/names to new values. When working with an Index object directly, rather than via a DataFrame, Index.set_names() can be used to change the names. mi = pd.MultiIndex.from_product([[1, 2], [\"a\", \"b\"]], names=[\"x\", \"y\"]) mi.names FrozenList(['x', 'y']) mi2 = mi.rename(\"new name\", level=0) mi2 MultiIndex([(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')], names=['new name', 'y']) You cannot set the names of the MultiIndex via a level. mi.levels[0].name = \"name via level\" --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[100], line 1 ----> 1 mi.levels[0].name =", "prev_chunk_id": "chunk_371", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_374", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Evaluation order matters#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Evaluation order matters#", "content": "Evaluation order matters# When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the SettingWithCopyWarning because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option mode.chained_assignment to one of these values: - 'warn', the default, means aSettingWithCopyWarningis printed. - 'raise'means pandas will raise aSettingWithCopyErroryou have to deal with. - Nonewill suppress the warnings entirely. dfb = pd.DataFrame({'a': ['one', 'one', 'two', .....: 'three', 'two', 'one', 'six'], .....: 'c': np.arange(7)}) .....: # This will show the SettingWithCopyWarning # but the frame values will be set dfb['c'][dfb['a'].str.startswith('o')] = 42 This however is operating on a copy and will not work. with pd.option_context('mode.chained_assignment','warn'): .....: dfb[dfb['a'].str.startswith('o')]['c'] = 42 .....: A chained assignment can also crop up in setting in a mixed dtype frame. The following is the recommended access method using .loc for multiple items (using mask) and a single item using a fixed index: dfc = pd.DataFrame({'a': ['one', 'one', 'two', .....: 'three', 'two', 'one', 'six'], .....: 'c': np.arange(7)}) .....: dfd = dfc.copy() # Setting multiple items using a mask mask = dfd['a'].str.startswith('o') dfd.loc[mask, 'c'] = 42 dfd a c 0 one 42 1 one 42 2 two 2 3 three 3 4 two 4 5 one 42 6 six 6 # Setting a single item dfd = dfc.copy() dfd.loc[2, 'a'] = 11 dfd a c 0 one 0 1 one 1 2 11 2 3 three 3 4 two 4 5 one 5 6 six 6 The following can work at", "prev_chunk_id": "chunk_372", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_375", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Renaming names of an Index or MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Renaming names of an Index or MultiIndex#", "content": "\"name via level\" File ~/work/pandas/pandas/pandas/core/indexes/base.py:1697, in Index.name(self, value) 1693 @name.setter 1694 def name(self, value: Hashable) -> None: 1695 if self._no_setting_name: 1696 # Used in MultiIndex.levels to avoid silently ignoring name updates. -> 1697 raise RuntimeError( 1698 \"Cannot set name on a level of a MultiIndex. Use \" 1699 \"'MultiIndex.set_names' instead.\" 1700 ) 1701 maybe_extract_name(value, None, type(self)) 1702 self._name = value RuntimeError: Cannot set name on a level of a MultiIndex. Use 'MultiIndex.set_names' instead. Use Index.set_names() instead.", "prev_chunk_id": "chunk_373", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_376", "url": "https://pandas.pydata.org/docs/user_guide/indexing.html", "title": "Evaluation order matters#", "page_title": "Indexing and selecting data — pandas 2.3.1 documentation", "breadcrumbs": "Evaluation order matters#", "content": "times, but it is not guaranteed to, and therefore should be avoided: dfd = dfc.copy() dfd['a'][2] = 111 dfd a c 0 one 0 1 one 1 2 111 2 3 three 3 4 two 4 5 one 5 6 six 6 Last, the subsequent example will not work at all, and so should be avoided: with pd.option_context('mode.chained_assignment','raise'): .....: dfd.loc[0]['a'] = 1111 .....: --------------------------------------------------------------------------- SettingWithCopyError Traceback (most recent call last) <ipython-input-400-32ce785aaa5b> in ?() 1 with pd.option_context('mode.chained_assignment','raise'): ----> 2 dfd.loc[0]['a'] = 1111 ~/work/pandas/pandas/pandas/core/series.py in ?(self, key, value) 1293 ) 1294 1295 check_dict_or_set_indexers(key) 1296 key = com.apply_if_callable(key, self) -> 1297 cacher_needs_updating = self._check_is_chained_assignment_possible() 1298 1299 if key is Ellipsis: 1300 key = slice(None) ~/work/pandas/pandas/pandas/core/series.py in ?(self) 1498 ref = self._get_cacher() 1499 if ref is not None and ref._is_mixed_type: 1500 self._check_setitem_copy(t=\"referent\", force=True) 1501 return True -> 1502 return super()._check_is_chained_assignment_possible() ~/work/pandas/pandas/pandas/core/generic.py in ?(self) 4414 single-dtype meaning that the cacher should be updated following 4415 setting. 4416 \"\"\" 4417 if self._is_copy: -> 4418 self._check_setitem_copy(t=\"referent\") 4419 return False ~/work/pandas/pandas/pandas/core/generic.py in ?(self, t, force) 4488 \"indexing.html#returning-a-view-versus-a-copy\" 4489 ) 4490 4491 if value == \"raise\": -> 4492 raise SettingWithCopyError(t) 4493 if value == \"warn\": 4494 warnings.warn(t, SettingWithCopyWarning, stacklevel=find_stack_level()) SettingWithCopyError: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy", "prev_chunk_id": "chunk_374", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_377", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Sorting a MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Sorting a MultiIndex#", "content": "Sorting a MultiIndex# For MultiIndex-ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use sort_index(). import random random.shuffle(tuples) s = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples)) s bar one 0.206053 foo one -0.251905 qux two -2.213588 one 1.063327 bar two 1.266143 foo two 0.299368 baz two -0.863838 one 0.408204 dtype: float64 s.sort_index() bar one 0.206053 two 1.266143 baz one 0.408204 two -0.863838 foo one -0.251905 two 0.299368 qux one 1.063327 two -2.213588 dtype: float64 s.sort_index(level=0) bar one 0.206053 two 1.266143 baz one 0.408204 two -0.863838 foo one -0.251905 two 0.299368 qux one 1.063327 two -2.213588 dtype: float64 s.sort_index(level=1) bar one 0.206053 baz one 0.408204 foo one -0.251905 qux one 1.063327 bar two 1.266143 baz two -0.863838 foo two 0.299368 qux two -2.213588 dtype: float64 You may also pass a level name to sort_index if the MultiIndex levels are named. s.index = s.index.set_names([\"L1\", \"L2\"]) s.sort_index(level=\"L1\") L1 L2 bar one 0.206053 two 1.266143 baz one 0.408204 two -0.863838 foo one -0.251905 two 0.299368 qux one 1.063327 two -2.213588 dtype: float64 s.sort_index(level=\"L2\") L1 L2 bar one 0.206053 baz one 0.408204 foo one -0.251905 qux one 1.063327 bar two 1.266143 baz two -0.863838 foo two 0.299368 qux two -2.213588 dtype: float64 On higher dimensional objects, you can sort any of the other axes by level if they have a MultiIndex: df.T.sort_index(level=1, axis=1) one zero one zero x x y y 0 0.600178 2.410179 1.519970 0.132885 1 0.274230 1.450520 -0.493662 -0.023688 Indexing will work even if the data are not sorted, but will be rather inefficient (and show a PerformanceWarning). It will also return a copy of the data rather than a view: dfm = pd.DataFrame( .....: {\"jim\": [0, 0, 1, 1], \"joe\": [\"x\", \"x\", \"z\", \"y\"], \"jolie\": np.random.rand(4)} .....: ) .....: dfm = dfm.set_index([\"jim\", \"joe\"]) dfm jolie jim", "prev_chunk_id": "chunk_375", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_378", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Sorting a MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Sorting a MultiIndex#", "content": "joe 0 x 0.490671 x 0.120248 1 z 0.537020 y 0.110968 dfm.loc[(1, 'z')] jolie jim joe 1 z 0.53702 Furthermore, if you try to index something that is not fully lexsorted, this can raise: dfm.loc[(0, 'y'):(1, 'z')] --------------------------------------------------------------------------- UnsortedIndexError Traceback (most recent call last) Cell In[116], line 1 ----> 1 dfm.loc[(0, 'y'):(1, 'z')] File ~/work/pandas/pandas/pandas/core/indexing.py:1191, in _LocationIndexer.__getitem__(self, key) 1189 maybe_callable = com.apply_if_callable(key, self.obj) 1190 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable) -> 1191 return self._getitem_axis(maybe_callable, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1411, in _LocIndexer._getitem_axis(self, key, axis) 1409 if isinstance(key, slice): 1410 self._validate_key(key, axis) -> 1411 return self._get_slice_axis(key, axis=axis) 1412 elif com.is_bool_indexer(key): 1413 return self._getbool_axis(key, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1443, in _LocIndexer._get_slice_axis(self, slice_obj, axis) 1440 return obj.copy(deep=False) 1442 labels = obj._get_axis(axis) -> 1443 indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step) 1445 if isinstance(indexer, slice): 1446 return self.obj._slice(indexer, axis=axis) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer(self, start, end, step) 6664 def slice_indexer( 6665 self, 6666 start: Hashable | None = None, 6667 end: Hashable | None = None, 6668 step: int | None = None, 6669 ) -> slice: 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice, end_slice = self.slice_locs(start, end, step=step) 6710 # return a slice 6711 if not is_scalar(start_slice): File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2923, in MultiIndex.slice_locs(self, start, end, step) 2871 \"\"\" 2872 For an ordered MultiIndex, compute the slice locations for input 2873 labels. (...) 2919 sequence of such. 2920 \"\"\" 2921 # This function adds nothing to its parent implementation (the magic 2922 # happens in get_slice_bound method), but it adds meaningful doc. -> 2923 return super().slice_locs(start, end, step) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6934, in Index.slice_locs(self, start, end, step) 6932 start_slice = None 6933 if start is not None: -> 6934 start_slice = self.get_slice_bound(start, \"left\") 6935 if start_slice is None: 6936 start_slice = 0 File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2867, in MultiIndex.get_slice_bound(self, label, side) 2865 if not isinstance(label,", "prev_chunk_id": "chunk_377", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_379", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Sorting a MultiIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Sorting a MultiIndex#", "content": "tuple): 2866 label = (label,) -> 2867 return self._partial_tup_index(label, side=side) File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2927, in MultiIndex._partial_tup_index(self, tup, side) 2925 def _partial_tup_index(self, tup: tuple, side: Literal[\"left\", \"right\"] = \"left\"): 2926 if len(tup) > self._lexsort_depth: -> 2927 raise UnsortedIndexError( 2928 f\"Key length ({len(tup)}) was greater than MultiIndex lexsort depth \" 2929 f\"({self._lexsort_depth})\" 2930 ) 2932 n = len(tup) 2933 start, end = 0, len(self) UnsortedIndexError: 'Key length (2) was greater than MultiIndex lexsort depth (1)' The is_monotonic_increasing() method on a MultiIndex shows if the index is sorted: dfm.index.is_monotonic_increasing False dfm = dfm.sort_index() dfm jolie jim joe 0 x 0.490671 x 0.120248 1 y 0.110968 z 0.537020 dfm.index.is_monotonic_increasing True And now selection works as expected. dfm.loc[(0, \"y\"):(1, \"z\")] jolie jim joe 1 y 0.110968 z 0.537020", "prev_chunk_id": "chunk_378", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_380", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Take methods#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Take methods#", "content": "Take methods# Similar to NumPy ndarrays, pandas Index, Series, and DataFrame also provides the take() method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions. take will also accept negative integers as relative positions to the end of the object. index = pd.Index(np.random.randint(0, 1000, 10)) index Index([214, 502, 712, 567, 786, 175, 993, 133, 758, 329], dtype='int64') positions = [0, 9, 3] index[positions] Index([214, 329, 567], dtype='int64') index.take(positions) Index([214, 329, 567], dtype='int64') ser = pd.Series(np.random.randn(10)) ser.iloc[positions] 0 -0.179666 9 1.824375 3 0.392149 dtype: float64 ser.take(positions) 0 -0.179666 9 1.824375 3 0.392149 dtype: float64 For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions. frm = pd.DataFrame(np.random.randn(5, 3)) frm.take([1, 4, 3]) 0 1 2 1 -1.237881 0.106854 -1.276829 4 0.629675 -1.425966 1.857704 3 0.979542 -1.633678 0.615855 frm.take([0, 2], axis=1) 0 2 0 0.595974 0.601544 1 -1.237881 -1.276829 2 -0.767101 1.499591 3 0.979542 0.615855 4 0.629675 1.857704 It is important to note that the take method on pandas objects are not intended to work on boolean indices and may return unexpected results. arr = np.random.randn(10) arr.take([False, False, True, True]) array([-1.1935, -1.1935, 0.6775, 0.6775]) arr[[0, 1]] array([-1.1935, 0.6775]) ser = pd.Series(np.random.randn(10)) ser.take([False, False, True, True]) 0 0.233141 0 0.233141 1 -0.223540 1 -0.223540 dtype: float64 ser.iloc[[0, 1]] 0 0.233141 1 -0.223540 dtype: float64 Finally, as a small note on performance, because the take method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing. arr = np.random.randn(10000, 5) indexer = np.arange(10000) random.shuffle(indexer) %timeit arr[indexer] .....: %timeit arr.take(indexer, axis=0) .....: 256 us +- 12.1 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each) 79.8 us +-", "prev_chunk_id": "chunk_379", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_381", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Take methods#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Take methods#", "content": "2.36 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each) ser = pd.Series(arr[:, 0]) %timeit ser.iloc[indexer] .....: %timeit ser.take(indexer) .....: 148 us +- 10 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each) 136 us +- 15.1 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each)", "prev_chunk_id": "chunk_380", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_382", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Index types#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Index types#", "content": "Index types# We have discussed MultiIndex in the previous sections pretty extensively. Documentation about DatetimeIndex and PeriodIndex are shown here, and documentation about TimedeltaIndex is found here. In the following sub-sections we will highlight some other index types.", "prev_chunk_id": "chunk_381", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_383", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "CategoricalIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "CategoricalIndex#", "content": "CategoricalIndex# CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. from pandas.api.types import CategoricalDtype df = pd.DataFrame({\"A\": np.arange(6), \"B\": list(\"aabbca\")}) df[\"B\"] = df[\"B\"].astype(CategoricalDtype(list(\"cab\"))) df A B 0 0 a 1 1 a 2 2 b 3 3 b 4 4 c 5 5 a df.dtypes A int64 B category dtype: object df[\"B\"].cat.categories Index(['c', 'a', 'b'], dtype='object') Setting the index will create a CategoricalIndex. df2 = df.set_index(\"B\") df2.index CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Indexing with __getitem__/.iloc/.loc works similarly to an Index with duplicates. The indexers must be in the category or the operation will raise a KeyError. df2.loc[\"a\"] A B a 0 a 1 a 5 The CategoricalIndex is preserved after indexing: df2.loc[\"a\"].index CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Sorting the index will sort by the order of the categories (recall that we created the index with CategoricalDtype(list('cab')), so the sorted order is cab). df2.sort_index() A B c 4 a 0 a 1 a 5 b 2 b 3 Groupby operations on the index will preserve the index nature as well. df2.groupby(level=0, observed=True).sum() A B c 4 a 6 b 5 df2.groupby(level=0, observed=True).sum().index CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old Index; indexing with a Categorical will return a CategoricalIndex, indexed according to the categories of the passed Categorical dtype. This allows one to arbitrarily index these even with values not in the categories, similarly to how you can reindex any pandas index. df3 = pd.DataFrame( .....: {\"A\": np.arange(3), \"B\":", "prev_chunk_id": "chunk_382", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_384", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "CategoricalIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "CategoricalIndex#", "content": "pd.Series(list(\"abc\")).astype(\"category\")} .....: ) .....: df3 = df3.set_index(\"B\") df3 A B a 0 b 1 c 2 df3.reindex([\"a\", \"e\"]) A B a 0.0 e NaN df3.reindex([\"a\", \"e\"]).index Index(['a', 'e'], dtype='object', name='B') df3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\"))) A B a 0.0 e NaN df3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\"))).index CategoricalIndex(['a', 'e'], categories=['a', 'b', 'e'], ordered=False, dtype='category', name='B')", "prev_chunk_id": "chunk_383", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_385", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "RangeIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "RangeIndex#", "content": "RangeIndex# RangeIndex is a sub-class of Index that provides the default index for all DataFrame and Series objects. RangeIndex is an optimized version of Index that can represent a monotonic ordered set. These are analogous to Python range types. A RangeIndex will always have an int64 dtype. idx = pd.RangeIndex(5) idx RangeIndex(start=0, stop=5, step=1) RangeIndex is the default index for all DataFrame and Series objects: ser = pd.Series([1, 2, 3]) ser.index RangeIndex(start=0, stop=3, step=1) df = pd.DataFrame([[1, 2], [3, 4]]) df.index RangeIndex(start=0, stop=2, step=1) df.columns RangeIndex(start=0, stop=2, step=1) A RangeIndex will behave similarly to a Index with an int64 dtype and operations on a RangeIndex, whose result cannot be represented by a RangeIndex, but should have an integer dtype, will be converted to an Index with int64. For example: idx[[0, 2]] Index([0, 2], dtype='int64')", "prev_chunk_id": "chunk_384", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_386", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "IntervalIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "IntervalIndex#", "content": "IntervalIndex# IntervalIndex together with its own dtype, IntervalDtype as well as the Interval scalar type, allow first-class support in pandas for interval notation. The IntervalIndex allows some unique indexing and is also used as a return type for the categories in cut() and qcut().", "prev_chunk_id": "chunk_385", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_387", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Indexing with an IntervalIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with an IntervalIndex#", "content": "Indexing with an IntervalIndex# An IntervalIndex can be used in Series and in DataFrame as the index. df = pd.DataFrame( .....: {\"A\": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4]) .....: ) .....: df A (0, 1] 1 (1, 2] 2 (2, 3] 3 (3, 4] 4 Label based indexing via .loc along the edges of an interval works as you would expect, selecting that particular interval. df.loc[2] A 2 Name: (1, 2], dtype: int64 df.loc[[2, 3]] A (1, 2] 2 (2, 3] 3 If you select a label contained within an interval, this will also select the interval. df.loc[2.5] A 3 Name: (2, 3], dtype: int64 df.loc[[2.5, 3.5]] A (2, 3] 3 (3, 4] 4 Selecting using an Interval will only return exact matches. df.loc[pd.Interval(1, 2)] A 2 Name: (1, 2], dtype: int64 Trying to select an Interval that is not exactly contained in the IntervalIndex will raise a KeyError. df.loc[pd.Interval(0.5, 2.5)] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Cell In[189], line 1 ----> 1 df.loc[pd.Interval(0.5, 2.5)] File ~/work/pandas/pandas/pandas/core/indexing.py:1191, in _LocationIndexer.__getitem__(self, key) 1189 maybe_callable = com.apply_if_callable(key, self.obj) 1190 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable) -> 1191 return self._getitem_axis(maybe_callable, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1431, in _LocIndexer._getitem_axis(self, key, axis) 1429 # fall thru to straight lookup 1430 self._validate_key(key, axis) -> 1431 return self._get_label(key, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1381, in _LocIndexer._get_label(self, label, axis) 1379 def _get_label(self, label, axis: AxisInt): 1380 # GH#5567 this will fail if the label is not present in the axis. -> 1381 return self.obj.xs(label, axis=axis) File ~/work/pandas/pandas/pandas/core/generic.py:4320, in NDFrame.xs(self, key, axis, level, drop_level) 4318 new_index = index[loc] 4319 else: -> 4320 loc = index.get_loc(key) 4322 if isinstance(loc, np.ndarray): 4323 if loc.dtype == np.bool_: File ~/work/pandas/pandas/pandas/core/indexes/interval.py:679, in IntervalIndex.get_loc(self, key) 677 matches = mask.sum() 678 if matches == 0: --> 679 raise KeyError(key) 680 if matches == 1: 681 return mask.argmax() KeyError: Interval(0.5, 2.5, closed='right')", "prev_chunk_id": "chunk_386", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_388", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Indexing with an IntervalIndex#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with an IntervalIndex#", "content": "Selecting all Intervals that overlap a given Interval can be performed using the overlaps() method to create a boolean indexer. idxr = df.index.overlaps(pd.Interval(0.5, 2.5)) idxr array([ True, True, True, False]) df[idxr] A (0, 1] 1 (1, 2] 2 (2, 3] 3", "prev_chunk_id": "chunk_387", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_389", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Binning data with cut and qcut#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Binning data with cut and qcut#", "content": "Binning data with cut and qcut# cut() and qcut() both return a Categorical object, and the bins they create are stored as an IntervalIndex in its .categories attribute. c = pd.cut(range(4), bins=2) c [(-0.003, 1.5], (-0.003, 1.5], (1.5, 3.0], (1.5, 3.0]] Categories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]] c.categories IntervalIndex([(-0.003, 1.5], (1.5, 3.0]], dtype='interval[float64, right]') cut() also accepts an IntervalIndex for its bins argument, which enables a useful pandas idiom. First, We call cut() with some data and bins set to a fixed number, to generate the bins. Then, we pass the values of .categories as the bins argument in subsequent calls to cut(), supplying new data which will be binned into the same bins. pd.cut([0, 3, 5, 1], bins=c.categories) [(-0.003, 1.5], (1.5, 3.0], NaN, (-0.003, 1.5]] Categories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]] Any value which falls outside all bins will be assigned a NaN value.", "prev_chunk_id": "chunk_388", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_390", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Generating ranges of intervals#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Generating ranges of intervals#", "content": "Generating ranges of intervals# If we need intervals on a regular frequency, we can use the interval_range() function to create an IntervalIndex using various combinations of start, end, and periods. The default frequency for interval_range is a 1 for numeric intervals, and calendar day for datetime-like intervals: pd.interval_range(start=0, end=5) IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]], dtype='interval[int64, right]') pd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4) IntervalIndex([(2017-01-01 00:00:00, 2017-01-02 00:00:00], (2017-01-02 00:00:00, 2017-01-03 00:00:00], (2017-01-03 00:00:00, 2017-01-04 00:00:00], (2017-01-04 00:00:00, 2017-01-05 00:00:00]], dtype='interval[datetime64[ns], right]') pd.interval_range(end=pd.Timedelta(\"3 days\"), periods=3) IntervalIndex([(0 days 00:00:00, 1 days 00:00:00], (1 days 00:00:00, 2 days 00:00:00], (2 days 00:00:00, 3 days 00:00:00]], dtype='interval[timedelta64[ns], right]') The freq parameter can used to specify non-default frequencies, and can utilize a variety of frequency aliases with datetime-like intervals: pd.interval_range(start=0, periods=5, freq=1.5) IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0], (6.0, 7.5]], dtype='interval[float64, right]') pd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4, freq=\"W\") IntervalIndex([(2017-01-01 00:00:00, 2017-01-08 00:00:00], (2017-01-08 00:00:00, 2017-01-15 00:00:00], (2017-01-15 00:00:00, 2017-01-22 00:00:00], (2017-01-22 00:00:00, 2017-01-29 00:00:00]], dtype='interval[datetime64[ns], right]') pd.interval_range(start=pd.Timedelta(\"0 days\"), periods=3, freq=\"9h\") IntervalIndex([(0 days 00:00:00, 0 days 09:00:00], (0 days 09:00:00, 0 days 18:00:00], (0 days 18:00:00, 1 days 03:00:00]], dtype='interval[timedelta64[ns], right]') Additionally, the closed parameter can be used to specify which side(s) the intervals are closed on. Intervals are closed on the right side by default. pd.interval_range(start=0, end=4, closed=\"both\") IntervalIndex([[0, 1], [1, 2], [2, 3], [3, 4]], dtype='interval[int64, both]') pd.interval_range(start=0, end=4, closed=\"neither\") IntervalIndex([(0, 1), (1, 2), (2, 3), (3, 4)], dtype='interval[int64, neither]') Specifying start, end, and periods will generate a range of evenly spaced intervals from start to end inclusively, with periods number of elements in the resulting IntervalIndex: pd.interval_range(start=0, end=6, periods=4) IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]], dtype='interval[float64, right]') pd.interval_range(pd.Timestamp(\"2018-01-01\"), pd.Timestamp(\"2018-02-28\"), periods=3) IntervalIndex([(2018-01-01 00:00:00, 2018-01-20 08:00:00], (2018-01-20 08:00:00, 2018-02-08 16:00:00], (2018-02-08 16:00:00, 2018-02-28 00:00:00]], dtype='interval[datetime64[ns], right]')", "prev_chunk_id": "chunk_389", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_391", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Integer indexing#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Integer indexing#", "content": "Integer indexing# Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index only label-based indexing is possible with the standard tools like .loc. The following code will generate exceptions: s = pd.Series(range(5)) s[-1] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/range.py:413, in RangeIndex.get_loc(self, key) 412 try: --> 413 return self._range.index(new_key) 414 except ValueError as err: ValueError: -1 is not in range The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) Cell In[208], line 1 ----> 1 s[-1] File ~/work/pandas/pandas/pandas/core/series.py:1130, in Series.__getitem__(self, key) 1127 return self._values[key] 1129 elif key_is_scalar: -> 1130 return self._get_value(key) 1132 # Convert generator to list before going through hashable part 1133 # (We will iterate through the generator there to check for slices) 1134 if is_iterator(key): File ~/work/pandas/pandas/pandas/core/series.py:1246, in Series._get_value(self, label, takeable) 1243 return self._values[label] 1245 # Similar to Index.get_value, but we do not fall back to positional -> 1246 loc = self.index.get_loc(label) 1248 if is_integer(loc): 1249 return self._values[loc] File ~/work/pandas/pandas/pandas/core/indexes/range.py:415, in RangeIndex.get_loc(self, key) 413 return self._range.index(new_key) 414 except ValueError as err: --> 415 raise KeyError(key) from err 416 if isinstance(key, Hashable): 417 raise KeyError(key) KeyError: -1 df = pd.DataFrame(np.random.randn(5, 4)) df 0 1 2 3 0 -0.435772 -1.188928 -0.808286 -0.284634 1 -1.815703 1.347213 -0.243487 0.514704 2 1.162969 -0.287725 -0.179734 0.993962 3 -0.212673 0.909872 -0.733333 -0.349893 4 0.456434 -0.306735 0.553396 0.166221 df.loc[-2:] 0 1 2 3 0 -0.435772 -1.188928 -0.808286 -0.284634 1 -1.815703 1.347213 -0.243487 0.514704 2 1.162969 -0.287725 -0.179734 0.993962 3 -0.212673 0.909872 -0.733333 -0.349893 4 0.456434 -0.306735 0.553396 0.166221 This deliberate decision was made to prevent ambiguities and subtle bugs", "prev_chunk_id": "chunk_390", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_392", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Integer indexing#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Integer indexing#", "content": "(many users reported finding bugs when the API change was made to stop “falling back” on position-based indexing).", "prev_chunk_id": "chunk_391", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_393", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Non-monotonic indexes require exact matches#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Non-monotonic indexes require exact matches#", "content": "Non-monotonic indexes require exact matches# If the index of a Series or DataFrame is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python list. Monotonicity of an index can be tested with the is_monotonic_increasing() and is_monotonic_decreasing() attributes. df = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=[\"data\"], data=list(range(5))) df.index.is_monotonic_increasing True # no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4: df.loc[0:4, :] data 2 0 3 1 3 2 4 3 # slice is are outside the index, so empty DataFrame is returned df.loc[13:15, :] Empty DataFrame Columns: [data] Index: [] On the other hand, if the index is not monotonic, then both slice bounds must be unique members of the index. df = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=[\"data\"], data=list(range(6))) df.index.is_monotonic_increasing False # OK because 2 and 4 are in the index df.loc[2:4, :] data 2 0 3 1 1 2 4 3 # 0 is not in the index df.loc[0:4, :] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key) 3811 try: -> 3812 return self._engine.get_loc(casted_key) 3813 except KeyError as err: File ~/work/pandas/pandas/pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc() File ~/work/pandas/pandas/pandas/_libs/index.pyx:191, in pandas._libs.index.IndexEngine.get_loc() File ~/work/pandas/pandas/pandas/_libs/index.pyx:234, in pandas._libs.index.IndexEngine._get_loc_duplicates() File ~/work/pandas/pandas/pandas/_libs/index.pyx:242, in pandas._libs.index.IndexEngine._maybe_get_bool_indexer() File ~/work/pandas/pandas/pandas/_libs/index.pyx:134, in pandas._libs.index._unpack_bool_indexer() KeyError: 0 The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) Cell In[219], line 1 ----> 1 df.loc[0:4, :] File ~/work/pandas/pandas/pandas/core/indexing.py:1184, in _LocationIndexer.__getitem__(self, key) 1182 if self._is_scalar_access(key): 1183 return self.obj._get_value(*key, takeable=self._takeable) -> 1184 return self._getitem_tuple(key) 1185 else: 1186 # we by definition only have the 0th axis 1187 axis = self.axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1377, in _LocIndexer._getitem_tuple(self, tup) 1374 if self._multi_take_opportunity(tup): 1375 return self._multi_take(tup) -> 1377 return self._getitem_tuple_same_dim(tup) File ~/work/pandas/pandas/pandas/core/indexing.py:1020, in _LocationIndexer._getitem_tuple_same_dim(self, tup)", "prev_chunk_id": "chunk_392", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_394", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Non-monotonic indexes require exact matches#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Non-monotonic indexes require exact matches#", "content": "1017 if com.is_null_slice(key): 1018 continue -> 1020 retval = getattr(retval, self.name)._getitem_axis(key, axis=i) 1021 # We should never have retval.ndim < self.ndim, as that should 1022 # be handled by the _getitem_lowerdim call above. 1023 assert retval.ndim == self.ndim File ~/work/pandas/pandas/pandas/core/indexing.py:1411, in _LocIndexer._getitem_axis(self, key, axis) 1409 if isinstance(key, slice): 1410 self._validate_key(key, axis) -> 1411 return self._get_slice_axis(key, axis=axis) 1412 elif com.is_bool_indexer(key): 1413 return self._getbool_axis(key, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1443, in _LocIndexer._get_slice_axis(self, slice_obj, axis) 1440 return obj.copy(deep=False) 1442 labels = obj._get_axis(axis) -> 1443 indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step) 1445 if isinstance(indexer, slice): 1446 return self.obj._slice(indexer, axis=axis) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer(self, start, end, step) 6664 def slice_indexer( 6665 self, 6666 start: Hashable | None = None, 6667 end: Hashable | None = None, 6668 step: int | None = None, 6669 ) -> slice: 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice, end_slice = self.slice_locs(start, end, step=step) 6710 # return a slice 6711 if not is_scalar(start_slice): File ~/work/pandas/pandas/pandas/core/indexes/base.py:6934, in Index.slice_locs(self, start, end, step) 6932 start_slice = None 6933 if start is not None: -> 6934 start_slice = self.get_slice_bound(start, \"left\") 6935 if start_slice is None: 6936 start_slice = 0 File ~/work/pandas/pandas/pandas/core/indexes/base.py:6859, in Index.get_slice_bound(self, label, side) 6856 return self._searchsorted_monotonic(label, side) 6857 except ValueError: 6858 # raise the original KeyError -> 6859 raise err 6861 if isinstance(slc, np.ndarray): 6862 # get_loc may return a boolean array, which 6863 # is OK as long as they are representable by a slice. 6864 assert is_bool_dtype(slc.dtype) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6853, in Index.get_slice_bound(self, label, side) 6851 # we need to look up the label 6852 try: -> 6853 slc = self.get_loc(label) 6854 except KeyError as err: 6855 try: File ~/work/pandas/pandas/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key) 3814 if isinstance(casted_key, slice) or ( 3815 isinstance(casted_key, abc.Iterable) 3816 and any(isinstance(x, slice) for x in casted_key)", "prev_chunk_id": "chunk_393", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_395", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Non-monotonic indexes require exact matches#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Non-monotonic indexes require exact matches#", "content": "3817 ): 3818 raise InvalidIndexError(key) -> 3819 raise KeyError(key) from err 3820 except TypeError: 3821 # If we have a listlike key, _check_indexing_error will raise 3822 # InvalidIndexError. Otherwise we fall through and re-raise 3823 # the TypeError. 3824 self._check_indexing_error(key) KeyError: 0 # 3 is not a unique label df.loc[2:3, :] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Cell In[220], line 1 ----> 1 df.loc[2:3, :] File ~/work/pandas/pandas/pandas/core/indexing.py:1184, in _LocationIndexer.__getitem__(self, key) 1182 if self._is_scalar_access(key): 1183 return self.obj._get_value(*key, takeable=self._takeable) -> 1184 return self._getitem_tuple(key) 1185 else: 1186 # we by definition only have the 0th axis 1187 axis = self.axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1377, in _LocIndexer._getitem_tuple(self, tup) 1374 if self._multi_take_opportunity(tup): 1375 return self._multi_take(tup) -> 1377 return self._getitem_tuple_same_dim(tup) File ~/work/pandas/pandas/pandas/core/indexing.py:1020, in _LocationIndexer._getitem_tuple_same_dim(self, tup) 1017 if com.is_null_slice(key): 1018 continue -> 1020 retval = getattr(retval, self.name)._getitem_axis(key, axis=i) 1021 # We should never have retval.ndim < self.ndim, as that should 1022 # be handled by the _getitem_lowerdim call above. 1023 assert retval.ndim == self.ndim File ~/work/pandas/pandas/pandas/core/indexing.py:1411, in _LocIndexer._getitem_axis(self, key, axis) 1409 if isinstance(key, slice): 1410 self._validate_key(key, axis) -> 1411 return self._get_slice_axis(key, axis=axis) 1412 elif com.is_bool_indexer(key): 1413 return self._getbool_axis(key, axis=axis) File ~/work/pandas/pandas/pandas/core/indexing.py:1443, in _LocIndexer._get_slice_axis(self, slice_obj, axis) 1440 return obj.copy(deep=False) 1442 labels = obj._get_axis(axis) -> 1443 indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step) 1445 if isinstance(indexer, slice): 1446 return self.obj._slice(indexer, axis=axis) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer(self, start, end, step) 6664 def slice_indexer( 6665 self, 6666 start: Hashable | None = None, 6667 end: Hashable | None = None, 6668 step: int | None = None, 6669 ) -> slice: 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice, end_slice = self.slice_locs(start, end, step=step) 6710 # return a slice 6711 if not is_scalar(start_slice): File ~/work/pandas/pandas/pandas/core/indexes/base.py:6940, in Index.slice_locs(self, start, end, step) 6938 end_slice = None 6939 if end is", "prev_chunk_id": "chunk_394", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_396", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Non-monotonic indexes require exact matches#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Non-monotonic indexes require exact matches#", "content": "not None: -> 6940 end_slice = self.get_slice_bound(end, \"right\") 6941 if end_slice is None: 6942 end_slice = len(self) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6867, in Index.get_slice_bound(self, label, side) 6865 slc = lib.maybe_booleans_to_slice(slc.view(\"u1\")) 6866 if isinstance(slc, np.ndarray): -> 6867 raise KeyError( 6868 f\"Cannot get {side} slice bound for non-unique \" 6869 f\"label: {repr(original_label)}\" 6870 ) 6872 if isinstance(slc, slice): 6873 if side == \"left\": KeyError: 'Cannot get right slice bound for non-unique label: 3' Index.is_monotonic_increasing and Index.is_monotonic_decreasing only check that an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with the is_unique() attribute. weakly_monotonic = pd.Index([\"a\", \"b\", \"c\", \"c\"]) weakly_monotonic Index(['a', 'b', 'c', 'c'], dtype='object') weakly_monotonic.is_monotonic_increasing True weakly_monotonic.is_monotonic_increasing & weakly_monotonic.is_unique False", "prev_chunk_id": "chunk_395", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_397", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Endpoints are inclusive#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Endpoints are inclusive#", "content": "Endpoints are inclusive# Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas is inclusive. The primary reason for this is that it is often not possible to easily determine the “successor” or next element after a particular label in an index. For example, consider the following Series: s = pd.Series(np.random.randn(6), index=list(\"abcdef\")) s a -0.101684 b -0.734907 c -0.130121 d -0.476046 e 0.759104 f 0.213379 dtype: float64 Suppose we wished to slice from c to e, using integers this would be accomplished as such: s[2:5] c -0.130121 d -0.476046 e 0.759104 dtype: float64 However, if you only had c and e, determining the next element in the index can be somewhat complicated. For example, the following does not work: s.loc['c':'e' + 1] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[228], line 1 ----> 1 s.loc['c':'e' + 1] TypeError: can only concatenate str (not \"int\") to str A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the design choice to make label-based slicing include both endpoints: s.loc[\"c\":\"e\"] c -0.130121 d -0.476046 e 0.759104 dtype: float64 This is most definitely a “practicality beats purity” sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works.", "prev_chunk_id": "chunk_396", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_398", "url": "https://pandas.pydata.org/docs/user_guide/advanced.html", "title": "Indexing potentially changes underlying Series dtype#", "page_title": "MultiIndex / advanced indexing — pandas 2.3.1 documentation", "breadcrumbs": "Indexing potentially changes underlying Series dtype#", "content": "Indexing potentially changes underlying Series dtype# The different indexing operation can potentially change the dtype of a Series. series1 = pd.Series([1, 2, 3]) series1.dtype dtype('int64') res = series1.reindex([0, 4]) res.dtype dtype('float64') res 0 1.0 4 NaN dtype: float64 series2 = pd.Series([True]) series2.dtype dtype('bool') res = series2.reindex_like(series1) res.dtype dtype('O') res 0 True 1 NaN 2 NaN dtype: object This is because the (re)indexing operations above silently inserts NaNs and the dtype changes accordingly. This can cause some issues when using numpy ufuncs such as numpy.logical_and. See the GH 2388 for a more detailed discussion.", "prev_chunk_id": "chunk_397", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_399", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Copy-on-Write (CoW)#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Copy-on-Write (CoW)#", "content": "Copy-on-Write (CoW)# Copy-on-Write was first introduced in version 1.5.0. Starting from version 2.0 most of the optimizations that become possible through CoW are implemented and supported. All possible optimizations are supported starting from pandas 2.1. CoW will be enabled by default in version 3.0. CoW will lead to more predictable behavior since it is not possible to update more than one object with one statement, e.g. indexing operations or methods won’t have side-effects. Additionally, through delaying copies as long as possible, the average performance and memory usage will improve.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_400", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Previous behavior#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Previous behavior#", "content": "Previous behavior# pandas indexing behavior is tricky to understand. Some operations return views while other return copies. Depending on the result of the operation, mutating one object might accidentally mutate another: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) subset = df[\"foo\"] subset.iloc[0] = 100 df foo bar 0 100 4 1 2 5 2 3 6 Mutating subset, e.g. updating its values, also updates df. The exact behavior is hard to predict. Copy-on-Write solves accidentally modifying more than one object, it explicitly disallows this. With CoW enabled, df is unchanged: pd.options.mode.copy_on_write = True df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) subset = df[\"foo\"] subset.iloc[0] = 100 df foo bar 0 1 4 1 2 5 2 3 6 The following sections will explain what this means and how it impacts existing applications.", "prev_chunk_id": "chunk_399", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_401", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Migrating to Copy-on-Write#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Migrating to Copy-on-Write#", "content": "Migrating to Copy-on-Write# Copy-on-Write will be the default and only mode in pandas 3.0. This means that users need to migrate their code to be compliant with CoW rules. The default mode in pandas will raise warnings for certain cases that will actively change behavior and thus change user intended behavior. We added another mode, e.g. pd.options.mode.copy_on_write = \"warn\" that will warn for every operation that will change behavior with CoW. We expect this mode to be very noisy, since many cases that we don’t expect that they will influence users will also emit a warning. We recommend checking this mode and analyzing the warnings, but it is not necessary to address all of these warning. The first two items of the following lists are the only cases that need to be addressed to make existing code work with CoW. The following few items describe the user visible changes: Chained assignment will never work loc should be used as an alternative. Check the chained assignment section for more details. Accessing the underlying array of a pandas object will return a read-only view ser = pd.Series([1, 2, 3]) ser.to_numpy() array([1, 2, 3]) This example returns a NumPy array that is a view of the Series object. This view can be modified and thus also modify the pandas object. This is not compliant with CoW rules. The returned array is set to non-writeable to protect against this behavior. Creating a copy of this array allows modification. You can also make the array writeable again if you don’t care about the pandas object anymore. See the section about read-only NumPy arrays for more details. Only one pandas object is updated at once The following code snippet updates both df and subset without CoW: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})", "prev_chunk_id": "chunk_400", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_402", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Migrating to Copy-on-Write#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Migrating to Copy-on-Write#", "content": "subset = df[\"foo\"] subset.iloc[0] = 100 df foo bar 0 1 4 1 2 5 2 3 6 This won’t be possible anymore with CoW, since the CoW rules explicitly forbid this. This includes updating a single column as a Series and relying on the change propagating back to the parent DataFrame. This statement can be rewritten into a single statement with loc or iloc if this behavior is necessary. DataFrame.where() is another suitable alternative for this case. Updating a column selected from a DataFrame with an inplace method will also not work anymore. df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df[\"foo\"].replace(1, 5, inplace=True) df foo bar 0 1 4 1 2 5 2 3 6 This is another form of chained assignment. This can generally be rewritten in 2 different forms: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df.replace({\"foo\": {1: 5}}, inplace=True) df foo bar 0 5 4 1 2 5 2 3 6 A different alternative would be to not use inplace: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df[\"foo\"] = df[\"foo\"].replace(1, 5) df foo bar 0 5 4 1 2 5 2 3 6 Constructors now copy NumPy arrays by default The Series and DataFrame constructors will now copy NumPy array by default when not otherwise specified. This was changed to avoid mutating a pandas object when the NumPy array is changed inplace outside of pandas. You can set copy=False to avoid this copy.", "prev_chunk_id": "chunk_401", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_403", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Description#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Description#", "content": "Description# CoW means that any DataFrame or Series derived from another in any way always behaves as a copy. As a consequence, we can only change the values of an object through modifying the object itself. CoW disallows updating a DataFrame or a Series that shares data with another DataFrame or Series object inplace. This avoids side-effects when modifying values and hence, most methods can avoid actually copying the data and only trigger a copy when necessary. The following example will operate inplace with CoW: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df.iloc[0, 0] = 100 df foo bar 0 100 4 1 2 5 2 3 6 The object df does not share any data with any other object and hence no copy is triggered when updating the values. In contrast, the following operation triggers a copy of the data under CoW: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df2 = df.reset_index(drop=True) df2.iloc[0, 0] = 100 df foo bar 0 1 4 1 2 5 2 3 6 df2 foo bar 0 100 4 1 2 5 2 3 6 reset_index returns a lazy copy with CoW while it copies the data without CoW. Since both objects, df and df2 share the same data, a copy is triggered when modifying df2. The object df still has the same values as initially while df2 was modified. If the object df isn’t needed anymore after performing the reset_index operation, you can emulate an inplace-like operation through assigning the output of reset_index to the same variable: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df = df.reset_index(drop=True) df.iloc[0, 0] = 100 df foo bar 0 100 4 1 2 5 2 3 6 The initial object gets out of scope as soon as the", "prev_chunk_id": "chunk_402", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_404", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Description#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Description#", "content": "result of reset_index is reassigned and hence df does not share data with any other object. No copy is necessary when modifying the object. This is generally true for all methods listed in Copy-on-Write optimizations. Previously, when operating on views, the view and the parent object was modified: with pd.option_context(\"mode.copy_on_write\", False): ....: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) ....: view = df[:] ....: df.iloc[0, 0] = 100 ....: df foo bar 0 100 4 1 2 5 2 3 6 view foo bar 0 100 4 1 2 5 2 3 6 CoW triggers a copy when df is changed to avoid mutating view as well: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) view = df[:] df.iloc[0, 0] = 100 df foo bar 0 100 4 1 2 5 2 3 6 view foo bar 0 1 4 1 2 5 2 3 6", "prev_chunk_id": "chunk_403", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_405", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Chained Assignment#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Chained Assignment#", "content": "Chained Assignment# Chained assignment references a technique where an object is updated through two subsequent indexing operations, e.g. with pd.option_context(\"mode.copy_on_write\", False): ....: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) ....: df[\"foo\"][df[\"bar\"] > 5] = 100 ....: df ....: The column foo is updated where the column bar is greater than 5. This violates the CoW principles though, because it would have to modify the view df[\"foo\"] and df in one step. Hence, chained assignment will consistently never work and raise a ChainedAssignmentError warning with CoW enabled: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]}) df[\"foo\"][df[\"bar\"] > 5] = 100 With copy on write this can be done by using loc. df.loc[df[\"bar\"] > 5, \"foo\"] = 100", "prev_chunk_id": "chunk_404", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_406", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Read-only NumPy arrays#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Read-only NumPy arrays#", "content": "Read-only NumPy arrays# Accessing the underlying NumPy array of a DataFrame will return a read-only array if the array shares data with the initial DataFrame: The array is a copy if the initial DataFrame consists of more than one array: df = pd.DataFrame({\"a\": [1, 2], \"b\": [1.5, 2.5]}) df.to_numpy() array([[1. , 1.5], [2. , 2.5]]) The array shares data with the DataFrame if the DataFrame consists of only one NumPy array: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}) df.to_numpy() array([[1, 3], [2, 4]]) This array is read-only, which means that it can’t be modified inplace: arr = df.to_numpy() arr[0, 0] = 100 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[54], line 1 ----> 1 arr[0, 0] = 100 ValueError: assignment destination is read-only The same holds true for a Series, since a Series always consists of a single array. There are two potential solution to this: - Trigger a copy manually if you want to avoid updating DataFrames that share memory with your array. - Make the array writeable. This is a more performant solution but circumvents Copy-on-Write rules, so it should be used with caution. arr = df.to_numpy() arr.flags.writeable = True arr[0, 0] = 100 arr array([[100, 3], [ 2, 4]])", "prev_chunk_id": "chunk_405", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_407", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Patterns to avoid#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Patterns to avoid#", "content": "Patterns to avoid# No defensive copy will be performed if two objects share the same data while you are modifying one object inplace. df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) df2 = df.reset_index(drop=True) df2.iloc[0, 0] = 100 This creates two objects that share data and thus the setitem operation will trigger a copy. This is not necessary if the initial object df isn’t needed anymore. Simply reassigning to the same variable will invalidate the reference that is held by the object. df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) df = df.reset_index(drop=True) df.iloc[0, 0] = 100 No copy is necessary in this example. Creating multiple references keeps unnecessary references alive and thus will hurt performance with Copy-on-Write.", "prev_chunk_id": "chunk_406", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_408", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "Copy-on-Write optimizations#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "Copy-on-Write optimizations#", "content": "Copy-on-Write optimizations# A new lazy copy mechanism that defers the copy until the object in question is modified and only if this object shares data with another object. This mechanism was added to methods that don’t require a copy of the underlying data. Popular examples are DataFrame.drop() for axis=1 and DataFrame.rename(). These methods return views when Copy-on-Write is enabled, which provides a significant performance improvement compared to the regular execution.", "prev_chunk_id": "chunk_407", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_409", "url": "https://pandas.pydata.org/docs/user_guide/copy_on_write.html", "title": "How to enable CoW#", "page_title": "Copy-on-Write (CoW) — pandas 2.3.1 documentation", "breadcrumbs": "How to enable CoW#", "content": "How to enable CoW# Copy-on-Write can be enabled through the configuration option copy_on_write. The option can be turned on __globally__ through either of the following: pd.set_option(\"mode.copy_on_write\", True) pd.options.mode.copy_on_write = True", "prev_chunk_id": "chunk_408", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_410", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge, join, concatenate and compare#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge, join, concatenate and compare#", "content": "Merge, join, concatenate and compare# pandas provides various methods for combining and comparing Series or DataFrame. - concat(): Merge multipleSeriesorDataFrameobjects along a shared index or column - DataFrame.join(): Merge multipleDataFrameobjects along the columns - DataFrame.combine_first(): Update missing values with non-missing values in the same location - merge(): Combine twoSeriesorDataFrameobjects with SQL-style joining - merge_ordered(): Combine twoSeriesorDataFrameobjects along an ordered axis - merge_asof(): Combine twoSeriesorDataFrameobjects by near instead of exact matching keys - Series.compare()andDataFrame.compare(): Show differences in values between twoSeriesorDataFrameobjects", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_411", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "concat()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "concat()#", "content": "concat()# The concat() function concatenates an arbitrary amount of Series or DataFrame objects along an axis while performing optional set logic (union or intersection) of the indexes on the other axes. Like numpy.concatenate, concat() takes a list or dict of homogeneously-typed objects and concatenates them. df1 = pd.DataFrame( ...: { ...: \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"], ...: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], ...: \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], ...: \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"], ...: }, ...: index=[0, 1, 2, 3], ...: ) ...: df2 = pd.DataFrame( ...: { ...: \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"], ...: \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"], ...: \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"], ...: \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"], ...: }, ...: index=[4, 5, 6, 7], ...: ) ...: df3 = pd.DataFrame( ...: { ...: \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"], ...: \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"], ...: \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"], ...: \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"], ...: }, ...: index=[8, 9, 10, 11], ...: ) ...: frames = [df1, df2, df3] result = pd.concat(frames) result A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11", "prev_chunk_id": "chunk_410", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_412", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Joining logic of the resulting axis#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Joining logic of the resulting axis#", "content": "Joining logic of the resulting axis# The join keyword specifies how to handle axis values that don’t exist in the first DataFrame. join='outer' takes the union of all axis values df4 = pd.DataFrame( ...: { ...: \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"], ...: \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"], ...: \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"], ...: }, ...: index=[2, 3, 6, 7], ...: ) ...: result = pd.concat([df1, df4], axis=1) result A B C D B D F 0 A0 B0 C0 D0 NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3 6 NaN NaN NaN NaN B6 D6 F6 7 NaN NaN NaN NaN B7 D7 F7 join='inner' takes the intersection of the axis values result = pd.concat([df1, df4], axis=1, join=\"inner\") result A B C D B D F 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3 To perform an effective “left” join using the exact index from the original DataFrame, result can be reindexed. result = pd.concat([df1, df4], axis=1).reindex(df1.index) result A B C D B D F 0 A0 B0 C0 D0 NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3", "prev_chunk_id": "chunk_411", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_413", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Ignoring indexes on the concatenation axis#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Ignoring indexes on the concatenation axis#", "content": "Ignoring indexes on the concatenation axis# For DataFrame objects which don’t have a meaningful index, the ignore_index ignores overlapping indexes. result = pd.concat([df1, df4], ignore_index=True, sort=False) result A B C D F 0 A0 B0 C0 D0 NaN 1 A1 B1 C1 D1 NaN 2 A2 B2 C2 D2 NaN 3 A3 B3 C3 D3 NaN 4 NaN B2 NaN D2 F2 5 NaN B3 NaN D3 F3 6 NaN B6 NaN D6 F6 7 NaN B7 NaN D7 F7", "prev_chunk_id": "chunk_412", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_414", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Concatenating Series and DataFrame together#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating Series and DataFrame together#", "content": "Concatenating Series and DataFrame together# You can concatenate a mix of Series and DataFrame objects. The Series will be transformed to DataFrame with the column name as the name of the Series. s1 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], name=\"X\") result = pd.concat([df1, s1], axis=1) result A B C D X 0 A0 B0 C0 D0 X0 1 A1 B1 C1 D1 X1 2 A2 B2 C2 D2 X2 3 A3 B3 C3 D3 X3 Unnamed Series will be numbered consecutively. s2 = pd.Series([\"_0\", \"_1\", \"_2\", \"_3\"]) result = pd.concat([df1, s2, s2, s2], axis=1) result A B C D 0 1 2 0 A0 B0 C0 D0 _0 _0 _0 1 A1 B1 C1 D1 _1 _1 _1 2 A2 B2 C2 D2 _2 _2 _2 3 A3 B3 C3 D3 _3 _3 _3 ignore_index=True will drop all name references. result = pd.concat([df1, s1], axis=1, ignore_index=True) result 0 1 2 3 4 0 A0 B0 C0 D0 X0 1 A1 B1 C1 D1 X1 2 A2 B2 C2 D2 X2 3 A3 B3 C3 D3 X3", "prev_chunk_id": "chunk_413", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_415", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Resulting keys#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Resulting keys#", "content": "Resulting keys# The keys argument adds another axis level to the resulting index or column (creating a MultiIndex) associate specific keys with each original DataFrame. result = pd.concat(frames, keys=[\"x\", \"y\", \"z\"]) result A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 result.loc[\"y\"] A B C D 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 The keys argument cane override the column names when creating a new DataFrame based on existing Series. s3 = pd.Series([0, 1, 2, 3], name=\"foo\") s4 = pd.Series([0, 1, 2, 3]) s5 = pd.Series([0, 1, 4, 5]) pd.concat([s3, s4, s5], axis=1) foo 0 1 0 0 0 0 1 1 1 1 2 2 2 4 3 3 3 5 pd.concat([s3, s4, s5], axis=1, keys=[\"red\", \"blue\", \"yellow\"]) red blue yellow 0 0 0 0 1 1 1 1 2 2 2 4 3 3 3 5 You can also pass a dict to concat() in which case the dict keys will be used for the keys argument unless other keys argument is specified: pieces = {\"x\": df1, \"y\": df2, \"z\": df3} result = pd.concat(pieces) result A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8", "prev_chunk_id": "chunk_414", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_416", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Resulting keys#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Resulting keys#", "content": "9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 result = pd.concat(pieces, keys=[\"z\", \"y\"]) result A B C D z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 The MultiIndex created has levels that are constructed from the passed keys and the index of the DataFrame pieces: result.index.levels FrozenList([['z', 'y'], [4, 5, 6, 7, 8, 9, 10, 11]]) levels argument allows specifying resulting levels associated with the keys result = pd.concat( ....: pieces, keys=[\"x\", \"y\", \"z\"], levels=[[\"z\", \"y\", \"x\", \"w\"]], names=[\"group_key\"] ....: ) ....: result A B C D group_key x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 result.index.levels FrozenList([['z', 'y', 'x', 'w'], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])", "prev_chunk_id": "chunk_415", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_417", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Appending rows to a DataFrame#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Appending rows to a DataFrame#", "content": "Appending rows to a DataFrame# If you have a Series that you want to append as a single row to a DataFrame, you can convert the row into a DataFrame and use concat() s2 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], index=[\"A\", \"B\", \"C\", \"D\"]) result = pd.concat([df1, s2.to_frame().T], ignore_index=True) result A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 X0 X1 X2 X3", "prev_chunk_id": "chunk_416", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_418", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "merge()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "merge()#", "content": "merge()# merge() performs join operations similar to relational databases like SQL. Users who are familiar with SQL but new to pandas can reference a comparison with SQL.", "prev_chunk_id": "chunk_417", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_419", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge types#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge types#", "content": "Merge types# merge() implements common SQL style joining operations. - one-to-one: joining twoDataFrameobjects on their indexes which must contain unique values. - many-to-one: joining a unique index to one or more columns in a differentDataFrame. - many-to-many: joining columns on columns. For a many-to-many join, if a key combination appears more than once in both tables, the DataFrame will have the Cartesian product of the associated data. left = pd.DataFrame( ....: { ....: \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"], ....: \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"], ....: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], ....: } ....: ) ....: right = pd.DataFrame( ....: { ....: \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"], ....: \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], ....: \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"], ....: } ....: ) ....: result = pd.merge(left, right, on=\"key\") result key A B C D 0 K0 A0 B0 C0 D0 1 K1 A1 B1 C1 D1 2 K2 A2 B2 C2 D2 3 K3 A3 B3 C3 D3 The how argument to merge() specifies which keys are included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names: Merge method | SQL Join Name | Description left | LEFT OUTER JOIN | Use keys from left frame only right | RIGHT OUTER JOIN | Use keys from right frame only outer | FULL OUTER JOIN | Use union of keys from both frames inner | INNER JOIN | Use intersection of keys from both frames cross | CROSS JOIN | Create the cartesian product of rows of both frames left = pd.DataFrame( ....: { ....: \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"], ....: \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"], ....: \"A\": [\"A0\", \"A1\",", "prev_chunk_id": "chunk_418", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_420", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge types#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge types#", "content": "\"A2\", \"A3\"], ....: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], ....: } ....: ) ....: right = pd.DataFrame( ....: { ....: \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"], ....: \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"], ....: \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], ....: \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"], ....: } ....: ) ....: result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"]) result key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"]) result key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 3 K2 K0 NaN NaN C3 D3 result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"]) result key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K0 NaN NaN C3 D3 5 K2 K1 A3 B3 NaN NaN result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"]) result key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 result = pd.merge(left, right, how=\"cross\") result key1_x key2_x A B key1_y key2_y C D 0 K0 K0 A0 B0 K0 K0 C0 D0 1 K0 K0 A0 B0 K1 K0 C1 D1 2 K0 K0 A0 B0 K1 K0 C2 D2 3 K0 K0 A0 B0 K2 K0 C3 D3 4 K0 K1 A1 B1 K0 K0 C0 D0 .. ... ... .. .. ... ... .. .. 11 K1 K0", "prev_chunk_id": "chunk_419", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_421", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge types#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge types#", "content": "A2 B2 K2 K0 C3 D3 12 K2 K1 A3 B3 K0 K0 C0 D0 13 K2 K1 A3 B3 K1 K0 C1 D1 14 K2 K1 A3 B3 K1 K0 C2 D2 15 K2 K1 A3 B3 K2 K0 C3 D3 [16 rows x 8 columns] You can Series and a DataFrame with a MultiIndex if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using Series.reset_index() before merging df = pd.DataFrame({\"Let\": [\"A\", \"B\", \"C\"], \"Num\": [1, 2, 3]}) df Let Num 0 A 1 1 B 2 2 C 3 ser = pd.Series( ....: [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], ....: index=pd.MultiIndex.from_arrays( ....: [[\"A\", \"B\", \"C\"] * 2, [1, 2, 3, 4, 5, 6]], names=[\"Let\", \"Num\"] ....: ), ....: ) ....: ser Let Num A 1 a B 2 b C 3 c A 4 d B 5 e C 6 f dtype: object pd.merge(df, ser.reset_index(), on=[\"Let\", \"Num\"]) Let Num 0 0 A 1 a 1 B 2 b 2 C 3 c Performing an outer join with duplicate join keys in DataFrame left = pd.DataFrame({\"A\": [1, 2], \"B\": [2, 2]}) right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]}) result = pd.merge(left, right, on=\"B\", how=\"outer\") result A_x B A_y 0 1 2 4 1 1 2 5 2 1 2 6 3 2 2 4 4 2 2 5 5 2 2 6", "prev_chunk_id": "chunk_420", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_422", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge key uniqueness#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge key uniqueness#", "content": "Merge key uniqueness# The validate argument checks whether the uniqueness of merge keys. Key uniqueness is checked before merge operations and can protect against memory overflows and unexpected key duplication. left = pd.DataFrame({\"A\": [1, 2], \"B\": [1, 2]}) right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]}) result = pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_one\") --------------------------------------------------------------------------- MergeError Traceback (most recent call last) Cell In[71], line 1 ----> 1 result = pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_one\") File ~/work/pandas/pandas/pandas/core/reshape/merge.py:170, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate) 155 return _cross_merge( 156 left_df, 157 right_df, (...) 167 copy=copy, 168 ) 169 else: --> 170 op = _MergeOperation( 171 left_df, 172 right_df, 173 how=how, 174 on=on, 175 left_on=left_on, 176 right_on=right_on, 177 left_index=left_index, 178 right_index=right_index, 179 sort=sort, 180 suffixes=suffixes, 181 indicator=indicator, 182 validate=validate, 183 ) 184 return op.get_result(copy=copy) File ~/work/pandas/pandas/pandas/core/reshape/merge.py:813, in _MergeOperation.__init__(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate) 809 # If argument passed to validate, 810 # check if columns specified as unique 811 # are in fact unique. 812 if validate is not None: --> 813 self._validate_validate_kwd(validate) File ~/work/pandas/pandas/pandas/core/reshape/merge.py:1658, in _MergeOperation._validate_validate_kwd(self, validate) 1654 raise MergeError( 1655 \"Merge keys are not unique in left dataset; not a one-to-one merge\" 1656 ) 1657 if not right_unique: -> 1658 raise MergeError( 1659 \"Merge keys are not unique in right dataset; not a one-to-one merge\" 1660 ) 1662 elif validate in [\"one_to_many\", \"1:m\"]: 1663 if not left_unique: MergeError: Merge keys are not unique in right dataset; not a one-to-one merge If the user is aware of the duplicates in the right DataFrame but wants to ensure there are no duplicates in the left DataFrame, one can use the validate='one_to_many' argument instead, which will not raise an exception. pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_many\") A_x B A_y 0", "prev_chunk_id": "chunk_421", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_423", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge key uniqueness#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge key uniqueness#", "content": "1 1 NaN 1 2 2 4.0 2 2 2 5.0 3 2 2 6.0", "prev_chunk_id": "chunk_422", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_424", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merge result indicator#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merge result indicator#", "content": "Merge result indicator# merge() accepts the argument indicator. If True, a Categorical-type column called _merge will be added to the output object that takes on values: df1 = pd.DataFrame({\"col1\": [0, 1], \"col_left\": [\"a\", \"b\"]}) df2 = pd.DataFrame({\"col1\": [1, 2, 2], \"col_right\": [2, 2, 2]}) pd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=True) col1 col_left col_right _merge 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only A string argument to indicator will use the value as the name for the indicator column. pd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=\"indicator_column\") col1 col_left col_right indicator_column 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only", "prev_chunk_id": "chunk_423", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_425", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Overlapping value columns#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Overlapping value columns#", "content": "Overlapping value columns# The merge suffixes argument takes a tuple of list of strings to append to overlapping column names in the input DataFrame to disambiguate the result columns: left = pd.DataFrame({\"k\": [\"K0\", \"K1\", \"K2\"], \"v\": [1, 2, 3]}) right = pd.DataFrame({\"k\": [\"K0\", \"K0\", \"K3\"], \"v\": [4, 5, 6]}) result = pd.merge(left, right, on=\"k\") result k v_x v_y 0 K0 1 4 1 K0 1 5 result = pd.merge(left, right, on=\"k\", suffixes=(\"_l\", \"_r\")) result k v_l v_r 0 K0 1 4 1 K0 1 5", "prev_chunk_id": "chunk_424", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_426", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "DataFrame.join()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame.join()#", "content": "DataFrame.join()# DataFrame.join() combines the columns of multiple, potentially differently-indexed DataFrame into a single result DataFrame. left = pd.DataFrame( ....: {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=[\"K0\", \"K1\", \"K2\"] ....: ) ....: right = pd.DataFrame( ....: {\"C\": [\"C0\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D2\", \"D3\"]}, index=[\"K0\", \"K2\", \"K3\"] ....: ) ....: result = left.join(right) result A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 result = left.join(right, how=\"outer\") result A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 K3 NaN NaN C3 D3 result = left.join(right, how=\"inner\") result A B C D K0 A0 B0 C0 D0 K2 A2 B2 C2 D2 DataFrame.join() takes an optional on argument which may be a column or multiple column names that the passed DataFrame is to be aligned. left = pd.DataFrame( ....: { ....: \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"], ....: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], ....: \"key\": [\"K0\", \"K1\", \"K0\", \"K1\"], ....: } ....: ) ....: right = pd.DataFrame({\"C\": [\"C0\", \"C1\"], \"D\": [\"D0\", \"D1\"]}, index=[\"K0\", \"K1\"]) result = left.join(right, on=\"key\") result A B key C D 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K0 C0 D0 3 A3 B3 K1 C1 D1 result = pd.merge( ....: left, right, left_on=\"key\", right_index=True, how=\"left\", sort=False ....: ) ....: result A B key C D 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K0 C0 D0 3 A3 B3 K1 C1 D1 To join on multiple keys, the passed DataFrame must have a MultiIndex: left = pd.DataFrame( ....: { ....: \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"], ....: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], ....: \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"], ....: \"key2\": [\"K0\", \"K1\", \"K0\",", "prev_chunk_id": "chunk_425", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_427", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "DataFrame.join()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame.join()#", "content": "\"K1\"], ....: } ....: ) ....: index = pd.MultiIndex.from_tuples( ....: [(\"K0\", \"K0\"), (\"K1\", \"K0\"), (\"K2\", \"K0\"), (\"K2\", \"K1\")] ....: ) ....: right = pd.DataFrame( ....: {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=index ....: ) ....: result = left.join(right, on=[\"key1\", \"key2\"]) result A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A1 B1 K0 K1 NaN NaN 2 A2 B2 K1 K0 C1 D1 3 A3 B3 K2 K1 C3 D3 The default for DataFrame.join is to perform a left join which uses only the keys found in the calling DataFrame. Other join types can be specified with how. result = left.join(right, on=[\"key1\", \"key2\"], how=\"inner\") result A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 2 A2 B2 K1 K0 C1 D1 3 A3 B3 K2 K1 C3 D3", "prev_chunk_id": "chunk_426", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_428", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Joining a single Index to a MultiIndex#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Joining a single Index to a MultiIndex#", "content": "Joining a single Index to a MultiIndex# You can join a DataFrame with a Index to a DataFrame with a MultiIndex on a level. The name of the Index with match the level name of the MultiIndex. left = pd.DataFrame( .....: {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, .....: index=pd.Index([\"K0\", \"K1\", \"K2\"], name=\"key\"), .....: ) .....: index = pd.MultiIndex.from_tuples( .....: [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")], .....: names=[\"key\", \"Y\"], .....: ) .....: right = pd.DataFrame( .....: {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, .....: index=index, .....: ) .....: result = left.join(right, how=\"inner\") result A B C D key Y K0 Y0 A0 B0 C0 D0 K1 Y1 A1 B1 C1 D1 K2 Y2 A2 B2 C2 D2 Y3 A2 B2 C3 D3", "prev_chunk_id": "chunk_427", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_429", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Joining with two MultiIndex#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Joining with two MultiIndex#", "content": "Joining with two MultiIndex# The MultiIndex of the input argument must be completely used in the join and is a subset of the indices in the left argument. leftindex = pd.MultiIndex.from_product( .....: [list(\"abc\"), list(\"xy\"), [1, 2]], names=[\"abc\", \"xy\", \"num\"] .....: ) .....: left = pd.DataFrame({\"v1\": range(12)}, index=leftindex) left v1 abc xy num a x 1 0 2 1 y 1 2 2 3 b x 1 4 2 5 y 1 6 2 7 c x 1 8 2 9 y 1 10 2 11 rightindex = pd.MultiIndex.from_product( .....: [list(\"abc\"), list(\"xy\")], names=[\"abc\", \"xy\"] .....: ) .....: right = pd.DataFrame({\"v2\": [100 * i for i in range(1, 7)]}, index=rightindex) right v2 abc xy a x 100 y 200 b x 300 y 400 c x 500 y 600 left.join(right, on=[\"abc\", \"xy\"], how=\"inner\") v1 v2 abc xy num a x 1 0 100 2 1 100 y 1 2 200 2 3 200 b x 1 4 300 2 5 300 y 1 6 400 2 7 400 c x 1 8 500 2 9 500 y 1 10 600 2 11 600 leftindex = pd.MultiIndex.from_tuples( .....: [(\"K0\", \"X0\"), (\"K0\", \"X1\"), (\"K1\", \"X2\")], names=[\"key\", \"X\"] .....: ) .....: left = pd.DataFrame( .....: {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=leftindex .....: ) .....: rightindex = pd.MultiIndex.from_tuples( .....: [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")], names=[\"key\", \"Y\"] .....: ) .....: right = pd.DataFrame( .....: {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=rightindex .....: ) .....: result = pd.merge( .....: left.reset_index(), right.reset_index(), on=[\"key\"], how=\"inner\" .....: ).set_index([\"key\", \"X\", \"Y\"]) .....: result A B C D key X Y K0 X0 Y0 A0 B0 C0 D0 X1 Y0 A1 B1 C0 D0 K1 X2 Y1 A2 B2 C1 D1", "prev_chunk_id": "chunk_428", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_430", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Merging on a combination of columns and index levels#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Merging on a combination of columns and index levels#", "content": "Merging on a combination of columns and index levels# Strings passed as the on, left_on, and right_on parameters may refer to either column names or index level names. This enables merging DataFrame instances on a combination of index levels and columns without resetting indexes. left_index = pd.Index([\"K0\", \"K0\", \"K1\", \"K2\"], name=\"key1\") left = pd.DataFrame( .....: { .....: \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"], .....: \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"], .....: \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"], .....: }, .....: index=left_index, .....: ) .....: right_index = pd.Index([\"K0\", \"K1\", \"K2\", \"K2\"], name=\"key1\") right = pd.DataFrame( .....: { .....: \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], .....: \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"], .....: \"key2\": [\"K0\", \"K0\", \"K0\", \"K1\"], .....: }, .....: index=right_index, .....: ) .....: result = left.merge(right, on=[\"key1\", \"key2\"]) result A B key2 C D key1 K0 A0 B0 K0 C0 D0 K1 A2 B2 K0 C1 D1 K2 A3 B3 K1 C3 D3", "prev_chunk_id": "chunk_429", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_431", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "Joining multiple DataFrame#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "Joining multiple DataFrame#", "content": "Joining multiple DataFrame# A list or tuple of :class:`DataFrame` can also be passed to join() to join them together on their indexes. right2 = pd.DataFrame({\"v\": [7, 8, 9]}, index=[\"K1\", \"K1\", \"K2\"]) result = left.join([right, right2])", "prev_chunk_id": "chunk_430", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_432", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "DataFrame.combine_first()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame.combine_first()#", "content": "DataFrame.combine_first()# DataFrame.combine_first() update missing values from one DataFrame with the non-missing values in another DataFrame in the corresponding location. df1 = pd.DataFrame( .....: [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]] .....: ) .....: df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2]) result = df1.combine_first(df2) result 0 1 2 0 NaN 3.0 5.0 1 -4.6 NaN -8.2 2 -5.0 7.0 4.0", "prev_chunk_id": "chunk_431", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_433", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "merge_ordered()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "merge_ordered()#", "content": "merge_ordered()# merge_ordered() combines order data such as numeric or time series data with optional filling of missing data with fill_method. left = pd.DataFrame( .....: {\"k\": [\"K0\", \"K1\", \"K1\", \"K2\"], \"lv\": [1, 2, 3, 4], \"s\": [\"a\", \"b\", \"c\", \"d\"]} .....: ) .....: right = pd.DataFrame({\"k\": [\"K1\", \"K2\", \"K4\"], \"rv\": [1, 2, 3]}) pd.merge_ordered(left, right, fill_method=\"ffill\", left_by=\"s\") k lv s rv 0 K0 1.0 a NaN 1 K1 1.0 a 1.0 2 K2 1.0 a 2.0 3 K4 1.0 a 3.0 4 K1 2.0 b 1.0 5 K2 2.0 b 2.0 6 K4 2.0 b 3.0 7 K1 3.0 c 1.0 8 K2 3.0 c 2.0 9 K4 3.0 c 3.0 10 K1 NaN d 1.0 11 K2 4.0 d 2.0 12 K4 4.0 d 3.0", "prev_chunk_id": "chunk_432", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_434", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "merge_asof()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "merge_asof()#", "content": "merge_asof()# merge_asof() is similar to an ordered left-join except that mactches are on the nearest key rather than equal keys. For each row in the left DataFrame, the last row in the right DataFrame are selected where the on key is less than the left’s key. Both DataFrame must be sorted by the key. Optionally an merge_asof() can perform a group-wise merge by matching the by key in addition to the nearest match on the on key. trades = pd.DataFrame( .....: { .....: \"time\": pd.to_datetime( .....: [ .....: \"20160525 13:30:00.023\", .....: \"20160525 13:30:00.038\", .....: \"20160525 13:30:00.048\", .....: \"20160525 13:30:00.048\", .....: \"20160525 13:30:00.048\", .....: ] .....: ), .....: \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"], .....: \"price\": [51.95, 51.95, 720.77, 720.92, 98.00], .....: \"quantity\": [75, 155, 100, 100, 100], .....: }, .....: columns=[\"time\", \"ticker\", \"price\", \"quantity\"], .....: ) .....: quotes = pd.DataFrame( .....: { .....: \"time\": pd.to_datetime( .....: [ .....: \"20160525 13:30:00.023\", .....: \"20160525 13:30:00.023\", .....: \"20160525 13:30:00.030\", .....: \"20160525 13:30:00.041\", .....: \"20160525 13:30:00.048\", .....: \"20160525 13:30:00.049\", .....: \"20160525 13:30:00.072\", .....: \"20160525 13:30:00.075\", .....: ] .....: ), .....: \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"], .....: \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01], .....: \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03], .....: }, .....: columns=[\"time\", \"ticker\", \"bid\", \"ask\"], .....: ) .....: trades time ticker price quantity 0 2016-05-25 13:30:00.023 MSFT 51.95 75 1 2016-05-25 13:30:00.038 MSFT 51.95 155 2 2016-05-25 13:30:00.048 GOOG 720.77 100 3 2016-05-25 13:30:00.048 GOOG 720.92 100 4 2016-05-25 13:30:00.048 AAPL 98.00 100 quotes time ticker bid ask 0 2016-05-25 13:30:00.023 GOOG 720.50 720.93 1 2016-05-25 13:30:00.023 MSFT 51.95 51.96 2 2016-05-25 13:30:00.030 MSFT 51.97 51.98 3 2016-05-25 13:30:00.041 MSFT 51.99 52.00 4 2016-05-25 13:30:00.048 GOOG 720.50 720.93 5 2016-05-25 13:30:00.049 AAPL 97.99 98.01 6 2016-05-25 13:30:00.072 GOOG 720.50 720.88 7 2016-05-25 13:30:00.075", "prev_chunk_id": "chunk_433", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_435", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "merge_asof()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "merge_asof()#", "content": "MSFT 52.01 52.03 pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\") time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 51.95 51.96 1 2016-05-25 13:30:00.038 MSFT 51.95 155 51.97 51.98 2 2016-05-25 13:30:00.048 GOOG 720.77 100 720.50 720.93 3 2016-05-25 13:30:00.048 GOOG 720.92 100 720.50 720.93 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN merge_asof() within 2ms between the quote time and the trade time. pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")) time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 51.95 51.96 1 2016-05-25 13:30:00.038 MSFT 51.95 155 NaN NaN 2 2016-05-25 13:30:00.048 GOOG 720.77 100 720.50 720.93 3 2016-05-25 13:30:00.048 GOOG 720.92 100 720.50 720.93 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN merge_asof() within 10ms between the quote time and the trade time and exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time. pd.merge_asof( .....: trades, .....: quotes, .....: on=\"time\", .....: by=\"ticker\", .....: tolerance=pd.Timedelta(\"10ms\"), .....: allow_exact_matches=False, .....: ) .....: time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 NaN NaN 1 2016-05-25 13:30:00.038 MSFT 51.95 155 51.97 51.98 2 2016-05-25 13:30:00.048 GOOG 720.77 100 NaN NaN 3 2016-05-25 13:30:00.048 GOOG 720.92 100 NaN NaN 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN", "prev_chunk_id": "chunk_434", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_436", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "compare()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "compare()#", "content": "compare()# The Series.compare() and DataFrame.compare() methods allow you to compare two DataFrame or Series, respectively, and summarize their differences. df = pd.DataFrame( .....: { .....: \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"], .....: \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0], .....: \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0], .....: }, .....: columns=[\"col1\", \"col2\", \"col3\"], .....: ) .....: df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 df2 = df.copy() df2.loc[0, \"col1\"] = \"c\" df2.loc[2, \"col3\"] = 4.0 df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 df.compare(df2) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 By default, if two corresponding values are equal, they will be shown as NaN. Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns. Stack the differences on rows. df.compare(df2, align_axis=0) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep all original rows and columns with keep_shape=True df.compare(df2, keep_shape=True) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all the original values even if they are equal. df.compare(df2, keep_shape=True, keep_equal=True) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4", "prev_chunk_id": "chunk_435", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_437", "url": "https://pandas.pydata.org/docs/user_guide/merging.html", "title": "compare()#", "page_title": "Merge, join, concatenate and compare — pandas 2.3.1 documentation", "breadcrumbs": "compare()#", "content": "a a 5.0 5.0 5.0 5.0", "prev_chunk_id": "chunk_436", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_438", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Reshaping and pivot tables#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Reshaping and pivot tables#", "content": "Reshaping and pivot tables# pandas provides methods for manipulating a Series and DataFrame to alter the representation of the data for further data processing or data summarization. - pivot()andpivot_table(): Group unique values within one or more discrete categories. - stack()andunstack(): Pivot a column or row level to the opposite axis respectively. - melt()andwide_to_long(): Unpivot a wideDataFrameto a long format. - get_dummies()andfrom_dummies(): Conversions with indicator variables. - explode(): Convert a column of list-like values to individual rows. - crosstab(): Calculate a cross-tabulation of multiple 1 dimensional factor arrays. - cut(): Transform continuous variables to discrete, categorical values - factorize(): Encode 1 dimensional variables into integer labels.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_439", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "pivot()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "pivot()#", "content": "pivot()# Data is often stored in so-called “stacked” or “record” format. In a “record” or “wide” format, typically there is one row for each subject. In the “stacked” or “long” format there are multiple rows for each subject where applicable. data = { ...: \"value\": range(12), ...: \"variable\": [\"A\"] * 3 + [\"B\"] * 3 + [\"C\"] * 3 + [\"D\"] * 3, ...: \"date\": pd.to_datetime([\"2020-01-03\", \"2020-01-04\", \"2020-01-05\"] * 4) ...: } ...: df = pd.DataFrame(data) To perform time series operations with each unique variable, a better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()): pivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\") pivoted variable A B C D date 2020-01-03 0 3 6 9 2020-01-04 1 4 7 10 2020-01-05 2 5 8 11 If the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column: df[\"value2\"] = df[\"value\"] * 2 pivoted = df.pivot(index=\"date\", columns=\"variable\") pivoted value value2 variable A B C D A B C D date 2020-01-03 0 3 6 9 0 6 12 18 2020-01-04 1 4 7 10 2 8 14 20 2020-01-05 2 5 8 11 4 10 16 22 You can then select subsets from the pivoted DataFrame: pivoted[\"value2\"] variable A B C D date 2020-01-03 0 6 12 18 2020-01-04 2 8 14 20 2020-01-05 4 10 16 22 Note that this returns a view on the underlying data in the case where the data are homogeneously-typed.", "prev_chunk_id": "chunk_438", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_440", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "pivot_table()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "pivot_table()#", "content": "pivot_table()# While pivot() provides general purpose pivoting with various data types, pandas also provides pivot_table() or pivot_table() for pivoting with aggregation of numeric data. The function pivot_table() can be used to create spreadsheet-style pivot tables. See the cookbook for some advanced strategies. import datetime df = pd.DataFrame( ....: { ....: \"A\": [\"one\", \"one\", \"two\", \"three\"] * 6, ....: \"B\": [\"A\", \"B\", \"C\"] * 8, ....: \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 4, ....: \"D\": np.random.randn(24), ....: \"E\": np.random.randn(24), ....: \"F\": [datetime.datetime(2013, i, 1) for i in range(1, 13)] ....: + [datetime.datetime(2013, i, 15) for i in range(1, 13)], ....: } ....: ) ....: df A B C D E F 0 one A foo 0.469112 0.404705 2013-01-01 1 one B foo -0.282863 0.577046 2013-02-01 2 two C foo -1.509059 -1.715002 2013-03-01 3 three A bar -1.135632 -1.039268 2013-04-01 4 one B bar 1.212112 -0.370647 2013-05-01 .. ... .. ... ... ... ... 19 three B foo -1.087401 -0.472035 2013-08-15 20 one C foo -0.673690 -0.013960 2013-09-15 21 one A bar 0.113648 -0.362543 2013-10-15 22 two B bar -1.478427 -0.006154 2013-11-15 23 three C bar 0.524988 -0.923061 2013-12-15 [24 rows x 6 columns] pd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"]) C bar foo A B one A -0.995460 0.595334 B 0.393570 -0.494817 C 0.196903 -0.767769 three A -0.431886 NaN B NaN -1.065818 C 0.798396 NaN two A NaN 0.197720 B -0.986678 NaN C NaN -1.274317 pd.pivot_table( ....: df, values=[\"D\", \"E\"], ....: index=[\"B\"], ....: columns=[\"A\", \"C\"], ....: aggfunc=\"sum\", ....: ) ....: D ... E A one three ... three two C bar foo bar ... foo bar foo B ... A -1.990921 1.190667 -0.863772 ... NaN NaN -1.067650 B 0.787140 -0.989634 NaN ... 0.372851 1.63741 NaN C 0.393806 -1.535539 1.596791 ... NaN NaN -3.491906 [3 rows x 12 columns] pd.pivot_table( ....: df,", "prev_chunk_id": "chunk_439", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_441", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "pivot_table()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "pivot_table()#", "content": "values=\"E\", ....: index=[\"B\", \"C\"], ....: columns=[\"A\"], ....: aggfunc=[\"sum\", \"mean\"], ....: ) ....: sum mean A one three two one three two B C A bar -0.471593 -2.008182 NaN -0.235796 -1.004091 NaN foo 0.761726 NaN -1.067650 0.380863 NaN -0.533825 B bar -1.665170 NaN 1.637410 -0.832585 NaN 0.818705 foo -0.097554 0.372851 NaN -0.048777 0.186425 NaN C bar -0.744154 -2.392449 NaN -0.372077 -1.196224 NaN foo 1.061810 NaN -3.491906 0.530905 NaN -1.745953 The result is a DataFrame potentially having a MultiIndex on the index or column. If the values column name is not given, the pivot table will include all of the data in an additional level of hierarchy in the columns: pd.pivot_table(df[[\"A\", \"B\", \"C\", \"D\", \"E\"]], index=[\"A\", \"B\"], columns=[\"C\"]) D E C bar foo bar foo A B one A -0.995460 0.595334 -0.235796 0.380863 B 0.393570 -0.494817 -0.832585 -0.048777 C 0.196903 -0.767769 -0.372077 0.530905 three A -0.431886 NaN -1.004091 NaN B NaN -1.065818 NaN 0.186425 C 0.798396 NaN -1.196224 NaN two A NaN 0.197720 NaN -0.533825 B -0.986678 NaN 0.818705 NaN C NaN -1.274317 NaN -1.745953 Also, you can use Grouper for index and columns keywords. For detail of Grouper, see Grouping with a Grouper specification. pd.pivot_table(df, values=\"D\", index=pd.Grouper(freq=\"ME\", key=\"F\"), columns=\"C\") C bar foo F 2013-01-31 NaN 0.595334 2013-02-28 NaN -0.494817 2013-03-31 NaN -1.274317 2013-04-30 -0.431886 NaN 2013-05-31 0.393570 NaN 2013-06-30 0.196903 NaN 2013-07-31 NaN 0.197720 2013-08-31 NaN -1.065818 2013-09-30 NaN -0.767769 2013-10-31 -0.995460 NaN 2013-11-30 -0.986678 NaN 2013-12-31 0.798396 NaN", "prev_chunk_id": "chunk_440", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_442", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Adding margins#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Adding margins#", "content": "Adding margins# Passing margins=True to pivot_table() will add a row and column with an All label with partial group aggregates across the categories on the rows and columns: table = df.pivot_table( ....: index=[\"A\", \"B\"], ....: columns=\"C\", ....: values=[\"D\", \"E\"], ....: margins=True, ....: aggfunc=\"std\" ....: ) ....: table D E C bar foo All bar foo All A B one A 1.568517 0.178504 1.293926 0.179247 0.033718 0.371275 B 1.157593 0.299748 0.860059 0.653280 0.885047 0.779837 C 0.523425 0.133049 0.638297 1.111310 0.770555 0.938819 three A 0.995247 NaN 0.995247 0.049748 NaN 0.049748 B NaN 0.030522 0.030522 NaN 0.931203 0.931203 C 0.386657 NaN 0.386657 0.386312 NaN 0.386312 two A NaN 0.111032 0.111032 NaN 1.146201 1.146201 B 0.695438 NaN 0.695438 1.166526 NaN 1.166526 C NaN 0.331975 0.331975 NaN 0.043771 0.043771 All 1.014073 0.713941 0.871016 0.881376 0.984017 0.923568 Additionally, you can call DataFrame.stack() to display a pivoted DataFrame as having a multi-level index: table.stack(future_stack=True) D E A B C one A bar 1.568517 0.179247 foo 0.178504 0.033718 All 1.293926 0.371275 B bar 1.157593 0.653280 foo 0.299748 0.885047 ... ... ... two C foo 0.331975 0.043771 All 0.331975 0.043771 All bar 1.014073 0.881376 foo 0.713941 0.984017 All 0.871016 0.923568 [30 rows x 2 columns]", "prev_chunk_id": "chunk_441", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_443", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "stack() and unstack()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "stack() and unstack()#", "content": "stack() and unstack()# Closely related to the pivot() method are the related stack() and unstack() methods available on Series and DataFrame. These methods are designed to work together with MultiIndex objects (see the section on hierarchical indexing). - stack(): “pivot” a level of the (possibly hierarchical) column labels, returning aDataFramewith an index with a new inner-most level of row labels. - unstack(): (inverse operation ofstack()) “pivot” a level of the (possibly hierarchical) row index to the column axis, producing a reshapedDataFramewith a new inner-most level of column labels. tuples = [ ....: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ....: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ....: ] ....: index = pd.MultiIndex.from_arrays(tuples, names=[\"first\", \"second\"]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"]) df2 = df[:4] df2 A B first second bar one 0.895717 0.805244 two -1.206412 2.565646 baz one 1.431256 1.340309 two -1.170299 -0.226169 The stack() function “compresses” a level in the DataFrame columns to produce either: - ASeries, in the case of aIndexin the columns. - ADataFrame, in the case of aMultiIndexin the columns. If the columns have a MultiIndex, you can choose which level to stack. The stacked level becomes the new lowest level in a MultiIndex on the columns: stacked = df2.stack(future_stack=True) stacked first second bar one A 0.895717 B 0.805244 two A -1.206412 B 2.565646 baz one A 1.431256 B 1.340309 two A -1.170299 B -0.226169 dtype: float64 With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level: stacked.unstack() A B first second bar one 0.895717 0.805244 two -1.206412 2.565646 baz one 1.431256 1.340309 two -1.170299 -0.226169 stacked.unstack(1) second one two first bar A 0.895717 -1.206412 B 0.805244 2.565646 baz A 1.431256 -1.170299 B 1.340309 -0.226169 stacked.unstack(0) first", "prev_chunk_id": "chunk_442", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_444", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "stack() and unstack()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "stack() and unstack()#", "content": "bar baz second one A 0.895717 1.431256 B 0.805244 1.340309 two A -1.206412 -1.170299 B 2.565646 -0.226169 If the indexes have names, you can use the level names instead of specifying the level numbers: stacked.unstack(\"second\") second one two first bar A 0.895717 -1.206412 B 0.805244 2.565646 baz A 1.431256 -1.170299 B 1.340309 -0.226169 Notice that the stack() and unstack() methods implicitly sort the index levels involved. Hence a call to stack() and then unstack(), or vice versa, will result in a sorted copy of the original DataFrame or Series: index = pd.MultiIndex.from_product([[2, 1], [\"a\", \"b\"]]) df = pd.DataFrame(np.random.randn(4), index=index, columns=[\"A\"]) df A 2 a -1.413681 b 1.607920 1 a 1.024180 b 0.569605 all(df.unstack().stack(future_stack=True) == df.sort_index()) True", "prev_chunk_id": "chunk_443", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_445", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Multiple levels#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Multiple levels#", "content": "Multiple levels# You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually. columns = pd.MultiIndex.from_tuples( ....: [ ....: (\"A\", \"cat\", \"long\"), ....: (\"B\", \"cat\", \"long\"), ....: (\"A\", \"dog\", \"short\"), ....: (\"B\", \"dog\", \"short\"), ....: ], ....: names=[\"exp\", \"animal\", \"hair_length\"], ....: ) ....: df = pd.DataFrame(np.random.randn(4, 4), columns=columns) df exp A B A B animal cat cat dog dog hair_length long long short short 0 0.875906 -2.211372 0.974466 -2.006747 1 -0.410001 -0.078638 0.545952 -1.219217 2 -1.226825 0.769804 -1.281247 -0.727707 3 -0.121306 -0.097883 0.695775 0.341734 df.stack(level=[\"animal\", \"hair_length\"], future_stack=True) exp A B animal hair_length 0 cat long 0.875906 -2.211372 dog short 0.974466 -2.006747 1 cat long -0.410001 -0.078638 dog short 0.545952 -1.219217 2 cat long -1.226825 0.769804 dog short -1.281247 -0.727707 3 cat long -0.121306 -0.097883 dog short 0.695775 0.341734 The list of levels can contain either level names or level numbers but not a mixture of the two. # df.stack(level=['animal', 'hair_length'], future_stack=True) # from above is equivalent to: df.stack(level=[1, 2], future_stack=True) exp A B animal hair_length 0 cat long 0.875906 -2.211372 dog short 0.974466 -2.006747 1 cat long -0.410001 -0.078638 dog short 0.545952 -1.219217 2 cat long -1.226825 0.769804 dog short -1.281247 -0.727707 3 cat long -0.121306 -0.097883 dog short 0.695775 0.341734", "prev_chunk_id": "chunk_444", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_446", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Missing data#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Missing data#", "content": "Missing data# Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type. columns = pd.MultiIndex.from_tuples( ....: [ ....: (\"A\", \"cat\"), ....: (\"B\", \"dog\"), ....: (\"B\", \"cat\"), ....: (\"A\", \"dog\"), ....: ], ....: names=[\"exp\", \"animal\"], ....: ) ....: index = pd.MultiIndex.from_product( ....: [(\"bar\", \"baz\", \"foo\", \"qux\"), (\"one\", \"two\")], names=[\"first\", \"second\"] ....: ) ....: df = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns) df3 = df.iloc[[0, 1, 4, 7], [1, 2]] df3 exp B animal dog cat first second bar one -1.110336 -0.619976 two 0.687738 0.176444 foo one 1.314232 0.690579 qux two 0.380396 0.084844 df3.unstack() exp B animal dog cat second one two one two first bar -1.110336 0.687738 -0.619976 0.176444 foo 1.314232 NaN 0.690579 NaN qux NaN 0.380396 NaN 0.084844 The missing value can be filled with a specific value with the fill_value argument. df3.unstack(fill_value=-1e9) exp B animal dog cat second one two one two first bar -1.110336e+00 6.877384e-01 -6.199759e-01 1.764443e-01 foo 1.314232e+00 -1.000000e+09 6.905793e-01 -1.000000e+09 qux -1.000000e+09 3.803956e-01 -1.000000e+09 8.484421e-02", "prev_chunk_id": "chunk_445", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_447", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "melt() and wide_to_long()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "melt() and wide_to_long()#", "content": "melt() and wide_to_long()# The top-level melt() function and the corresponding DataFrame.melt() are useful to massage a DataFrame into a format where one or more columns are identifier variables, while all other columns, considered measured variables, are “unpivoted” to the row axis, leaving just two non-identifier columns, “variable” and “value”. The names of those columns can be customized by supplying the var_name and value_name parameters. cheese = pd.DataFrame( ....: { ....: \"first\": [\"John\", \"Mary\"], ....: \"last\": [\"Doe\", \"Bo\"], ....: \"height\": [5.5, 6.0], ....: \"weight\": [130, 150], ....: } ....: ) ....: cheese first last height weight 0 John Doe 5.5 130 1 Mary Bo 6.0 150 cheese.melt(id_vars=[\"first\", \"last\"]) first last variable value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 cheese.melt(id_vars=[\"first\", \"last\"], var_name=\"quantity\") first last quantity value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 When transforming a DataFrame using melt(), the index will be ignored. The original index values can be kept by setting the ignore_index=False parameter to False (default is True). ignore_index=False will however duplicate index values. index = pd.MultiIndex.from_tuples([(\"person\", \"A\"), (\"person\", \"B\")]) cheese = pd.DataFrame( ....: { ....: \"first\": [\"John\", \"Mary\"], ....: \"last\": [\"Doe\", \"Bo\"], ....: \"height\": [5.5, 6.0], ....: \"weight\": [130, 150], ....: }, ....: index=index, ....: ) ....: cheese first last height weight person A John Doe 5.5 130 B Mary Bo 6.0 150 cheese.melt(id_vars=[\"first\", \"last\"]) first last variable value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 cheese.melt(id_vars=[\"first\", \"last\"], ignore_index=False) first last variable value person A John Doe height 5.5 B Mary Bo height 6.0 A John Doe weight 130.0 B Mary Bo weight 150.0 wide_to_long() is similar to", "prev_chunk_id": "chunk_446", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_448", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "melt() and wide_to_long()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "melt() and wide_to_long()#", "content": "melt() with more customization for column matching. dft = pd.DataFrame( ....: { ....: \"A1970\": {0: \"a\", 1: \"b\", 2: \"c\"}, ....: \"A1980\": {0: \"d\", 1: \"e\", 2: \"f\"}, ....: \"B1970\": {0: 2.5, 1: 1.2, 2: 0.7}, ....: \"B1980\": {0: 3.2, 1: 1.3, 2: 0.1}, ....: \"X\": dict(zip(range(3), np.random.randn(3))), ....: } ....: ) ....: dft[\"id\"] = dft.index dft A1970 A1980 B1970 B1980 X id 0 a d 2.5 3.2 1.519970 0 1 b e 1.2 1.3 -0.493662 1 2 c f 0.7 0.1 0.600178 2 pd.wide_to_long(dft, [\"A\", \"B\"], i=\"id\", j=\"year\") X A B id year 0 1970 1.519970 a 2.5 1 1970 -0.493662 b 1.2 2 1970 0.600178 c 0.7 0 1980 1.519970 d 3.2 1 1980 -0.493662 e 1.3 2 1980 0.600178 f 0.1", "prev_chunk_id": "chunk_447", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_449", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "get_dummies() and from_dummies()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "get_dummies() and from_dummies()#", "content": "get_dummies() and from_dummies()# To convert categorical variables of a Series into a “dummy” or “indicator”, get_dummies() creates a new DataFrame with columns of the unique variables and the values representing the presence of those variables per row. df = pd.DataFrame({\"key\": list(\"bbacab\"), \"data1\": range(6)}) pd.get_dummies(df[\"key\"]) a b c 0 False True False 1 False True False 2 True False False 3 False False True 4 True False False 5 False True False df[\"key\"].str.get_dummies() a b c 0 0 1 0 1 0 1 0 2 1 0 0 3 0 0 1 4 1 0 0 5 0 1 0 prefix adds a prefix to the the column names which is useful for merging the result with the original DataFrame: dummies = pd.get_dummies(df[\"key\"], prefix=\"key\") dummies key_a key_b key_c 0 False True False 1 False True False 2 True False False 3 False False True 4 True False False 5 False True False df[[\"data1\"]].join(dummies) data1 key_a key_b key_c 0 0 False True False 1 1 False True False 2 2 True False False 3 3 False False True 4 4 True False False 5 5 False True False This function is often used along with discretization functions like cut(): values = np.random.randn(10) values array([ 0.2742, 0.1329, -0.0237, 2.4102, 1.4505, 0.2061, -0.2519, -2.2136, 1.0633, 1.2661]) bins = [0, 0.2, 0.4, 0.6, 0.8, 1] pd.get_dummies(pd.cut(values, bins)) (0.0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1.0] 0 False True False False False 1 True False False False False 2 False False False False False 3 False False False False False 4 False False False False False 5 False True False False False 6 False False False False False 7 False False False False False 8 False False False False False 9 False False False False False get_dummies() also accepts a DataFrame. By default, object,", "prev_chunk_id": "chunk_448", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_450", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "get_dummies() and from_dummies()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "get_dummies() and from_dummies()#", "content": "string, or categorical type columns are encoded as dummy variables with other columns unaltered. df = pd.DataFrame({\"A\": [\"a\", \"b\", \"a\"], \"B\": [\"c\", \"c\", \"b\"], \"C\": [1, 2, 3]}) pd.get_dummies(df) C A_a A_b B_b B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False Specifying the columns keyword will encode a column of any type. pd.get_dummies(df, columns=[\"A\"]) B C A_a A_b 0 c 1 True False 1 c 2 False True 2 b 3 True False As with the Series version, you can pass values for the prefix and prefix_sep. By default the column name is used as the prefix and _ as the prefix separator. You can specify prefix and prefix_sep in 3 ways: - string: Use the same value forprefixorprefix_sepfor each column to be encoded. - list: Must be the same length as the number of columns being encoded. - dict: Mapping column name to prefix. simple = pd.get_dummies(df, prefix=\"new_prefix\") simple C new_prefix_a new_prefix_b new_prefix_b new_prefix_c 0 1 True False False True 1 2 False True False True 2 3 True False True False from_list = pd.get_dummies(df, prefix=[\"from_A\", \"from_B\"]) from_list C from_A_a from_A_b from_B_b from_B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False from_dict = pd.get_dummies(df, prefix={\"B\": \"from_B\", \"A\": \"from_A\"}) from_dict C from_A_a from_A_b from_B_b from_B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False To avoid collinearity when feeding the result to statistical models, specify drop_first=True. s = pd.Series(list(\"abcaa\")) pd.get_dummies(s) a b c 0 True False False 1 False True False 2 False False True 3 True False False 4 True False False pd.get_dummies(s, drop_first=True) b c 0 False False 1 True False 2 False True 3 False False 4", "prev_chunk_id": "chunk_449", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_451", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "get_dummies() and from_dummies()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "get_dummies() and from_dummies()#", "content": "False False When a column contains only one level, it will be omitted in the result. df = pd.DataFrame({\"A\": list(\"aaaaa\"), \"B\": list(\"ababc\")}) pd.get_dummies(df) A_a B_a B_b B_c 0 True True False False 1 True False True False 2 True True False False 3 True False True False 4 True False False True pd.get_dummies(df, drop_first=True) B_b B_c 0 False False 1 True False 2 False False 3 True False 4 False True The values can be cast to a different type using the dtype argument. df = pd.DataFrame({\"A\": list(\"abc\"), \"B\": [1.1, 2.2, 3.3]}) pd.get_dummies(df, dtype=np.float32).dtypes B float64 A_a float32 A_b float32 A_c float32 dtype: object from_dummies() converts the output of get_dummies() back into a Series of categorical values from indicator values. df = pd.DataFrame({\"prefix_a\": [0, 1, 0], \"prefix_b\": [1, 0, 1]}) df prefix_a prefix_b 0 0 1 1 1 0 2 0 1 pd.from_dummies(df, sep=\"_\") prefix 0 b 1 a 2 b Dummy coded data only requires k - 1 categories to be included, in this case the last category is the default category. The default category can be modified with default_category. df = pd.DataFrame({\"prefix_a\": [0, 1, 0]}) df prefix_a 0 0 1 1 2 0 pd.from_dummies(df, sep=\"_\", default_category=\"b\") prefix 0 b 1 a 2 b", "prev_chunk_id": "chunk_450", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_452", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "explode()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "explode()#", "content": "explode()# For a DataFrame column with nested, list-like values, explode() will transform each list-like value to a separate row. The resulting Index will be duplicated corresponding to the index label from the original row: keys = [\"panda1\", \"panda2\", \"panda3\"] values = [[\"eats\", \"shoots\"], [\"shoots\", \"leaves\"], [\"eats\", \"leaves\"]] df = pd.DataFrame({\"keys\": keys, \"values\": values}) df keys values 0 panda1 [eats, shoots] 1 panda2 [shoots, leaves] 2 panda3 [eats, leaves] df[\"values\"].explode() 0 eats 0 shoots 1 shoots 1 leaves 2 eats 2 leaves Name: values, dtype: object DataFrame.explode can also explode the column in the DataFrame. df.explode(\"values\") keys values 0 panda1 eats 0 panda1 shoots 1 panda2 shoots 1 panda2 leaves 2 panda3 eats 2 panda3 leaves Series.explode() will replace empty lists with a missing value indicator and preserve scalar entries. s = pd.Series([[1, 2, 3], \"foo\", [], [\"a\", \"b\"]]) s 0 [1, 2, 3] 1 foo 2 [] 3 [a, b] dtype: object s.explode() 0 1 0 2 0 3 1 foo 2 NaN 3 a 3 b dtype: object A comma-separated string value can be split into individual values in a list and then exploded to a new row. df = pd.DataFrame([{\"var1\": \"a,b,c\", \"var2\": 1}, {\"var1\": \"d,e,f\", \"var2\": 2}]) df.assign(var1=df.var1.str.split(\",\")).explode(\"var1\") var1 var2 0 a 1 0 b 1 0 c 1 1 d 2 1 e 2 1 f 2", "prev_chunk_id": "chunk_451", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_453", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "crosstab()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "crosstab()#", "content": "crosstab()# Use crosstab() to compute a cross-tabulation of two (or more) factors. By default crosstab() computes a frequency table of the factors unless an array of values and an aggregation function are passed. Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified a = np.array([\"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\"], dtype=object) b = np.array([\"one\", \"one\", \"two\", \"one\", \"two\", \"one\"], dtype=object) c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\"], dtype=object) pd.crosstab(a, [b, c], rownames=[\"a\"], colnames=[\"b\", \"c\"]) b one two c dull shiny dull shiny a bar 1 0 0 1 foo 2 1 1 0 If crosstab() receives only two Series, it will provide a frequency table. df = pd.DataFrame( .....: {\"A\": [1, 2, 2, 2, 2], \"B\": [3, 3, 4, 4, 4], \"C\": [1, 1, np.nan, 1, 1]} .....: ) .....: df A B C 0 1 3 1.0 1 2 3 1.0 2 2 4 NaN 3 2 4 1.0 4 2 4 1.0 pd.crosstab(df[\"A\"], df[\"B\"]) B 3 4 A 1 1 0 2 1 3 crosstab() can also summarize to Categorical data. foo = pd.Categorical([\"a\", \"b\"], categories=[\"a\", \"b\", \"c\"]) bar = pd.Categorical([\"d\", \"e\"], categories=[\"d\", \"e\", \"f\"]) pd.crosstab(foo, bar) col_0 d e row_0 a 1 0 b 0 1 For Categorical data, to include all of data categories even if the actual data does not contain any instances of a particular category, use dropna=False. pd.crosstab(foo, bar, dropna=False) col_0 d e f row_0 a 1 0 0 b 0 1 0 c 0 0 0", "prev_chunk_id": "chunk_452", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_454", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Normalization#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Normalization#", "content": "Normalization# Frequency tables can also be normalized to show percentages rather than counts using the normalize argument: pd.crosstab(df[\"A\"], df[\"B\"], normalize=True) B 3 4 A 1 0.2 0.0 2 0.2 0.6 normalize can also normalize values within each row or within each column: pd.crosstab(df[\"A\"], df[\"B\"], normalize=\"columns\") B 3 4 A 1 0.5 0.0 2 0.5 1.0 crosstab() can also accept a third Series and an aggregation function (aggfunc) that will be applied to the values of the third Series within each group defined by the first two Series: pd.crosstab(df[\"A\"], df[\"B\"], values=df[\"C\"], aggfunc=\"sum\") B 3 4 A 1 1.0 NaN 2 1.0 2.0", "prev_chunk_id": "chunk_453", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_455", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "Adding margins#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "Adding margins#", "content": "Adding margins# margins=True will add a row and column with an All label with partial group aggregates across the categories on the rows and columns: pd.crosstab( .....: df[\"A\"], df[\"B\"], values=df[\"C\"], aggfunc=\"sum\", normalize=True, margins=True .....: ) .....: B 3 4 All A 1 0.25 0.0 0.25 2 0.25 0.5 0.75 All 0.50 0.5 1.00", "prev_chunk_id": "chunk_454", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_456", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "cut()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "cut()#", "content": "cut()# The cut() function computes groupings for the values of the input array and is often used to transform continuous variables to discrete or categorical variables: An integer bins will form equal-width bins. ages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60]) pd.cut(ages, bins=3) [(9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (26.667, 43.333], (43.333, 60.0], (43.333, 60.0]] Categories (3, interval[float64, right]): [(9.95, 26.667] < (26.667, 43.333] < (43.333, 60.0]] A list of ordered bin edges will assign an interval for each variable. pd.cut(ages, bins=[0, 18, 35, 70]) [(0, 18], (0, 18], (0, 18], (0, 18], (18, 35], (18, 35], (18, 35], (35, 70], (35, 70]] Categories (3, interval[int64, right]): [(0, 18] < (18, 35] < (35, 70]] If the bins keyword is an IntervalIndex, then these will be used to bin the passed data. pd.cut(ages, bins=pd.IntervalIndex.from_breaks([0, 40, 70])) [(0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (40, 70], (40, 70]] Categories (2, interval[int64, right]): [(0, 40] < (40, 70]]", "prev_chunk_id": "chunk_455", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_457", "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html", "title": "factorize()#", "page_title": "Reshaping and pivot tables — pandas 2.3.1 documentation", "breadcrumbs": "factorize()#", "content": "factorize()# factorize() encodes 1 dimensional values into integer labels. Missing values are encoded as -1. x = pd.Series([\"A\", \"A\", np.nan, \"B\", 3.14, np.inf]) x 0 A 1 A 2 NaN 3 B 4 3.14 5 inf dtype: object labels, uniques = pd.factorize(x) labels array([ 0, 0, -1, 1, 2, 3]) uniques Index(['A', 'B', 3.14, inf], dtype='object') Categorical will similarly encode 1 dimensional values for further categorical operations pd.Categorical(x) ['A', 'A', NaN, 'B', 3.14, inf] Categories (4, object): [3.14, inf, 'A', 'B']", "prev_chunk_id": "chunk_456", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_458", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Text data types#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Text data types#", "content": "Text data types# There are two ways to store text data in pandas: - object-dtype NumPy array. - StringDtypeextension type. We recommend using StringDtype to store text data. Prior to pandas 1.0, object dtype was the only option. This was unfortunate for many reasons: - You can accidentally store amixtureof strings and non-strings in anobjectdtype array. It’s better to have a dedicated dtype. - objectdtype breaks dtype-specific operations likeDataFrame.select_dtypes(). There isn’t a clear way to selectjusttext while excluding non-text but still object-dtype columns. - When reading code, the contents of anobjectdtype array is less clear than'string'. Currently, the performance of object dtype arrays of strings and arrays.StringArray are about the same. We expect future enhancements to significantly increase the performance and lower the memory overhead of StringArray. For backwards-compatibility, object dtype remains the default type we infer a list of strings to pd.Series([\"a\", \"b\", \"c\"]) 0 a 1 b 2 c dtype: object To explicitly request string dtype, specify the dtype pd.Series([\"a\", \"b\", \"c\"], dtype=\"string\") 0 a 1 b 2 c dtype: string pd.Series([\"a\", \"b\", \"c\"], dtype=pd.StringDtype()) 0 a 1 b 2 c dtype: string Or astype after the Series or DataFrame is created s = pd.Series([\"a\", \"b\", \"c\"]) s 0 a 1 b 2 c dtype: object s.astype(\"string\") 0 a 1 b 2 c dtype: string You can also use StringDtype/\"string\" as the dtype on non-string data and it will be converted to string dtype: s = pd.Series([\"a\", 2, np.nan], dtype=\"string\") s 0 a 1 2 2 <NA> dtype: string type(s[1]) str or convert from existing pandas data: s1 = pd.Series([1, 2, np.nan], dtype=\"Int64\") s1 0 1 1 2 2 <NA> dtype: Int64 s2 = s1.astype(\"string\") s2 0 1 1 2 2 <NA> dtype: string type(s2[0]) str", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_459", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Behavior differences#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Behavior differences#", "content": "Behavior differences# These are places where the behavior of StringDtype objects differ from object dtype - ForStringDtype,string accessor methodsthat returnnumericoutput will always return a nullable integer dtype, rather than either int or float dtype, depending on the presence of NA values. Methods returningbooleanoutput will return a nullable boolean dtype.In [15]:s=pd.Series([\"a\",None,\"b\"],dtype=\"string\")In [16]:sOut[16]:0 a1 <NA>2 bdtype: stringIn [17]:s.str.count(\"a\")Out[17]:0 11 <NA>2 0dtype: Int64In [18]:s.dropna().str.count(\"a\")Out[18]:0 12 0dtype: Int64Both outputs areInt64dtype. Compare that with object-dtypeIn [19]:s2=pd.Series([\"a\",None,\"b\"],dtype=\"object\")In [20]:s2.str.count(\"a\")Out[20]:0 1.01 NaN2 0.0dtype: float64In [21]:s2.dropna().str.count(\"a\")Out[21]:0 12 0dtype: int64When NA values are present, the output dtype is float64. Similarly for methods returning boolean values.In [22]:s.str.isdigit()Out[22]:0 False1 <NA>2 Falsedtype: booleanIn [23]:s.str.match(\"a\")Out[23]:0 True1 <NA>2 Falsedtype: boolean - Some string methods, likeSeries.str.decode()are not available onStringArraybecauseStringArrayonly holds strings, not bytes. - In comparison operations,arrays.StringArrayandSeriesbacked by aStringArraywill return an object withBooleanDtype, rather than abooldtype object. Missing values in aStringArraywill propagate in comparison operations, rather than always comparing unequal likenumpy.nan. Everything else that follows in the rest of this document applies equally to string and object dtype.", "prev_chunk_id": "chunk_458", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_460", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "String methods#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "String methods#", "content": "String methods# Series and Index are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods: s = pd.Series( ....: [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\" ....: ) ....: s.str.lower() 0 a 1 b 2 c 3 aaba 4 baca 5 <NA> 6 caba 7 dog 8 cat dtype: string s.str.upper() 0 A 1 B 2 C 3 AABA 4 BACA 5 <NA> 6 CABA 7 DOG 8 CAT dtype: string s.str.len() 0 1 1 1 2 1 3 4 4 4 5 <NA> 6 4 7 3 8 3 dtype: Int64 idx = pd.Index([\" jack\", \"jill \", \" jesse \", \"frank\"]) idx.str.strip() Index(['jack', 'jill', 'jesse', 'frank'], dtype='object') idx.str.lstrip() Index(['jack', 'jill ', 'jesse ', 'frank'], dtype='object') idx.str.rstrip() Index([' jack', 'jill', ' jesse', 'frank'], dtype='object') The string methods on Index are especially useful for cleaning up or transforming DataFrame columns. For instance, you may have columns with leading or trailing whitespace: df = pd.DataFrame( ....: np.random.randn(3, 2), columns=[\" Column A \", \" Column B \"], index=range(3) ....: ) ....: df Column A Column B 0 0.469112 -0.282863 1 -1.509059 -1.135632 2 1.212112 -0.173215 Since df.columns is an Index object, we can use the .str accessor df.columns.str.strip() Index(['Column A', 'Column B'], dtype='object') df.columns.str.lower() Index([' column a ', ' column b '], dtype='object') These string methods can then be used to clean up the columns as needed. Here we are removing leading and trailing whitespaces, lower casing all names, and replacing any remaining whitespaces with underscores: df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\") df column_a column_b 0 0.469112 -0.282863 1 -1.509059 -1.135632 2 1.212112", "prev_chunk_id": "chunk_459", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_461", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "String methods#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "String methods#", "content": "-0.173215", "prev_chunk_id": "chunk_460", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_462", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Splitting and replacing strings#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Splitting and replacing strings#", "content": "Splitting and replacing strings# Methods like split return a Series of lists: s2 = pd.Series([\"a_b_c\", \"c_d_e\", np.nan, \"f_g_h\"], dtype=\"string\") s2.str.split(\"_\") 0 [a, b, c] 1 [c, d, e] 2 <NA> 3 [f, g, h] dtype: object Elements in the split lists can be accessed using get or [] notation: s2.str.split(\"_\").str.get(1) 0 b 1 d 2 <NA> 3 g dtype: object s2.str.split(\"_\").str[1] 0 b 1 d 2 <NA> 3 g dtype: object It is easy to expand this to return a DataFrame using expand. s2.str.split(\"_\", expand=True) 0 1 2 0 a b c 1 c d e 2 <NA> <NA> <NA> 3 f g h When original Series has StringDtype, the output columns will all be StringDtype as well. It is also possible to limit the number of splits: s2.str.split(\"_\", expand=True, n=1) 0 1 0 a b_c 1 c d_e 2 <NA> <NA> 3 f g_h rsplit is similar to split except it works in the reverse direction, i.e., from the end of the string to the beginning of the string: s2.str.rsplit(\"_\", expand=True, n=1) 0 1 0 a_b c 1 c_d e 2 <NA> <NA> 3 f_g h replace optionally uses regular expressions: s3 = pd.Series( ....: [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", \"\", np.nan, \"CABA\", \"dog\", \"cat\"], ....: dtype=\"string\", ....: ) ....: s3 0 A 1 B 2 C 3 Aaba 4 Baca 5 6 <NA> 7 CABA 8 dog 9 cat dtype: string s3.str.replace(\"^.a|dog\", \"XX-XX \", case=False, regex=True) 0 A 1 B 2 C 3 XX-XX ba 4 XX-XX ca 5 6 <NA> 7 XX-XX BA 8 XX-XX 9 XX-XX t dtype: string Single character pattern with regex=True will also be treated as regular expressions: s4 = pd.Series([\"a.b\", \".\", \"b\", np.nan, \"\"], dtype=\"string\") s4 0 a.b 1 . 2 b 3 <NA> 4 dtype: string s4.str.replace(\".\", \"a\", regex=True) 0 aaa", "prev_chunk_id": "chunk_461", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_463", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Splitting and replacing strings#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Splitting and replacing strings#", "content": "1 a 2 a 3 <NA> 4 dtype: string If you want literal replacement of a string (equivalent to str.replace()), you can set the optional regex parameter to False, rather than escaping each character. In this case both pat and repl must be strings: dollars = pd.Series([\"12\", \"-$10\", \"$10,000\"], dtype=\"string\") # These lines are equivalent dollars.str.replace(r\"-\\$\", \"-\", regex=True) 0 12 1 -10 2 $10,000 dtype: string dollars.str.replace(\"-$\", \"-\", regex=False) 0 12 1 -10 2 $10,000 dtype: string The replace method can also take a callable as replacement. It is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string. # Reverse every lowercase alphabetic word pat = r\"[a-z]+\" def repl(m): ....: return m.group(0)[::-1] ....: pd.Series([\"foo 123\", \"bar baz\", np.nan], dtype=\"string\").str.replace( ....: pat, repl, regex=True ....: ) ....: 0 oof 123 1 rab zab 2 <NA> dtype: string # Using regex groups pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\" def repl(m): ....: return m.group(\"two\").swapcase() ....: pd.Series([\"Foo Bar Baz\", np.nan], dtype=\"string\").str.replace( ....: pat, repl, regex=True ....: ) ....: 0 bAR 1 <NA> dtype: string The replace method also accepts a compiled regular expression object from re.compile() as a pattern. All flags should be included in the compiled regular expression object. import re regex_pat = re.compile(r\"^.a|dog\", flags=re.IGNORECASE) s3.str.replace(regex_pat, \"XX-XX \", regex=True) 0 A 1 B 2 C 3 XX-XX ba 4 XX-XX ca 5 6 <NA> 7 XX-XX BA 8 XX-XX 9 XX-XX t dtype: string Including a flags argument when calling replace with a compiled regular expression object will raise a ValueError. s3.str.replace(regex_pat, 'XX-XX ', flags=re.IGNORECASE) --------------------------------------------------------------------------- ValueError: case and flags cannot be set when pat is a compiled regex removeprefix and removesuffix have the same effect as str.removeprefix and str.removesuffix added in Python 3.9 <https://docs.python.org/3/library/stdtypes.html#str.removeprefix>`__: s = pd.Series([\"str_foo\", \"str_bar\", \"no_prefix\"]) s.str.removeprefix(\"str_\") 0 foo", "prev_chunk_id": "chunk_462", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_464", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Splitting and replacing strings#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Splitting and replacing strings#", "content": "1 bar 2 no_prefix dtype: object s = pd.Series([\"foo_str\", \"bar_str\", \"no_suffix\"]) s.str.removesuffix(\"_str\") 0 foo 1 bar 2 no_suffix dtype: object", "prev_chunk_id": "chunk_463", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_465", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenation#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenation#", "content": "Concatenation# There are several ways to concatenate a Series or Index, either with itself or others, all based on cat(), resp. Index.str.cat.", "prev_chunk_id": "chunk_464", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_466", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenating a single Series into a string#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating a single Series into a string#", "content": "Concatenating a single Series into a string# The content of a Series (or Index) can be concatenated: s = pd.Series([\"a\", \"b\", \"c\", \"d\"], dtype=\"string\") s.str.cat(sep=\",\") 'a,b,c,d' If not specified, the keyword sep for the separator defaults to the empty string, sep='': s.str.cat() 'abcd' By default, missing values are ignored. Using na_rep, they can be given a representation: t = pd.Series([\"a\", \"b\", np.nan, \"d\"], dtype=\"string\") t.str.cat(sep=\",\") 'a,b,d' t.str.cat(sep=\",\", na_rep=\"-\") 'a,b,-,d'", "prev_chunk_id": "chunk_465", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_467", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenating a Series and something list-like into a Series#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating a Series and something list-like into a Series#", "content": "Concatenating a Series and something list-like into a Series# The first argument to cat() can be a list-like object, provided that it matches the length of the calling Series (or Index). s.str.cat([\"A\", \"B\", \"C\", \"D\"]) 0 aA 1 bB 2 cC 3 dD dtype: string Missing values on either side will result in missing values in the result as well, unless na_rep is specified: s.str.cat(t) 0 aa 1 bb 2 <NA> 3 dd dtype: string s.str.cat(t, na_rep=\"-\") 0 aa 1 bb 2 c- 3 dd dtype: string", "prev_chunk_id": "chunk_466", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_468", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenating a Series and something array-like into a Series#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating a Series and something array-like into a Series#", "content": "Concatenating a Series and something array-like into a Series# The parameter others can also be two-dimensional. In this case, the number or rows must match the lengths of the calling Series (or Index). d = pd.concat([t, s], axis=1) s 0 a 1 b 2 c 3 d dtype: string d 0 1 0 a a 1 b b 2 <NA> c 3 d d s.str.cat(d, na_rep=\"-\") 0 aaa 1 bbb 2 c-c 3 ddd dtype: string", "prev_chunk_id": "chunk_467", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_469", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenating a Series and an indexed object into a Series, with alignment#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating a Series and an indexed object into a Series, with alignment#", "content": "Concatenating a Series and an indexed object into a Series, with alignment# For concatenation with a Series or DataFrame, it is possible to align the indexes before concatenation by setting the join-keyword. u = pd.Series([\"b\", \"d\", \"a\", \"c\"], index=[1, 3, 0, 2], dtype=\"string\") s 0 a 1 b 2 c 3 d dtype: string u 1 b 3 d 0 a 2 c dtype: string s.str.cat(u) 0 aa 1 bb 2 cc 3 dd dtype: string s.str.cat(u, join=\"left\") 0 aa 1 bb 2 cc 3 dd dtype: string The usual options are available for join (one of 'left', 'outer', 'inner', 'right'). In particular, alignment also means that the different lengths do not need to coincide anymore. v = pd.Series([\"z\", \"a\", \"b\", \"d\", \"e\"], index=[-1, 0, 1, 3, 4], dtype=\"string\") s 0 a 1 b 2 c 3 d dtype: string v -1 z 0 a 1 b 3 d 4 e dtype: string s.str.cat(v, join=\"left\", na_rep=\"-\") 0 aa 1 bb 2 c- 3 dd dtype: string s.str.cat(v, join=\"outer\", na_rep=\"-\") -1 -z 0 aa 1 bb 2 c- 3 dd 4 -e dtype: string The same alignment can be used when others is a DataFrame: f = d.loc[[3, 2, 1, 0], :] s 0 a 1 b 2 c 3 d dtype: string f 0 1 3 d d 2 <NA> c 1 b b 0 a a s.str.cat(f, join=\"left\", na_rep=\"-\") 0 aaa 1 bbb 2 c-c 3 ddd dtype: string", "prev_chunk_id": "chunk_468", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_470", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Concatenating a Series and many objects into a Series#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating a Series and many objects into a Series#", "content": "Concatenating a Series and many objects into a Series# Several array-like items (specifically: Series, Index, and 1-dimensional variants of np.ndarray) can be combined in a list-like container (including iterators, dict-views, etc.). s 0 a 1 b 2 c 3 d dtype: string u 1 b 3 d 0 a 2 c dtype: string s.str.cat([u, u.to_numpy()], join=\"left\") 0 aab 1 bbd 2 cca 3 ddc dtype: string All elements without an index (e.g. np.ndarray) within the passed list-like must match in length to the calling Series (or Index), but Series and Index may have arbitrary length (as long as alignment is not disabled with join=None): v -1 z 0 a 1 b 3 d 4 e dtype: string s.str.cat([v, u, u.to_numpy()], join=\"outer\", na_rep=\"-\") -1 -z-- 0 aaab 1 bbbd 2 c-ca 3 dddc 4 -e-- dtype: string If using join='right' on a list-like of others that contains different indexes, the union of these indexes will be used as the basis for the final concatenation: u.loc[[3]] 3 d dtype: string v.loc[[-1, 0]] -1 z 0 a dtype: string s.str.cat([u.loc[[3]], v.loc[[-1, 0]]], join=\"right\", na_rep=\"-\") 3 dd- -1 --z 0 a-a dtype: string", "prev_chunk_id": "chunk_469", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_471", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Indexing with .str#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with .str#", "content": "Indexing with .str# You can use [] notation to directly index by position locations. If you index past the end of the string, the result will be a NaN. s = pd.Series( .....: [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\" .....: ) .....: s.str[0] 0 A 1 B 2 C 3 A 4 B 5 <NA> 6 C 7 d 8 c dtype: string s.str[1] 0 <NA> 1 <NA> 2 <NA> 3 a 4 a 5 <NA> 6 A 7 o 8 a dtype: string", "prev_chunk_id": "chunk_470", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_472", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Extract first match in each subject (extract)#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Extract first match in each subject (extract)#", "content": "Extract first match in each subject (extract)# The extract method accepts a regular expression with at least one capture group. Extracting a regular expression with more than one group returns a DataFrame with one column per group. pd.Series( .....: [\"a1\", \"b2\", \"c3\"], .....: dtype=\"string\", .....: ).str.extract(r\"([ab])(\\d)\", expand=False) .....: 0 1 0 a 1 1 b 2 2 <NA> <NA> Elements that do not match return a row filled with NaN. Thus, a Series of messy strings can be “converted” into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating get() to access tuples or re.match objects. The dtype of the result is always object, even if no match is found and the result only contains NaN. Named groups like pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract( .....: r\"(?P<letter>[ab])(?P<digit>\\d)\", expand=False .....: ) .....: letter digit 0 a 1 1 b 2 2 <NA> <NA> and optional groups like pd.Series( .....: [\"a1\", \"b2\", \"3\"], .....: dtype=\"string\", .....: ).str.extract(r\"([ab])?(\\d)\", expand=False) .....: 0 1 0 a 1 1 b 2 2 <NA> 3 can also be used. Note that any capture group names in the regular expression will be used for column names; otherwise capture group numbers will be used. Extracting a regular expression with one group returns a DataFrame with one column if expand=True. pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=True) 0 0 1 1 2 2 <NA> It returns a Series if expand=False. pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=False) 0 1 1 2 2 <NA> dtype: string Calling on an Index with a regex with exactly one capture group returns a DataFrame with one column if expand=True. s = pd.Series([\"a1\", \"b2\", \"c3\"], [\"A11\", \"B22\", \"C33\"], dtype=\"string\") s A11 a1 B22 b2 C33 c3 dtype: string s.index.str.extract(\"(?P<letter>[a-zA-Z])\", expand=True) letter 0 A 1 B 2 C It returns an Index if expand=False. s.index.str.extract(\"(?P<letter>[a-zA-Z])\", expand=False) Index(['A', 'B',", "prev_chunk_id": "chunk_471", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_473", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Extract first match in each subject (extract)#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Extract first match in each subject (extract)#", "content": "'C'], dtype='object', name='letter') Calling on an Index with a regex with more than one capture group returns a DataFrame if expand=True. s.index.str.extract(\"(?P<letter>[a-zA-Z])([0-9]+)\", expand=True) letter 1 0 A 11 1 B 22 2 C 33 It raises ValueError if expand=False. s.index.str.extract(\"(?P<letter>[a-zA-Z])([0-9]+)\", expand=False) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[116], line 1 ----> 1 s.index.str.extract(\"(?P<letter>[a-zA-Z])([0-9]+)\", expand=False) File ~/work/pandas/pandas/pandas/core/strings/accessor.py:140, in forbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper(self, *args, **kwargs) 135 msg = ( 136 f\"Cannot use .str.{func_name} with values of \" 137 f\"inferred dtype '{self._inferred_dtype}'.\" 138 ) 139 raise TypeError(msg) --> 140 return func(self, *args, **kwargs) File ~/work/pandas/pandas/pandas/core/strings/accessor.py:2771, in StringMethods.extract(self, pat, flags, expand) 2768 raise ValueError(\"pattern contains no capture groups\") 2770 if not expand and regex.groups > 1 and isinstance(self._data, ABCIndex): -> 2771 raise ValueError(\"only one regex group is supported with Index\") 2773 obj = self._data 2774 result_dtype = _result_dtype(obj) ValueError: only one regex group is supported with Index The table below summarizes the behavior of extract(expand=False) (input subject in first column, number of groups in regex in first row) | 1 group | >1 group Index | Index | ValueError Series | Series | DataFrame", "prev_chunk_id": "chunk_472", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_474", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Extract all matches in each subject (extractall)#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Extract all matches in each subject (extractall)#", "content": "Extract all matches in each subject (extractall)# Unlike extract (which returns only the first match), s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"], dtype=\"string\") s A a1a2 B b1 C c1 dtype: string two_groups = \"(?P<letter>[a-z])(?P<digit>[0-9])\" s.str.extract(two_groups, expand=True) letter digit A a 1 B b 1 C c 1 the extractall method returns every match. The result of extractall is always a DataFrame with a MultiIndex on its rows. The last level of the MultiIndex is named match and indicates the order in the subject. s.str.extractall(two_groups) letter digit match A 0 a 1 1 a 2 B 0 b 1 C 0 c 1 When each subject string in the Series has exactly one match, s = pd.Series([\"a3\", \"b3\", \"c2\"], dtype=\"string\") s 0 a3 1 b3 2 c2 dtype: string then extractall(pat).xs(0, level='match') gives the same result as extract(pat). extract_result = s.str.extract(two_groups, expand=True) extract_result letter digit 0 a 3 1 b 3 2 c 2 extractall_result = s.str.extractall(two_groups) extractall_result letter digit match 0 0 a 3 1 0 b 3 2 0 c 2 extractall_result.xs(0, level=\"match\") letter digit 0 a 3 1 b 3 2 c 2 Index also supports .str.extractall. It returns a DataFrame which has the same result as a Series.str.extractall with a default index (starts from 0). pd.Index([\"a1a2\", \"b1\", \"c1\"]).str.extractall(two_groups) letter digit match 0 0 a 1 1 a 2 1 0 b 1 2 0 c 1 pd.Series([\"a1a2\", \"b1\", \"c1\"], dtype=\"string\").str.extractall(two_groups) letter digit match 0 0 a 1 1 a 2 1 0 b 1 2 0 c 1", "prev_chunk_id": "chunk_473", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_475", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Testing for strings that match or contain a pattern#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Testing for strings that match or contain a pattern#", "content": "Testing for strings that match or contain a pattern# You can check whether elements contain a pattern: pattern = r\"[0-9][a-z]\" pd.Series( .....: [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"], .....: dtype=\"string\", .....: ).str.contains(pattern) .....: 0 False 1 False 2 True 3 True 4 True 5 True dtype: boolean Or whether elements match a pattern: pd.Series( .....: [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"], .....: dtype=\"string\", .....: ).str.match(pattern) .....: 0 False 1 False 2 True 3 True 4 False 5 True dtype: boolean pd.Series( .....: [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"], .....: dtype=\"string\", .....: ).str.fullmatch(pattern) .....: 0 False 1 False 2 True 3 True 4 False 5 False dtype: boolean Methods like match, fullmatch, contains, startswith, and endswith take an extra na argument so missing values can be considered True or False: s4 = pd.Series( .....: [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\" .....: ) .....: s4.str.contains(\"A\", na=False) 0 True 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False dtype: boolean", "prev_chunk_id": "chunk_474", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_476", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Creating indicator variables#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Creating indicator variables#", "content": "Creating indicator variables# You can extract dummy variables from string columns. For example if they are separated by a '|': s = pd.Series([\"a\", \"a|b\", np.nan, \"a|c\"], dtype=\"string\") s.str.get_dummies(sep=\"|\") a b c 0 1 0 0 1 1 1 0 2 0 0 0 3 1 0 1 String Index also supports get_dummies which returns a MultiIndex. idx = pd.Index([\"a\", \"a|b\", np.nan, \"a|c\"]) idx.str.get_dummies(sep=\"|\") MultiIndex([(1, 0, 0), (1, 1, 0), (0, 0, 0), (1, 0, 1)], names=['a', 'b', 'c']) See also get_dummies().", "prev_chunk_id": "chunk_475", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_477", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Method summary#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Method summary#", "content": "Method summary# Method | Description cat() | Concatenate strings split() | Split strings on delimiter rsplit() | Split strings on delimiter working from the end of the string get() | Index into each element (retrieve i-th element) join() | Join strings in each element of the Series with passed separator get_dummies() | Split strings on the delimiter returning DataFrame of dummy variables contains() | Return boolean array if each string contains pattern/regex replace() | Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence removeprefix() | Remove prefix from string i.e. only remove if string starts with prefix. removesuffix() | Remove suffix from string i.e. only remove if string ends with suffix. repeat() | Duplicate values (s.str.repeat(3) equivalent to x * 3) pad() | Add whitespace to the sides of strings center() | Equivalent to str.center ljust() | Equivalent to str.ljust rjust() | Equivalent to str.rjust zfill() | Equivalent to str.zfill wrap() | Split long strings into lines with length less than a given width slice() | Slice each string in the Series slice_replace() | Replace slice in each string with passed value count() | Count occurrences of pattern startswith() | Equivalent to str.startswith(pat) for each element endswith() | Equivalent to str.endswith(pat) for each element findall() | Compute list of all occurrences of pattern/regex for each string match() | Call re.match on each element returning matched groups as list extract() | Call re.search on each element returning DataFrame with one row for each element and one column for each regex capture group extractall() | Call re.findall on each element returning DataFrame with one row for each match and one column for each regex capture group len() | Compute string lengths strip() | Equivalent to str.strip rstrip() | Equivalent to str.rstrip lstrip() |", "prev_chunk_id": "chunk_476", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_478", "url": "https://pandas.pydata.org/docs/user_guide/text.html", "title": "Method summary#", "page_title": "Working with text data — pandas 2.3.1 documentation", "breadcrumbs": "Method summary#", "content": "Equivalent to str.lstrip partition() | Equivalent to str.partition rpartition() | Equivalent to str.rpartition lower() | Equivalent to str.lower casefold() | Equivalent to str.casefold upper() | Equivalent to str.upper find() | Equivalent to str.find rfind() | Equivalent to str.rfind index() | Equivalent to str.index rindex() | Equivalent to str.rindex capitalize() | Equivalent to str.capitalize swapcase() | Equivalent to str.swapcase normalize() | Return Unicode normal form. Equivalent to unicodedata.normalize translate() | Equivalent to str.translate isalnum() | Equivalent to str.isalnum isalpha() | Equivalent to str.isalpha isdigit() | Equivalent to str.isdigit isspace() | Equivalent to str.isspace islower() | Equivalent to str.islower isupper() | Equivalent to str.isupper istitle() | Equivalent to str.istitle isnumeric() | Equivalent to str.isnumeric isdecimal() | Equivalent to str.isdecimal", "prev_chunk_id": "chunk_477", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_479", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Values considered “missing”#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Values considered “missing”#", "content": "Values considered “missing”# pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type. numpy.nan for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to np.float64 or object. pd.Series([1, 2], dtype=np.int64).reindex([0, 1, 2]) 0 1.0 1 2.0 2 NaN dtype: float64 pd.Series([True, False], dtype=np.bool_).reindex([0, 1, 2]) 0 True 1 False 2 NaN dtype: object NaT for NumPy np.datetime64, np.timedelta64, and PeriodDtype. For typing applications, use api.types.NaTType. pd.Series([1, 2], dtype=np.dtype(\"timedelta64[ns]\")).reindex([0, 1, 2]) 0 0 days 00:00:00.000000001 1 0 days 00:00:00.000000002 2 NaT dtype: timedelta64[ns] pd.Series([1, 2], dtype=np.dtype(\"datetime64[ns]\")).reindex([0, 1, 2]) 0 1970-01-01 00:00:00.000000001 1 1970-01-01 00:00:00.000000002 2 NaT dtype: datetime64[ns] pd.Series([\"2020\", \"2020\"], dtype=pd.PeriodDtype(\"D\")).reindex([0, 1, 2]) 0 2020-01-01 1 2020-01-01 2 NaT dtype: period[D] NA for StringDtype, Int64Dtype (and other bit widths), Float64Dtype`(and other bit widths), :class:`BooleanDtype and ArrowDtype. These types will maintain the original data type of the data. For typing applications, use api.types.NAType. pd.Series([1, 2], dtype=\"Int64\").reindex([0, 1, 2]) 0 1 1 2 2 <NA> dtype: Int64 pd.Series([True, False], dtype=\"boolean[pyarrow]\").reindex([0, 1, 2]) 0 True 1 False 2 <NA> dtype: bool[pyarrow] To detect these missing value, use the isna() or notna() methods. ser = pd.Series([pd.Timestamp(\"2020-01-01\"), pd.NaT]) ser 0 2020-01-01 1 NaT dtype: datetime64[ns] pd.isna(ser) 0 False 1 True dtype: bool", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_480", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "NA semantics#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "NA semantics#", "content": "NA semantics# Starting from pandas 1.0, an experimental NA value (singleton) is available to represent scalar missing values. The goal of NA is provide a “missing” indicator that can be used consistently across data types (instead of np.nan, None or pd.NaT depending on the data type). For example, when having missing values in a Series with the nullable integer dtype, it will use NA: s = pd.Series([1, 2, None], dtype=\"Int64\") s 0 1 1 2 2 <NA> dtype: Int64 s[2] <NA> s[2] is pd.NA True Currently, pandas does not yet use those data types using NA by default a DataFrame or Series, so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the conversion section.", "prev_chunk_id": "chunk_479", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_481", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Propagation in arithmetic and comparison operations#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Propagation in arithmetic and comparison operations#", "content": "Propagation in arithmetic and comparison operations# In general, missing values propagate in operations involving NA. When one of the operands is unknown, the outcome of the operation is also unknown. For example, NA propagates in arithmetic operations, similarly to np.nan: pd.NA + 1 <NA> \"a\" * pd.NA <NA> There are a few special cases when the result is known, even when one of the operands is NA. pd.NA ** 0 1 1 ** pd.NA 1 In equality and comparison operations, NA also propagates. This deviates from the behaviour of np.nan, where comparisons with np.nan always return False. pd.NA == 1 <NA> pd.NA == pd.NA <NA> pd.NA < 2.5 <NA> To check if a value is equal to NA, use isna() pd.isna(pd.NA) True", "prev_chunk_id": "chunk_480", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_482", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Logical operations#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Logical operations#", "content": "Logical operations# For logical operations, NA follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required. For example, for the logical “or” operation (|), if one of the operands is True, we already know the result will be True, regardless of the other value (so regardless the missing value would be True or False). In this case, NA does not propagate: True | False True True | pd.NA True pd.NA | True True On the other hand, if one of the operands is False, the result depends on the value of the other operand. Therefore, in this case NA propagates: False | True True False | False False False | pd.NA <NA> The behaviour of the logical “and” operation (&) can be derived using similar logic (where now NA will not propagate if one of the operands is already False): False & True False False & False False False & pd.NA False True & True True True & False False True & pd.NA <NA>", "prev_chunk_id": "chunk_481", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_483", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "NA in a boolean context#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "NA in a boolean context#", "content": "NA in a boolean context# Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. bool(pd.NA) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[45], line 1 ----> 1 bool(pd.NA) File ~/work/pandas/pandas/pandas/_libs/missing.pyx:392, in pandas._libs.missing.NAType.__bool__() TypeError: boolean value of NA is ambiguous This also means that NA cannot be used in a context where it is evaluated to a boolean, such as if condition: ... where condition can potentially be NA. In such cases, isna() can be used to check for NA or condition being NA can be avoided, for example by filling missing values beforehand. A similar situation occurs when using Series or DataFrame objects in if statements, see Using if/truth statements with pandas.", "prev_chunk_id": "chunk_482", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_484", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "NumPy ufuncs#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "NumPy ufuncs#", "content": "NumPy ufuncs# pandas.NA implements NumPy’s __array_ufunc__ protocol. Most ufuncs work with NA, and generally return NA: np.log(pd.NA) <NA> np.add(pd.NA, 1) <NA> See DataFrame interoperability with NumPy functions for more on ufuncs.", "prev_chunk_id": "chunk_483", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_485", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Conversion#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Conversion#", "content": "Conversion# If you have a DataFrame or Series using np.nan, Series.convert_dtypes() and DataFrame.convert_dtypes() in DataFrame that can convert data to use the data types that use NA such as Int64Dtype or ArrowDtype. This is especially helpful after reading in data sets from IO methods where data types were inferred. In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns. import io data = io.StringIO(\"a,b\\n,True\\n2,\") df = pd.read_csv(data) df.dtypes a float64 b object dtype: object df_conv = df.convert_dtypes() df_conv a b 0 <NA> True 1 2 <NA> df_conv.dtypes a Int64 b boolean dtype: object", "prev_chunk_id": "chunk_484", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_486", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Inserting missing data#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Inserting missing data#", "content": "Inserting missing data# You can insert missing values by simply assigning to a Series or DataFrame. The missing value sentinel used will be chosen based on the dtype. ser = pd.Series([1., 2., 3.]) ser.loc[0] = None ser 0 NaN 1 2.0 2 3.0 dtype: float64 ser = pd.Series([pd.Timestamp(\"2021\"), pd.Timestamp(\"2021\")]) ser.iloc[0] = np.nan ser 0 NaT 1 2021-01-01 dtype: datetime64[ns] ser = pd.Series([True, False], dtype=\"boolean[pyarrow]\") ser.iloc[0] = None ser 0 <NA> 1 False dtype: bool[pyarrow] For object types, pandas will use the value given: s = pd.Series([\"a\", \"b\", \"c\"], dtype=object) s.loc[0] = None s.loc[1] = np.nan s 0 None 1 NaN 2 c dtype: object", "prev_chunk_id": "chunk_485", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_487", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Calculations with missing data#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Calculations with missing data#", "content": "Calculations with missing data# Missing values propagate through arithmetic operations between pandas objects. ser1 = pd.Series([np.nan, np.nan, 2, 3]) ser2 = pd.Series([np.nan, 1, np.nan, 4]) ser1 0 NaN 1 NaN 2 2.0 3 3.0 dtype: float64 ser2 0 NaN 1 1.0 2 NaN 3 4.0 dtype: float64 ser1 + ser2 0 NaN 1 NaN 2 NaN 3 7.0 dtype: float64 The descriptive statistics and computational methods discussed in the data structure overview (and listed here and here) are all account for missing data. When summing data, NA values or empty data will be treated as zero. pd.Series([np.nan]).sum() 0.0 pd.Series([], dtype=\"float64\").sum() 0.0 When taking the product, NA values or empty data will be treated as 1. pd.Series([np.nan]).prod() 1.0 pd.Series([], dtype=\"float64\").prod() 1.0 Cumulative methods like cumsum() and cumprod() ignore NA values by default preserve them in the result. This behavior can be changed with skipna - Cumulative methods likecumsum()andcumprod()ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, useskipna=False. ser = pd.Series([1, np.nan, 3, np.nan]) ser 0 1.0 1 NaN 2 3.0 3 NaN dtype: float64 ser.cumsum() 0 1.0 1 NaN 2 4.0 3 NaN dtype: float64 ser.cumsum(skipna=False) 0 1.0 1 NaN 2 NaN 3 NaN dtype: float64", "prev_chunk_id": "chunk_486", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_488", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Dropping missing data#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Dropping missing data#", "content": "Dropping missing data# dropna() dropa rows or columns with missing data. df = pd.DataFrame([[np.nan, 1, 2], [1, 2, np.nan], [1, 2, 3]]) df 0 1 2 0 NaN 1 2.0 1 1.0 2 NaN 2 1.0 2 3.0 df.dropna() 0 1 2 2 1.0 2 3.0 df.dropna(axis=1) 1 0 1 1 2 2 2 ser = pd.Series([1, pd.NA], dtype=\"int64[pyarrow]\") ser.dropna() 0 1 dtype: int64[pyarrow]", "prev_chunk_id": "chunk_487", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_489", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Filling by value#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Filling by value#", "content": "Filling by value# fillna() replaces NA values with non-NA data. Replace NA with a scalar value data = {\"np\": [1.0, np.nan, np.nan, 2], \"arrow\": pd.array([1.0, pd.NA, pd.NA, 2], dtype=\"float64[pyarrow]\")} df = pd.DataFrame(data) df np arrow 0 1.0 1.0 1 NaN <NA> 2 NaN <NA> 3 2.0 2.0 df.fillna(0) np arrow 0 1.0 1.0 1 0.0 0.0 2 0.0 0.0 3 2.0 2.0 Fill gaps forward or backward df.ffill() np arrow 0 1.0 1.0 1 1.0 1.0 2 1.0 1.0 3 2.0 2.0 df.bfill() np arrow 0 1.0 1.0 1 2.0 2.0 2 2.0 2.0 3 2.0 2.0 Limit the number of NA values filled df.ffill(limit=1) np arrow 0 1.0 1.0 1 1.0 1.0 2 NaN <NA> 3 2.0 2.0 NA values can be replaced with corresponding value from a Series or DataFrame where the index and column aligns between the original object and the filled object. dff = pd.DataFrame(np.arange(30, dtype=np.float64).reshape(10, 3), columns=list(\"ABC\")) dff.iloc[3:5, 0] = np.nan dff.iloc[4:6, 1] = np.nan dff.iloc[5:8, 2] = np.nan dff A B C 0 0.0 1.0 2.0 1 3.0 4.0 5.0 2 6.0 7.0 8.0 3 NaN 10.0 11.0 4 NaN NaN 14.0 5 15.0 NaN NaN 6 18.0 19.0 NaN 7 21.0 22.0 NaN 8 24.0 25.0 26.0 9 27.0 28.0 29.0 dff.fillna(dff.mean()) A B C 0 0.00 1.0 2.000000 1 3.00 4.0 5.000000 2 6.00 7.0 8.000000 3 14.25 10.0 11.000000 4 14.25 14.5 14.000000 5 15.00 14.5 13.571429 6 18.00 19.0 13.571429 7 21.00 22.0 13.571429 8 24.00 25.0 26.000000 9 27.00 28.0 29.000000", "prev_chunk_id": "chunk_488", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_490", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Interpolation#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Interpolation#", "content": "Interpolation# DataFrame.interpolate() and Series.interpolate() fills NA values using various interpolation methods. df = pd.DataFrame( .....: { .....: \"A\": [1, 2.1, np.nan, 4.7, 5.6, 6.8], .....: \"B\": [0.25, np.nan, np.nan, 4, 12.2, 14.4], .....: } .....: ) .....: df A B 0 1.0 0.25 1 2.1 NaN 2 NaN NaN 3 4.7 4.00 4 5.6 12.20 5 6.8 14.40 df.interpolate() A B 0 1.0 0.25 1 2.1 1.50 2 3.4 2.75 3 4.7 4.00 4 5.6 12.20 5 6.8 14.40 idx = pd.date_range(\"2020-01-01\", periods=10, freq=\"D\") data = np.random.default_rng(2).integers(0, 10, 10).astype(np.float64) ts = pd.Series(data, index=idx) ts.iloc[[1, 2, 5, 6, 9]] = np.nan ts 2020-01-01 8.0 2020-01-02 NaN 2020-01-03 NaN 2020-01-04 2.0 2020-01-05 4.0 2020-01-06 NaN 2020-01-07 NaN 2020-01-08 0.0 2020-01-09 3.0 2020-01-10 NaN Freq: D, dtype: float64 ts.plot() <Axes: > ts.interpolate() 2020-01-01 8.000000 2020-01-02 6.000000 2020-01-03 4.000000 2020-01-04 2.000000 2020-01-05 4.000000 2020-01-06 2.666667 2020-01-07 1.333333 2020-01-08 0.000000 2020-01-09 3.000000 2020-01-10 3.000000 Freq: D, dtype: float64 ts.interpolate().plot() <Axes: > Interpolation relative to a Timestamp in the DatetimeIndex is available by setting method=\"time\" ts2 = ts.iloc[[0, 1, 3, 7, 9]] ts2 2020-01-01 8.0 2020-01-02 NaN 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 NaN dtype: float64 ts2.interpolate() 2020-01-01 8.0 2020-01-02 5.0 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 0.0 dtype: float64 ts2.interpolate(method=\"time\") 2020-01-01 8.0 2020-01-02 6.0 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 0.0 dtype: float64 For a floating-point index, use method='values': idx = [0.0, 1.0, 10.0] ser = pd.Series([0.0, np.nan, 10.0], idx) ser 0.0 0.0 1.0 NaN 10.0 10.0 dtype: float64 ser.interpolate() 0.0 0.0 1.0 5.0 10.0 10.0 dtype: float64 ser.interpolate(method=\"values\") 0.0 0.0 1.0 1.0 10.0 10.0 dtype: float64 If you have scipy installed, you can pass the name of a 1-d interpolation routine to method. as specified in the scipy interpolation documentation and reference guide. The appropriate interpolation method will depend on the data type. When interpolating via a", "prev_chunk_id": "chunk_489", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_491", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Interpolation#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Interpolation#", "content": "polynomial or spline approximation, you must also specify the degree or order of the approximation: df.interpolate(method=\"spline\", order=2) A B 0 1.000000 0.250000 1 2.100000 -0.428598 2 3.404545 1.206900 3 4.700000 4.000000 4 5.600000 12.200000 5 6.800000 14.400000 df.interpolate(method=\"polynomial\", order=2) A B 0 1.000000 0.250000 1 2.100000 -2.703846 2 3.451351 -1.453846 3 4.700000 4.000000 4 5.600000 12.200000 5 6.800000 14.400000 Comparing several methods. np.random.seed(2) ser = pd.Series(np.arange(1, 10.1, 0.25) ** 2 + np.random.randn(37)) missing = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29]) ser.iloc[missing] = np.nan methods = [\"linear\", \"quadratic\", \"cubic\"] df = pd.DataFrame({m: ser.interpolate(method=m) for m in methods}) df.plot() <Axes: > Interpolating new observations from expanding data with Series.reindex(). ser = pd.Series(np.sort(np.random.uniform(size=100))) # interpolate at new_index new_index = ser.index.union(pd.Index([49.25, 49.5, 49.75, 50.25, 50.5, 50.75])) interp_s = ser.reindex(new_index).interpolate(method=\"pchip\") interp_s.loc[49:51] 49.00 0.471410 49.25 0.476841 49.50 0.481780 49.75 0.485998 50.00 0.489266 50.25 0.491814 50.50 0.493995 50.75 0.495763 51.00 0.497074 dtype: float64", "prev_chunk_id": "chunk_490", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_492", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Interpolation limits#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Interpolation limits#", "content": "Interpolation limits# interpolate() accepts a limit keyword argument to limit the number of consecutive NaN values filled since the last valid observation ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13, np.nan, np.nan]) ser 0 NaN 1 NaN 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 NaN 8 NaN dtype: float64 ser.interpolate() 0 NaN 1 NaN 2 5.0 3 7.0 4 9.0 5 11.0 6 13.0 7 13.0 8 13.0 dtype: float64 ser.interpolate(limit=1) 0 NaN 1 NaN 2 5.0 3 7.0 4 NaN 5 NaN 6 13.0 7 13.0 8 NaN dtype: float64 By default, NaN values are filled in a forward direction. Use limit_direction parameter to fill backward or from both directions. ser.interpolate(limit=1, limit_direction=\"backward\") 0 NaN 1 5.0 2 5.0 3 NaN 4 NaN 5 11.0 6 13.0 7 NaN 8 NaN dtype: float64 ser.interpolate(limit=1, limit_direction=\"both\") 0 NaN 1 5.0 2 5.0 3 7.0 4 NaN 5 11.0 6 13.0 7 13.0 8 NaN dtype: float64 ser.interpolate(limit_direction=\"both\") 0 5.0 1 5.0 2 5.0 3 7.0 4 9.0 5 11.0 6 13.0 7 13.0 8 13.0 dtype: float64 By default, NaN values are filled whether they are surrounded by existing valid values or outside existing valid values. The limit_area parameter restricts filling to either inside or outside values. # fill one consecutive inside value in both directions ser.interpolate(limit_direction=\"both\", limit_area=\"inside\", limit=1) 0 NaN 1 NaN 2 5.0 3 7.0 4 NaN 5 11.0 6 13.0 7 NaN 8 NaN dtype: float64 # fill all consecutive outside values backward ser.interpolate(limit_direction=\"backward\", limit_area=\"outside\") 0 5.0 1 5.0 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 NaN 8 NaN dtype: float64 # fill all consecutive outside values in both directions ser.interpolate(limit_direction=\"both\", limit_area=\"outside\") 0 5.0 1 5.0 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 13.0 8", "prev_chunk_id": "chunk_491", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_493", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Interpolation limits#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Interpolation limits#", "content": "13.0 dtype: float64", "prev_chunk_id": "chunk_492", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_494", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Replacing values#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Replacing values#", "content": "Replacing values# Series.replace() and DataFrame.replace() can be used similar to Series.fillna() and DataFrame.fillna() to replace or insert missing values. df = pd.DataFrame(np.eye(3)) df 0 1 2 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 df_missing = df.replace(0, np.nan) df_missing 0 1 2 0 1.0 NaN NaN 1 NaN 1.0 NaN 2 NaN NaN 1.0 df_filled = df_missing.replace(np.nan, 2) df_filled 0 1 2 0 1.0 2.0 2.0 1 2.0 1.0 2.0 2 2.0 2.0 1.0 Replacing more than one value is possible by passing a list. df_filled.replace([1, 44], [2, 28]) 0 1 2 0 2.0 2.0 2.0 1 2.0 2.0 2.0 2 2.0 2.0 2.0 Replacing using a mapping dict. df_filled.replace({1: 44, 2: 28}) 0 1 2 0 44.0 28.0 28.0 1 28.0 44.0 28.0 2 28.0 28.0 44.0", "prev_chunk_id": "chunk_493", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_495", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Regular expression replacement#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Regular expression replacement#", "content": "Regular expression replacement# Replace the ‘.’ with NaN d = {\"a\": list(range(4)), \"b\": list(\"ab..\"), \"c\": [\"a\", \"b\", np.nan, \"d\"]} df = pd.DataFrame(d) df.replace(\".\", np.nan) a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Replace the ‘.’ with NaN with regular expression that removes surrounding whitespace df.replace(r\"\\s*\\.\\s*\", np.nan, regex=True) a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Replace with a list of regexes. df.replace([r\"\\.\", r\"(a)\"], [\"dot\", r\"\\1stuff\"], regex=True) a b c 0 0 astuff astuff 1 1 b b 2 2 dot NaN 3 3 dot d Replace with a regex in a mapping dict. df.replace({\"b\": r\"\\s*\\.\\s*\"}, {\"b\": np.nan}, regex=True) a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Pass nested dictionaries of regular expressions that use the regex keyword. df.replace({\"b\": {\"b\": r\"\"}}, regex=True) a b c 0 0 a a 1 1 b 2 2 . NaN 3 3 . d df.replace(regex={\"b\": {r\"\\s*\\.\\s*\": np.nan}}) a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d df.replace({\"b\": r\"\\s*(\\.)\\s*\"}, {\"b\": r\"\\1ty\"}, regex=True) a b c 0 0 a a 1 1 b b 2 2 .ty NaN 3 3 .ty d Pass a list of regular expressions that will replace matches with a scalar. df.replace([r\"\\s*\\.\\s*\", r\"a|b\"], \"placeholder\", regex=True) a b c 0 0 placeholder placeholder 1 1 placeholder placeholder 2 2 placeholder NaN 3 3 placeholder d All of the regular expression examples can also be passed with the to_replace argument as the regex argument. In this case the value argument must be passed explicitly by name or regex must be a nested dictionary. df.replace(regex=[r\"\\s*\\.\\s*\", r\"a|b\"], value=\"placeholder\") a b c 0 0 placeholder", "prev_chunk_id": "chunk_494", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_496", "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html", "title": "Regular expression replacement#", "page_title": "Working with missing data — pandas 2.3.1 documentation", "breadcrumbs": "Regular expression replacement#", "content": "placeholder 1 1 placeholder placeholder 2 2 placeholder NaN 3 3 placeholder d", "prev_chunk_id": "chunk_495", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_497", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate Labels#", "content": "Duplicate Labels# Index objects are not required to be unique; you can have duplicate row or column labels. This may be a bit confusing at first. If you’re familiar with SQL, you know that row labels are similar to a primary key on a table, and you would never want duplicates in a SQL table. But one of pandas’ roles is to clean messy, real-world data before it goes to some downstream system. And real-world data has duplicates, even in fields that are supposed to be unique. This section describes how duplicate labels change the behavior of certain operations, and how prevent duplicates from arising during operations, or to detect them if they do. import pandas as pd import numpy as np", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_498", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Consequences of Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Consequences of Duplicate Labels#", "content": "Consequences of Duplicate Labels# Some pandas methods (Series.reindex() for example) just don’t work with duplicates present. The output can’t be determined, and so pandas raises. s1 = pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]) s1.reindex([\"a\", \"b\", \"c\"]) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[4], line 1 ----> 1 s1.reindex([\"a\", \"b\", \"c\"]) File ~/work/pandas/pandas/pandas/core/series.py:5164, in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance) 5147 @doc( 5148 NDFrame.reindex, # type: ignore[has-type] 5149 klass=_shared_doc_kwargs[\"klass\"], (...) 5162 tolerance=None, 5163 ) -> Series: -> 5164 return super().reindex( 5165 index=index, 5166 method=method, 5167 copy=copy, 5168 level=level, 5169 fill_value=fill_value, 5170 limit=limit, 5171 tolerance=tolerance, 5172 ) File ~/work/pandas/pandas/pandas/core/generic.py:5629, in NDFrame.reindex(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance) 5626 return self._reindex_multi(axes, copy, fill_value) 5628 # perform the reindex on the axes -> 5629 return self._reindex_axes( 5630 axes, level, limit, tolerance, method, fill_value, copy 5631 ).__finalize__(self, method=\"reindex\") File ~/work/pandas/pandas/pandas/core/generic.py:5652, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy) 5649 continue 5651 ax = self._get_axis(a) -> 5652 new_index, indexer = ax.reindex( 5653 labels, level=level, limit=limit, tolerance=tolerance, method=method 5654 ) 5656 axis = self._get_axis_number(a) 5657 obj = obj._reindex_with_indexers( 5658 {axis: [new_index, indexer]}, 5659 fill_value=fill_value, 5660 copy=copy, 5661 allow_dups=False, 5662 ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:4436, in Index.reindex(self, target, method, level, limit, tolerance) 4433 raise ValueError(\"cannot handle a non-unique multi-index!\") 4434 elif not self.is_unique: 4435 # GH#42568 -> 4436 raise ValueError(\"cannot reindex on an axis with duplicate labels\") 4437 else: 4438 indexer, _ = self.get_indexer_non_unique(target) ValueError: cannot reindex on an axis with duplicate labels Other methods, like indexing, can give very surprising results. Typically indexing with a scalar will reduce dimensionality. Slicing a DataFrame with a scalar will return a Series. Slicing a Series with a scalar will return a scalar. But with duplicates, this isn’t the case. df1 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"A\", \"B\"]) df1 A", "prev_chunk_id": "chunk_497", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_499", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Consequences of Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Consequences of Duplicate Labels#", "content": "A B 0 0 1 2 1 3 4 5 We have duplicates in the columns. If we slice 'B', we get back a Series df1[\"B\"] # a series 0 2 1 5 Name: B, dtype: int64 But slicing 'A' returns a DataFrame df1[\"A\"] # a DataFrame A A 0 0 1 1 3 4 This applies to row labels as well df2 = pd.DataFrame({\"A\": [0, 1, 2]}, index=[\"a\", \"a\", \"b\"]) df2 A a 0 a 1 b 2 df2.loc[\"b\", \"A\"] # a scalar 2 df2.loc[\"a\", \"A\"] # a Series a 0 a 1 Name: A, dtype: int64", "prev_chunk_id": "chunk_498", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_500", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Duplicate Label Detection#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate Label Detection#", "content": "Duplicate Label Detection# You can check whether an Index (storing the row or column labels) is unique with Index.is_unique: df2 A a 0 a 1 b 2 df2.index.is_unique False df2.columns.is_unique True Index.duplicated() will return a boolean ndarray indicating whether a label is repeated. df2.index.duplicated() array([False, True, False]) Which can be used as a boolean filter to drop duplicate rows. df2.loc[~df2.index.duplicated(), :] A a 0 b 2 If you need additional logic to handle duplicate labels, rather than just dropping the repeats, using groupby() on the index is a common trick. For example, we’ll resolve duplicates by taking the average of all rows with the same label. df2.groupby(level=0).mean() A a 0.5 b 2.0", "prev_chunk_id": "chunk_499", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_501", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Disallowing Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Disallowing Duplicate Labels#", "content": "Disallowing Duplicate Labels# As noted above, handling duplicates is an important feature when reading in raw data. That said, you may want to avoid introducing duplicates as part of a data processing pipeline (from methods like pandas.concat(), rename(), etc.). Both Series and DataFrame disallow duplicate labels by calling .set_flags(allows_duplicate_labels=False). (the default is to allow them). If there are duplicate labels, an exception will be raised. pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]).set_flags(allows_duplicate_labels=False) --------------------------------------------------------------------------- DuplicateLabelError Traceback (most recent call last) Cell In[19], line 1 ----> 1 pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]).set_flags(allows_duplicate_labels=False) File ~/work/pandas/pandas/pandas/core/generic.py:508, in NDFrame.set_flags(self, copy, allows_duplicate_labels) 506 df = self.copy(deep=copy and not using_copy_on_write()) 507 if allows_duplicate_labels is not None: --> 508 df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels 509 return df File ~/work/pandas/pandas/pandas/core/flags.py:109, in Flags.__setitem__(self, key, value) 107 if key not in self._keys: 108 raise ValueError(f\"Unknown flag {key}. Must be one of {self._keys}\") --> 109 setattr(self, key, value) File ~/work/pandas/pandas/pandas/core/flags.py:96, in Flags.allows_duplicate_labels(self, value) 94 if not value: 95 for ax in obj.axes: ---> 96 ax._maybe_check_unique() 98 self._allows_duplicate_labels = value File ~/work/pandas/pandas/pandas/core/indexes/base.py:716, in Index._maybe_check_unique(self) 713 duplicates = self._format_duplicate_message() 714 msg += f\"\\n{duplicates}\" --> 716 raise DuplicateLabelError(msg) DuplicateLabelError: Index has duplicates. positions label b [1, 2] This applies to both row and column labels for a DataFrame pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"B\", \"C\"],).set_flags( ....: allows_duplicate_labels=False ....: ) ....: A B C 0 0 1 2 1 3 4 5 This attribute can be checked or set with allows_duplicate_labels, which indicates whether that object can have duplicate labels. df = pd.DataFrame({\"A\": [0, 1, 2, 3]}, index=[\"x\", \"y\", \"X\", \"Y\"]).set_flags( ....: allows_duplicate_labels=False ....: ) ....: df A x 0 y 1 X 2 Y 3 df.flags.allows_duplicate_labels False DataFrame.set_flags() can be used to return a new DataFrame with attributes like allows_duplicate_labels set to some value df2 = df.set_flags(allows_duplicate_labels=True) df2.flags.allows_duplicate_labels True The new DataFrame returned is a", "prev_chunk_id": "chunk_500", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_502", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Disallowing Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Disallowing Duplicate Labels#", "content": "view on the same data as the old DataFrame. Or the property can just be set directly on the same object df2.flags.allows_duplicate_labels = False df2.flags.allows_duplicate_labels False When processing raw, messy data you might initially read in the messy data (which potentially has duplicate labels), deduplicate, and then disallow duplicates going forward, to ensure that your data pipeline doesn’t introduce duplicates. >>> raw = pd.read_csv(\"...\") >>> deduplicated = raw.groupby(level=0).first() # remove duplicates >>> deduplicated.flags.allows_duplicate_labels = False # disallow going forward Setting allows_duplicate_labels=False on a Series or DataFrame with duplicate labels or performing an operation that introduces duplicate labels on a Series or DataFrame that disallows duplicates will raise an errors.DuplicateLabelError. df.rename(str.upper) --------------------------------------------------------------------------- DuplicateLabelError Traceback (most recent call last) Cell In[28], line 1 ----> 1 df.rename(str.upper) File ~/work/pandas/pandas/pandas/core/frame.py:5774, in DataFrame.rename(self, mapper, index, columns, axis, copy, inplace, level, errors) 5643 def rename( 5644 self, 5645 mapper: Renamer | None = None, (...) 5653 errors: IgnoreRaise = \"ignore\", 5654 ) -> DataFrame | None: 5655 \"\"\" 5656 Rename columns or index labels. 5657 (...) 5772 4 3 6 5773 \"\"\" -> 5774 return super()._rename( 5775 mapper=mapper, 5776 index=index, 5777 columns=columns, 5778 axis=axis, 5779 copy=copy, 5780 inplace=inplace, 5781 level=level, 5782 errors=errors, 5783 ) File ~/work/pandas/pandas/pandas/core/generic.py:1140, in NDFrame._rename(self, mapper, index, columns, axis, copy, inplace, level, errors) 1138 return None 1139 else: -> 1140 return result.__finalize__(self, method=\"rename\") File ~/work/pandas/pandas/pandas/core/generic.py:6281, in NDFrame.__finalize__(self, other, method, **kwargs) 6274 if other.attrs: 6275 # We want attrs propagation to have minimal performance 6276 # impact if attrs are not used; i.e. attrs is an empty dict. 6277 # One could make the deepcopy unconditionally, but a deepcopy 6278 # of an empty dict is 50x more expensive than the empty check. 6279 self.attrs = deepcopy(other.attrs) -> 6281 self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels 6282 # For subclasses using _metadata. 6283 for name in set(self._metadata) &", "prev_chunk_id": "chunk_501", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_503", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Disallowing Duplicate Labels#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Disallowing Duplicate Labels#", "content": "set(other._metadata): File ~/work/pandas/pandas/pandas/core/flags.py:96, in Flags.allows_duplicate_labels(self, value) 94 if not value: 95 for ax in obj.axes: ---> 96 ax._maybe_check_unique() 98 self._allows_duplicate_labels = value File ~/work/pandas/pandas/pandas/core/indexes/base.py:716, in Index._maybe_check_unique(self) 713 duplicates = self._format_duplicate_message() 714 msg += f\"\\n{duplicates}\" --> 716 raise DuplicateLabelError(msg) DuplicateLabelError: Index has duplicates. positions label X [0, 2] Y [1, 3] This error message contains the labels that are duplicated, and the numeric positions of all the duplicates (including the “original”) in the Series or DataFrame", "prev_chunk_id": "chunk_502", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_504", "url": "https://pandas.pydata.org/docs/user_guide/duplicates.html", "title": "Duplicate Label Propagation#", "page_title": "Duplicate Labels — pandas 2.3.1 documentation", "breadcrumbs": "Duplicate Label Propagation#", "content": "Duplicate Label Propagation# In general, disallowing duplicates is “sticky”. It’s preserved through operations. s1 = pd.Series(0, index=[\"a\", \"b\"]).set_flags(allows_duplicate_labels=False) s1 a 0 b 0 dtype: int64 s1.head().rename({\"a\": \"b\"}) --------------------------------------------------------------------------- DuplicateLabelError Traceback (most recent call last) Cell In[31], line 1 ----> 1 s1.head().rename({\"a\": \"b\"}) File ~/work/pandas/pandas/pandas/core/series.py:5101, in Series.rename(self, index, axis, copy, inplace, level, errors) 5094 axis = self._get_axis_number(axis) 5096 if callable(index) or is_dict_like(index): 5097 # error: Argument 1 to \"_rename\" of \"NDFrame\" has incompatible 5098 # type \"Union[Union[Mapping[Any, Hashable], Callable[[Any], 5099 # Hashable]], Hashable, None]\"; expected \"Union[Mapping[Any, 5100 # Hashable], Callable[[Any], Hashable], None]\" -> 5101 return super()._rename( 5102 index, # type: ignore[arg-type] 5103 copy=copy, 5104 inplace=inplace, 5105 level=level, 5106 errors=errors, 5107 ) 5108 else: 5109 return self._set_name(index, inplace=inplace, deep=copy) File ~/work/pandas/pandas/pandas/core/generic.py:1140, in NDFrame._rename(self, mapper, index, columns, axis, copy, inplace, level, errors) 1138 return None 1139 else: -> 1140 return result.__finalize__(self, method=\"rename\") File ~/work/pandas/pandas/pandas/core/generic.py:6281, in NDFrame.__finalize__(self, other, method, **kwargs) 6274 if other.attrs: 6275 # We want attrs propagation to have minimal performance 6276 # impact if attrs are not used; i.e. attrs is an empty dict. 6277 # One could make the deepcopy unconditionally, but a deepcopy 6278 # of an empty dict is 50x more expensive than the empty check. 6279 self.attrs = deepcopy(other.attrs) -> 6281 self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels 6282 # For subclasses using _metadata. 6283 for name in set(self._metadata) & set(other._metadata): File ~/work/pandas/pandas/pandas/core/flags.py:96, in Flags.allows_duplicate_labels(self, value) 94 if not value: 95 for ax in obj.axes: ---> 96 ax._maybe_check_unique() 98 self._allows_duplicate_labels = value File ~/work/pandas/pandas/pandas/core/indexes/base.py:716, in Index._maybe_check_unique(self) 713 duplicates = self._format_duplicate_message() 714 msg += f\"\\n{duplicates}\" --> 716 raise DuplicateLabelError(msg) DuplicateLabelError: Index has duplicates. positions label b [0, 1]", "prev_chunk_id": "chunk_503", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_505", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Categorical data#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Categorical data#", "content": "Categorical data# This is an introduction to pandas categorical data type, including a short comparison with R’s factor. Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales. In contrast to statistical categorical variables, categorical data might have an order (e.g. ‘strongly agree’ vs ‘agree’ or ‘first observation’ vs. ‘second observation’), but numerical operations (additions, divisions, …) are not possible. All values of categorical data are either in categories or np.nan. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array. The categorical data type is useful in the following cases: - A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, seehere. - The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, seehere. - As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types). See also the API docs on categoricals.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_506", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Series creation#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Series creation#", "content": "Series creation# Categorical Series or columns in a DataFrame can be created in several ways: By specifying dtype=\"category\" when constructing a Series: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\") s 0 a 1 b 2 c 3 a dtype: category Categories (3, object): ['a', 'b', 'c'] By converting an existing Series or column to a category dtype: df = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]}) df[\"B\"] = df[\"A\"].astype(\"category\") df A B 0 a a 1 b b 2 c c 3 a a By using special functions, such as cut(), which groups data into discrete bins. See the example on tiling in the docs. df = pd.DataFrame({\"value\": np.random.randint(0, 100, 20)}) labels = [\"{0} - {1}\".format(i, i + 9) for i in range(0, 100, 10)] df[\"group\"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels) df.head(10) value group 0 65 60 - 69 1 49 40 - 49 2 56 50 - 59 3 43 40 - 49 4 43 40 - 49 5 91 90 - 99 6 32 30 - 39 7 87 80 - 89 8 36 30 - 39 9 8 0 - 9 By passing a pandas.Categorical object to a Series or assigning it to a DataFrame. raw_cat = pd.Categorical( ....: [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False ....: ) ....: s = pd.Series(raw_cat) s 0 NaN 1 b 2 c 3 NaN dtype: category Categories (3, object): ['b', 'c', 'd'] df = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]}) df[\"B\"] = raw_cat df A B 0 a NaN 1 b b 2 c c 3 a NaN Categorical data has a specific category dtype: df.dtypes A object B category dtype: object", "prev_chunk_id": "chunk_505", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_507", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "DataFrame creation#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame creation#", "content": "DataFrame creation# Similar to the previous section where a single column was converted to categorical, all columns in a DataFrame can be batch converted to categorical either during or after construction. This can be done during construction by specifying dtype=\"category\" in the DataFrame constructor: df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")}, dtype=\"category\") df.dtypes A category B category dtype: object Note that the categories present in each column differ; the conversion is done column by column, so only labels present in a given column are categories: df[\"A\"] 0 a 1 b 2 c 3 a Name: A, dtype: category Categories (3, object): ['a', 'b', 'c'] df[\"B\"] 0 b 1 c 2 c 3 d Name: B, dtype: category Categories (3, object): ['b', 'c', 'd'] Analogously, all columns in an existing DataFrame can be batch converted using DataFrame.astype(): df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")}) df_cat = df.astype(\"category\") df_cat.dtypes A category B category dtype: object This conversion is likewise done column by column: df_cat[\"A\"] 0 a 1 b 2 c 3 a Name: A, dtype: category Categories (3, object): ['a', 'b', 'c'] df_cat[\"B\"] 0 b 1 c 2 c 3 d Name: B, dtype: category Categories (3, object): ['b', 'c', 'd']", "prev_chunk_id": "chunk_506", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_508", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Controlling behavior#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Controlling behavior#", "content": "Controlling behavior# In the examples above where we passed dtype='category', we used the default behavior: - Categories are inferred from the data. - Categories are unordered. To control those behaviors, instead of passing 'category', use an instance of CategoricalDtype. from pandas.api.types import CategoricalDtype s = pd.Series([\"a\", \"b\", \"c\", \"a\"]) cat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True) s_cat = s.astype(cat_type) s_cat 0 NaN 1 b 2 c 3 NaN dtype: category Categories (3, object): ['b' < 'c' < 'd'] Similarly, a CategoricalDtype can be used with a DataFrame to ensure that categories are consistent among all columns. from pandas.api.types import CategoricalDtype df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")}) cat_type = CategoricalDtype(categories=list(\"abcd\"), ordered=True) df_cat = df.astype(cat_type) df_cat[\"A\"] 0 a 1 b 2 c 3 a Name: A, dtype: category Categories (4, object): ['a' < 'b' < 'c' < 'd'] df_cat[\"B\"] 0 b 1 c 2 c 3 d Name: B, dtype: category Categories (4, object): ['a' < 'b' < 'c' < 'd'] If you already have codes and categories, you can use the from_codes() constructor to save the factorize step during normal constructor mode: splitter = np.random.choice([0, 1], 5, p=[0.5, 0.5]) s = pd.Series(pd.Categorical.from_codes(splitter, categories=[\"train\", \"test\"]))", "prev_chunk_id": "chunk_507", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_509", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Regaining original data#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Regaining original data#", "content": "Regaining original data# To get back to the original Series or NumPy array, use Series.astype(original_dtype) or np.asarray(categorical): s = pd.Series([\"a\", \"b\", \"c\", \"a\"]) s 0 a 1 b 2 c 3 a dtype: object s2 = s.astype(\"category\") s2 0 a 1 b 2 c 3 a dtype: category Categories (3, object): ['a', 'b', 'c'] s2.astype(str) 0 a 1 b 2 c 3 a dtype: object np.asarray(s2) array(['a', 'b', 'c', 'a'], dtype=object)", "prev_chunk_id": "chunk_508", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_510", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "CategoricalDtype#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "CategoricalDtype#", "content": "CategoricalDtype# A categorical’s type is fully described by - categories: a sequence of unique values and no missing values - ordered: a boolean This information can be stored in a CategoricalDtype. The categories argument is optional, which implies that the actual categories should be inferred from whatever is present in the data when the pandas.Categorical is created. The categories are assumed to be unordered by default. from pandas.api.types import CategoricalDtype CategoricalDtype([\"a\", \"b\", \"c\"]) CategoricalDtype(categories=['a', 'b', 'c'], ordered=False, categories_dtype=object) CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True) CategoricalDtype(categories=['a', 'b', 'c'], ordered=True, categories_dtype=object) CategoricalDtype() CategoricalDtype(categories=None, ordered=False, categories_dtype=None) A CategoricalDtype can be used in any place pandas expects a dtype. For example pandas.read_csv(), pandas.DataFrame.astype(), or in the Series constructor.", "prev_chunk_id": "chunk_509", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_511", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Equality semantics#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Equality semantics#", "content": "Equality semantics# Two instances of CategoricalDtype compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the categories is not considered. c1 = CategoricalDtype([\"a\", \"b\", \"c\"], ordered=False) # Equal, since order is not considered when ordered=False c1 == CategoricalDtype([\"b\", \"c\", \"a\"], ordered=False) True # Unequal, since the second CategoricalDtype is ordered c1 == CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True) False All instances of CategoricalDtype compare equal to the string 'category'. c1 == \"category\" True", "prev_chunk_id": "chunk_510", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_512", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Description#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Description#", "content": "Description# Using describe() on categorical data will produce similar output to a Series or DataFrame of type string. cat = pd.Categorical([\"a\", \"c\", \"c\", np.nan], categories=[\"b\", \"a\", \"c\"]) df = pd.DataFrame({\"cat\": cat, \"s\": [\"a\", \"c\", \"c\", np.nan]}) df.describe() cat s count 3 3 unique 2 2 top c c freq 2 2 df[\"cat\"].describe() count 3 unique 2 top c freq 2 Name: cat, dtype: object", "prev_chunk_id": "chunk_511", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_513", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Working with categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Working with categories#", "content": "Working with categories# Categorical data has a categories and a ordered property, which list their possible values and whether the ordering matters or not. These properties are exposed as s.cat.categories and s.cat.ordered. If you don’t manually specify categories and ordering, they are inferred from the passed arguments. s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\") s.cat.categories Index(['a', 'b', 'c'], dtype='object') s.cat.ordered False It’s also possible to pass in the categories in a specific order: s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"c\", \"b\", \"a\"])) s.cat.categories Index(['c', 'b', 'a'], dtype='object') s.cat.ordered False", "prev_chunk_id": "chunk_512", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_514", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Renaming categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Renaming categories#", "content": "Renaming categories# Renaming categories is done by using the rename_categories() method: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\") s 0 a 1 b 2 c 3 a dtype: category Categories (3, object): ['a', 'b', 'c'] new_categories = [\"Group %s\" % g for g in s.cat.categories] s = s.cat.rename_categories(new_categories) s 0 Group a 1 Group b 2 Group c 3 Group a dtype: category Categories (3, object): ['Group a', 'Group b', 'Group c'] # You can also pass a dict-like object to map the renaming s = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"}) s 0 Group a 1 Group b 2 Group c 3 Group a dtype: category Categories (3, object): ['Group a', 'Group b', 'Group c'] Categories must be unique or a ValueError is raised: try: ....: s = s.cat.rename_categories([1, 1, 1]) ....: except ValueError as e: ....: print(\"ValueError:\", str(e)) ....: ValueError: Categorical categories must be unique Categories must also not be NaN or a ValueError is raised: try: ....: s = s.cat.rename_categories([1, 2, np.nan]) ....: except ValueError as e: ....: print(\"ValueError:\", str(e)) ....: ValueError: Categorical categories cannot be null", "prev_chunk_id": "chunk_513", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_515", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Appending new categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Appending new categories#", "content": "Appending new categories# Appending categories can be done by using the add_categories() method: s = s.cat.add_categories([4]) s.cat.categories Index(['Group a', 'Group b', 'Group c', 4], dtype='object') s 0 Group a 1 Group b 2 Group c 3 Group a dtype: category Categories (4, object): ['Group a', 'Group b', 'Group c', 4]", "prev_chunk_id": "chunk_514", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_516", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Removing categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Removing categories#", "content": "Removing categories# Removing categories can be done by using the remove_categories() method. Values which are removed are replaced by np.nan.: s = s.cat.remove_categories([4]) s 0 Group a 1 Group b 2 Group c 3 Group a dtype: category Categories (3, object): ['Group a', 'Group b', 'Group c']", "prev_chunk_id": "chunk_515", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_517", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Removing unused categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Removing unused categories#", "content": "Removing unused categories# Removing unused categories can also be done: s = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"], categories=[\"a\", \"b\", \"c\", \"d\"])) s 0 a 1 b 2 a dtype: category Categories (4, object): ['a', 'b', 'c', 'd'] s.cat.remove_unused_categories() 0 a 1 b 2 a dtype: category Categories (2, object): ['a', 'b']", "prev_chunk_id": "chunk_516", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_518", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Setting categories#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Setting categories#", "content": "Setting categories# If you want to do remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, use set_categories(). s = pd.Series([\"one\", \"two\", \"four\", \"-\"], dtype=\"category\") s 0 one 1 two 2 four 3 - dtype: category Categories (4, object): ['-', 'four', 'one', 'two'] s = s.cat.set_categories([\"one\", \"two\", \"three\", \"four\"]) s 0 one 1 two 2 four 3 NaN dtype: category Categories (4, object): ['one', 'two', 'three', 'four']", "prev_chunk_id": "chunk_517", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_519", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Sorting and order#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Sorting and order#", "content": "Sorting and order# If categorical data is ordered (s.cat.ordered == True), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, .min()/.max() will raise a TypeError. s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=False)) s = s.sort_values() s = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True)) s = s.sort_values() s 0 a 3 a 1 b 2 c dtype: category Categories (3, object): ['a' < 'b' < 'c'] s.min(), s.max() ('a', 'c') You can set categorical data to be ordered by using as_ordered() or unordered by using as_unordered(). These will by default return a new object. s.cat.as_ordered() 0 a 3 a 1 b 2 c dtype: category Categories (3, object): ['a' < 'b' < 'c'] s.cat.as_unordered() 0 a 3 a 1 b 2 c dtype: category Categories (3, object): ['a', 'b', 'c'] Sorting will use the order defined by categories, not any lexical order present on the data type. This is even true for strings and numeric data: s = pd.Series([1, 2, 3, 1], dtype=\"category\") s = s.cat.set_categories([2, 3, 1], ordered=True) s 0 1 1 2 2 3 3 1 dtype: category Categories (3, int64): [2 < 3 < 1] s = s.sort_values() s 1 2 2 3 0 1 3 1 dtype: category Categories (3, int64): [2 < 3 < 1] s.min(), s.max() (2, 1)", "prev_chunk_id": "chunk_518", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_520", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Reordering#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Reordering#", "content": "Reordering# Reordering the categories is possible via the Categorical.reorder_categories() and the Categorical.set_categories() methods. For Categorical.reorder_categories(), all old categories must be included in the new categories and no new categories are allowed. This will necessarily make the sort order the same as the categories order. s = pd.Series([1, 2, 3, 1], dtype=\"category\") s = s.cat.reorder_categories([2, 3, 1], ordered=True) s 0 1 1 2 2 3 3 1 dtype: category Categories (3, int64): [2 < 3 < 1] s = s.sort_values() s 1 2 2 3 0 1 3 1 dtype: category Categories (3, int64): [2 < 3 < 1] s.min(), s.max() (2, 1)", "prev_chunk_id": "chunk_519", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_521", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Multi column sorting#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Multi column sorting#", "content": "Multi column sorting# A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns. The ordering of the categorical is determined by the categories of that column. dfs = pd.DataFrame( .....: { .....: \"A\": pd.Categorical( .....: list(\"bbeebbaa\"), .....: categories=[\"e\", \"a\", \"b\"], .....: ordered=True, .....: ), .....: \"B\": [1, 2, 1, 2, 2, 1, 2, 1], .....: } .....: ) .....: dfs.sort_values(by=[\"A\", \"B\"]) A B 2 e 1 3 e 2 7 a 1 6 a 2 0 b 1 5 b 1 1 b 2 4 b 2 Reordering the categories changes a future sort. dfs[\"A\"] = dfs[\"A\"].cat.reorder_categories([\"a\", \"b\", \"e\"]) dfs.sort_values(by=[\"A\", \"B\"]) A B 7 a 1 6 a 2 0 b 1 5 b 1 1 b 2 4 b 2 2 e 1 3 e 2", "prev_chunk_id": "chunk_520", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_522", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Comparisons#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Comparisons#", "content": "Comparisons# Comparing categorical data with other objects is possible in three cases: - Comparing equality (==and!=) to a list-like object (list, Series, array, …) of the same length as the categorical data. - All comparisons (==,!=,>,>=,<, and<=) of categorical data to another categorical Series, whenordered==Trueand thecategoriesare the same. - All comparisons of a categorical data to a scalar. All other comparisons, especially “non-equality” comparisons of two categoricals with different categories or a categorical with any list-like object, will raise a TypeError. cat = pd.Series([1, 2, 3]).astype(CategoricalDtype([3, 2, 1], ordered=True)) cat_base = pd.Series([2, 2, 2]).astype(CategoricalDtype([3, 2, 1], ordered=True)) cat_base2 = pd.Series([2, 2, 2]).astype(CategoricalDtype(ordered=True)) cat 0 1 1 2 2 3 dtype: category Categories (3, int64): [3 < 2 < 1] cat_base 0 2 1 2 2 2 dtype: category Categories (3, int64): [3 < 2 < 1] cat_base2 0 2 1 2 2 2 dtype: category Categories (1, int64): [2] Comparing to a categorical with the same categories and ordering or to a scalar works: cat > cat_base 0 True 1 False 2 False dtype: bool cat > 2 0 True 1 False 2 False dtype: bool Equality comparisons work with any list-like object of same length and scalars: cat == cat_base 0 False 1 True 2 False dtype: bool cat == np.array([1, 2, 3]) 0 True 1 True 2 True dtype: bool cat == 2 0 False 1 True 2 False dtype: bool This doesn’t work because the categories are not the same: try: .....: cat > cat_base2 .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: Categoricals can only be compared if 'categories' are the same. If you want to do a “non-equality” comparison of a categorical series with a list-like object which is not categorical data, you need to be explicit and convert the categorical data back", "prev_chunk_id": "chunk_521", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_523", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Comparisons#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Comparisons#", "content": "to the original values: base = np.array([1, 2, 3]) try: .....: cat > base .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: Cannot compare a Categorical for op __gt__ with type <class 'numpy.ndarray'>. If you want to compare values, use 'np.asarray(cat) <op> other'. np.asarray(cat) > base array([False, False, False]) When you compare two unordered categoricals with the same categories, the order is not considered: c1 = pd.Categorical([\"a\", \"b\"], categories=[\"a\", \"b\"], ordered=False) c2 = pd.Categorical([\"a\", \"b\"], categories=[\"b\", \"a\"], ordered=False) c1 == c2 array([ True, True])", "prev_chunk_id": "chunk_522", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_524", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Operations#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# Apart from Series.min(), Series.max() and Series.mode(), the following operations are possible with categorical data: Series methods like Series.value_counts() will use all categories, even if some categories are not present in the data: s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"c\"], categories=[\"c\", \"a\", \"b\", \"d\"])) s.value_counts() c 2 a 1 b 1 d 0 Name: count, dtype: int64 DataFrame methods like DataFrame.sum() also show “unused” categories when observed=False. columns = pd.Categorical( .....: [\"One\", \"One\", \"Two\"], categories=[\"One\", \"Two\", \"Three\"], ordered=True .....: ) .....: df = pd.DataFrame( .....: data=[[1, 2, 3], [4, 5, 6]], .....: columns=pd.MultiIndex.from_arrays([[\"A\", \"B\", \"B\"], columns]), .....: ).T .....: df.groupby(level=1, observed=False).sum() 0 1 One 3 9 Two 3 6 Three 0 0 Groupby will also show “unused” categories when observed=False: cats = pd.Categorical( .....: [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"], categories=[\"a\", \"b\", \"c\", \"d\"] .....: ) .....: df = pd.DataFrame({\"cats\": cats, \"values\": [1, 2, 2, 2, 3, 4, 5]}) df.groupby(\"cats\", observed=False).mean() values cats a 1.0 b 2.0 c 4.0 d NaN cats2 = pd.Categorical([\"a\", \"a\", \"b\", \"b\"], categories=[\"a\", \"b\", \"c\"]) df2 = pd.DataFrame( .....: { .....: \"cats\": cats2, .....: \"B\": [\"c\", \"d\", \"c\", \"d\"], .....: \"values\": [1, 2, 3, 4], .....: } .....: ) .....: df2.groupby([\"cats\", \"B\"], observed=False).mean() values cats B a c 1.0 d 2.0 b c 3.0 d 4.0 c c NaN d NaN Pivot tables: raw_cat = pd.Categorical([\"a\", \"a\", \"b\", \"b\"], categories=[\"a\", \"b\", \"c\"]) df = pd.DataFrame({\"A\": raw_cat, \"B\": [\"c\", \"d\", \"c\", \"d\"], \"values\": [1, 2, 3, 4]}) pd.pivot_table(df, values=\"values\", index=[\"A\", \"B\"], observed=False) values A B a c 1.0 d 2.0 b c 3.0 d 4.0", "prev_chunk_id": "chunk_523", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_525", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Data munging#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Data munging#", "content": "Data munging# The optimized pandas data access methods .loc, .iloc, .at, and .iat, work as normal. The only difference is the return type (for getting) and that only values already in categories can be assigned.", "prev_chunk_id": "chunk_524", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_526", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Getting#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Getting#", "content": "Getting# If the slicing operation returns either a DataFrame or a column of type Series, the category dtype is preserved. idx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"]) cats = pd.Series([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"], dtype=\"category\", index=idx) values = [1, 2, 2, 2, 3, 4, 5] df = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx) df.iloc[2:4, :] cats values j b 2 k b 2 df.iloc[2:4, :].dtypes cats category values int64 dtype: object df.loc[\"h\":\"j\", \"cats\"] h a i b j b Name: cats, dtype: category Categories (3, object): ['a', 'b', 'c'] df[df[\"cats\"] == \"b\"] cats values i b 2 j b 2 k b 2 An example where the category type is not preserved is if you take one single row: the resulting Series is of dtype object: # get the complete \"h\" row as a Series df.loc[\"h\", :] cats a values 1 Name: h, dtype: object Returning a single item from categorical data will also return the value, not a categorical of length “1”. df.iat[0, 0] 'a' df[\"cats\"] = df[\"cats\"].cat.rename_categories([\"x\", \"y\", \"z\"]) df.at[\"h\", \"cats\"] # returns a string 'x' To get a single value Series of type category, you pass in a list with a single value: df.loc[[\"h\"], \"cats\"] h x Name: cats, dtype: category Categories (3, object): ['x', 'y', 'z']", "prev_chunk_id": "chunk_525", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_527", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "String and datetime accessors#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "String and datetime accessors#", "content": "String and datetime accessors# The accessors .dt and .str will work if the s.cat.categories are of an appropriate type: str_s = pd.Series(list(\"aabb\")) str_cat = str_s.astype(\"category\") str_cat 0 a 1 a 2 b 3 b dtype: category Categories (2, object): ['a', 'b'] str_cat.str.contains(\"a\") 0 True 1 True 2 False 3 False dtype: bool date_s = pd.Series(pd.date_range(\"1/1/2015\", periods=5)) date_cat = date_s.astype(\"category\") date_cat 0 2015-01-01 1 2015-01-02 2 2015-01-03 3 2015-01-04 4 2015-01-05 dtype: category Categories (5, datetime64[ns]): [2015-01-01, 2015-01-02, 2015-01-03, 2015-01-04, 2015-01-05] date_cat.dt.day 0 1 1 2 2 3 3 4 4 5 dtype: int32 That means, that the returned values from methods and properties on the accessors of a Series and the returned values from methods and properties on the accessors of this Series transformed to one of type category will be equal: ret_s = str_s.str.contains(\"a\") ret_cat = str_cat.str.contains(\"a\") ret_s.dtype == ret_cat.dtype True ret_s == ret_cat 0 True 1 True 2 True 3 True dtype: bool", "prev_chunk_id": "chunk_526", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_528", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Setting#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Setting#", "content": "Setting# Setting values in a categorical column (or Series) works as long as the value is included in the categories: idx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"]) cats = pd.Categorical([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]) values = [1, 1, 1, 1, 1, 1, 1] df = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx) df.iloc[2:4, :] = [[\"b\", 2], [\"b\", 2]] df cats values h a 1 i a 1 j b 2 k b 2 l a 1 m a 1 n a 1 try: .....: df.iloc[2:4, :] = [[\"c\", 3], [\"c\", 3]] .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: Cannot setitem on a Categorical with a new category, set the categories first Setting values by assigning categorical data will also check that the categories match: df.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"a\", \"a\"], categories=[\"a\", \"b\"]) df cats values h a 1 i a 1 j a 2 k a 2 l a 1 m a 1 n a 1 try: .....: df.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\", \"c\"]) .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: Cannot set a Categorical with another, without identical categories Assigning a Categorical to parts of a column of other types will use the values: df = pd.DataFrame({\"a\": [1, 1, 1, 1, 1], \"b\": [\"a\", \"a\", \"a\", \"a\", \"a\"]}) df.loc[1:2, \"a\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\"]) df.loc[2:3, \"b\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\"]) df a b 0 1 a 1 b a 2 b b 3 1 b 4 1 a df.dtypes a object b object dtype: object", "prev_chunk_id": "chunk_527", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_529", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Merging / concatenation#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Merging / concatenation#", "content": "Merging / concatenation# By default, combining Series or DataFrames which contain the same categories results in category dtype, otherwise results will depend on the dtype of the underlying categories. Merges that result in non-categorical dtypes will likely have higher memory usage. Use .astype or union_categoricals to ensure category results. from pandas.api.types import union_categoricals # same categories s1 = pd.Series([\"a\", \"b\"], dtype=\"category\") s2 = pd.Series([\"a\", \"b\", \"a\"], dtype=\"category\") pd.concat([s1, s2]) 0 a 1 b 0 a 1 b 2 a dtype: category Categories (2, object): ['a', 'b'] # different categories s3 = pd.Series([\"b\", \"c\"], dtype=\"category\") pd.concat([s1, s3]) 0 a 1 b 0 b 1 c dtype: object # Output dtype is inferred based on categories values int_cats = pd.Series([1, 2], dtype=\"category\") float_cats = pd.Series([3.0, 4.0], dtype=\"category\") pd.concat([int_cats, float_cats]) 0 1.0 1 2.0 0 3.0 1 4.0 dtype: float64 pd.concat([s1, s3]).astype(\"category\") 0 a 1 b 0 b 1 c dtype: category Categories (3, object): ['a', 'b', 'c'] union_categoricals([s1.array, s3.array]) ['a', 'b', 'b', 'c'] Categories (3, object): ['a', 'b', 'c'] The following table summarizes the results of merging Categoricals: arg1 | arg2 | identical | result category | category | True | category category (object) | category (object) | False | object (dtype is inferred) category (int) | category (float) | False | float (dtype is inferred)", "prev_chunk_id": "chunk_528", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_530", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Unioning#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Unioning#", "content": "Unioning# If you want to combine categoricals that do not necessarily have the same categories, the union_categoricals() function will combine a list-like of categoricals. The new categories will be the union of the categories being combined. from pandas.api.types import union_categoricals a = pd.Categorical([\"b\", \"c\"]) b = pd.Categorical([\"a\", \"b\"]) union_categoricals([a, b]) ['b', 'c', 'a', 'b'] Categories (3, object): ['b', 'c', 'a'] By default, the resulting categories will be ordered as they appear in the data. If you want the categories to be lexsorted, use sort_categories=True argument. union_categoricals([a, b], sort_categories=True) ['b', 'c', 'a', 'b'] Categories (3, object): ['a', 'b', 'c'] union_categoricals also works with the “easy” case of combining two categoricals of the same categories and order information (e.g. what you could also append for). a = pd.Categorical([\"a\", \"b\"], ordered=True) b = pd.Categorical([\"a\", \"b\", \"a\"], ordered=True) union_categoricals([a, b]) ['a', 'b', 'a', 'b', 'a'] Categories (2, object): ['a' < 'b'] The below raises TypeError because the categories are ordered and not identical. a = pd.Categorical([\"a\", \"b\"], ordered=True) b = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True) union_categoricals([a, b]) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[205], line 1 ----> 1 union_categoricals([a, b]) File ~/work/pandas/pandas/pandas/core/dtypes/concat.py:341, in union_categoricals(to_union, sort_categories, ignore_order) 339 if all(c.ordered for c in to_union): 340 msg = \"to union ordered Categoricals, all categories must be the same\" --> 341 raise TypeError(msg) 342 raise TypeError(\"Categorical.ordered must be the same\") 344 if ignore_order: TypeError: to union ordered Categoricals, all categories must be the same Ordered categoricals with different categories or orderings can be combined by using the ignore_ordered=True argument. a = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True) b = pd.Categorical([\"c\", \"b\", \"a\"], ordered=True) union_categoricals([a, b], ignore_order=True) ['a', 'b', 'c', 'c', 'b', 'a'] Categories (3, object): ['a', 'b', 'c'] union_categoricals() also works with a CategoricalIndex, or Series containing categorical data, but note that the resulting array will always be", "prev_chunk_id": "chunk_529", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_531", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Unioning#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Unioning#", "content": "a plain Categorical: a = pd.Series([\"b\", \"c\"], dtype=\"category\") b = pd.Series([\"a\", \"b\"], dtype=\"category\") union_categoricals([a, b]) ['b', 'c', 'a', 'b'] Categories (3, object): ['b', 'c', 'a']", "prev_chunk_id": "chunk_530", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_532", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Getting data in/out#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Getting data in/out#", "content": "Getting data in/out# You can write data that contains category dtypes to a HDFStore. See here for an example and caveats. It is also possible to write data to and reading data from Stata format files. See here for an example and caveats. Writing to a CSV file will convert the data, effectively removing any information about the categorical (categories and ordering). So if you read back the CSV file you have to convert the relevant columns back to category and assign the right categories and categories ordering. import io s = pd.Series(pd.Categorical([\"a\", \"b\", \"b\", \"a\", \"a\", \"d\"])) # rename the categories s = s.cat.rename_categories([\"very good\", \"good\", \"bad\"]) # reorder the categories and add missing categories s = s.cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]) df = pd.DataFrame({\"cats\": s, \"vals\": [1, 2, 3, 4, 5, 6]}) csv = io.StringIO() df.to_csv(csv) df2 = pd.read_csv(io.StringIO(csv.getvalue())) df2.dtypes Unnamed: 0 int64 cats object vals int64 dtype: object df2[\"cats\"] 0 very good 1 good 2 good 3 very good 4 very good 5 bad Name: cats, dtype: object # Redo the category df2[\"cats\"] = df2[\"cats\"].astype(\"category\") df2[\"cats\"] = df2[\"cats\"].cat.set_categories( .....: [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"] .....: ) .....: df2.dtypes Unnamed: 0 int64 cats category vals int64 dtype: object df2[\"cats\"] 0 very good 1 good 2 good 3 very good 4 very good 5 bad Name: cats, dtype: category Categories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good'] The same holds for writing to a SQL database with to_sql.", "prev_chunk_id": "chunk_531", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_533", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Missing data#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Missing data#", "content": "Missing data# pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section. Missing values should not be included in the Categorical’s categories, only in the values. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical’s codes, missing values will always have a code of -1. s = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\") # only two categories s 0 a 1 b 2 NaN 3 a dtype: category Categories (2, object): ['a', 'b'] s.cat.codes 0 0 1 1 2 -1 3 0 dtype: int8 Methods for working with missing data, e.g. isna(), fillna(), dropna(), all work normally: s = pd.Series([\"a\", \"b\", np.nan], dtype=\"category\") s 0 a 1 b 2 NaN dtype: category Categories (2, object): ['a', 'b'] pd.isna(s) 0 False 1 False 2 True dtype: bool s.fillna(\"a\") 0 a 1 b 2 a dtype: category Categories (2, object): ['a', 'b']", "prev_chunk_id": "chunk_532", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_534", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Differences to R’s factor#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Differences to R’s factor#", "content": "Differences to R’s factor# The following differences to R’s factor functions can be observed: - R’slevelsare namedcategories. - R’slevelsare always of type string, whilecategoriesin pandas can be of any dtype. - It’s not possible to specify labels at creation time. Uses.cat.rename_categories(new_labels)afterwards. - In contrast to R’sfactorfunction, using categorical data as the sole input to create a new categorical series willnotremove unused categories but create a new categorical series which is equal to the passed in one! - R allows for missing values to be included in itslevels(pandas’categories). pandas does not allowNaNcategories, but missing values can still be in thevalues.", "prev_chunk_id": "chunk_533", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_535", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Memory usage#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Memory usage#", "content": "Memory usage# The memory usage of a Categorical is proportional to the number of categories plus the length of the data. In contrast, an object dtype is a constant times the length of the data. s = pd.Series([\"foo\", \"bar\"] * 1000) # object dtype s.nbytes 16000 # category dtype s.astype(\"category\").nbytes 2016", "prev_chunk_id": "chunk_534", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_536", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Categorical is not a numpy array#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Categorical is not a numpy array#", "content": "Categorical is not a numpy array# Currently, categorical data and the underlying Categorical is implemented as a Python object and not as a low-level NumPy array dtype. This leads to some problems. NumPy itself doesn’t know about the new dtype: try: .....: np.dtype(\"category\") .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: data type 'category' not understood dtype = pd.Categorical([\"a\"]).dtype try: .....: np.dtype(dtype) .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: Cannot interpret 'CategoricalDtype(categories=['a'], ordered=False, categories_dtype=object)' as a data type Dtype comparisons work: dtype == np.str_ False np.str_ == dtype False To check if a Series contains Categorical data, use hasattr(s, 'cat'): hasattr(pd.Series([\"a\"], dtype=\"category\"), \"cat\") True hasattr(pd.Series([\"a\"]), \"cat\") False Using NumPy functions on a Series of type category should not work as Categoricals are not numeric data (even in the case that .categories is numeric). s = pd.Series(pd.Categorical([1, 2, 3, 4])) try: .....: np.sum(s) .....: except TypeError as e: .....: print(\"TypeError:\", str(e)) .....: TypeError: 'Categorical' with dtype category does not support reduction 'sum'", "prev_chunk_id": "chunk_535", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_537", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "dtype in apply#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "dtype in apply#", "content": "dtype in apply# pandas currently does not preserve the dtype in apply functions: If you apply along rows you get a Series of object dtype (same as getting a row -> getting one element will return a basic type) and applying along columns will also convert to object. NaN values are unaffected. You can use fillna to handle missing values before applying a function. df = pd.DataFrame( .....: { .....: \"a\": [1, 2, 3, 4], .....: \"b\": [\"a\", \"b\", \"c\", \"d\"], .....: \"cats\": pd.Categorical([1, 2, 3, 2]), .....: } .....: ) .....: df.apply(lambda row: type(row[\"cats\"]), axis=1) 0 <class 'int'> 1 <class 'int'> 2 <class 'int'> 3 <class 'int'> dtype: object df.apply(lambda col: col.dtype, axis=0) a int64 b object cats category dtype: object", "prev_chunk_id": "chunk_536", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_538", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Categorical index#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Categorical index#", "content": "Categorical index# CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. See the advanced indexing docs for a more detailed explanation. Setting the index will create a CategoricalIndex: cats = pd.Categorical([1, 2, 3, 4], categories=[4, 2, 3, 1]) strings = [\"a\", \"b\", \"c\", \"d\"] values = [4, 2, 3, 1] df = pd.DataFrame({\"strings\": strings, \"values\": values}, index=cats) df.index CategoricalIndex([1, 2, 3, 4], categories=[4, 2, 3, 1], ordered=False, dtype='category') # This now sorts by the categories order df.sort_index() strings values 4 d 1 2 b 2 3 c 3 1 a 4", "prev_chunk_id": "chunk_537", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_539", "url": "https://pandas.pydata.org/docs/user_guide/categorical.html", "title": "Side effects#", "page_title": "Categorical data — pandas 2.3.1 documentation", "breadcrumbs": "Side effects#", "content": "Side effects# Constructing a Series from a Categorical will not copy the input Categorical. This means that changes to the Series will in most cases change the original Categorical: cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10]) s = pd.Series(cat, name=\"cat\") cat [1, 2, 3, 10] Categories (5, int64): [1, 2, 3, 4, 10] s.iloc[0:2] = 10 cat [10, 10, 3, 10] Categories (5, int64): [1, 2, 3, 4, 10] Use copy=True to prevent such a behaviour or simply don’t reuse Categoricals: cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10]) s = pd.Series(cat, name=\"cat\", copy=True) cat [1, 2, 3, 10] Categories (5, int64): [1, 2, 3, 4, 10] s.iloc[0:2] = 10 cat [1, 2, 3, 10] Categories (5, int64): [1, 2, 3, 4, 10]", "prev_chunk_id": "chunk_538", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_540", "url": "https://pandas.pydata.org/docs/user_guide/integer_na.html", "title": "Nullable integer data type#", "page_title": "Nullable integer data type — pandas 2.3.1 documentation", "breadcrumbs": "Nullable integer data type#", "content": "Nullable integer data type# In Working with missing data, we saw that pandas primarily uses NaN to represent missing data. Because NaN is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_541", "url": "https://pandas.pydata.org/docs/user_guide/integer_na.html", "title": "Construction#", "page_title": "Nullable integer data type — pandas 2.3.1 documentation", "breadcrumbs": "Construction#", "content": "Construction# pandas can represent integer data with possibly missing values using arrays.IntegerArray. This is an extension type implemented within pandas. arr = pd.array([1, 2, None], dtype=pd.Int64Dtype()) arr <IntegerArray> [1, 2, <NA>] Length: 3, dtype: Int64 Or the string alias \"Int64\" (note the capital \"I\") to differentiate from NumPy’s 'int64' dtype: pd.array([1, 2, np.nan], dtype=\"Int64\") <IntegerArray> [1, 2, <NA>] Length: 3, dtype: Int64 All NA-like values are replaced with pandas.NA. pd.array([1, 2, np.nan, None, pd.NA], dtype=\"Int64\") <IntegerArray> [1, 2, <NA>, <NA>, <NA>] Length: 5, dtype: Int64 This array can be stored in a DataFrame or Series like any NumPy array. pd.Series(arr) 0 1 1 2 2 <NA> dtype: Int64 You can also pass the list-like object to the Series constructor with the dtype.", "prev_chunk_id": "chunk_540", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_542", "url": "https://pandas.pydata.org/docs/user_guide/integer_na.html", "title": "Operations#", "page_title": "Nullable integer data type — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed. s = pd.Series([1, 2, None], dtype=\"Int64\") # arithmetic s + 1 0 2 1 3 2 <NA> dtype: Int64 # comparison s == 1 0 True 1 False 2 <NA> dtype: boolean # slicing operation s.iloc[1:3] 1 2 2 <NA> dtype: Int64 # operate with other dtypes s + s.iloc[1:3].astype(\"Int8\") 0 <NA> 1 4 2 <NA> dtype: Int64 # coerce when needed s + 0.01 0 1.01 1 2.01 2 <NA> dtype: Float64 These dtypes can operate as part of a DataFrame. df = pd.DataFrame({\"A\": s, \"B\": [1, 1, 3], \"C\": list(\"aab\")}) df A B C 0 1 1 a 1 2 1 a 2 <NA> 3 b df.dtypes A Int64 B int64 C object dtype: object These dtypes can be merged, reshaped & casted. pd.concat([df[[\"A\"]], df[[\"B\", \"C\"]]], axis=1).dtypes A Int64 B int64 C object dtype: object df[\"A\"].astype(float) 0 1.0 1 2.0 2 NaN Name: A, dtype: float64 Reduction and groupby operations such as sum() work as well. df.sum(numeric_only=True) A 3 B 5 dtype: Int64 df.sum() A 3 B 5 C aab dtype: object df.groupby(\"B\").A.sum() B 1 3 3 0 Name: A, dtype: Int64", "prev_chunk_id": "chunk_541", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_543", "url": "https://pandas.pydata.org/docs/user_guide/integer_na.html", "title": "Scalar NA Value#", "page_title": "Nullable integer data type — pandas 2.3.1 documentation", "breadcrumbs": "Scalar NA Value#", "content": "Scalar NA Value# arrays.IntegerArray uses pandas.NA as its scalar missing value. Slicing a single element that’s missing will return pandas.NA a = pd.array([1, None], dtype=\"Int64\") a[1] <NA>", "prev_chunk_id": "chunk_542", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_544", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Cookbook#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Cookbook#", "content": "Cookbook# This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_545", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Idioms#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Idioms#", "content": "Idioms# These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: df = pd.DataFrame( ...: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ...: ) ...: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50", "prev_chunk_id": "chunk_544", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_546", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "if-then…#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "if-then…#", "content": "if-then…# An if-then on one column df.loc[df.AAA >= 5, \"BBB\"] = -1 df AAA BBB CCC 0 4 10 100 1 5 -1 50 2 6 -1 -30 3 7 -1 -50 An if-then with assignment to 2 columns: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555 df AAA BBB CCC 0 4 10 100 1 5 555 555 2 6 555 555 3 7 555 555 Add another line with different logic, to do the -else df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000 df AAA BBB CCC 0 4 2000 2000 1 5 555 555 2 6 555 555 3 7 555 555 Or use pandas where after you’ve set up a mask df_mask = pd.DataFrame( ...: {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2} ...: ) ...: df.where(df_mask, -1000) AAA BBB CCC 0 4 -1000 2000 1 5 -1000 -1000 2 6 -1000 555 3 7 -1000 -1000 if-then-else using NumPy’s where() df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\") df AAA BBB CCC logic 0 4 10 100 low 1 5 20 50 low 2 6 30 -30 high 3 7 40 -50 high", "prev_chunk_id": "chunk_545", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_547", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Splitting#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Splitting#", "content": "Splitting# Split a frame with a boolean criterion df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df[df.AAA <= 5] AAA BBB CCC 0 4 10 100 1 5 20 50 df[df.AAA > 5] AAA BBB CCC 2 6 30 -30 3 7 40 -50", "prev_chunk_id": "chunk_546", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_548", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Building criteria#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Building criteria#", "content": "Building criteria# Select with multi-column criteria df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 …and (without assignment returns a Series) df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"] 0 4 1 5 Name: AAA, dtype: int64 …or (without assignment returns a Series) df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"] 0 4 1 5 2 6 3 7 Name: AAA, dtype: int64 …or (with assignment modifies the DataFrame.) df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 999 df AAA BBB CCC 0 999 10 100 1 5 20 50 2 999 30 -30 3 999 40 -50 Select rows with data closest to certain value using argsort df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 aValue = 43.0 df.loc[(df.CCC - aValue).abs().argsort()] AAA BBB CCC 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 Dynamically reduce a list of criteria using a binary operators df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 Crit1 = df.AAA <= 5.5 Crit2 = df.BBB == 10.0 Crit3 = df.CCC > -40.0 One could hard code: AllCrit = Crit1 & Crit2 & Crit3 …Or it can be done with a list of dynamically built criteria import", "prev_chunk_id": "chunk_547", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_549", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Building criteria#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Building criteria#", "content": "functools CritList = [Crit1, Crit2, Crit3] AllCrit = functools.reduce(lambda x, y: x & y, CritList) df[AllCrit] AAA BBB CCC 0 4 10 100", "prev_chunk_id": "chunk_548", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_550", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Dataframes#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Dataframes#", "content": "Dataframes# The indexing docs. Using both row labels and value conditionals df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))] AAA BBB CCC 0 4 10 100 2 6 30 -30 Use loc for label-oriented slicing and iloc positional slicing GH 2904 df = pd.DataFrame( ....: {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}, ....: index=[\"foo\", \"bar\", \"boo\", \"kar\"], ....: ) ....: There are 2 explicit slicing methods, with a third general case - Positional-oriented (Python slicing style : exclusive of end) - Label-oriented (Non-Python slicing style : inclusive of end) - General (Either slicing style : depends on if the slice contains labels or positions) df.loc[\"bar\":\"kar\"] # Label AAA BBB CCC bar 5 20 50 boo 6 30 -30 kar 7 40 -50 # Generic df[0:3] AAA BBB CCC foo 4 10 100 bar 5 20 50 boo 6 30 -30 df[\"bar\":\"kar\"] AAA BBB CCC bar 5 20 50 boo 6 30 -30 kar 7 40 -50 Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4]) # Note index starts at 1. df2.iloc[1:3] # Position-oriented AAA BBB CCC 2 5 20 50 3 6 30 -30 df2.loc[1:3] # Label-oriented AAA BBB CCC 1 4 10 100 2 5 20 50 3 6 30 -30 Using inverse operator (~) to take the complement of a mask df = pd.DataFrame( ....: {\"AAA\": [4, 5,", "prev_chunk_id": "chunk_549", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_551", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Dataframes#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Dataframes#", "content": "6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ....: ) ....: df AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))] AAA BBB CCC 1 5 20 50 3 7 40 -50", "prev_chunk_id": "chunk_550", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_552", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "New columns#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "New columns#", "content": "New columns# Efficiently and dynamically creating new columns using DataFrame.map (previously named applymap) df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]}) df AAA BBB CCC 0 1 1 2 1 2 1 1 2 1 2 3 3 3 2 1 source_cols = df.columns # Or some subset would work too new_cols = [str(x) + \"_cat\" for x in source_cols] categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"} df[new_cols] = df[source_cols].map(categories.get) df AAA BBB CCC AAA_cat BBB_cat CCC_cat 0 1 1 2 Alpha Alpha Beta 1 2 1 1 Beta Alpha Alpha 2 1 2 3 Alpha Beta Charlie 3 3 2 1 Charlie Beta Alpha Keep other columns when using min() with groupby df = pd.DataFrame( ....: {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]} ....: ) ....: df AAA BBB 0 1 2 1 1 1 2 1 3 3 2 4 4 2 5 5 2 1 6 3 2 7 3 3 Method 1 : idxmin() to get the index of the minimums df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()] AAA BBB 1 1 1 5 2 1 6 3 2 Method 2 : sort then take first of each df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first() AAA BBB 0 1 1 1 2 1 2 3 2 Notice the same results, with the exception of the index.", "prev_chunk_id": "chunk_551", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_553", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Multiindexing#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Multiindexing#", "content": "Multiindexing# The multindexing docs. Creating a MultiIndex from a labeled frame df = pd.DataFrame( ....: { ....: \"row\": [0, 1, 2], ....: \"One_X\": [1.1, 1.1, 1.1], ....: \"One_Y\": [1.2, 1.2, 1.2], ....: \"Two_X\": [1.11, 1.11, 1.11], ....: \"Two_Y\": [1.22, 1.22, 1.22], ....: } ....: ) ....: df row One_X One_Y Two_X Two_Y 0 0 1.1 1.2 1.11 1.22 1 1 1.1 1.2 1.11 1.22 2 2 1.1 1.2 1.11 1.22 # As Labelled Index df = df.set_index(\"row\") df One_X One_Y Two_X Two_Y row 0 1.1 1.2 1.11 1.22 1 1.1 1.2 1.11 1.22 2 1.1 1.2 1.11 1.22 # With Hierarchical Columns df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns]) df One Two X Y X Y row 0 1.1 1.2 1.11 1.22 1 1.1 1.2 1.11 1.22 2 1.1 1.2 1.11 1.22 # Now stack & Reset df = df.stack(0, future_stack=True).reset_index(1) df level_1 X Y row 0 One 1.10 1.20 0 Two 1.11 1.22 1 One 1.10 1.20 1 Two 1.11 1.22 2 One 1.10 1.20 2 Two 1.11 1.22 # And fix the labels (Notice the label 'level_1' got added automatically) df.columns = [\"Sample\", \"All_X\", \"All_Y\"] df Sample All_X All_Y row 0 One 1.10 1.20 0 Two 1.11 1.22 1 One 1.10 1.20 1 Two 1.11 1.22 2 One 1.10 1.20 2 Two 1.11 1.22", "prev_chunk_id": "chunk_552", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_554", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Arithmetic#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Arithmetic#", "content": "Arithmetic# Performing arithmetic with a MultiIndex that needs broadcasting cols = pd.MultiIndex.from_tuples( ....: [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]] ....: ) ....: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols) df A B C O I O I O I n 0.469112 -0.282863 -1.509059 -1.135632 1.212112 -0.173215 m 0.119209 -1.044236 -0.861849 -2.104569 -0.494929 1.071804 df = df.div(df[\"C\"], level=1) df A B C O I O I O I n 0.387021 1.633022 -1.244983 6.556214 1.0 1.0 m -0.240860 -0.974279 1.741358 -1.963577 1.0 1.0", "prev_chunk_id": "chunk_553", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_555", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Slicing#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Slicing#", "content": "Slicing# Slicing a MultiIndex with xs coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")] index = pd.MultiIndex.from_tuples(coords) df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"]) df MyData AA one 11 six 22 BB one 33 two 44 six 55 To take the cross section of the 1st level and 1st axis the index: # Note : level and axis are optional, and default to zero df.xs(\"BB\", level=0, axis=0) MyData one 33 two 44 six 55 …and now the 2nd level of the 1st axis. df.xs(\"six\", level=1, axis=0) MyData AA 22 BB 55 Slicing a MultiIndex with xs, method #2 import itertools index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"])) headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"])) indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"]) cols = pd.MultiIndex.from_tuples(headr) # Notice these are un-named data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)] df = pd.DataFrame(data, indx, cols) df Exams Labs I II I II Student Course Ada Comp 70 71 72 73 Math 71 73 75 74 Sci 72 75 75 75 Quinn Comp 73 74 75 76 Math 74 76 78 77 Sci 75 78 78 78 Violet Comp 76 77 78 79 Math 77 79 81 80 Sci 78 81 81 81 All = slice(None) df.loc[\"Violet\"] Exams Labs I II I II Course Comp 76 77 78 79 Math 77 79 81 80 Sci 78 81 81 81 df.loc[(All, \"Math\"), All] Exams Labs I II I II Student Course Ada Math 71 73 75 74 Quinn Math 74 76 78 77 Violet Math 77 79 81 80 df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All] Exams Labs I II I II Student Course Ada Math 71 73 75 74 Quinn Math 74 76 78 77 df.loc[(All, \"Math\"), (\"Exams\")] I II", "prev_chunk_id": "chunk_554", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_556", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Slicing#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Slicing#", "content": "Student Course Ada Math 71 73 Quinn Math 74 76 Violet Math 77 79 df.loc[(All, \"Math\"), (All, \"II\")] Exams Labs II II Student Course Ada Math 73 74 Quinn Math 76 77 Violet Math 79 80 Setting portions of a MultiIndex with xs", "prev_chunk_id": "chunk_555", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_557", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Sorting#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Sorting#", "content": "Sorting# Sort by specific column or an ordered list of columns, with a MultiIndex df.sort_values(by=(\"Labs\", \"II\"), ascending=False) Exams Labs I II I II Student Course Violet Sci 78 81 81 81 Math 77 79 81 80 Comp 76 77 78 79 Quinn Sci 75 78 78 78 Math 74 76 78 77 Comp 73 74 75 76 Ada Sci 72 75 75 75 Math 71 73 75 74 Comp 70 71 72 73 Partial selection, the need for sortedness GH 2995", "prev_chunk_id": "chunk_556", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_558", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Levels#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Levels#", "content": "Levels# Prepending a level to a multiindex Flatten Hierarchical columns", "prev_chunk_id": "chunk_557", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_559", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Missing data#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Missing data#", "content": "Missing data# The missing data docs. Fill forward a reversed timeseries df = pd.DataFrame( .....: np.random.randn(6, 1), .....: index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"), .....: columns=list(\"A\"), .....: ) .....: df.loc[df.index[3], \"A\"] = np.nan df A 2013-08-01 0.721555 2013-08-02 -0.706771 2013-08-05 -1.039575 2013-08-06 NaN 2013-08-07 -0.424972 2013-08-08 0.567020 df.bfill() A 2013-08-01 0.721555 2013-08-02 -0.706771 2013-08-05 -1.039575 2013-08-06 -0.424972 2013-08-07 -0.424972 2013-08-08 0.567020 cumsum reset at NaN values", "prev_chunk_id": "chunk_558", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_560", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Replace#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Replace#", "content": "Replace# Using replace with backrefs", "prev_chunk_id": "chunk_559", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_561", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Grouping#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Grouping#", "content": "Grouping# The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns df = pd.DataFrame( .....: { .....: \"animal\": \"cat dog cat fish dog cat cat\".split(), .....: \"size\": list(\"SSMMMLL\"), .....: \"weight\": [8, 10, 11, 1, 20, 12, 12], .....: \"adult\": [False] * 5 + [True] * 2, .....: } .....: ) .....: df animal size weight adult 0 cat S 8 False 1 dog S 10 False 2 cat M 11 False 3 fish M 1 False 4 dog M 20 False 5 cat L 12 True 6 cat L 12 True # List the size of the animals with the highest weight. df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()], include_groups=False) animal cat L dog M fish M dtype: object Using get_group gb = df.groupby(\"animal\") gb.get_group(\"cat\") animal size weight adult 0 cat S 8 False 2 cat M 11 False 5 cat L 12 True 6 cat L 12 True Apply to different items in a group def GrowUp(x): .....: avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5) .....: avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25) .....: avg_weight += sum(x[x[\"size\"] == \"L\"].weight) .....: avg_weight /= len(x) .....: return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"]) .....: expected_df = gb.apply(GrowUp, include_groups=False) expected_df size weight adult animal cat L 12.4375 True dog L 20.0000 True fish L 1.2500 True Expanding apply S = pd.Series([i / 100.0 for i in range(1, 11)]) def cum_ret(x, y): .....: return x * (1 + y) .....: def red(x): .....: return functools.reduce(cum_ret, x, 1.0) .....: S.expanding().apply(red, raw=True) 0 1.010000 1 1.030200 2 1.061106 3 1.103550 4 1.158728 5 1.228251 6 1.314229 7 1.419367 8 1.547110 9 1.701821 dtype: float64 Replacing some values with mean of the rest of a group df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1,", "prev_chunk_id": "chunk_560", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_562", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Grouping#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Grouping#", "content": "2]}) gb = df.groupby(\"A\") def replace(g): .....: mask = g < 0 .....: return g.where(~mask, g[~mask].mean()) .....: gb.transform(replace) B 0 1 1 1 2 1 3 2 Sort groups by aggregated data df = pd.DataFrame( .....: { .....: \"code\": [\"foo\", \"bar\", \"baz\"] * 2, .....: \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62], .....: \"flag\": [False, True] * 3, .....: } .....: ) .....: code_groups = df.groupby(\"code\") agg_n_sort_order = code_groups[[\"data\"]].transform(\"sum\").sort_values(by=\"data\") sorted_df = df.loc[agg_n_sort_order.index] sorted_df code data flag 1 bar -0.21 True 4 bar -0.59 False 0 foo 0.16 False 3 foo 0.45 True 2 baz 0.33 False 5 baz 0.62 True Create multiple aggregated columns rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\") ts = pd.Series(data=list(range(10)), index=rng) def MyCust(x): .....: if len(x) > 2: .....: return x.iloc[1] * 1.234 .....: return pd.NaT .....: mhc = {\"Mean\": \"mean\", \"Max\": \"max\", \"Custom\": MyCust} ts.resample(\"5min\").apply(mhc) Mean Max Custom 2014-10-07 00:00:00 1.0 2 1.234 2014-10-07 00:05:00 3.5 4 NaT 2014-10-07 00:10:00 6.0 7 7.404 2014-10-07 00:15:00 8.5 9 NaT ts 2014-10-07 00:00:00 0 2014-10-07 00:02:00 1 2014-10-07 00:04:00 2 2014-10-07 00:06:00 3 2014-10-07 00:08:00 4 2014-10-07 00:10:00 5 2014-10-07 00:12:00 6 2014-10-07 00:14:00 7 2014-10-07 00:16:00 8 2014-10-07 00:18:00 9 Freq: 2min, dtype: int64 Create a value counts column and reassign back to the DataFrame df = pd.DataFrame( .....: {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]} .....: ) .....: df Color Value 0 Red 100 1 Red 150 2 Red 50 3 Blue 50 df[\"Counts\"] = df.groupby([\"Color\"]).transform(len) df Color Value Counts 0 Red 100 3 1 Red 150 3 2 Red 50 3 3 Blue 50 1 Shift groups of the values in a column based on the index df = pd.DataFrame( .....: {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]}, .....: index=[ .....: \"Last Gunfighter\", .....: \"Last Gunfighter\",", "prev_chunk_id": "chunk_561", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_563", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Grouping#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Grouping#", "content": ".....: \"Last Gunfighter\", .....: \"Paynter\", .....: \"Paynter\", .....: \"Paynter\", .....: ], .....: ) .....: df line_race beyer Last Gunfighter 10 99 Last Gunfighter 10 102 Last Gunfighter 8 103 Paynter 10 103 Paynter 10 88 Paynter 8 100 df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1) df line_race beyer beyer_shifted Last Gunfighter 10 99 NaN Last Gunfighter 10 102 99.0 Last Gunfighter 8 103 102.0 Paynter 10 103 NaN Paynter 10 88 103.0 Paynter 8 100 88.0 Select row with maximum value from each group df = pd.DataFrame( .....: { .....: \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"], .....: \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"], .....: \"no\": [1, 2, 1, 2, 1], .....: } .....: ).set_index([\"host\", \"service\"]) .....: mask = df.groupby(level=0).agg(\"idxmax\") df_count = df.loc[mask[\"no\"]].reset_index() df_count host service no 0 other web 2 1 that mail 1 2 this mail 2 Grouping like Python’s itertools.groupby df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"]) df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]} df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum() 0 0 1 1 2 0 3 1 4 2 5 3 6 0 7 1 8 2 Name: A, dtype: int64", "prev_chunk_id": "chunk_562", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_564", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Expanding data#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Expanding data#", "content": "Expanding data# Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval", "prev_chunk_id": "chunk_563", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_565", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Splitting#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Splitting#", "content": "Splitting# Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. df = pd.DataFrame( .....: data={ .....: \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"], .....: \"Data\": np.random.randn(9), .....: } .....: ) .....: dfs = list( .....: zip( .....: *df.groupby( .....: (1 * (df[\"Case\"] == \"B\")) .....: .cumsum() .....: .rolling(window=3, min_periods=1) .....: .median() .....: ) .....: ) .....: )[-1] .....: dfs[0] Case Data 0 A 0.276232 1 A -1.087401 2 A -0.673690 3 B 0.113648 dfs[1] Case Data 4 A -1.478427 5 A 0.524988 6 B 0.404705 dfs[2] Case Data 7 A 0.577046 8 A -1.715002", "prev_chunk_id": "chunk_564", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_566", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Pivot#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Pivot#", "content": "Pivot# The Pivot docs. Partial sums and subtotals df = pd.DataFrame( .....: data={ .....: \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"], .....: \"City\": [ .....: \"Toronto\", .....: \"Montreal\", .....: \"Vancouver\", .....: \"Calgary\", .....: \"Edmonton\", .....: \"Winnipeg\", .....: \"Windsor\", .....: ], .....: \"Sales\": [13, 6, 16, 8, 4, 3, 1], .....: } .....: ) .....: table = pd.pivot_table( .....: df, .....: values=[\"Sales\"], .....: index=[\"Province\"], .....: columns=[\"City\"], .....: aggfunc=\"sum\", .....: margins=True, .....: ) .....: table.stack(\"City\", future_stack=True) Sales Province City AL Calgary 8.0 Edmonton 4.0 Montreal NaN Toronto NaN Vancouver NaN ... ... All Toronto 13.0 Vancouver 16.0 Windsor 1.0 Winnipeg 3.0 All 51.0 [48 rows x 1 columns] Frequency table like plyr in R grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78] df = pd.DataFrame( .....: { .....: \"ID\": [\"x%d\" % r for r in range(10)], .....: \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"], .....: \"ExamYear\": [ .....: \"2007\", .....: \"2007\", .....: \"2007\", .....: \"2008\", .....: \"2008\", .....: \"2008\", .....: \"2008\", .....: \"2009\", .....: \"2009\", .....: \"2009\", .....: ], .....: \"Class\": [ .....: \"algebra\", .....: \"stats\", .....: \"bio\", .....: \"algebra\", .....: \"algebra\", .....: \"stats\", .....: \"stats\", .....: \"algebra\", .....: \"bio\", .....: \"bio\", .....: ], .....: \"Participated\": [ .....: \"yes\", .....: \"yes\", .....: \"yes\", .....: \"yes\", .....: \"no\", .....: \"yes\", .....: \"yes\", .....: \"yes\", .....: \"yes\", .....: \"yes\", .....: ], .....: \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades], .....: \"Employed\": [ .....: True, .....: True, .....: True, .....: False, .....: False, .....: False, .....: False, .....: True, .....: True, .....: False, .....: ], .....: \"Grade\": grades, .....: } .....: ) .....: df.groupby(\"ExamYear\").agg( .....: { .....: \"Participated\": lambda x: x.value_counts()[\"yes\"], .....: \"Passed\": lambda x: sum(x == \"yes\"), .....: \"Employed\": lambda x: sum(x), .....: \"Grade\": lambda x: sum(x) / len(x), .....:", "prev_chunk_id": "chunk_565", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_567", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Pivot#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Pivot#", "content": "} .....: ) .....: Participated Passed Employed Grade ExamYear 2007 3 2 3 74.000000 2008 3 3 0 68.500000 2009 3 2 2 60.666667 Plot pandas DataFrame with year over year data To create year and month cross tabulation: df = pd.DataFrame( .....: {\"value\": np.random.randn(36)}, .....: index=pd.date_range(\"2011-01-01\", freq=\"ME\", periods=36), .....: ) .....: pd.pivot_table( .....: df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\" .....: ) .....: 2011 2012 2013 1 -1.039268 -0.968914 2.565646 2 -0.370647 -1.294524 1.431256 3 -1.157892 0.413738 1.340309 4 -1.344312 0.276662 -1.170299 5 0.844885 -0.472035 -0.226169 6 1.075770 -0.013960 0.410835 7 -0.109050 -0.362543 0.813850 8 1.643563 -0.006154 0.132003 9 -1.469388 -0.923061 -0.827317 10 0.357021 0.895717 -0.076467 11 -0.674600 0.805244 -1.187678 12 -1.776904 -1.206412 1.130127", "prev_chunk_id": "chunk_566", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_568", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Apply#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Apply#", "content": "Apply# Rolling apply to organize - Turning embedded lists into a MultiIndex frame df = pd.DataFrame( .....: data={ .....: \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]], .....: \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]], .....: }, .....: index=[\"I\", \"II\", \"III\"], .....: ) .....: def SeriesFromSubList(aList): .....: return pd.Series(aList) .....: df_orgz = pd.concat( .....: {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()} .....: ) .....: df_orgz 0 1 2 3 I A 2 4 8 16.0 B a b c NaN II A 100 200 NaN NaN B jj kk NaN NaN III A 10 20.0 30.0 NaN B ccc NaN NaN NaN Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned df = pd.DataFrame( .....: data=np.random.randn(2000, 2) / 10000, .....: index=pd.date_range(\"2001-01-01\", periods=2000), .....: columns=[\"A\", \"B\"], .....: ) .....: df A B 2001-01-01 -0.000144 -0.000141 2001-01-02 0.000161 0.000102 2001-01-03 0.000057 0.000088 2001-01-04 -0.000221 0.000097 2001-01-05 -0.000201 -0.000041 ... ... ... 2006-06-19 0.000040 -0.000235 2006-06-20 -0.000123 -0.000021 2006-06-21 -0.000113 0.000114 2006-06-22 0.000136 0.000109 2006-06-23 0.000027 0.000030 [2000 rows x 2 columns] def gm(df, const): .....: v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const .....: return v.iloc[-1] .....: s = pd.Series( .....: { .....: df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5) .....: for i in range(len(df) - 50) .....: } .....: ) .....: s 2001-01-01 0.000930 2001-01-02 0.002615 2001-01-03 0.001281 2001-01-04 0.001117 2001-01-05 0.002772 ... 2006-04-30 0.003296 2006-05-01 0.002629 2006-05-02 0.002081 2006-05-03 0.004247 2006-05-04 0.003928 Length: 1950, dtype: float64 Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) rng = pd.date_range(start=\"2014-01-01\", periods=100) df = pd.DataFrame( .....: { .....: \"Open\": np.random.randn(len(rng)), .....: \"Close\": np.random.randn(len(rng)), .....: \"Volume\": np.random.randint(100,", "prev_chunk_id": "chunk_567", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_569", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Apply#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Apply#", "content": "2000, len(rng)), .....: }, .....: index=rng, .....: ) .....: df Open Close Volume 2014-01-01 -1.611353 -0.492885 1219 2014-01-02 -3.000951 0.445794 1054 2014-01-03 -0.138359 -0.076081 1381 2014-01-04 0.301568 1.198259 1253 2014-01-05 0.276381 -0.669831 1728 ... ... ... ... 2014-04-06 -0.040338 0.937843 1188 2014-04-07 0.359661 -0.285908 1864 2014-04-08 0.060978 1.714814 941 2014-04-09 1.759055 -0.455942 1065 2014-04-10 0.138185 -1.147008 1453 [100 rows x 3 columns] def vwap(bars): .....: return (bars.Close * bars.Volume).sum() / bars.Volume.sum() .....: window = 5 s = pd.concat( .....: [ .....: (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]])) .....: for i in range(len(df) - window) .....: ] .....: ) .....: s.round(2) 2014-01-06 0.02 2014-01-07 0.11 2014-01-08 0.10 2014-01-09 0.07 2014-01-10 -0.29 ... 2014-04-06 -0.63 2014-04-07 -0.02 2014-04-08 -0.03 2014-04-09 0.34 2014-04-10 0.29 Length: 95, dtype: float64", "prev_chunk_id": "chunk_568", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_570", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Timeseries#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Timeseries#", "content": "Timeseries# Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex dates = pd.date_range(\"2000-01-01\", periods=5) dates.to_period(freq=\"M\").to_timestamp() DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_569", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_571", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Resampling#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Resampling#", "content": "Resampling# The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH 3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby", "prev_chunk_id": "chunk_570", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_572", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Merge#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Merge#", "content": "Merge# The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) rng = pd.date_range(\"2000-01-01\", periods=6) df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"]) df2 = df1.copy() Depending on df construction, ignore_index may be needed df = pd.concat([df1, df2], ignore_index=True) df A B C 0 -0.870117 -0.479265 -0.790855 1 0.144817 1.726395 -0.464535 2 -0.821906 1.597605 0.187307 3 -0.128342 -1.511638 -0.289858 4 0.399194 -1.430030 -0.639760 5 1.115116 -2.012600 1.810662 6 -0.870117 -0.479265 -0.790855 7 0.144817 1.726395 -0.464535 8 -0.821906 1.597605 0.187307 9 -0.128342 -1.511638 -0.289858 10 0.399194 -1.430030 -0.639760 11 1.115116 -2.012600 1.810662 Self Join of a DataFrame GH 2996 df = pd.DataFrame( .....: data={ .....: \"Area\": [\"A\"] * 5 + [\"C\"] * 2, .....: \"Bins\": [110] * 2 + [160] * 3 + [40] * 2, .....: \"Test_0\": [0, 1, 0, 1, 2, 0, 1], .....: \"Data\": np.random.randn(7), .....: } .....: ) .....: df Area Bins Test_0 Data 0 A 110 0 -0.433937 1 A 110 1 -0.160552 2 A 160 0 0.744434 3 A 160 1 1.754213 4 A 160 2 0.000850 5 C 40 0 0.342243 6 C 40 1 1.070599 df[\"Test_1\"] = df[\"Test_0\"] - 1 pd.merge( .....: df, .....: df, .....: left_on=[\"Bins\", \"Area\", \"Test_0\"], .....: right_on=[\"Bins\", \"Area\", \"Test_1\"], .....: suffixes=(\"_L\", \"_R\"), .....: ) .....: Area Bins Test_0_L Data_L Test_1_L Test_0_R Data_R Test_1_R 0 A 110 0 -0.433937 -1 1 -0.160552 0 1 A 160 0 0.744434 -1 1 1.754213 0 2 A 160 1 1.754213 0 2 0.000850 1 3 C 40 0 0.342243 -1 1 1.070599 0 How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range", "prev_chunk_id": "chunk_571", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_573", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Plotting#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Plotting#", "content": "Plotting# The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable df = pd.DataFrame( .....: { .....: \"stratifying_var\": np.random.uniform(0, 100, 20), .....: \"price\": np.random.normal(100, 5, 20), .....: } .....: ) .....: df[\"quartiles\"] = pd.qcut( .....: df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"] .....: ) .....: df.boxplot(column=\"price\", by=\"quartiles\") <Axes: title={'center': 'price'}, xlabel='quartiles'>", "prev_chunk_id": "chunk_572", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_574", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Data in/out#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Data in/out#", "content": "Data in/out# Performance comparison of SQL vs HDF5", "prev_chunk_id": "chunk_573", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_575", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "CSV#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "CSV#", "content": "CSV# The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH 2886 Write a multi-row index CSV without writing duplicates", "prev_chunk_id": "chunk_574", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_576", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Reading multiple files to create a single DataFrame#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Reading multiple files to create a single DataFrame#", "content": "Reading multiple files to create a single DataFrame# The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): for i in range(3): .....: data = pd.DataFrame(np.random.randn(10, 4)) .....: data.to_csv(\"file_{}.csv\".format(i)) .....: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"] result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True) You can use the same approach to read all files matching a pattern. Here is an example using glob: import glob import os files = glob.glob(\"file_*.csv\") result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True) Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.", "prev_chunk_id": "chunk_575", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_577", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Parsing date components in multi-columns#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Parsing date components in multi-columns#", "content": "Parsing date components in multi-columns# Parsing date components in multi-columns is faster with a format i = pd.date_range(\"20000101\", periods=10000) df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day}) df.head() year month day 0 2000 1 1 1 2000 1 2 2 2000 1 3 3 2000 1 4 4 2000 1 5 %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d') .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1) .....: ds.head() .....: %timeit pd.to_datetime(ds) .....: 2.42 ms +- 35.9 us per loop (mean +- std. dev. of 7 runs, 100 loops each) 1.08 ms +- 6.03 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each)", "prev_chunk_id": "chunk_576", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_578", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Skip row between header and data#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Skip row between header and data#", "content": "Skip row between header and data# data = \"\"\";;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: date;Param1;Param2;Param4;Param5 .....: ;m²;°C;m²;m .....: ;;;; .....: 01.01.1990 00:00;1;1;2;3 .....: 01.01.1990 01:00;5;3;4;5 .....: 01.01.1990 02:00;9;5;6;7 .....: 01.01.1990 03:00;13;7;8;9 .....: 01.01.1990 04:00;17;9;10;11 .....: 01.01.1990 05:00;21;11;12;13 .....: \"\"\" .....:", "prev_chunk_id": "chunk_577", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_579", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Option 1: pass rows explicitly to skip rows#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Option 1: pass rows explicitly to skip rows#", "content": "Option 1: pass rows explicitly to skip rows# from io import StringIO pd.read_csv( .....: StringIO(data), .....: sep=\";\", .....: skiprows=[11, 12], .....: index_col=0, .....: parse_dates=True, .....: header=10, .....: ) .....: Param1 Param2 Param4 Param5 date 1990-01-01 00:00:00 1 1 2 3 1990-01-01 01:00:00 5 3 4 5 1990-01-01 02:00:00 9 5 6 7 1990-01-01 03:00:00 13 7 8 9 1990-01-01 04:00:00 17 9 10 11 1990-01-01 05:00:00 21 11 12 13", "prev_chunk_id": "chunk_578", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_580", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Option 2: read column names and then data#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Option 2: read column names and then data#", "content": "Option 2: read column names and then data# pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object') columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns pd.read_csv( .....: StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns .....: ) .....: Param1 Param2 Param4 Param5 date 1990-01-01 00:00:00 1 1 2 3 1990-01-01 01:00:00 5 3 4 5 1990-01-01 02:00:00 9 5 6 7 1990-01-01 03:00:00 13 7 8 9 1990-01-01 04:00:00 17 9 10 11 1990-01-01 05:00:00 21 11 12 13", "prev_chunk_id": "chunk_579", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_581", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "SQL#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "SQL#", "content": "SQL# The SQL docs Reading from databases with SQL", "prev_chunk_id": "chunk_580", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_582", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Excel#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Excel#", "content": "Excel# The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH 19842#issuecomment-892150745", "prev_chunk_id": "chunk_581", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_583", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "HTML#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "HTML#", "content": "HTML# Reading HTML tables from a server that cannot handle the default request header", "prev_chunk_id": "chunk_582", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_584", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "HDFStore#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "HDFStore#", "content": "HDFStore# The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH 3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node df = pd.DataFrame(np.random.randn(8, 3)) store = pd.HDFStore(\"test.h5\") store.put(\"df\", df) # you can store an arbitrary Python object via pickle store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10} store.get_storer(\"df\").attrs.my_attribute {'A': 10} You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\") df = pd.DataFrame(np.random.randn(8, 3)) store[\"test\"] = df # only after closing the store, data is written to disk: store.close()", "prev_chunk_id": "chunk_583", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_585", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Binary files#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Binary files#", "content": "Binary files# pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, #include <stdio.h> #include <stdint.h> typedef struct _Data { int32_t count; double avg; float scale; } Data; int main(int argc, const char *argv[]) { size_t n = 10; Data d[n]; for (int i = 0; i < n; ++i) { d[i].count = i; d[i].avg = i + 1.0; d[i].scale = (float) i + 2.0f; } FILE *file = fopen(\"binary.dat\", \"wb\"); fwrite(&d, sizeof(Data), n, file); fclose(file); return 0; } the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: names = \"count\", \"avg\", \"scale\" # note that the offsets are larger than the size of the type because of # struct padding offsets = 0, 8, 16 formats = \"i4\", \"f8\", \"f4\" dt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True) df = pd.DataFrame(np.fromfile(\"binary.dat\", dt))", "prev_chunk_id": "chunk_584", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_586", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Computation#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Computation#", "content": "Computation# Numerical integration (sample-based) of a time series", "prev_chunk_id": "chunk_585", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_587", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Correlation#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Correlation#", "content": "Correlation# Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: df = pd.DataFrame(np.random.random(size=(100, 5))) corr_mat = df.corr() mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1) corr_mat.where(mask) 0 1 2 3 4 0 NaN NaN NaN NaN NaN 1 -0.079861 NaN NaN NaN NaN 2 -0.236573 0.183801 NaN NaN NaN 3 -0.013795 -0.051975 0.037235 NaN NaN 4 -0.031974 0.118342 -0.073499 -0.02063 NaN The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. def distcorr(x, y): .....: n = len(x) .....: a = np.zeros(shape=(n, n)) .....: b = np.zeros(shape=(n, n)) .....: for i in range(n): .....: for j in range(i + 1, n): .....: a[i, j] = abs(x[i] - x[j]) .....: b[i, j] = abs(y[i] - y[j]) .....: a += a.T .....: b += b.T .....: a_bar = np.vstack([np.nanmean(a, axis=0)] * n) .....: b_bar = np.vstack([np.nanmean(b, axis=0)] * n) .....: A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean()) .....: B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean()) .....: cov_ab = np.sqrt(np.nansum(A * B)) / n .....: std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n) .....: std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n) .....: return cov_ab / std_a / std_b .....: df = pd.DataFrame(np.random.normal(size=(100, 3))) df.corr(method=distcorr) 0 1 2 0 1.000000 0.197613 0.216328 1 0.197613 1.000000 0.208749 2 0.216328 0.208749 1.000000", "prev_chunk_id": "chunk_586", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_588", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Timedeltas#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Timedeltas#", "content": "Timedeltas# The Timedeltas docs. Using timedeltas import datetime s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\")) s - s.max() 0 -2 days 1 -1 days 2 0 days dtype: timedelta64[ns] s.max() - s 0 2 days 1 1 days 2 0 days dtype: timedelta64[ns] s - datetime.datetime(2011, 1, 1, 3, 5) 0 364 days 20:55:00 1 365 days 20:55:00 2 366 days 20:55:00 dtype: timedelta64[ns] s + datetime.timedelta(minutes=5) 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] datetime.datetime(2011, 1, 1, 3, 5) - s 0 -365 days +03:05:00 1 -366 days +03:05:00 2 -367 days +03:05:00 dtype: timedelta64[ns] datetime.timedelta(minutes=5) + s 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] Adding and subtracting deltas and dates deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)]) df = pd.DataFrame({\"A\": s, \"B\": deltas}) df A B 0 2012-01-01 0 days 1 2012-01-02 1 days 2 2012-01-03 2 days df[\"New Dates\"] = df[\"A\"] + df[\"B\"] df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"] df A B New Dates Delta 0 2012-01-01 0 days 2012-01-01 0 days 1 2012-01-02 1 days 2012-01-03 -1 days 2 2012-01-03 2 days 2012-01-05 -2 days df.dtypes A datetime64[ns] B timedelta64[ns] New Dates datetime64[ns] Delta timedelta64[ns] dtype: object Another example Values can be set to NaT using np.nan, similar to datetime y = s - s.shift() y 0 NaT 1 1 days 2 1 days dtype: timedelta64[ns] y[1] = np.nan y 0 NaT 1 NaT 2 1 days dtype: timedelta64[ns]", "prev_chunk_id": "chunk_587", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_589", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Creating example data#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Creating example data#", "content": "Creating example data# To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: def expand_grid(data_dict): .....: rows = itertools.product(*data_dict.values()) .....: return pd.DataFrame.from_records(rows, columns=data_dict.keys()) .....: df = expand_grid( .....: {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]} .....: ) .....: df height weight sex 0 60 100 Male 1 60 100 Female 2 60 140 Male 3 60 140 Female 4 60 180 Male 5 60 180 Female 6 70 100 Male 7 70 100 Female 8 70 140 Male 9 70 140 Female 10 70 180 Male 11 70 180 Female", "prev_chunk_id": "chunk_588", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_590", "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html", "title": "Constant series#", "page_title": "Cookbook — pandas 2.3.1 documentation", "breadcrumbs": "Constant series#", "content": "Constant series# To assess if a series has a constant value, we can check if series.nunique() <= 1. However, a more performant approach, that does not count all unique values first, is: v = s.to_numpy() is_constant = v.shape[0] == 0 or (s[0] == s).all() This approach assumes that the series does not contain missing values. For the case that we would drop NA values, we can simply remove those values first: v = s.dropna().to_numpy() is_constant = v.shape[0] == 0 or (s[0] == s).all() If missing values are considered distinct from any other value, then one could use: v = s.to_numpy() is_constant = v.shape[0] == 0 or (s[0] == s).all() or not pd.notna(v).any() (Note that this example does not disambiguate between np.nan, pd.NA and None)", "prev_chunk_id": "chunk_589", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_591", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "DataFrame memory usage#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame memory usage#", "content": "DataFrame memory usage# The memory usage of a DataFrame (including the index) is shown when calling the info(). A configuration option, display.memory_usage (see the list of options), specifies if the DataFrame memory usage will be displayed when invoking the info() method. For example, the memory usage of the DataFrame below is shown when calling info(): dtypes = [ ...: \"int64\", ...: \"float64\", ...: \"datetime64[ns]\", ...: \"timedelta64[ns]\", ...: \"complex128\", ...: \"object\", ...: \"bool\", ...: ] ...: n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\"categorical\"] = df[\"object\"].astype(\"category\") df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 5000 entries, 0 to 4999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int64 5000 non-null int64 1 float64 5000 non-null float64 2 datetime64[ns] 5000 non-null datetime64[ns] 3 timedelta64[ns] 5000 non-null timedelta64[ns] 4 complex128 5000 non-null complex128 5 object 5000 non-null object 6 bool 5000 non-null bool 7 categorical 5000 non-null category dtypes: bool(1), category(1), complex128(1), datetime64[ns](1), float64(1), int64(1), object(1), timedelta64[ns](1) memory usage: 288.2+ KB The + symbol indicates that the true memory usage could be higher, because pandas does not count the memory used by values in columns with dtype=object. Passing memory_usage='deep' will enable a more accurate memory usage report, accounting for the full usage of the contained objects. This is optional as it can be expensive to do this deeper introspection. df.info(memory_usage=\"deep\") <class 'pandas.core.frame.DataFrame'> RangeIndex: 5000 entries, 0 to 4999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int64 5000 non-null int64 1 float64 5000 non-null float64 2 datetime64[ns] 5000 non-null datetime64[ns] 3 timedelta64[ns] 5000 non-null timedelta64[ns] 4 complex128 5000 non-null complex128 5 object 5000 non-null object 6 bool 5000 non-null bool 7 categorical 5000 non-null category dtypes: bool(1), category(1), complex128(1), datetime64[ns](1), float64(1), int64(1), object(1), timedelta64[ns](1) memory", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_592", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "DataFrame memory usage#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame memory usage#", "content": "usage: 424.7 KB By default the display option is set to True but can be explicitly overridden by passing the memory_usage argument when invoking info(). The memory usage of each column can be found by calling the memory_usage() method. This returns a Series with an index represented by column names and memory usage of each column shown in bytes. For the DataFrame above, the memory usage of each column and the total memory usage can be found with the memory_usage() method: df.memory_usage() Index 128 int64 40000 float64 40000 datetime64[ns] 40000 timedelta64[ns] 40000 complex128 80000 object 40000 bool 5000 categorical 9968 dtype: int64 # total memory usage of dataframe df.memory_usage().sum() 295096 By default the memory usage of the DataFrame index is shown in the returned Series, the memory usage of the index can be suppressed by passing the index=False argument: df.memory_usage(index=False) int64 40000 float64 40000 datetime64[ns] 40000 timedelta64[ns] 40000 complex128 80000 object 40000 bool 5000 categorical 9968 dtype: int64 The memory usage displayed by the info() method utilizes the memory_usage() method to determine the memory usage of a DataFrame while also formatting the output in human-readable units (base-2 representation; i.e. 1KB = 1024 bytes). See also Categorical Memory Usage.", "prev_chunk_id": "chunk_591", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_593", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Using if/truth statements with pandas#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Using if/truth statements with pandas#", "content": "Using if/truth statements with pandas# pandas follows the NumPy convention of raising an error when you try to convert something to a bool. This happens in an if-statement or when using the boolean operations: and, or, and not. It is not clear what the result of the following code should be: >>> if pd.Series([False, True, False]): ... pass Should it be True because it’s not zero-length, or False because there are False values? It is unclear, so instead, pandas raises a ValueError: if pd.Series([False, True, False]): ....: print(\"I was true\") ....: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-11-5c782b38cd2f> in ?() ----> 1 if pd.Series([False, True, False]): 2 print(\"I was true\") ~/work/pandas/pandas/pandas/core/generic.py in ?(self) 1575 @final 1576 def __nonzero__(self) -> NoReturn: -> 1577 raise ValueError( 1578 f\"The truth value of a {type(self).__name__} is ambiguous. \" 1579 \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\" 1580 ) ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). You need to explicitly choose what you want to do with the DataFrame, e.g. use any(), all() or empty(). Alternatively, you might want to compare if the pandas object is None: if pd.Series([False, True, False]) is not None: ....: print(\"I was not None\") ....: I was not None Below is how to check if any of the values are True: if pd.Series([False, True, False]).any(): ....: print(\"I am any\") ....: I am any", "prev_chunk_id": "chunk_592", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_594", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Bitwise boolean#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Bitwise boolean#", "content": "Bitwise boolean# Bitwise boolean operators like == and != return a boolean Series which performs an element-wise comparison when compared to a scalar. s = pd.Series(range(5)) s == 4 0 False 1 False 2 False 3 False 4 True dtype: bool See boolean comparisons for more examples.", "prev_chunk_id": "chunk_593", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_595", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Using the in operator#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Using the in operator#", "content": "Using the in operator# Using the Python in operator on a Series tests for membership in the index, not membership among the values. s = pd.Series(range(5), index=list(\"abcde\")) 2 in s False 'b' in s True If this behavior is surprising, keep in mind that using in on a Python dictionary tests keys, not values, and Series are dict-like. To test for membership in the values, use the method isin(): s.isin([2]) a False b False c True d False e False dtype: bool s.isin([2]).any() True For DataFrame, likewise, in applies to the column axis, testing for membership in the list of column names.", "prev_chunk_id": "chunk_594", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_596", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Mutating with User Defined Function (UDF) methods#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Mutating with User Defined Function (UDF) methods#", "content": "Mutating with User Defined Function (UDF) methods# This section applies to pandas methods that take a UDF. In particular, the methods DataFrame.apply(), DataFrame.aggregate(), DataFrame.transform(), and DataFrame.filter(). It is a general rule in programming that one should not mutate a container while it is being iterated over. Mutation will invalidate the iterator, causing unexpected behavior. Consider the example: values = [0, 1, 2, 3, 4, 5] n_removed = 0 for k, value in enumerate(values): ....: idx = k - n_removed ....: if value % 2 == 1: ....: del values[idx] ....: n_removed += 1 ....: else: ....: values[idx] = value + 1 ....: values [1, 4, 5] One probably would have expected that the result would be [1, 3, 5]. When using a pandas method that takes a UDF, internally pandas is often iterating over the DataFrame or other pandas object. Therefore, if the UDF mutates (changes) the DataFrame, unexpected behavior can arise. Here is a similar example with DataFrame.apply(): def f(s): ....: s.pop(\"a\") ....: return s ....: df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) df.apply(f, axis=\"columns\") --------------------------------------------------------------------------- KeyError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key) 3811 try: -> 3812 return self._engine.get_loc(casted_key) 3813 except KeyError as err: File ~/work/pandas/pandas/pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc() File ~/work/pandas/pandas/pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc() File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item() File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: 'a' The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) Cell In[27], line 1 ----> 1 df.apply(f, axis=\"columns\") File ~/work/pandas/pandas/pandas/core/frame.py:10381, in DataFrame.apply(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs) 10367 from pandas.core.apply import frame_apply 10369 op = frame_apply( 10370 self, 10371 func=func, (...) 10379 kwargs=kwargs, 10380 ) > 10381 return op.apply().__finalize__(self, method=\"apply\") File ~/work/pandas/pandas/pandas/core/apply.py:916, in FrameApply.apply(self) 913 elif self.raw: 914 return self.apply_raw(engine=self.engine, engine_kwargs=self.engine_kwargs) --> 916 return self.apply_standard() File ~/work/pandas/pandas/pandas/core/apply.py:1063, in FrameApply.apply_standard(self)", "prev_chunk_id": "chunk_595", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_597", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Mutating with User Defined Function (UDF) methods#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Mutating with User Defined Function (UDF) methods#", "content": "1061 def apply_standard(self): 1062 if self.engine == \"python\": -> 1063 results, res_index = self.apply_series_generator() 1064 else: 1065 results, res_index = self.apply_series_numba() File ~/work/pandas/pandas/pandas/core/apply.py:1081, in FrameApply.apply_series_generator(self) 1078 with option_context(\"mode.chained_assignment\", None): 1079 for i, v in enumerate(series_gen): 1080 # ignore SettingWithCopy here in case the user mutates -> 1081 results[i] = self.func(v, *self.args, **self.kwargs) 1082 if isinstance(results[i], ABCSeries): 1083 # If we have a view on v, we need to make a copy because 1084 # series_generator will swap out the underlying data 1085 results[i] = results[i].copy(deep=False) Cell In[25], line 2, in f(s) 1 def f(s): ----> 2 s.pop(\"a\") 3 return s File ~/work/pandas/pandas/pandas/core/series.py:5402, in Series.pop(self, item) 5377 def pop(self, item: Hashable) -> Any: 5378 \"\"\" 5379 Return item and drops from series. Raise KeyError if not found. 5380 (...) 5400 dtype: int64 5401 \"\"\" -> 5402 return super().pop(item=item) File ~/work/pandas/pandas/pandas/core/generic.py:947, in NDFrame.pop(self, item) 946 def pop(self, item: Hashable) -> Series | Any: --> 947 result = self[item] 948 del self[item] 950 return result File ~/work/pandas/pandas/pandas/core/series.py:1130, in Series.__getitem__(self, key) 1127 return self._values[key] 1129 elif key_is_scalar: -> 1130 return self._get_value(key) 1132 # Convert generator to list before going through hashable part 1133 # (We will iterate through the generator there to check for slices) 1134 if is_iterator(key): File ~/work/pandas/pandas/pandas/core/series.py:1246, in Series._get_value(self, label, takeable) 1243 return self._values[label] 1245 # Similar to Index.get_value, but we do not fall back to positional -> 1246 loc = self.index.get_loc(label) 1248 if is_integer(loc): 1249 return self._values[loc] File ~/work/pandas/pandas/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key) 3814 if isinstance(casted_key, slice) or ( 3815 isinstance(casted_key, abc.Iterable) 3816 and any(isinstance(x, slice) for x in casted_key) 3817 ): 3818 raise InvalidIndexError(key) -> 3819 raise KeyError(key) from err 3820 except TypeError: 3821 # If we have a listlike key, _check_indexing_error will raise 3822 # InvalidIndexError. Otherwise we fall through and re-raise 3823 # the TypeError. 3824 self._check_indexing_error(key) KeyError:", "prev_chunk_id": "chunk_596", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_598", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Mutating with User Defined Function (UDF) methods#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Mutating with User Defined Function (UDF) methods#", "content": "'a' To resolve this issue, one can make a copy so that the mutation does not apply to the container being iterated over. values = [0, 1, 2, 3, 4, 5] n_removed = 0 for k, value in enumerate(values.copy()): ....: idx = k - n_removed ....: if value % 2 == 1: ....: del values[idx] ....: n_removed += 1 ....: else: ....: values[idx] = value + 1 ....: values [1, 3, 5] def f(s): ....: s = s.copy() ....: s.pop(\"a\") ....: return s ....: df = pd.DataFrame({\"a\": [1, 2, 3], 'b': [4, 5, 6]}) df.apply(f, axis=\"columns\") b 0 4 1 5 2 6", "prev_chunk_id": "chunk_597", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_599", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "np.nan as the NA representation for NumPy types#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "np.nan as the NA representation for NumPy types#", "content": "np.nan as the NA representation for NumPy types# For lack of NA (missing) support from the ground up in NumPy and Python in general, NA could have been represented with: - Amasked arraysolution: an array of data and an array of boolean values indicating whether a value is there or is missing. - Using a special sentinel value, bit pattern, or set of sentinel values to denoteNAacross the dtypes. The special value np.nan (Not-A-Number) was chosen as the NA value for NumPy types, and there are API functions like DataFrame.isna() and DataFrame.notna() which can be used across the dtypes to detect NA values. However, this choice has a downside of coercing missing integer data as float types as shown in Support for integer NA.", "prev_chunk_id": "chunk_598", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_600", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "NA type promotions for NumPy types#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "NA type promotions for NumPy types#", "content": "NA type promotions for NumPy types# When introducing NAs into an existing Series or DataFrame via reindex() or some other means, boolean and integer types will be promoted to a different dtype in order to store the NAs. The promotions are summarized in this table: Typeclass | Promotion dtype for storing NAs floating | no change object | no change integer | cast to float64 boolean | cast to object", "prev_chunk_id": "chunk_599", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_601", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Support for integer NA#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Support for integer NA#", "content": "Support for integer NA# In the absence of high performance NA support being built into NumPy from the ground up, the primary casualty is the ability to represent NAs in integer arrays. For example: s = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\")) s a 1 b 2 c 3 d 4 e 5 dtype: int64 s.dtype dtype('int64') s2 = s.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"]) s2 a 1.0 b 2.0 c 3.0 f NaN u NaN dtype: float64 s2.dtype dtype('float64') This trade-off is made largely for memory and performance reasons, and also so that the resulting Series continues to be “numeric”. If you need to represent integers with possibly missing values, use one of the nullable-integer extension dtypes provided by pandas or pyarrow - Int8Dtype - Int16Dtype - Int32Dtype - Int64Dtype - ArrowDtype s_int = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"), dtype=pd.Int64Dtype()) s_int a 1 b 2 c 3 d 4 e 5 dtype: Int64 s_int.dtype Int64Dtype() s2_int = s_int.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"]) s2_int a 1 b 2 c 3 f <NA> u <NA> dtype: Int64 s2_int.dtype Int64Dtype() s_int_pa = pd.Series([1, 2, None], dtype=\"int64[pyarrow]\") s_int_pa 0 1 1 2 2 <NA> dtype: int64[pyarrow] See Nullable integer data type and PyArrow Functionality for more.", "prev_chunk_id": "chunk_600", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_602", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Why not make NumPy like R?#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Why not make NumPy like R?#", "content": "Why not make NumPy like R?# Many people have suggested that NumPy should simply emulate the NA support present in the more domain-specific statistical programming language R. Part of the reason is the NumPy type hierarchy. The R language, by contrast, only has a handful of built-in data types: integer, numeric (floating-point), character, and boolean. NA types are implemented by reserving special bit patterns for each type to be used as the missing value. While doing this with the full NumPy type hierarchy would be possible, it would be a more substantial trade-off (especially for the 8- and 16-bit data types) and implementation undertaking. However, R NA semantics are now available by using masked NumPy types such as Int64Dtype or PyArrow types (ArrowDtype).", "prev_chunk_id": "chunk_601", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_603", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Differences with NumPy#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Differences with NumPy#", "content": "Differences with NumPy# For Series and DataFrame objects, var() normalizes by N-1 to produce unbiased estimates of the population variance, while NumPy’s numpy.var() normalizes by N, which measures the variance of the sample. Note that cov() normalizes by N-1 in both pandas and NumPy.", "prev_chunk_id": "chunk_602", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_604", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Thread-safety#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Thread-safety#", "content": "Thread-safety# pandas is not 100% thread safe. The known issues relate to the copy() method. If you are doing a lot of copying of DataFrame objects shared among threads, we recommend holding locks inside the threads where the data copying occurs. See this link for more information.", "prev_chunk_id": "chunk_603", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_605", "url": "https://pandas.pydata.org/docs/user_guide/gotchas.html", "title": "Byte-ordering issues#", "page_title": "Frequently Asked Questions (FAQ) — pandas 2.3.1 documentation", "breadcrumbs": "Byte-ordering issues#", "content": "Byte-ordering issues# Occasionally you may have to deal with data that were created on a machine with a different byte order than the one on which you are running Python. A common symptom of this issue is an error like: Traceback ... ValueError: Big-endian buffer not supported on little-endian compiler To deal with this issue you should convert the underlying NumPy array to the native system byte order before passing it to Series or DataFrame constructors using something similar to the following: x = np.array(list(range(10)), \">i4\") # big endian newx = x.byteswap().view(x.dtype.newbyteorder()) # force native byteorder s = pd.Series(newx) See the NumPy documentation on byte order for more details.", "prev_chunk_id": "chunk_604", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_606", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Migration guide for the new string data type (pandas 3.0)#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Migration guide for the new string data type (pandas 3.0)#", "content": "Migration guide for the new string data type (pandas 3.0)# The upcoming pandas 3.0 release introduces a new, default string data type. This will most likely cause some work when upgrading to pandas 3.0, and this page provides an overview of the issues you might run into and gives guidance on how to address them. This new dtype is already available in the pandas 2.3 release, and you can enable it with: pd.options.future.infer_string = True This allows you to test your code before the final 3.0 release.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_607", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Background#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Background#", "content": "Background# Historically, pandas has always used the NumPy object dtype as the default to store text data. This has two primary drawbacks. First, object dtype is not specific to strings: any Python object can be stored in an object-dtype array, not just strings, and seeing object as the dtype for a column with strings is confusing for users. Second, this is not always very efficient (both performance wise and for memory usage). Since pandas 1.0, an opt-in string data type has been available, but this has not yet been made the default, and uses the pd.NA scalar to represent missing values. Pandas 3.0 changes the default dtype for strings to a new string data type, a variant of the existing optional string data type but using NaN as the missing value indicator, to be consistent with the other default data types. To improve performance, the new string data type will use the pyarrow package by default, if installed (and otherwise it uses object dtype under the hood as a fallback). See PDEP-14: Dedicated string data type for pandas 3.0 for more background and details.", "prev_chunk_id": "chunk_606", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_608", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Brief introduction to the new default string dtype#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Brief introduction to the new default string dtype#", "content": "Brief introduction to the new default string dtype# By default, pandas will infer this new string dtype instead of object dtype for string data (when creating pandas objects, such as in constructors or IO functions). Being a default dtype means that the string dtype will be used in IO methods or constructors when the dtype is being inferred and the input is inferred to be string data: >>> pd.Series([\"a\", \"b\", None]) 0 a 1 b 2 NaN dtype: str It can also be specified explicitly using the \"str\" alias: >>> pd.Series([\"a\", \"b\", None], dtype=\"str\") 0 a 1 b 2 NaN dtype: str Similarly, functions like read_csv(), read_parquet(), and others will now use the new string dtype when reading string data. In contrast to the current object dtype, the new string dtype will only store strings. This also means that it will raise an error if you try to store a non-string value in it (see below for more details). Missing values with the new string dtype are always represented as NaN (np.nan), and the missing value behavior is similar to other default dtypes. This new string dtype should otherwise behave the same as the existing object dtype users are used to. For example, all string-specific methods through the str accessor will work the same: >>> ser = pd.Series([\"a\", \"b\", None], dtype=\"str\") >>> ser.str.upper() 0 A 1 B 2 NaN dtype: str", "prev_chunk_id": "chunk_607", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_609", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "The dtype is no longer object dtype#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "The dtype is no longer object dtype#", "content": "The dtype is no longer object dtype# When inferring or reading string data, the data type of the resulting DataFrame column or Series will silently start being the new \"str\" dtype instead of \"object\" dtype, and this can have some impact on your code.", "prev_chunk_id": "chunk_608", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_610", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Checking the dtype#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Checking the dtype#", "content": "Checking the dtype# When checking the dtype, code might currently do something like: >>> ser = pd.Series([\"a\", \"b\", \"c\"]) >>> ser.dtype == \"object\" to check for columns with string data (by checking for the dtype being \"object\"). This will no longer work in pandas 3+, since ser.dtype will now be \"str\" with the new default string dtype, and the above check will return False. To check for columns with string data, you should instead use: >>> ser.dtype == \"str\" How to write compatible code For code that should work on both pandas 2.x and 3.x, you can use the pandas.api.types.is_string_dtype() function: >>> pd.api.types.is_string_dtype(ser.dtype) True This will return True for both the object dtype and the string dtypes.", "prev_chunk_id": "chunk_609", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_611", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Hardcoded use of object dtype#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Hardcoded use of object dtype#", "content": "Hardcoded use of object dtype# If you have code where the dtype is hardcoded in constructors, like >>> pd.Series([\"a\", \"b\", \"c\"], dtype=\"object\") this will keep using the object dtype. You will want to update this code to ensure you get the benefits of the new string dtype. How to write compatible code? First, in many cases it can be sufficient to remove the specific data type, and let pandas do the inference. But if you want to be specific, you can specify the \"str\" dtype: >>> pd.Series([\"a\", \"b\", \"c\"], dtype=\"str\") This is actually compatible with pandas 2.x as well, since in pandas < 3, dtype=\"str\" was essentially treated as an alias for object dtype.", "prev_chunk_id": "chunk_610", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_612", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "The missing value sentinel is now always NaN#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "The missing value sentinel is now always NaN#", "content": "The missing value sentinel is now always NaN# When using object dtype, multiple possible missing value sentinels are supported, including None and np.nan. With the new default string dtype, the missing value sentinel is always NaN (np.nan): # with object dtype, None is preserved as None and seen as missing >>> ser = pd.Series([\"a\", \"b\", None], dtype=\"object\") >>> ser 0 a 1 b 2 None dtype: object >>> print(ser[2]) None # with the new string dtype, any missing value like None is coerced to NaN >>> ser = pd.Series([\"a\", \"b\", None], dtype=\"str\") >>> ser 0 a 1 b 2 NaN dtype: str >>> print(ser[2]) nan Generally this should be no problem when relying on missing value behavior in pandas methods (for example, ser.isna() will give the same result as before). But when you relied on the exact value of None being present, that can impact your code. How to write compatible code? When checking for a missing value, instead of checking for the exact value of None or np.nan, you should use the pandas.isna() function. This is the most robust way to check for missing values, as it will work regardless of the dtype and the exact missing value sentinel: >>> pd.isna(ser[2]) True One caveat: this function works both on scalars and on array-likes, and in the latter case it will return an array of bools. When using it in a Boolean context (for example, if pd.isna(..): ..) be sure to only pass a scalar to it.", "prev_chunk_id": "chunk_611", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_613", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "“setitem” operations will now raise an error for non-string data#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "“setitem” operations will now raise an error for non-string data#", "content": "“setitem” operations will now raise an error for non-string data# With the new string dtype, any attempt to set a non-string value in a Series or DataFrame will raise an error: >>> ser = pd.Series([\"a\", \"b\", None], dtype=\"str\") >>> ser[1] = 2.5 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ... TypeError: Invalid value '2.5' for dtype 'str'. Value should be a string or missing value, got 'float' instead. If you relied on the flexible nature of object dtype being able to hold any Python object, but your initial data was inferred as strings, your code might be impacted by this change. How to write compatible code? You can update your code to ensure you only set string values in such columns, or otherwise you can explicitly ensure the column has object dtype first. This can be done by specifying the dtype explicitly in the constructor, or by using the astype() method: >>> ser = pd.Series([\"a\", \"b\", None], dtype=\"str\") >>> ser = ser.astype(\"object\") >>> ser[1] = 2.5 This astype(\"object\") call will be redundant when using pandas 2.x, but this code will work for all versions.", "prev_chunk_id": "chunk_612", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_614", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "Invalid unicode input#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "Invalid unicode input#", "content": "Invalid unicode input# Python allows to have a built-in str object that represents invalid unicode data. And since the object dtype can hold any Python object, you can have a pandas Series with such invalid unicode data: >>> ser = pd.Series([\"\\u2600\", \"\\ud83d\"], dtype=object) >>> ser 0 ☀ 1 \\ud83d dtype: object However, when using the string dtype using pyarrow under the hood, this can only store valid unicode data, and otherwise it will raise an error: >>> ser = pd.Series([\"\\u2600\", \"\\ud83d\"]) --------------------------------------------------------------------------- UnicodeEncodeError Traceback (most recent call last) ... UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 0: surrogates not allowed If you want to keep the previous behaviour, you can explicitly specify dtype=object to keep working with object dtype. When you have byte data that you want to convert to strings using decode(), the decode() method now has a dtype parameter to be able to specify object dtype instead of the default of string dtype for this use case.", "prev_chunk_id": "chunk_613", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_615", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "astype(str) preserving missing values#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "astype(str) preserving missing values#", "content": "astype(str) preserving missing values# This is a long standing “bug” or misfeature, as discussed in pandas-dev/pandas#25353. With pandas < 3, when using astype(str) (using the built-in str(), not astype(\"str\")!), the operation would convert every element to a string, including the missing values: # OLD behavior in pandas < 3 >>> ser = pd.Series([\"a\", np.nan], dtype=object) >>> ser 0 a 1 NaN dtype: object >>> ser.astype(str) 0 a 1 nan dtype: object >>> ser.astype(str).to_numpy() array(['a', 'nan'], dtype=object) Note how NaN (np.nan) was converted to the string \"nan\". This was not the intended behavior, and it was inconsistent with how other dtypes handled missing values. With pandas 3, this behavior has been fixed, and now astype(str) is an alias for astype(\"str\"), i.e. casting to the new string dtype, which will preserve the missing values: # NEW behavior in pandas 3 >>> pd.options.future.infer_string = True >>> ser = pd.Series([\"a\", np.nan], dtype=object) >>> ser.astype(str) 0 a 1 NaN dtype: str >>> ser.astype(str).values array(['a', nan], dtype=object) If you want to preserve the old behaviour of converting every object to a string, you can use ser.map(str) instead.", "prev_chunk_id": "chunk_614", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_616", "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html", "title": "prod() raising for string data#", "page_title": "Migration guide for the new string data type (pandas 3.0) — pandas 2.3.1 documentation", "breadcrumbs": "prod() raising for string data#", "content": "prod() raising for string data# In pandas < 3, calling the prod() method on a Series with string data would generally raise an error, except when the Series was empty or contained only a single string (potentially with missing values): >>> ser = pd.Series([\"a\", None], dtype=object) >>> ser.prod() 'a' When the Series contains multiple strings, it will raise a TypeError. This behaviour stays the same in pandas 3 when using the flexible object dtype. But by virtue of using the new string dtype, this will generally consistently raise an error regardless of the number of strings: >>> ser = pd.Series([\"a\", None], dtype=\"str\") >>> ser.prod() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ... TypeError: Cannot perform reduction 'prod' with string dtype", "prev_chunk_id": "chunk_615", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_617", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Sparse data structures#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Sparse data structures#", "content": "Sparse data structures# pandas provides data structures for efficiently storing sparse data. These are not necessarily sparse in the typical “mostly 0”. Rather, you can view these objects as being “compressed” where any data matching a specific value (NaN / missing value, though any value can be chosen, including 0) is omitted. The compressed values are not actually stored in the array. arr = np.random.randn(10) arr[2:-2] = np.nan ts = pd.Series(pd.arrays.SparseArray(arr)) ts 0 0.469112 1 -0.282863 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 -0.861849 9 -2.104569 dtype: Sparse[float64, nan] Notice the dtype, Sparse[float64, nan]. The nan means that elements in the array that are nan aren’t actually stored, only the non-nan elements are. Those non-nan elements have a float64 dtype. The sparse objects exist for memory efficiency reasons. Suppose you had a large, mostly NA DataFrame: df = pd.DataFrame(np.random.randn(10000, 4)) df.iloc[:9998] = np.nan sdf = df.astype(pd.SparseDtype(\"float\", np.nan)) sdf.head() 0 1 2 3 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN sdf.dtypes 0 Sparse[float64, nan] 1 Sparse[float64, nan] 2 Sparse[float64, nan] 3 Sparse[float64, nan] dtype: object sdf.sparse.density 0.0002 As you can see, the density (% of values that have not been “compressed”) is extremely low. This sparse object takes up much less memory on disk (pickled) and in the Python interpreter. 'dense : {:0.2f} bytes'.format(df.memory_usage().sum() / 1e3) 'dense : 320.13 bytes' 'sparse: {:0.2f} bytes'.format(sdf.memory_usage().sum() / 1e3) 'sparse: 0.22 bytes' Functionally, their behavior should be nearly identical to their dense counterparts.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_618", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "SparseArray#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "SparseArray#", "content": "SparseArray# arrays.SparseArray is a ExtensionArray for storing an array of sparse values (see dtypes for more on extension arrays). It is a 1-dimensional ndarray-like object storing only values distinct from the fill_value: arr = np.random.randn(10) arr[2:5] = np.nan arr[7:8] = np.nan sparr = pd.arrays.SparseArray(arr) sparr [-1.9556635297215477, -1.6588664275960427, nan, nan, nan, 1.1589328886422277, 0.14529711373305043, nan, 0.6060271905134522, 1.3342113401317768] Fill: nan IntIndex Indices: array([0, 1, 5, 6, 8, 9], dtype=int32) A sparse array can be converted to a regular (dense) ndarray with numpy.asarray() np.asarray(sparr) array([-1.9557, -1.6589, nan, nan, nan, 1.1589, 0.1453, nan, 0.606 , 1.3342])", "prev_chunk_id": "chunk_617", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_619", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "SparseDtype#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "SparseDtype#", "content": "SparseDtype# The SparseArray.dtype property stores two pieces of information - The dtype of the non-sparse values - The scalar fill value sparr.dtype Sparse[float64, nan] A SparseDtype may be constructed by passing only a dtype pd.SparseDtype(np.dtype('datetime64[ns]')) Sparse[datetime64[ns], numpy.datetime64('NaT')] in which case a default fill value will be used (for NumPy dtypes this is often the “missing” value for that dtype). To override this default an explicit fill value may be passed instead pd.SparseDtype(np.dtype('datetime64[ns]'), ....: fill_value=pd.Timestamp('2017-01-01')) ....: Sparse[datetime64[ns], Timestamp('2017-01-01 00:00:00')] Finally, the string alias 'Sparse[dtype]' may be used to specify a sparse dtype in many places pd.array([1, 0, 0, 2], dtype='Sparse[int]') [1, 0, 0, 2] Fill: 0 IntIndex Indices: array([0, 3], dtype=int32)", "prev_chunk_id": "chunk_618", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_620", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Sparse accessor#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Sparse accessor#", "content": "Sparse accessor# pandas provides a .sparse accessor, similar to .str for string data, .cat for categorical data, and .dt for datetime-like data. This namespace provides attributes and methods that are specific to sparse data. s = pd.Series([0, 0, 1, 2], dtype=\"Sparse[int]\") s.sparse.density 0.5 s.sparse.fill_value 0 This accessor is available only on data with SparseDtype, and on the Series class itself for creating a Series with sparse data from a scipy COO matrix with. A .sparse accessor has been added for DataFrame as well. See Sparse accessor for more.", "prev_chunk_id": "chunk_619", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_621", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Sparse calculation#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Sparse calculation#", "content": "Sparse calculation# You can apply NumPy ufuncs to arrays.SparseArray and get a arrays.SparseArray as a result. arr = pd.arrays.SparseArray([1., np.nan, np.nan, -2., np.nan]) np.abs(arr) [1.0, nan, nan, 2.0, nan] Fill: nan IntIndex Indices: array([0, 3], dtype=int32) The ufunc is also applied to fill_value. This is needed to get the correct dense result. arr = pd.arrays.SparseArray([1., -1, -1, -2., -1], fill_value=-1) np.abs(arr) [1, 1, 1, 2.0, 1] Fill: 1 IntIndex Indices: array([3], dtype=int32) np.abs(arr).to_dense() array([1., 1., 1., 2., 1.]) Conversion To convert data from sparse to dense, use the .sparse accessors sdf.sparse.to_dense() 0 1 2 3 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN ... ... ... ... ... 9995 NaN NaN NaN NaN 9996 NaN NaN NaN NaN 9997 NaN NaN NaN NaN 9998 0.509184 -0.774928 -1.369894 -0.382141 9999 0.280249 -1.648493 1.490865 -0.890819 [10000 rows x 4 columns] From dense to sparse, use DataFrame.astype() with a SparseDtype. dense = pd.DataFrame({\"A\": [1, 0, 0, 1]}) dtype = pd.SparseDtype(int, fill_value=0) dense.astype(dtype) A 0 1 1 0 2 0 3 1", "prev_chunk_id": "chunk_620", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_622", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Interaction with scipy.sparse#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Interaction with scipy.sparse#", "content": "Interaction with scipy.sparse# Use DataFrame.sparse.from_spmatrix() to create a DataFrame with sparse values from a sparse matrix. from scipy.sparse import csr_matrix arr = np.random.random(size=(1000, 5)) arr[arr < .9] = 0 sp_arr = csr_matrix(arr) sp_arr <Compressed Sparse Row sparse matrix of dtype 'float64' with 517 stored elements and shape (1000, 5)> sdf = pd.DataFrame.sparse.from_spmatrix(sp_arr) sdf.head() 0 1 2 3 4 0 0.95638 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 0 0 4 0.999552 0 0 0.956153 0 sdf.dtypes 0 Sparse[float64, 0] 1 Sparse[float64, 0] 2 Sparse[float64, 0] 3 Sparse[float64, 0] 4 Sparse[float64, 0] dtype: object All sparse formats are supported, but matrices that are not in COOrdinate format will be converted, copying data as needed. To convert back to sparse SciPy matrix in COO format, you can use the DataFrame.sparse.to_coo() method: sdf.sparse.to_coo() <COOrdinate sparse matrix of dtype 'float64' with 517 stored elements and shape (1000, 5)> Series.sparse.to_coo() is implemented for transforming a Series with sparse values indexed by a MultiIndex to a scipy.sparse.coo_matrix. The method requires a MultiIndex with two or more levels. s = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan]) s.index = pd.MultiIndex.from_tuples( ....: [ ....: (1, 2, \"a\", 0), ....: (1, 2, \"a\", 1), ....: (1, 1, \"b\", 0), ....: (1, 1, \"b\", 1), ....: (2, 1, \"b\", 0), ....: (2, 1, \"b\", 1), ....: ], ....: names=[\"A\", \"B\", \"C\", \"D\"], ....: ) ....: ss = s.astype('Sparse') ss A B C D 1 2 a 0 3.0 1 NaN 1 b 0 1.0 1 3.0 2 1 b 0 NaN 1 NaN dtype: Sparse[float64, nan] In the example below, we transform the Series to a sparse representation of a 2-d array by specifying that the first and second MultiIndex levels define labels for the rows and the", "prev_chunk_id": "chunk_621", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_623", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Interaction with scipy.sparse#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Interaction with scipy.sparse#", "content": "third and fourth levels define labels for the columns. We also specify that the column and row labels should be sorted in the final sparse representation. A, rows, columns = ss.sparse.to_coo( ....: row_levels=[\"A\", \"B\"], column_levels=[\"C\", \"D\"], sort_labels=True ....: ) ....: A <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 4)> A.todense() matrix([[0., 0., 1., 3.], [3., 0., 0., 0.], [0., 0., 0., 0.]]) rows [(1, 1), (1, 2), (2, 1)] columns [('a', 0), ('a', 1), ('b', 0), ('b', 1)] Specifying different row and column labels (and not sorting them) yields a different sparse matrix: A, rows, columns = ss.sparse.to_coo( ....: row_levels=[\"A\", \"B\", \"C\"], column_levels=[\"D\"], sort_labels=False ....: ) ....: A <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 2)> A.todense() matrix([[3., 0.], [1., 3.], [0., 0.]]) rows [(1, 2, 'a'), (1, 1, 'b'), (2, 1, 'b')] columns [(0,), (1,)] A convenience method Series.sparse.from_coo() is implemented for creating a Series with sparse values from a scipy.sparse.coo_matrix. from scipy import sparse A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4)) A <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 4)> A.todense() matrix([[0., 0., 1., 2.], [3., 0., 0., 0.], [0., 0., 0., 0.]]) The default behaviour (with dense_index=False) simply returns a Series containing only the non-null entries. ss = pd.Series.sparse.from_coo(A) ss 0 2 1.0 3 2.0 1 0 3.0 dtype: Sparse[float64, nan] Specifying dense_index=True will result in an index that is the Cartesian product of the row and columns coordinates of the matrix. Note that this will consume a significant amount of memory (relative to dense_index=False) if the sparse matrix is large (and sparse) enough. ss_dense = pd.Series.sparse.from_coo(A, dense_index=True) ss_dense 1 0 3.0 2 NaN 3 NaN 0 0 NaN 2 1.0 3 2.0", "prev_chunk_id": "chunk_622", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_624", "url": "https://pandas.pydata.org/docs/user_guide/sparse.html", "title": "Interaction with scipy.sparse#", "page_title": "Sparse data structures — pandas 2.3.1 documentation", "breadcrumbs": "Interaction with scipy.sparse#", "content": "0 NaN 2 1.0 3 2.0 dtype: Sparse[float64, nan]", "prev_chunk_id": "chunk_623", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_625", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Scaling to large datasets#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Scaling to large datasets#", "content": "Scaling to large datasets# pandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies. This document provides a few recommendations for scaling your analysis to larger datasets. It’s a complement to Enhancing performance, which focuses on speeding up analysis for datasets that fit in memory.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_626", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Load less data#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Load less data#", "content": "Load less data# Suppose our raw dataset on disk has many columns. import pandas as pd import numpy as np def make_timeseries(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"1D\", seed=None): ...: index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\") ...: n = len(index) ...: state = np.random.RandomState(seed) ...: columns = { ...: \"name\": state.choice([\"Alice\", \"Bob\", \"Charlie\"], size=n), ...: \"id\": state.poisson(1000, size=n), ...: \"x\": state.rand(n) * 2 - 1, ...: \"y\": state.rand(n) * 2 - 1, ...: } ...: df = pd.DataFrame(columns, index=index, columns=sorted(columns)) ...: if df.index[-1] == end: ...: df = df.iloc[:-1] ...: return df ...: timeseries = [ ...: make_timeseries(freq=\"1min\", seed=i).rename(columns=lambda x: f\"{x}_{i}\") ...: for i in range(10) ...: ] ...: ts_wide = pd.concat(timeseries, axis=1) ts_wide.head() id_0 name_0 x_0 ... name_9 x_9 y_9 timestamp ... 2000-01-01 00:00:00 977 Alice -0.821225 ... Charlie -0.957208 -0.757508 2000-01-01 00:01:00 1018 Bob -0.219182 ... Alice -0.414445 -0.100298 2000-01-01 00:02:00 927 Alice 0.660908 ... Charlie -0.325838 0.581859 2000-01-01 00:03:00 997 Bob -0.852458 ... Bob 0.992033 -0.686692 2000-01-01 00:04:00 965 Bob 0.717283 ... Charlie -0.924556 -0.184161 [5 rows x 40 columns] ts_wide.to_parquet(\"timeseries_wide.parquet\") To load the columns we want, we have two options. Option 1 loads in all the data and then filters to what we need. columns = [\"id_0\", \"name_0\", \"x_0\", \"y_0\"] pd.read_parquet(\"timeseries_wide.parquet\")[columns] id_0 name_0 x_0 y_0 timestamp 2000-01-01 00:00:00 977 Alice -0.821225 0.906222 2000-01-01 00:01:00 1018 Bob -0.219182 0.350855 2000-01-01 00:02:00 927 Alice 0.660908 -0.798511 2000-01-01 00:03:00 997 Bob -0.852458 0.735260 2000-01-01 00:04:00 965 Bob 0.717283 0.393391 ... ... ... ... ... 2000-12-30 23:56:00 1037 Bob -0.814321 0.612836 2000-12-30 23:57:00 980 Bob 0.232195 -0.618828 2000-12-30 23:58:00 965 Alice -0.231131 0.026310 2000-12-30 23:59:00 984 Alice 0.942819 0.853128 2000-12-31 00:00:00 1003 Alice 0.201125 -0.136655 [525601 rows x 4 columns] Option 2 only loads the columns we request. pd.read_parquet(\"timeseries_wide.parquet\", columns=columns) id_0 name_0 x_0 y_0 timestamp 2000-01-01 00:00:00 977 Alice -0.821225 0.906222 2000-01-01 00:01:00 1018", "prev_chunk_id": "chunk_625", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_627", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Load less data#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Load less data#", "content": "Bob -0.219182 0.350855 2000-01-01 00:02:00 927 Alice 0.660908 -0.798511 2000-01-01 00:03:00 997 Bob -0.852458 0.735260 2000-01-01 00:04:00 965 Bob 0.717283 0.393391 ... ... ... ... ... 2000-12-30 23:56:00 1037 Bob -0.814321 0.612836 2000-12-30 23:57:00 980 Bob 0.232195 -0.618828 2000-12-30 23:58:00 965 Alice -0.231131 0.026310 2000-12-30 23:59:00 984 Alice 0.942819 0.853128 2000-12-31 00:00:00 1003 Alice 0.201125 -0.136655 [525601 rows x 4 columns] If we were to measure the memory usage of the two calls, we’d see that specifying columns uses about 1/10th the memory in this case. With pandas.read_csv(), you can specify usecols to limit the columns read into memory. Not all file formats that can be read by pandas provide an option to read a subset of columns.", "prev_chunk_id": "chunk_626", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_628", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Use efficient datatypes#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Use efficient datatypes#", "content": "Use efficient datatypes# The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as “low-cardinality” data). By using more efficient data types, you can store larger datasets in memory. ts = make_timeseries(freq=\"30s\", seed=0) ts.to_parquet(\"timeseries.parquet\") ts = pd.read_parquet(\"timeseries.parquet\") ts id name x y timestamp 2000-01-01 00:00:00 1041 Alice 0.889987 0.281011 2000-01-01 00:00:30 988 Bob -0.455299 0.488153 2000-01-01 00:01:00 1018 Alice 0.096061 0.580473 2000-01-01 00:01:30 992 Bob 0.142482 0.041665 2000-01-01 00:02:00 960 Bob -0.036235 0.802159 ... ... ... ... ... 2000-12-30 23:58:00 1022 Alice 0.266191 0.875579 2000-12-30 23:58:30 974 Alice -0.009826 0.413686 2000-12-30 23:59:00 1028 Charlie 0.307108 -0.656789 2000-12-30 23:59:30 1002 Alice 0.202602 0.541335 2000-12-31 00:00:00 987 Alice 0.200832 0.615972 [1051201 rows x 4 columns] Now, let’s inspect the data types and memory usage to see where we should focus our attention. ts.dtypes id int64 name object x float64 y float64 dtype: object ts.memory_usage(deep=True) # memory usage in bytes Index 8409608 id 8409608 name 65176434 x 8409608 y 8409608 dtype: int64 The name column is taking up much more memory than any other. It has just a few unique values, so it’s a good candidate for converting to a pandas.Categorical. With a pandas.Categorical, we store each unique name once and use space-efficient integers to know which specific name is used in each row. ts2 = ts.copy() ts2[\"name\"] = ts2[\"name\"].astype(\"category\") ts2.memory_usage(deep=True) Index 8409608 id 8409608 name 1051495 x 8409608 y 8409608 dtype: int64 We can go a bit further and downcast the numeric columns to their smallest types using pandas.to_numeric(). ts2[\"id\"] = pd.to_numeric(ts2[\"id\"], downcast=\"unsigned\") ts2[[\"x\", \"y\"]] = ts2[[\"x\", \"y\"]].apply(pd.to_numeric, downcast=\"float\") ts2.dtypes id uint16 name category x float32 y float32 dtype: object ts2.memory_usage(deep=True) Index 8409608 id 2102402 name 1051495 x 4204804 y 4204804 dtype: int64 reduction =", "prev_chunk_id": "chunk_627", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_629", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Use efficient datatypes#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Use efficient datatypes#", "content": "ts2.memory_usage(deep=True).sum() / ts.memory_usage(deep=True).sum() print(f\"{reduction:0.2f}\") 0.20 In all, we’ve reduced the in-memory footprint of this dataset to 1/5 of its original size. See Categorical data for more on pandas.Categorical and dtypes for an overview of all of pandas’ dtypes.", "prev_chunk_id": "chunk_628", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_630", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Use chunking#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Use chunking#", "content": "Use chunking# Some workloads can be achieved with chunking by splitting a large problem into a bunch of small problems. For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory. As long as each chunk fits in memory, you can work with datasets that are much larger than memory. Suppose we have an even larger “logical dataset” on disk that’s a directory of parquet files. Each file in the directory represents a different year of the entire dataset. import pathlib N = 12 starts = [f\"20{i:>02d}-01-01\" for i in range(N)] ends = [f\"20{i:>02d}-12-13\" for i in range(N)] pathlib.Path(\"data/timeseries\").mkdir(exist_ok=True) for i, (start, end) in enumerate(zip(starts, ends)): ....: ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i) ....: ts.to_parquet(f\"data/timeseries/ts-{i:0>2d}.parquet\") ....: data └── timeseries ├── ts-00.parquet ├── ts-01.parquet ├── ts-02.parquet ├── ts-03.parquet ├── ts-04.parquet ├── ts-05.parquet ├── ts-06.parquet ├── ts-07.parquet ├── ts-08.parquet ├── ts-09.parquet ├── ts-10.parquet └── ts-11.parquet Now we’ll implement an out-of-core pandas.Series.value_counts(). The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets. %%time ....: files = pathlib.Path(\"data/timeseries/\").glob(\"ts*.parquet\") ....: counts = pd.Series(dtype=int) ....: for path in files: ....: df = pd.read_parquet(path) ....: counts = counts.add(df[\"name\"].value_counts(), fill_value=0) ....: counts.astype(int) ....: CPU times: user 987 ms, sys: 38.5 ms, total: 1.03 s Wall time: 997 ms name Alice 1994645 Bob 1993692 Charlie 1994875 dtype: int64 Some readers, like pandas.read_csv(), offer parameters to control the chunksize when reading a single file. Manually chunking is an OK option for workflows that don’t require too sophisticated of operations. Some operations, like pandas.DataFrame.groupby(), are much harder to do chunkwise. In these cases, you may be better switching to a different library", "prev_chunk_id": "chunk_629", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_631", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Use chunking#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Use chunking#", "content": "that implements these out-of-core algorithms for you.", "prev_chunk_id": "chunk_630", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_632", "url": "https://pandas.pydata.org/docs/user_guide/scale.html", "title": "Use Other Libraries#", "page_title": "Scaling to large datasets — pandas 2.3.1 documentation", "breadcrumbs": "Use Other Libraries#", "content": "Use Other Libraries# There are other libraries which provide similar APIs to pandas and work nicely with pandas DataFrame, and can give you the ability to scale your large dataset processing and analytics by parallel runtime, distributed memory, clustering, etc. You can find more information in the ecosystem page.", "prev_chunk_id": "chunk_631", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_633", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Enhancing performance#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Enhancing performance#", "content": "Enhancing performance# In this part of the tutorial, we will investigate how to speed up certain functions operating on pandas DataFrame using Cython, Numba and pandas.eval(). Generally, using Cython and Numba can offer a larger speedup than using pandas.eval() but will require a lot more code.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_634", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Cython (writing C extensions for pandas)#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Cython (writing C extensions for pandas)#", "content": "Cython (writing C extensions for pandas)# For many use cases writing pandas in pure Python and NumPy is sufficient. In some computationally heavy applications however, it can be possible to achieve sizable speed-ups by offloading work to cython. This tutorial assumes you have refactored as much as possible in Python, for example by trying to remove for-loops and making use of NumPy vectorization. It’s always worth optimising in Python first. This tutorial walks through a “typical” process of cythonizing a slow computation. We use an example from the Cython documentation but in the context of pandas. Our final cythonized solution is around 100 times faster than the pure Python solution.", "prev_chunk_id": "chunk_633", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_635", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Pure Python#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Pure Python#", "content": "Pure Python# We have a DataFrame to which we want to apply a function row-wise. df = pd.DataFrame( ...: { ...: \"a\": np.random.randn(1000), ...: \"b\": np.random.randn(1000), ...: \"N\": np.random.randint(100, 1000, (1000)), ...: \"x\": \"x\", ...: } ...: ) ...: df a b N x 0 0.469112 -0.218470 585 x 1 -0.282863 -0.061645 841 x 2 -1.509059 -0.723780 251 x 3 -1.135632 0.551225 972 x 4 1.212112 -0.497767 181 x .. ... ... ... .. 995 -1.512743 0.874737 374 x 996 0.933753 1.120790 246 x 997 -0.308013 0.198768 157 x 998 -0.079915 1.757555 977 x 999 -1.010589 -1.115680 770 x [1000 rows x 4 columns] Here’s the function in pure Python: def f(x): ...: return x * (x - 1) ...: def integrate_f(a, b, N): ...: s = 0 ...: dx = (b - a) / N ...: for i in range(N): ...: s += f(a + i * dx) ...: return s * dx ...: We achieve our result by using DataFrame.apply() (row-wise): %timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1) 79.2 ms +- 531 us per loop (mean +- std. dev. of 7 runs, 10 loops each) Let’s take a look and see where the time is spent during this operation using the prun ipython magic function: # most time consuming 4 calls %prun -l 4 df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1) # noqa E999 605956 function calls (605938 primitive calls) in 0.176 seconds Ordered by: internal time List reduced from 163 to 4 due to restriction <4> ncalls tottime percall cumtime percall filename:lineno(function) 1000 0.101 0.000 0.155 0.000 <ipython-input-4-c2a74e076cf0>:1(integrate_f) 552423 0.054 0.000 0.054 0.000 <ipython-input-3-c138bdd570e3>:1(f) 3000 0.004 0.000 0.013 0.000 series.py:1104(__getitem__) 3000 0.002 0.000 0.006 0.000 series.py:1229(_get_value) By far the majority of time is spend inside either integrate_f or f, hence we’ll concentrate our efforts cythonizing these two functions.", "prev_chunk_id": "chunk_634", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_636", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Plain Cython#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Plain Cython#", "content": "Plain Cython# First we’re going to need to import the Cython magic function to IPython: %load_ext Cython Now, let’s simply copy our functions over to Cython: %%cython ...: def f_plain(x): ...: return x * (x - 1) ...: def integrate_f_plain(a, b, N): ...: s = 0 ...: dx = (b - a) / N ...: for i in range(N): ...: s += f_plain(a + i * dx) ...: return s * dx ...: %timeit df.apply(lambda x: integrate_f_plain(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1) 48.9 ms +- 1.56 ms per loop (mean +- std. dev. of 7 runs, 10 loops each) This has improved the performance compared to the pure Python approach by one-third.", "prev_chunk_id": "chunk_635", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_637", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Declaring C types#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Declaring C types#", "content": "Declaring C types# We can annotate the function variables and return types as well as use cdef and cpdef to improve performance: %%cython ....: cdef double f_typed(double x) except? -2: ....: return x * (x - 1) ....: cpdef double integrate_f_typed(double a, double b, int N): ....: cdef int i ....: cdef double s, dx ....: s = 0 ....: dx = (b - a) / N ....: for i in range(N): ....: s += f_typed(a + i * dx) ....: return s * dx ....: %timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1) 7.59 ms +- 22.8 us per loop (mean +- std. dev. of 7 runs, 100 loops each) Annotating the functions with C types yields an over ten times performance improvement compared to the original Python implementation.", "prev_chunk_id": "chunk_636", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_638", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Using ndarray#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Using ndarray#", "content": "Using ndarray# When re-profiling, time is spent creating a Series from each row, and calling __getitem__ from both the index and the series (three times for each row). These Python function calls are expensive and can be improved by passing an np.ndarray. %prun -l 4 df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1) 52533 function calls (52515 primitive calls) in 0.019 seconds Ordered by: internal time List reduced from 161 to 4 due to restriction <4> ncalls tottime percall cumtime percall filename:lineno(function) 3000 0.003 0.000 0.013 0.000 series.py:1104(__getitem__) 3000 0.002 0.000 0.006 0.000 series.py:1229(_get_value) 3000 0.002 0.000 0.003 0.000 indexing.py:2765(check_dict_or_set_indexers) 3000 0.002 0.000 0.002 0.000 base.py:3784(get_loc) %%cython ....: cimport numpy as np ....: import numpy as np ....: cdef double f_typed(double x) except? -2: ....: return x * (x - 1) ....: cpdef double integrate_f_typed(double a, double b, int N): ....: cdef int i ....: cdef double s, dx ....: s = 0 ....: dx = (b - a) / N ....: for i in range(N): ....: s += f_typed(a + i * dx) ....: return s * dx ....: cpdef np.ndarray[double] apply_integrate_f(np.ndarray col_a, np.ndarray col_b, ....: np.ndarray col_N): ....: assert (col_a.dtype == np.float64 ....: and col_b.dtype == np.float64 and col_N.dtype == np.dtype(int)) ....: cdef Py_ssize_t i, n = len(col_N) ....: assert (len(col_a) == len(col_b) == n) ....: cdef np.ndarray[double] res = np.empty(n) ....: for i in range(len(col_a)): ....: res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i]) ....: return res ....: Content of stderr: In file included from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h:1929, from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h:5, from /home/runner/.cache/ipython/cython/_cython_magic_e28ca0815e7a5856b3401a5b0db25fe9aa46818f3939827e614e4baf0ae7bfcc.c:1146: /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] 17 | #warning \"Using deprecated NumPy API, disable it with \" \\ | ^~~~~~~ This implementation creates an array of zeros and inserts the result of integrate_f_typed applied over each row. Looping over an ndarray", "prev_chunk_id": "chunk_637", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_639", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Using ndarray#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Using ndarray#", "content": "is faster in Cython than looping over a Series object. Since apply_integrate_f is typed to accept an np.ndarray, Series.to_numpy() calls are needed to utilize this function. %timeit apply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()) 836 us +- 1.2 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each) Performance has improved from the prior implementation by almost ten times.", "prev_chunk_id": "chunk_638", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_640", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Disabling compiler directives#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Disabling compiler directives#", "content": "Disabling compiler directives# The majority of the time is now spent in apply_integrate_f. Disabling Cython’s boundscheck and wraparound checks can yield more performance. %prun -l 4 apply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()) 78 function calls in 0.001 seconds Ordered by: internal time List reduced from 21 to 4 due to restriction <4> ncalls tottime percall cumtime percall filename:lineno(function) 1 0.001 0.001 0.001 0.001 <string>:1(<module>) 1 0.000 0.000 0.001 0.001 {built-in method builtins.exec} 3 0.000 0.000 0.000 0.000 frame.py:4067(__getitem__) 3 0.000 0.000 0.000 0.000 base.py:545(to_numpy) %%cython ....: cimport cython ....: cimport numpy as np ....: import numpy as np ....: cdef np.float64_t f_typed(np.float64_t x) except? -2: ....: return x * (x - 1) ....: cpdef np.float64_t integrate_f_typed(np.float64_t a, np.float64_t b, np.int64_t N): ....: cdef np.int64_t i ....: cdef np.float64_t s = 0.0, dx ....: dx = (b - a) / N ....: for i in range(N): ....: s += f_typed(a + i * dx) ....: return s * dx ....: @cython.boundscheck(False) ....: @cython.wraparound(False) ....: cpdef np.ndarray[np.float64_t] apply_integrate_f_wrap( ....: np.ndarray[np.float64_t] col_a, ....: np.ndarray[np.float64_t] col_b, ....: np.ndarray[np.int64_t] col_N ....: ): ....: cdef np.int64_t i, n = len(col_N) ....: assert len(col_a) == len(col_b) == n ....: cdef np.ndarray[np.float64_t] res = np.empty(n, dtype=np.float64) ....: for i in range(n): ....: res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i]) ....: return res ....: Content of stderr: In file included from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h:1929, from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h:5, from /home/runner/.cache/ipython/cython/_cython_magic_bbeb904a660b57c169919355c1c214518b81c71b7a8af05dfcb190be7557b758.c:1147: /home/runner/micromamba/envs/test/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] 17 | #warning \"Using deprecated NumPy API, disable it with \" \\ | ^~~~~~~ %timeit apply_integrate_f_wrap(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()) 623 us +- 322 ns per loop (mean +- std. dev. of 7 runs, 1,000 loops each) However, a loop indexer i accessing an invalid location in an array would cause a segfault because memory access isn’t checked. For more about boundscheck and", "prev_chunk_id": "chunk_639", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_641", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Disabling compiler directives#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Disabling compiler directives#", "content": "wraparound, see the Cython docs on compiler directives.", "prev_chunk_id": "chunk_640", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_642", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Numba (JIT compilation)#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Numba (JIT compilation)#", "content": "Numba (JIT compilation)# An alternative to statically compiling Cython code is to use a dynamic just-in-time (JIT) compiler with Numba. Numba allows you to write a pure Python function which can be JIT compiled to native machine instructions, similar in performance to C, C++ and Fortran, by decorating your function with @jit. Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool). Numba supports compilation of Python to run on either CPU or GPU hardware and is designed to integrate with the Python scientific software stack. Numba can be used in 2 ways with pandas: - Specify theengine=\"numba\"keyword in select pandas methods - Define your own Python function decorated with@jitand pass the underlying NumPy array ofSeriesorDataFrame(usingSeries.to_numpy()) into the function", "prev_chunk_id": "chunk_641", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_643", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "pandas Numba Engine#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "pandas Numba Engine#", "content": "pandas Numba Engine# If Numba is installed, one can specify engine=\"numba\" in select pandas methods to execute the method using Numba. Methods that support engine=\"numba\" will also have an engine_kwargs keyword that accepts a dictionary that allows one to specify \"nogil\", \"nopython\" and \"parallel\" keys with boolean values to pass into the @jit decorator. If engine_kwargs is not specified, it defaults to {\"nogil\": False, \"nopython\": True, \"parallel\": False} unless otherwise specified. If your compute hardware contains multiple CPUs, the largest performance gain can be realized by setting parallel to True to leverage more than 1 CPU. Internally, pandas leverages numba to parallelize computations over the columns of a DataFrame; therefore, this performance benefit is only beneficial for a DataFrame with a large number of columns. import numba numba.set_num_threads(1) df = pd.DataFrame(np.random.randn(10_000, 100)) roll = df.rolling(100) %timeit roll.mean(engine=\"numba\", engine_kwargs={\"parallel\": True}) 347 ms ± 26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) numba.set_num_threads(2) %timeit roll.mean(engine=\"numba\", engine_kwargs={\"parallel\": True}) 201 ms ± 2.97 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)", "prev_chunk_id": "chunk_642", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_644", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Custom Function Examples#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Custom Function Examples#", "content": "Custom Function Examples# A custom Python function decorated with @jit can be used with pandas objects by passing their NumPy array representations with Series.to_numpy(). import numba @numba.jit def f_plain(x): return x * (x - 1) @numba.jit def integrate_f_numba(a, b, N): s = 0 dx = (b - a) / N for i in range(N): s += f_plain(a + i * dx) return s * dx @numba.jit def apply_integrate_f_numba(col_a, col_b, col_N): n = len(col_N) result = np.empty(n, dtype=\"float64\") assert len(col_a) == len(col_b) == n for i in range(n): result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i]) return result def compute_numba(df): result = apply_integrate_f_numba( df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy() ) return pd.Series(result, index=df.index, name=\"result\") %timeit compute_numba(df) 1000 loops, best of 3: 798 us per loop In this example, using Numba was faster than Cython. Numba can also be used to write vectorized functions that do not require the user to explicitly loop over the observations of a vector; a vectorized function will be applied to each row automatically. Consider the following example of doubling each observation: import numba def double_every_value_nonumba(x): return x * 2 @numba.vectorize def double_every_value_withnumba(x): # noqa E501 return x * 2 # Custom function without numba %timeit df[\"col1_doubled\"] = df[\"a\"].apply(double_every_value_nonumba) # noqa E501 1000 loops, best of 3: 797 us per loop # Standard implementation (faster than a custom function) %timeit df[\"col1_doubled\"] = df[\"a\"] * 2 1000 loops, best of 3: 233 us per loop # Custom function with numba %timeit df[\"col1_doubled\"] = double_every_value_withnumba(df[\"a\"].to_numpy()) 1000 loops, best of 3: 145 us per loop", "prev_chunk_id": "chunk_643", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_645", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Caveats#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Caveats#", "content": "Caveats# Numba is best at accelerating functions that apply numerical functions to NumPy arrays. If you try to @jit a function that contains unsupported Python or NumPy code, compilation will revert object mode which will mostly likely not speed up your function. If you would prefer that Numba throw an error if it cannot compile a function in a way that speeds up your code, pass Numba the argument nopython=True (e.g. @jit(nopython=True)). For more on troubleshooting Numba modes, see the Numba troubleshooting page. Using parallel=True (e.g. @jit(parallel=True)) may result in a SIGABRT if the threading layer leads to unsafe behavior. You can first specify a safe threading layer before running a JIT function with parallel=True. Generally if the you encounter a segfault (SIGSEGV) while using Numba, please report the issue to the Numba issue tracker.", "prev_chunk_id": "chunk_644", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_646", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Expression evaluation via eval()#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Expression evaluation via eval()#", "content": "Expression evaluation via eval()# The top-level function pandas.eval() implements performant expression evaluation of Series and DataFrame. Expression evaluation allows operations to be expressed as strings and can potentially provide a performance improvement by evaluate arithmetic and boolean expression all at once for large DataFrame.", "prev_chunk_id": "chunk_645", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_647", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Supported syntax#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Supported syntax#", "content": "Supported syntax# These operations are supported by pandas.eval(): - Arithmetic operations except for the left shift (<<) and right shift (>>) operators, e.g.,df+2*pi/s**4%42-the_golden_ratio - Comparison operations, including chained comparisons, e.g.,2<df<df2 - Boolean operations, e.g.,df<df2anddf3<df4ornotdf_bool - listandtupleliterals, e.g.,[1,2]or(1,2) - Attribute access, e.g.,df.a - Subscript expressions, e.g.,df[0] - Simple variable evaluation, e.g.,pd.eval(\"df\")(this is not very useful) - Math functions:sin,cos,exp,log,expm1,log1p,sqrt,sinh,cosh,tanh,arcsin,arccos,arctan,arccosh,arcsinh,arctanh,abs,arctan2andlog10. The following Python syntax is not allowed: - ExpressionsFunction calls other than math functions.is/isnotoperationsifexpressionslambdaexpressionslist/set/dictcomprehensionsLiteraldictandsetexpressionsyieldexpressionsGenerator expressionsBoolean expressions consisting of only scalar values - StatementsNeithersimpleorcompoundstatements are allowed. This includesfor,while, andif.", "prev_chunk_id": "chunk_646", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_648", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Local variables#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Local variables#", "content": "Local variables# You must explicitly reference any local variable that you want to use in an expression by placing the @ character in front of the name. This mechanism is the same for both DataFrame.query() and DataFrame.eval(). For example, df = pd.DataFrame(np.random.randn(5, 2), columns=list(\"ab\")) newcol = np.random.randn(len(df)) df.eval(\"b + @newcol\") 0 -0.206122 1 -1.029587 2 0.519726 3 -2.052589 4 1.453210 dtype: float64 df.query(\"b < @newcol\") a b 1 0.160268 -0.848896 3 0.333758 -1.180355 4 0.572182 0.439895 If you don’t prefix the local variable with @, pandas will raise an exception telling you the variable is undefined. When using DataFrame.eval() and DataFrame.query(), this allows you to have a local variable and a DataFrame column with the same name in an expression. a = np.random.randn() df.query(\"@a < a\") a b 0 0.473349 0.891236 1 0.160268 -0.848896 2 0.803311 1.662031 3 0.333758 -1.180355 4 0.572182 0.439895 df.loc[a < df[\"a\"]] # same as the previous expression a b 0 0.473349 0.891236 1 0.160268 -0.848896 2 0.803311 1.662031 3 0.333758 -1.180355 4 0.572182 0.439895", "prev_chunk_id": "chunk_647", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_649", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "pandas.eval() parsers#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "pandas.eval() parsers#", "content": "pandas.eval() parsers# There are two different expression syntax parsers. The default 'pandas' parser allows a more intuitive syntax for expressing query-like operations (comparisons, conjunctions and disjunctions). In particular, the precedence of the & and | operators is made equal to the precedence of the corresponding boolean operations and and or. For example, the above conjunction can be written without parentheses. Alternatively, you can use the 'python' parser to enforce strict Python semantics. nrows, ncols = 20000, 100 df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)] expr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\" x = pd.eval(expr, parser=\"python\") expr_no_parens = \"df1 > 0 & df2 > 0 & df3 > 0 & df4 > 0\" y = pd.eval(expr_no_parens, parser=\"pandas\") np.all(x == y) True The same expression can be “anded” together with the word and as well: expr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\" x = pd.eval(expr, parser=\"python\") expr_with_ands = \"df1 > 0 and df2 > 0 and df3 > 0 and df4 > 0\" y = pd.eval(expr_with_ands, parser=\"pandas\") np.all(x == y) True The and and or operators here have the same precedence that they would in Python.", "prev_chunk_id": "chunk_648", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_650", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "pandas.eval() engines#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "pandas.eval() engines#", "content": "pandas.eval() engines# There are two different expression engines. The 'numexpr' engine is the more performant engine that can yield performance improvements compared to standard Python syntax for large DataFrame. This engine requires the optional dependency numexpr to be installed. The 'python' engine is generally not useful except for testing other evaluation engines against it. You will achieve no performance benefits using eval() with engine='python' and may incur a performance hit. %timeit df1 + df2 + df3 + df4 7.58 ms +- 62.4 us per loop (mean +- std. dev. of 7 runs, 100 loops each) %timeit pd.eval(\"df1 + df2 + df3 + df4\", engine=\"python\") 8.24 ms +- 38 us per loop (mean +- std. dev. of 7 runs, 100 loops each)", "prev_chunk_id": "chunk_649", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_651", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "The DataFrame.eval() method#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "The DataFrame.eval() method#", "content": "The DataFrame.eval() method# In addition to the top level pandas.eval() function you can also evaluate an expression in the “context” of a DataFrame. df = pd.DataFrame(np.random.randn(5, 2), columns=[\"a\", \"b\"]) df.eval(\"a + b\") 0 -0.161099 1 0.805452 2 0.747447 3 1.189042 4 -2.057490 dtype: float64 Any expression that is a valid pandas.eval() expression is also a valid DataFrame.eval() expression, with the added benefit that you don’t have to prefix the name of the DataFrame to the column(s) you’re interested in evaluating. In addition, you can perform assignment of columns within an expression. This allows for formulaic evaluation. The assignment target can be a new column name or an existing column name, and it must be a valid Python identifier. df = pd.DataFrame(dict(a=range(5), b=range(5, 10))) df = df.eval(\"c = a + b\") df = df.eval(\"d = a + b + c\") df = df.eval(\"a = 1\") df a b c d 0 1 5 5 10 1 1 6 7 14 2 1 7 9 18 3 1 8 11 22 4 1 9 13 26 A copy of the DataFrame with the new or modified columns is returned, and the original frame is unchanged. df a b c d 0 1 5 5 10 1 1 6 7 14 2 1 7 9 18 3 1 8 11 22 4 1 9 13 26 df.eval(\"e = a - c\") a b c d e 0 1 5 5 10 -4 1 1 6 7 14 -6 2 1 7 9 18 -8 3 1 8 11 22 -10 4 1 9 13 26 -12 df a b c d 0 1 5 5 10 1 1 6 7 14 2 1 7 9 18 3 1 8 11 22 4 1 9 13 26 Multiple column assignments can be performed by", "prev_chunk_id": "chunk_650", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_652", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "The DataFrame.eval() method#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "The DataFrame.eval() method#", "content": "using a multi-line string. df.eval( ....: \"\"\" ....: c = a + b ....: d = a + b + c ....: a = 1\"\"\", ....: ) ....: a b c d 0 1 5 6 12 1 1 6 7 14 2 1 7 8 16 3 1 8 9 18 4 1 9 10 20 The equivalent in standard Python would be df = pd.DataFrame(dict(a=range(5), b=range(5, 10))) df[\"c\"] = df[\"a\"] + df[\"b\"] df[\"d\"] = df[\"a\"] + df[\"b\"] + df[\"c\"] df[\"a\"] = 1 df a b c d 0 1 5 5 10 1 1 6 7 14 2 1 7 9 18 3 1 8 11 22 4 1 9 13 26", "prev_chunk_id": "chunk_651", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_653", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "eval() performance comparison#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "eval() performance comparison#", "content": "eval() performance comparison# pandas.eval() works well with expressions containing large arrays. nrows, ncols = 20000, 100 df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)] DataFrame arithmetic: %timeit df1 + df2 + df3 + df4 7.23 ms +- 70 us per loop (mean +- std. dev. of 7 runs, 100 loops each) %timeit pd.eval(\"df1 + df2 + df3 + df4\") 2.99 ms +- 46 us per loop (mean +- std. dev. of 7 runs, 100 loops each) DataFrame comparison: %timeit (df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0) 5.48 ms +- 39.4 us per loop (mean +- std. dev. of 7 runs, 100 loops each) %timeit pd.eval(\"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\") 8.7 ms +- 85.5 us per loop (mean +- std. dev. of 7 runs, 100 loops each) DataFrame arithmetic with unaligned axes. s = pd.Series(np.random.randn(50)) %timeit df1 + df2 + df3 + df4 + s 12.8 ms +- 101 us per loop (mean +- std. dev. of 7 runs, 100 loops each) %timeit pd.eval(\"df1 + df2 + df3 + df4 + s\") 3.71 ms +- 52.6 us per loop (mean +- std. dev. of 7 runs, 100 loops each) Here is a plot showing the running time of pandas.eval() as function of the size of the frame involved in the computation. The two lines are two different engines. You will only see the performance benefits of using the numexpr engine with pandas.eval() if your DataFrame has more than approximately 100,000 rows. This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn().", "prev_chunk_id": "chunk_652", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_654", "url": "https://pandas.pydata.org/docs/user_guide/enhancingperf.html", "title": "Expression evaluation limitations with numexpr#", "page_title": "Enhancing performance — pandas 2.3.1 documentation", "breadcrumbs": "Expression evaluation limitations with numexpr#", "content": "Expression evaluation limitations with numexpr# Expressions that would result in an object dtype or involve datetime operations because of NaT must be evaluated in Python space, but part of an expression can still be evaluated with numexpr. For example: df = pd.DataFrame( ....: {\"strings\": np.repeat(list(\"cba\"), 3), \"nums\": np.repeat(range(3), 3)} ....: ) ....: df strings nums 0 c 0 1 c 0 2 c 0 3 b 1 4 b 1 5 b 1 6 a 2 7 a 2 8 a 2 df.query(\"strings == 'a' and nums == 1\") Empty DataFrame Columns: [strings, nums] Index: [] The numeric part of the comparison (nums == 1) will be evaluated by numexpr and the object part of the comparison (\"strings == 'a') will be evaluated by Python.", "prev_chunk_id": "chunk_653", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_655", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Overview#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "Overview# pandas has an options API configure and customize global behavior related to DataFrame display, data behavior and more. Options have a full “dotted-style”, case-insensitive name (e.g. display.max_rows). You can get/set options directly as attributes of the top-level options attribute: import pandas as pd pd.options.display.max_rows 15 pd.options.display.max_rows = 999 pd.options.display.max_rows 999 The API is composed of 5 relevant functions, available directly from the pandas namespace: - get_option()/set_option()- get/set the value of a single option. - reset_option()- reset one or more options to their default value. - describe_option()- print the descriptions of one or more options. - option_context()- execute a codeblock with a set of options that revert to prior settings after execution. All of the functions above accept a regexp pattern (re.search style) as an argument, to match an unambiguous substring: pd.get_option(\"display.chop_threshold\") pd.set_option(\"display.chop_threshold\", 2) pd.get_option(\"display.chop_threshold\") 2 pd.set_option(\"chop\", 4) pd.get_option(\"display.chop_threshold\") 4 The following will not work because it matches multiple option names, e.g. display.max_colwidth, display.max_rows, display.max_columns: pd.get_option(\"max\") --------------------------------------------------------------------------- OptionError Traceback (most recent call last) Cell In[10], line 1 ----> 1 pd.get_option(\"max\") File ~/work/pandas/pandas/pandas/_config/config.py:274, in CallableDynamicDoc.__call__(self, *args, **kwds) 273 def __call__(self, *args, **kwds) -> T: --> 274 return self.__func__(*args, **kwds) File ~/work/pandas/pandas/pandas/_config/config.py:146, in _get_option(pat, silent) 145 def _get_option(pat: str, silent: bool = False) -> Any: --> 146 key = _get_single_key(pat, silent) 148 # walk the nested dict 149 root, k = _get_root(key) File ~/work/pandas/pandas/pandas/_config/config.py:134, in _get_single_key(pat, silent) 132 raise OptionError(f\"No such keys(s): {repr(pat)}\") 133 if len(keys) > 1: --> 134 raise OptionError(\"Pattern matched multiple keys\") 135 key = keys[0] 137 if not silent: OptionError: Pattern matched multiple keys", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_656", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "Available options# You can get a list of available options and their descriptions with describe_option(). When called with no argument describe_option() will print out the descriptions for all available options. pd.describe_option() compute.use_bottleneck : bool Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numba : bool Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexpr : bool Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_threshold : float or None if set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify : 'left'/'right' Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirst : boolean When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirst : boolean When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encoding : str/unicode Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf8] display.expand_frame_repr : boolean Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, `max_columns` is still respected, but the output will wrap-around across multiple \"pages\" if its width exceeds `display.width`. [default: True] [currently: True] display.float_format : callable The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter", "prev_chunk_id": "chunk_655", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_657", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "for an example. [default: None] [currently: None] display.html.border : int A ``border=value`` attribute is inserted in the ``<table>`` tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schema : boolean Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjax : boolean When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr : 'truncate'/'info' For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categories : int This sets the maximum number of categories pandas should output when printing out a `Categorical` or a Series of dtype \"category\". [default: 8] [currently: 8] display.max_columns : int If max_cols is exceeded, switch to truncate view. Depending on `large_repr`, objects are either centrally truncated or printed as a summary view. 'None' value means unlimited. In case python/IPython is running in a terminal and `large_repr` equals 'truncate' this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidth : int or None The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \"...\" placeholder is embedded in the output. A 'None' value means unlimited. [default: 50] [currently: 50] display.max_dir_items : int The number of items that will be added to `dir(...)`. 'None' value means unlimited. Because dir is", "prev_chunk_id": "chunk_656", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_658", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columns : int max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rows : int df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rows : int If max_rows is exceeded, switch to truncate view. Depending on `large_repr`, objects are either centrally truncated or printed as a summary view. 'None' value means unlimited. In case python/IPython is running in a terminal and `large_repr` equals 'truncate' this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_items : int or None When pretty-printing a long sequence, no more then `max_seq_items` will be printed. If items are omitted, they will be denoted by the addition of \"...\" to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usage : bool, string or None This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,'deep' [default: True] [currently: True] display.min_rows : int The numbers of rows to show in a truncated view (when `max_rows` is exceeded). Ignored when `max_rows` is set to None or 0. When set to None, follows", "prev_chunk_id": "chunk_657", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_659", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "the value of `max_rows`. [default: 10] [currently: 10] display.multi_sparse : boolean \"sparsify\" MultiIndex display (don't display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_html : boolean When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depth : int Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precision : int Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to ``precision`` in :meth:`numpy.set_printoptions`. [default: 6] [currently: 6] display.show_dimensions : boolean or 'truncate' Whether to print out dimensions at the end of DataFrame repr. If 'truncate' is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_wide : boolean Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_width : boolean Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.width : int Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated). [default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior", "prev_chunk_id": "chunk_658", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_660", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "which will *not* silently downcast results from Series and DataFrame `where`, `mask`, and `clip` methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated). [default: False] [currently: False] io.excel.ods.reader : string The default Excel reader engine for 'ods' files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writer : string The default Excel writer engine for 'ods' files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.reader : string The default Excel reader engine for 'xls' files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.reader : string The default Excel reader engine for 'xlsb' files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.reader : string The default Excel reader engine for 'xlsm' files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writer : string The default Excel writer engine for 'xlsm' files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.reader : string The default Excel reader engine for 'xlsx' files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writer : string The default Excel writer engine for 'xlsx' files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_format : format default format writing format, if None, then put will default to 'fixed' and append will default to 'table' [default: None] [currently: None] io.hdf.dropna_table : boolean drop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.engine : string The default parquet reader/writer engine. Available options: 'auto', 'pyarrow', 'fastparquet', the default is 'auto' [default: auto] [currently: auto] io.sql.engine : string The default sql reader/writer engine. Available options: 'auto', 'sqlalchemy', the default is 'auto' [default: auto] [currently: auto] mode.chained_assignment : string Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently:", "prev_chunk_id": "chunk_659", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_661", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "warn] mode.copy_on_write : bool Use new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the 'PANDAS_COPY_ON_WRITE' environment variable (if set to \"1\" for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_manager : string Internal data manager type; can be \"block\" or \"array\". Defaults to \"block\", unless overridden by the 'PANDAS_DATA_MANAGER' environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactive : boolean Whether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storage : string The default storage for StringDtype. [default: auto] [currently: auto] mode.use_inf_as_na : boolean True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backend : str The plotting backend to use. The default value is \"matplotlib\", the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_converters : bool or 'auto'. Whether to register converters with matplotlib's units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimal : str The character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escape : str, optional Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatter : str, callable, dict, optional A formatter object to be used as default within ``Styler.format``. [default: None] [currently: None] styler.format.na_rep : str, optional The string representation for values identified as missing. [default:", "prev_chunk_id": "chunk_660", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_662", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "None] [currently: None] styler.format.precision : int The precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousands : str, optional The character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjax : bool If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environment : str The environment to replace ``\\begin{table}``. If \"longtable\" is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrules : bool Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align : {\"r\", \"c\", \"l\", \"naive-l\", \"naive-r\"} The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \"\\|r\" will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align : {\"c\", \"t\", \"b\"} The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encoding : str The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columns : int, optional The maximum number of columns that will be rendered. May still be reduced to satisfy ``max_elements``, which takes precedence. [default: None] [currently: None] styler.render.max_elements : int The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rows : int, optional The maximum number of rows that will be rendered. May still be reduced to satisfy ``max_elements``, which takes precedence. [default: None] [currently: None] styler.render.repr : str Determine which output to use in Jupyter Notebook in {\"html\", \"latex\"}. [default: html] [currently: html] styler.sparse.columns : bool Whether to sparsify the display of hierarchical columns.", "prev_chunk_id": "chunk_661", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_663", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Available options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Available options#", "content": "Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.index : bool Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]", "prev_chunk_id": "chunk_662", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_664", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Getting and setting options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Getting and setting options#", "content": "Getting and setting options# As described above, get_option() and set_option() are available from the pandas namespace. To change an option, call set_option('option regex', new_value). pd.get_option(\"mode.sim_interactive\") False pd.set_option(\"mode.sim_interactive\", True) pd.get_option(\"mode.sim_interactive\") True You can use reset_option() to revert to a setting’s default value pd.get_option(\"display.max_rows\") 60 pd.set_option(\"display.max_rows\", 999) pd.get_option(\"display.max_rows\") 999 pd.reset_option(\"display.max_rows\") pd.get_option(\"display.max_rows\") 60 It’s also possible to reset multiple options at once (using a regex): pd.reset_option(\"^display\") option_context() context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the with block: with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", 5): ....: print(pd.get_option(\"display.max_rows\")) ....: print(pd.get_option(\"display.max_columns\")) ....: 10 5 print(pd.get_option(\"display.max_rows\")) 60 print(pd.get_option(\"display.max_columns\")) 0", "prev_chunk_id": "chunk_663", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_665", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Setting startup options in Python/IPython environment#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Setting startup options in Python/IPython environment#", "content": "Setting startup options in Python/IPython environment# Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient. To do this, create a .py or .ipy script in the startup directory of the desired profile. An example where the startup folder is in a default IPython profile can be found at: $IPYTHONDIR/profile_default/startup More information can be found in the IPython documentation. An example startup script for pandas is displayed below: import pandas as pd pd.set_option(\"display.max_rows\", 999) pd.set_option(\"display.precision\", 5)", "prev_chunk_id": "chunk_664", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_666", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Frequently used options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Frequently used options#", "content": "Frequently used options# The following is a demonstrates the more frequently used display options. display.max_rows and display.max_columns sets the maximum number of rows and columns displayed when a frame is pretty-printed. Truncated lines are replaced by an ellipsis. df = pd.DataFrame(np.random.randn(7, 2)) pd.set_option(\"display.max_rows\", 7) df 0 1 0 0.469112 -0.282863 1 -1.509059 -1.135632 2 1.212112 -0.173215 3 0.119209 -1.044236 4 -0.861849 -2.104569 5 -0.494929 1.071804 6 0.721555 -0.706771 pd.set_option(\"display.max_rows\", 5) df 0 1 0 0.469112 -0.282863 1 -1.509059 -1.135632 .. ... ... 5 -0.494929 1.071804 6 0.721555 -0.706771 [7 rows x 2 columns] pd.reset_option(\"display.max_rows\") Once the display.max_rows is exceeded, the display.min_rows options determines how many rows are shown in the truncated repr. pd.set_option(\"display.max_rows\", 8) pd.set_option(\"display.min_rows\", 4) # below max_rows -> all rows shown df = pd.DataFrame(np.random.randn(7, 2)) df 0 1 0 -1.039575 0.271860 1 -0.424972 0.567020 2 0.276232 -1.087401 3 -0.673690 0.113648 4 -1.478427 0.524988 5 0.404705 0.577046 6 -1.715002 -1.039268 # above max_rows -> only min_rows (4) rows shown df = pd.DataFrame(np.random.randn(9, 2)) df 0 1 0 -0.370647 -1.157892 1 -1.344312 0.844885 .. ... ... 7 0.276662 -0.472035 8 -0.013960 -0.362543 [9 rows x 2 columns] pd.reset_option(\"display.max_rows\") pd.reset_option(\"display.min_rows\") display.expand_frame_repr allows for the representation of a DataFrame to stretch across pages, wrapped over the all the columns. df = pd.DataFrame(np.random.randn(5, 10)) pd.set_option(\"expand_frame_repr\", True) df 0 1 2 ... 7 8 9 0 -0.006154 -0.923061 0.895717 ... 1.340309 -1.170299 -0.226169 1 0.410835 0.813850 0.132003 ... -1.436737 -1.413681 1.607920 2 1.024180 0.569605 0.875906 ... -0.078638 0.545952 -1.219217 3 -1.226825 0.769804 -1.281247 ... 0.341734 0.959726 -1.110336 4 -0.619976 0.149748 -0.732339 ... 0.301624 -2.179861 -1.369849 [5 rows x 10 columns] pd.set_option(\"expand_frame_repr\", False) df 0 1 2 3 4 5 6 7 8 9 0 -0.006154 -0.923061 0.895717 0.805244 -1.206412 2.565646 1.431256 1.340309 -1.170299 -0.226169 1 0.410835 0.813850 0.132003 -0.827317 -0.076467 -1.187678 1.130127 -1.436737 -1.413681", "prev_chunk_id": "chunk_665", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_667", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Frequently used options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Frequently used options#", "content": "1.607920 2 1.024180 0.569605 0.875906 -2.211372 0.974466 -2.006747 -0.410001 -0.078638 0.545952 -1.219217 3 -1.226825 0.769804 -1.281247 -0.727707 -0.121306 -0.097883 0.695775 0.341734 0.959726 -1.110336 4 -0.619976 0.149748 -0.732339 0.687738 0.176444 0.403310 -0.154951 0.301624 -2.179861 -1.369849 pd.reset_option(\"expand_frame_repr\") display.large_repr displays a DataFrame that exceed max_columns or max_rows as a truncated frame or summary. df = pd.DataFrame(np.random.randn(10, 10)) pd.set_option(\"display.max_rows\", 5) pd.set_option(\"large_repr\", \"truncate\") df 0 1 2 ... 7 8 9 0 -0.954208 1.462696 -1.743161 ... 0.995761 2.396780 0.014871 1 3.357427 -0.317441 -1.236269 ... 0.380396 0.084844 0.432390 .. ... ... ... ... ... ... ... 8 -0.303421 -0.858447 0.306996 ... 0.476720 0.473424 -0.242861 9 -0.014805 -0.284319 0.650776 ... 1.613616 0.464000 0.227371 [10 rows x 10 columns] pd.set_option(\"large_repr\", \"info\") df <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 10 non-null float64 1 1 10 non-null float64 2 2 10 non-null float64 3 3 10 non-null float64 4 4 10 non-null float64 5 5 10 non-null float64 6 6 10 non-null float64 7 7 10 non-null float64 8 8 10 non-null float64 9 9 10 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes pd.reset_option(\"large_repr\") pd.reset_option(\"display.max_rows\") display.max_colwidth sets the maximum width of columns. Cells of this length or longer will be truncated with an ellipsis. df = pd.DataFrame( ....: np.array( ....: [ ....: [\"foo\", \"bar\", \"bim\", \"uncomfortably long string\"], ....: [\"horse\", \"cow\", \"banana\", \"apple\"], ....: ] ....: ) ....: ) ....: pd.set_option(\"max_colwidth\", 40) df 0 1 2 3 0 foo bar bim uncomfortably long string 1 horse cow banana apple pd.set_option(\"max_colwidth\", 6) df 0 1 2 3 0 foo bar bim un... 1 horse cow ba... apple pd.reset_option(\"max_colwidth\") display.max_info_columns sets a threshold for the number of columns displayed when calling info(). df = pd.DataFrame(np.random.randn(10, 10)) pd.set_option(\"max_info_columns\", 11) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 10", "prev_chunk_id": "chunk_666", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_668", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Frequently used options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Frequently used options#", "content": "entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 10 non-null float64 1 1 10 non-null float64 2 2 10 non-null float64 3 3 10 non-null float64 4 4 10 non-null float64 5 5 10 non-null float64 6 6 10 non-null float64 7 7 10 non-null float64 8 8 10 non-null float64 9 9 10 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes pd.set_option(\"max_info_columns\", 5) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Columns: 10 entries, 0 to 9 dtypes: float64(10) memory usage: 928.0 bytes pd.reset_option(\"max_info_columns\") display.max_info_rows: info() will usually show null-counts for each column. For a large DataFrame, this can be quite slow. max_info_rows and max_info_cols limit this null check to the specified rows and columns respectively. The info() keyword argument show_counts=True will override this. df = pd.DataFrame(np.random.choice([0, 1, np.nan], size=(10, 10))) df 0 1 2 3 4 5 6 7 8 9 0 0.0 NaN 1.0 NaN NaN 0.0 NaN 0.0 NaN 1.0 1 1.0 NaN 1.0 1.0 1.0 1.0 NaN 0.0 0.0 NaN 2 0.0 NaN 1.0 0.0 0.0 NaN NaN NaN NaN 0.0 3 NaN NaN NaN 0.0 1.0 1.0 NaN 1.0 NaN 1.0 4 0.0 NaN NaN NaN 0.0 NaN NaN NaN 1.0 0.0 5 0.0 1.0 1.0 1.0 1.0 0.0 NaN NaN 1.0 0.0 6 1.0 1.0 1.0 NaN 1.0 NaN 1.0 0.0 NaN NaN 7 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 NaN 8 NaN NaN NaN 0.0 NaN NaN NaN NaN 1.0 NaN 9 0.0 NaN 0.0 NaN NaN 0.0 NaN 1.0 1.0 0.0 pd.set_option(\"max_info_rows\", 11) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 8 non-null float64 1 1 3 non-null", "prev_chunk_id": "chunk_667", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_669", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Frequently used options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Frequently used options#", "content": "float64 2 2 7 non-null float64 3 3 6 non-null float64 4 4 7 non-null float64 5 5 6 non-null float64 6 6 2 non-null float64 7 7 6 non-null float64 8 8 6 non-null float64 9 9 6 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes pd.set_option(\"max_info_rows\", 5) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Dtype --- ------ ----- 0 0 float64 1 1 float64 2 2 float64 3 3 float64 4 4 float64 5 5 float64 6 6 float64 7 7 float64 8 8 float64 9 9 float64 dtypes: float64(10) memory usage: 928.0 bytes pd.reset_option(\"max_info_rows\") display.precision sets the output display precision in terms of decimal places. df = pd.DataFrame(np.random.randn(5, 5)) pd.set_option(\"display.precision\", 7) df 0 1 2 3 4 0 -1.1506406 -0.7983341 -0.5576966 0.3813531 1.3371217 1 -1.5310949 1.3314582 -0.5713290 -0.0266708 -1.0856630 2 -1.1147378 -0.0582158 -0.4867681 1.6851483 0.1125723 3 -1.4953086 0.8984347 -0.1482168 -1.5960698 0.1596530 4 0.2621358 0.0362196 0.1847350 -0.2550694 -0.2710197 pd.set_option(\"display.precision\", 4) df 0 1 2 3 4 0 -1.1506 -0.7983 -0.5577 0.3814 1.3371 1 -1.5311 1.3315 -0.5713 -0.0267 -1.0857 2 -1.1147 -0.0582 -0.4868 1.6851 0.1126 3 -1.4953 0.8984 -0.1482 -1.5961 0.1597 4 0.2621 0.0362 0.1847 -0.2551 -0.2710 display.chop_threshold sets the rounding threshold to zero when displaying a Series or DataFrame. This setting does not change the precision at which the number is stored. df = pd.DataFrame(np.random.randn(6, 6)) pd.set_option(\"chop_threshold\", 0) df 0 1 2 3 4 5 0 1.2884 0.2946 -1.1658 0.8470 -0.6856 0.6091 1 -0.3040 0.6256 -0.0593 0.2497 1.1039 -1.0875 2 1.9980 -0.2445 0.1362 0.8863 -1.3507 -0.8863 3 -1.0133 1.9209 -0.3882 -2.3144 0.6655 0.4026 4 0.3996 -1.7660 0.8504 0.3881 0.9923 0.7441 5 -0.7398 -1.0549 -0.1796 0.6396 1.5850 1.9067 pd.set_option(\"chop_threshold\", 0.5) df 0 1 2 3 4 5 0 1.2884 0.0000 -1.1658 0.8470 -0.6856 0.6091 1 0.0000 0.6256 0.0000 0.0000 1.1039", "prev_chunk_id": "chunk_668", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_670", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Frequently used options#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Frequently used options#", "content": "-1.0875 2 1.9980 0.0000 0.0000 0.8863 -1.3507 -0.8863 3 -1.0133 1.9209 0.0000 -2.3144 0.6655 0.0000 4 0.0000 -1.7660 0.8504 0.0000 0.9923 0.7441 5 -0.7398 -1.0549 0.0000 0.6396 1.5850 1.9067 pd.reset_option(\"chop_threshold\") display.colheader_justify controls the justification of the headers. The options are 'right', and 'left'. df = pd.DataFrame( ....: np.array([np.random.randn(6), np.random.randint(1, 9, 6) * 0.1, np.zeros(6)]).T, ....: columns=[\"A\", \"B\", \"C\"], ....: dtype=\"float\", ....: ) ....: pd.set_option(\"colheader_justify\", \"right\") df A B C 0 0.1040 0.1 0.0 1 0.1741 0.5 0.0 2 -0.4395 0.4 0.0 3 -0.7413 0.8 0.0 4 -0.0797 0.4 0.0 5 -0.9229 0.3 0.0 pd.set_option(\"colheader_justify\", \"left\") df A B C 0 0.1040 0.1 0.0 1 0.1741 0.5 0.0 2 -0.4395 0.4 0.0 3 -0.7413 0.8 0.0 4 -0.0797 0.4 0.0 5 -0.9229 0.3 0.0 pd.reset_option(\"colheader_justify\")", "prev_chunk_id": "chunk_669", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_671", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Number formatting#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Number formatting#", "content": "Number formatting# pandas also allows you to set how numbers are displayed in the console. This option is not set through the set_options API. Use the set_eng_float_format function to alter the floating-point formatting of pandas objects to produce a particular format. import numpy as np pd.set_eng_float_format(accuracy=3, use_eng_prefix=True) s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) s / 1.0e3 a 303.638u b -721.084u c -622.696u d 648.250u e -1.945m dtype: float64 s / 1.0e6 a 303.638n b -721.084n c -622.696n d 648.250n e -1.945u dtype: float64 Use round() to specifically control rounding of an individual DataFrame", "prev_chunk_id": "chunk_670", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_672", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Unicode formatting#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Unicode formatting#", "content": "Unicode formatting# Some East Asian countries use Unicode characters whose width corresponds to two Latin characters. If a DataFrame or Series contains these characters, the default output mode may not align them properly. df = pd.DataFrame({\"国籍\": [\"UK\", \"日本\"], \"名前\": [\"Alice\", \"しのぶ\"]}) df 国籍 名前 0 UK Alice 1 日本 しのぶ Enabling display.unicode.east_asian_width allows pandas to check each character’s “East Asian Width” property. These characters can be aligned properly by setting this option to True. However, this will result in longer render times than the standard len function. pd.set_option(\"display.unicode.east_asian_width\", True) df 国籍 名前 0 UK Alice 1 日本 しのぶ In addition, Unicode characters whose width is “ambiguous” can either be 1 or 2 characters wide depending on the terminal setting or encoding. The option display.unicode.ambiguous_as_wide can be used to handle the ambiguity. By default, an “ambiguous” character’s width, such as “¡” (inverted exclamation) in the example below, is taken to be 1. df = pd.DataFrame({\"a\": [\"xxx\", \"¡¡\"], \"b\": [\"yyy\", \"¡¡\"]}) df a b 0 xxx yyy 1 ¡¡ ¡¡ Enabling display.unicode.ambiguous_as_wide makes pandas interpret these characters’ widths to be 2. (Note that this option will only be effective when display.unicode.east_asian_width is enabled.) However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly: pd.set_option(\"display.unicode.ambiguous_as_wide\", True) df a b 0 xxx yyy 1 ¡¡ ¡¡", "prev_chunk_id": "chunk_671", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_673", "url": "https://pandas.pydata.org/docs/user_guide/options.html", "title": "Table schema display#", "page_title": "Options and settings — pandas 2.3.1 documentation", "breadcrumbs": "Table schema display#", "content": "Table schema display# DataFrame and Series will publish a Table Schema representation by default. This can be enabled globally with the display.html.table_schema option: pd.set_option(\"display.html.table_schema\", True) Only 'display.max_rows' are serialized and published.", "prev_chunk_id": "chunk_672", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_674", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Time deltas#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Time deltas#", "content": "Time deltas# Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative. Timedelta is a subclass of datetime.timedelta, and behaves in a similar manner, but allows compatibility with np.timedelta64 types as well as a host of custom representation, parsing, and attributes.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_675", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Parsing#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Parsing#", "content": "Parsing# You can construct a Timedelta scalar through various arguments, including ISO 8601 Duration strings. import datetime # strings pd.Timedelta(\"1 days\") Timedelta('1 days 00:00:00') pd.Timedelta(\"1 days 00:00:00\") Timedelta('1 days 00:00:00') pd.Timedelta(\"1 days 2 hours\") Timedelta('1 days 02:00:00') pd.Timedelta(\"-1 days 2 min 3us\") Timedelta('-2 days +23:57:59.999997') # like datetime.timedelta # note: these MUST be specified as keyword arguments pd.Timedelta(days=1, seconds=1) Timedelta('1 days 00:00:01') # integers with a unit pd.Timedelta(1, unit=\"d\") Timedelta('1 days 00:00:00') # from a datetime.timedelta/np.timedelta64 pd.Timedelta(datetime.timedelta(days=1, seconds=1)) Timedelta('1 days 00:00:01') pd.Timedelta(np.timedelta64(1, \"ms\")) Timedelta('0 days 00:00:00.001000') # negative Timedeltas have this string repr # to be more consistent with datetime.timedelta conventions pd.Timedelta(\"-1us\") Timedelta('-1 days +23:59:59.999999') # a NaT pd.Timedelta(\"nan\") NaT pd.Timedelta(\"nat\") NaT # ISO 8601 Duration strings pd.Timedelta(\"P0DT0H1M0S\") Timedelta('0 days 00:01:00') pd.Timedelta(\"P0DT0H0M0.000000123S\") Timedelta('0 days 00:00:00.000000123') DateOffsets (Day, Hour, Minute, Second, Milli, Micro, Nano) can also be used in construction. pd.Timedelta(pd.offsets.Second(2)) Timedelta('0 days 00:00:02') Further, operations among the scalars yield another scalar Timedelta. pd.Timedelta(pd.offsets.Day(2)) + pd.Timedelta(pd.offsets.Second(2)) + pd.Timedelta( ....: \"00:00:00.000123\" ....: ) ....: Timedelta('2 days 00:00:02.000123')", "prev_chunk_id": "chunk_674", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_676", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "to_timedelta#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "to_timedelta#", "content": "to_timedelta# Using the top-level pd.to_timedelta, you can convert a scalar, array, list, or Series from a recognized timedelta format / value into a Timedelta type. It will construct Series if the input is a Series, a scalar if the input is scalar-like, otherwise it will output a TimedeltaIndex. You can parse a single string to a Timedelta: pd.to_timedelta(\"1 days 06:05:01.00003\") Timedelta('1 days 06:05:01.000030') pd.to_timedelta(\"15.5us\") Timedelta('0 days 00:00:00.000015500') or a list/array of strings: pd.to_timedelta([\"1 days 06:05:01.00003\", \"15.5us\", \"nan\"]) TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT], dtype='timedelta64[ns]', freq=None) The unit keyword argument specifies the unit of the Timedelta if the input is numeric: pd.to_timedelta(np.arange(5), unit=\"s\") TimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03', '0 days 00:00:04'], dtype='timedelta64[ns]', freq=None) pd.to_timedelta(np.arange(5), unit=\"d\") TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None)", "prev_chunk_id": "chunk_675", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_677", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Timedelta limitations#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Timedelta limitations#", "content": "Timedelta limitations# pandas represents Timedeltas in nanosecond resolution using 64 bit integers. As such, the 64 bit integer limits determine the Timedelta limits. pd.Timedelta.min Timedelta('-106752 days +00:12:43.145224193') pd.Timedelta.max Timedelta('106751 days 23:47:16.854775807')", "prev_chunk_id": "chunk_676", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_678", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Operations#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# You can operate on Series/DataFrames and construct timedelta64[ns] Series through subtraction operations on datetime64[ns] Series, or Timestamps. s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\")) td = pd.Series([pd.Timedelta(days=i) for i in range(3)]) df = pd.DataFrame({\"A\": s, \"B\": td}) df A B 0 2012-01-01 0 days 1 2012-01-02 1 days 2 2012-01-03 2 days df[\"C\"] = df[\"A\"] + df[\"B\"] df A B C 0 2012-01-01 0 days 2012-01-01 1 2012-01-02 1 days 2012-01-03 2 2012-01-03 2 days 2012-01-05 df.dtypes A datetime64[ns] B timedelta64[ns] C datetime64[ns] dtype: object s - s.max() 0 -2 days 1 -1 days 2 0 days dtype: timedelta64[ns] s - datetime.datetime(2011, 1, 1, 3, 5) 0 364 days 20:55:00 1 365 days 20:55:00 2 366 days 20:55:00 dtype: timedelta64[ns] s + datetime.timedelta(minutes=5) 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] s + pd.offsets.Minute(5) 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] s + pd.offsets.Minute(5) + pd.offsets.Milli(5) 0 2012-01-01 00:05:00.005 1 2012-01-02 00:05:00.005 2 2012-01-03 00:05:00.005 dtype: datetime64[ns] Operations with scalars from a timedelta64[ns] series: y = s - s[0] y 0 0 days 1 1 days 2 2 days dtype: timedelta64[ns] Series of timedeltas with NaT values are supported: y = s - s.shift() y 0 NaT 1 1 days 2 1 days dtype: timedelta64[ns] Elements can be set to NaT using np.nan analogously to datetimes: y[1] = np.nan y 0 NaT 1 NaT 2 1 days dtype: timedelta64[ns] Operands can also appear in a reversed order (a singular object operated with a Series): s.max() - s 0 2 days 1 1 days 2 0 days dtype: timedelta64[ns] datetime.datetime(2011, 1, 1, 3, 5) - s 0 -365 days +03:05:00 1 -366 days +03:05:00 2 -367 days +03:05:00 dtype: timedelta64[ns] datetime.timedelta(minutes=5) + s 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] min,", "prev_chunk_id": "chunk_677", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_679", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Operations#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "max and the corresponding idxmin, idxmax operations are supported on frames: A = s - pd.Timestamp(\"20120101\") - pd.Timedelta(\"00:05:05\") B = s - pd.Series(pd.date_range(\"2012-1-2\", periods=3, freq=\"D\")) df = pd.DataFrame({\"A\": A, \"B\": B}) df A B 0 -1 days +23:54:55 -1 days 1 0 days 23:54:55 -1 days 2 1 days 23:54:55 -1 days df.min() A -1 days +23:54:55 B -1 days +00:00:00 dtype: timedelta64[ns] df.min(axis=1) 0 -1 days 1 -1 days 2 -1 days dtype: timedelta64[ns] df.idxmin() A 0 B 0 dtype: int64 df.idxmax() A 2 B 0 dtype: int64 min, max, idxmin, idxmax operations are supported on Series as well. A scalar result will be a Timedelta. df.min().max() Timedelta('-1 days +23:54:55') df.min(axis=1).min() Timedelta('-1 days +00:00:00') df.min().idxmax() 'A' df.min(axis=1).idxmin() 0 You can fillna on timedeltas, passing a timedelta to get a particular value. y.fillna(pd.Timedelta(0)) 0 0 days 1 0 days 2 1 days dtype: timedelta64[ns] y.fillna(pd.Timedelta(10, unit=\"s\")) 0 0 days 00:00:10 1 0 days 00:00:10 2 1 days 00:00:00 dtype: timedelta64[ns] y.fillna(pd.Timedelta(\"-1 days, 00:00:05\")) 0 -1 days +00:00:05 1 -1 days +00:00:05 2 1 days 00:00:00 dtype: timedelta64[ns] You can also negate, multiply and use abs on Timedeltas: td1 = pd.Timedelta(\"-1 days 2 hours 3 seconds\") td1 Timedelta('-2 days +21:59:57') -1 * td1 Timedelta('1 days 02:00:03') -td1 Timedelta('1 days 02:00:03') abs(td1) Timedelta('1 days 02:00:03')", "prev_chunk_id": "chunk_678", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_680", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Reductions#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Reductions#", "content": "Reductions# Numeric reduction operation for timedelta64[ns] will return Timedelta objects. As usual NaT are skipped during evaluation. y2 = pd.Series( ....: pd.to_timedelta([\"-1 days +00:00:05\", \"nat\", \"-1 days +00:00:05\", \"1 days\"]) ....: ) ....: y2 0 -1 days +00:00:05 1 NaT 2 -1 days +00:00:05 3 1 days 00:00:00 dtype: timedelta64[ns] y2.mean() Timedelta('-1 days +16:00:03.333333334') y2.median() Timedelta('-1 days +00:00:05') y2.quantile(0.1) Timedelta('-1 days +00:00:05') y2.sum() Timedelta('-1 days +00:00:10')", "prev_chunk_id": "chunk_679", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_681", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Frequency conversion#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Frequency conversion#", "content": "Frequency conversion# Timedelta Series and TimedeltaIndex, and Timedelta can be converted to other frequencies by astyping to a specific timedelta dtype. december = pd.Series(pd.date_range(\"20121201\", periods=4)) january = pd.Series(pd.date_range(\"20130101\", periods=4)) td = january - december td[2] += datetime.timedelta(minutes=5, seconds=3) td[3] = np.nan td 0 31 days 00:00:00 1 31 days 00:00:00 2 31 days 00:05:03 3 NaT dtype: timedelta64[ns] # to seconds td.astype(\"timedelta64[s]\") 0 31 days 00:00:00 1 31 days 00:00:00 2 31 days 00:05:03 3 NaT dtype: timedelta64[s] For timedelta64 resolutions other than the supported “s”, “ms”, “us”, “ns”, an alternative is to divide by another timedelta object. Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division. # to days td / np.timedelta64(1, \"D\") 0 31.000000 1 31.000000 2 31.003507 3 NaN dtype: float64 Dividing or multiplying a timedelta64[ns] Series by an integer or integer Series yields another timedelta64[ns] dtypes Series. td * -1 0 -31 days +00:00:00 1 -31 days +00:00:00 2 -32 days +23:54:57 3 NaT dtype: timedelta64[ns] td * pd.Series([1, 2, 3, 4]) 0 31 days 00:00:00 1 62 days 00:00:00 2 93 days 00:15:09 3 NaT dtype: timedelta64[ns] Rounded division (floor-division) of a timedelta64[ns] Series by a scalar Timedelta gives a series of integers. td // pd.Timedelta(days=3, hours=4) 0 9.0 1 9.0 2 9.0 3 NaN dtype: float64 pd.Timedelta(days=3, hours=4) // td 0 0.0 1 0.0 2 0.0 3 NaN dtype: float64 The mod (%) and divmod operations are defined for Timedelta when operating with another timedelta-like or with a numeric argument. pd.Timedelta(hours=37) % datetime.timedelta(hours=2) Timedelta('0 days 01:00:00') # divmod against a timedelta-like returns a pair (int, Timedelta) divmod(datetime.timedelta(hours=2), pd.Timedelta(minutes=11)) (10, Timedelta('0 days 00:10:00')) # divmod against a numeric returns a pair (Timedelta, Timedelta) divmod(pd.Timedelta(hours=25), 86400000000000) (Timedelta('0 days 00:00:00.000000001'), Timedelta('0 days 01:00:00'))", "prev_chunk_id": "chunk_680", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_682", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Attributes#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Attributes#", "content": "Attributes# You can access various components of the Timedelta or TimedeltaIndex directly using the attributes days,seconds,microseconds,nanoseconds. These are identical to the values returned by datetime.timedelta, in that, for example, the .seconds attribute represents the number of seconds >= 0 and < 1 day. These are signed according to whether the Timedelta is signed. These operations can also be directly accessed via the .dt property of the Series as well. For a Series: td.dt.days 0 31.0 1 31.0 2 31.0 3 NaN dtype: float64 td.dt.seconds 0 0.0 1 0.0 2 303.0 3 NaN dtype: float64 You can access the value of the fields for a scalar Timedelta directly. tds = pd.Timedelta(\"31 days 5 min 3 sec\") tds.days 31 tds.seconds 303 (-tds).seconds 86097 You can use the .components property to access a reduced form of the timedelta. This returns a DataFrame indexed similarly to the Series. These are the displayed values of the Timedelta. td.dt.components days hours minutes seconds milliseconds microseconds nanoseconds 0 31.0 0.0 0.0 0.0 0.0 0.0 0.0 1 31.0 0.0 0.0 0.0 0.0 0.0 0.0 2 31.0 0.0 5.0 3.0 0.0 0.0 0.0 3 NaN NaN NaN NaN NaN NaN NaN td.dt.components.seconds 0 0.0 1 0.0 2 3.0 3 NaN Name: seconds, dtype: float64 You can convert a Timedelta to an ISO 8601 Duration string with the .isoformat method pd.Timedelta( ....: days=6, minutes=50, seconds=3, milliseconds=10, microseconds=10, nanoseconds=12 ....: ).isoformat() ....: 'P6DT0H50M3.010010012S'", "prev_chunk_id": "chunk_681", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_683", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "TimedeltaIndex#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "TimedeltaIndex#", "content": "TimedeltaIndex# To generate an index with time delta, you can use either the TimedeltaIndex or the timedelta_range() constructor. Using TimedeltaIndex you can pass string-like, Timedelta, timedelta, or np.timedelta64 objects. Passing np.nan/pd.NaT/nat will represent missing values. pd.TimedeltaIndex( ....: [ ....: \"1 days\", ....: \"1 days, 00:00:05\", ....: np.timedelta64(2, \"D\"), ....: datetime.timedelta(days=2, seconds=2), ....: ] ....: ) ....: TimedeltaIndex(['1 days 00:00:00', '1 days 00:00:05', '2 days 00:00:00', '2 days 00:00:02'], dtype='timedelta64[ns]', freq=None) The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation: pd.TimedeltaIndex([\"0 days\", \"10 days\", \"20 days\"], freq=\"infer\") TimedeltaIndex(['0 days', '10 days', '20 days'], dtype='timedelta64[ns]', freq='10D')", "prev_chunk_id": "chunk_682", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_684", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Generating ranges of time deltas#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Generating ranges of time deltas#", "content": "Generating ranges of time deltas# Similar to date_range(), you can construct regular ranges of a TimedeltaIndex using timedelta_range(). The default frequency for timedelta_range is calendar day: pd.timedelta_range(start=\"1 days\", periods=5) TimedeltaIndex(['1 days', '2 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq='D') Various combinations of start, end, and periods can be used with timedelta_range: pd.timedelta_range(start=\"1 days\", end=\"5 days\") TimedeltaIndex(['1 days', '2 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq='D') pd.timedelta_range(end=\"10 days\", periods=4) TimedeltaIndex(['7 days', '8 days', '9 days', '10 days'], dtype='timedelta64[ns]', freq='D') The freq parameter can passed a variety of frequency aliases: pd.timedelta_range(start=\"1 days\", end=\"2 days\", freq=\"30min\") TimedeltaIndex(['1 days 00:00:00', '1 days 00:30:00', '1 days 01:00:00', '1 days 01:30:00', '1 days 02:00:00', '1 days 02:30:00', '1 days 03:00:00', '1 days 03:30:00', '1 days 04:00:00', '1 days 04:30:00', '1 days 05:00:00', '1 days 05:30:00', '1 days 06:00:00', '1 days 06:30:00', '1 days 07:00:00', '1 days 07:30:00', '1 days 08:00:00', '1 days 08:30:00', '1 days 09:00:00', '1 days 09:30:00', '1 days 10:00:00', '1 days 10:30:00', '1 days 11:00:00', '1 days 11:30:00', '1 days 12:00:00', '1 days 12:30:00', '1 days 13:00:00', '1 days 13:30:00', '1 days 14:00:00', '1 days 14:30:00', '1 days 15:00:00', '1 days 15:30:00', '1 days 16:00:00', '1 days 16:30:00', '1 days 17:00:00', '1 days 17:30:00', '1 days 18:00:00', '1 days 18:30:00', '1 days 19:00:00', '1 days 19:30:00', '1 days 20:00:00', '1 days 20:30:00', '1 days 21:00:00', '1 days 21:30:00', '1 days 22:00:00', '1 days 22:30:00', '1 days 23:00:00', '1 days 23:30:00', '2 days 00:00:00'], dtype='timedelta64[ns]', freq='30min') pd.timedelta_range(start=\"1 days\", periods=5, freq=\"2D5h\") TimedeltaIndex(['1 days 00:00:00', '3 days 05:00:00', '5 days 10:00:00', '7 days 15:00:00', '9 days 20:00:00'], dtype='timedelta64[ns]', freq='53h') Specifying start, end, and periods will generate a range of evenly spaced timedeltas from start to end inclusively, with periods number of elements in the resulting TimedeltaIndex: pd.timedelta_range(\"0 days\", \"4 days\", periods=5) TimedeltaIndex(['0", "prev_chunk_id": "chunk_683", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_685", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Generating ranges of time deltas#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Generating ranges of time deltas#", "content": "days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None) pd.timedelta_range(\"0 days\", \"4 days\", periods=10) TimedeltaIndex(['0 days 00:00:00', '0 days 10:40:00', '0 days 21:20:00', '1 days 08:00:00', '1 days 18:40:00', '2 days 05:20:00', '2 days 16:00:00', '3 days 02:40:00', '3 days 13:20:00', '4 days 00:00:00'], dtype='timedelta64[ns]', freq=None)", "prev_chunk_id": "chunk_684", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_686", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Using the TimedeltaIndex#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Using the TimedeltaIndex#", "content": "Using the TimedeltaIndex# Similarly to other of the datetime-like indices, DatetimeIndex and PeriodIndex, you can use TimedeltaIndex as the index of pandas objects. s = pd.Series( .....: np.arange(100), .....: index=pd.timedelta_range(\"1 days\", periods=100, freq=\"h\"), .....: ) .....: s 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 .. 4 days 23:00:00 95 5 days 00:00:00 96 5 days 01:00:00 97 5 days 02:00:00 98 5 days 03:00:00 99 Freq: h, Length: 100, dtype: int64 Selections work similarly, with coercion on string-likes and slices: s[\"1 day\":\"2 day\"] 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 .. 2 days 19:00:00 43 2 days 20:00:00 44 2 days 21:00:00 45 2 days 22:00:00 46 2 days 23:00:00 47 Freq: h, Length: 48, dtype: int64 s[\"1 day 01:00:00\"] 1 s[pd.Timedelta(\"1 day 1h\")] 1 Furthermore you can use partial string selection and the range will be inferred: s[\"1 day\":\"1 day 5 hours\"] 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 1 days 05:00:00 5 Freq: h, dtype: int64", "prev_chunk_id": "chunk_685", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_687", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Operations#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Operations#", "content": "Operations# Finally, the combination of TimedeltaIndex with DatetimeIndex allow certain combination operations that are NaT preserving: tdi = pd.TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"]) tdi.to_list() [Timedelta('1 days 00:00:00'), NaT, Timedelta('2 days 00:00:00')] dti = pd.date_range(\"20130101\", periods=3) dti.to_list() [Timestamp('2013-01-01 00:00:00'), Timestamp('2013-01-02 00:00:00'), Timestamp('2013-01-03 00:00:00')] (dti + tdi).to_list() [Timestamp('2013-01-02 00:00:00'), NaT, Timestamp('2013-01-05 00:00:00')] (dti - tdi).to_list() [Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2013-01-01 00:00:00')]", "prev_chunk_id": "chunk_686", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_688", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Conversions#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Conversions#", "content": "Conversions# Similarly to frequency conversion on a Series above, you can convert these indices to yield another Index. tdi / np.timedelta64(1, \"s\") Index([86400.0, nan, 172800.0], dtype='float64') tdi.astype(\"timedelta64[s]\") TimedeltaIndex(['1 days', NaT, '2 days'], dtype='timedelta64[s]', freq=None) Scalars type ops work as well. These can potentially return a different type of index. # adding or timedelta and date -> datelike tdi + pd.Timestamp(\"20130101\") DatetimeIndex(['2013-01-02', 'NaT', '2013-01-03'], dtype='datetime64[ns]', freq=None) # subtraction of a date and a timedelta -> datelike # note that trying to subtract a date from a Timedelta will raise an exception (pd.Timestamp(\"20130101\") - tdi).to_list() [Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2012-12-30 00:00:00')] # timedelta + timedelta -> timedelta tdi + pd.Timedelta(\"10 days\") TimedeltaIndex(['11 days', NaT, '12 days'], dtype='timedelta64[ns]', freq=None) # division can result in a Timedelta if the divisor is an integer tdi / 2 TimedeltaIndex(['0 days 12:00:00', NaT, '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None) # or a float64 Index if the divisor is a Timedelta tdi / tdi[0] Index([1.0, nan, 2.0], dtype='float64')", "prev_chunk_id": "chunk_687", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_689", "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html", "title": "Resampling#", "page_title": "Time deltas — pandas 2.3.1 documentation", "breadcrumbs": "Resampling#", "content": "Resampling# Similar to timeseries resampling, we can resample with a TimedeltaIndex. s.resample(\"D\").mean() 1 days 11.5 2 days 35.5 3 days 59.5 4 days 83.5 5 days 97.5 Freq: D, dtype: float64", "prev_chunk_id": "chunk_688", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_690", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time series / date functionality#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time series / date functionality#", "content": "Time series / date functionality# pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy datetime64 and timedelta64 dtypes, pandas has consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data. For example, pandas supports: Parsing time series information from various sources and formats import datetime dti = pd.to_datetime( ...: [\"1/1/2018\", np.datetime64(\"2018-01-01\"), datetime.datetime(2018, 1, 1)] ...: ) ...: dti DatetimeIndex(['2018-01-01', '2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None) Generate sequences of fixed-frequency dates and time spans dti = pd.date_range(\"2018-01-01\", periods=3, freq=\"h\") dti DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00', '2018-01-01 02:00:00'], dtype='datetime64[ns]', freq='h') Manipulating and converting date times with timezone information dti = dti.tz_localize(\"UTC\") dti DatetimeIndex(['2018-01-01 00:00:00+00:00', '2018-01-01 01:00:00+00:00', '2018-01-01 02:00:00+00:00'], dtype='datetime64[ns, UTC]', freq='h') dti.tz_convert(\"US/Pacific\") DatetimeIndex(['2017-12-31 16:00:00-08:00', '2017-12-31 17:00:00-08:00', '2017-12-31 18:00:00-08:00'], dtype='datetime64[ns, US/Pacific]', freq='h') Resampling or converting a time series to a particular frequency idx = pd.date_range(\"2018-01-01\", periods=5, freq=\"h\") ts = pd.Series(range(len(idx)), index=idx) ts 2018-01-01 00:00:00 0 2018-01-01 01:00:00 1 2018-01-01 02:00:00 2 2018-01-01 03:00:00 3 2018-01-01 04:00:00 4 Freq: h, dtype: int64 ts.resample(\"2h\").mean() 2018-01-01 00:00:00 0.5 2018-01-01 02:00:00 2.5 2018-01-01 04:00:00 4.0 Freq: 2h, dtype: float64 Performing date and time arithmetic with absolute or relative time increments friday = pd.Timestamp(\"2018-01-05\") friday.day_name() 'Friday' # Add 1 day saturday = friday + pd.Timedelta(\"1 day\") saturday.day_name() 'Saturday' # Add 1 business day (Friday --> Monday) monday = friday + pd.offsets.BDay() monday.day_name() 'Monday' pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_691", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Overview#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "Overview# pandas captures 4 general time related concepts: - Date times: A specific date and time with timezone support. Similar todatetime.datetimefrom the standard library. - Time deltas: An absolute time duration. Similar todatetime.timedeltafrom the standard library. - Time spans: A span of time defined by a point in time and its associated frequency. - Date offsets: A relative time duration that respects calendar arithmetic. Similar todateutil.relativedelta.relativedeltafrom thedateutilpackage. Concept | Scalar Class | Array Class | pandas Data Type | Primary Creation Method Date times | Timestamp | DatetimeIndex | datetime64[ns] or datetime64[ns, tz] | to_datetime or date_range Time deltas | Timedelta | TimedeltaIndex | timedelta64[ns] | to_timedelta or timedelta_range Time spans | Period | PeriodIndex | period[freq] | Period or period_range Date offsets | DateOffset | None | None | DateOffset For time series data, it’s conventional to represent the time component in the index of a Series or DataFrame so manipulations can be performed with respect to the time element. pd.Series(range(3), index=pd.date_range(\"2000\", freq=\"D\", periods=3)) 2000-01-01 0 2000-01-02 1 2000-01-03 2 Freq: D, dtype: int64 However, Series and DataFrame can directly also support the time component as data itself. pd.Series(pd.date_range(\"2000\", freq=\"D\", periods=3)) 0 2000-01-01 1 2000-01-02 2 2000-01-03 dtype: datetime64[ns] Series and DataFrame have extended data type support and functionality for datetime, timedelta and Period data when passed into those constructors. DateOffset data however will be stored as object data. pd.Series(pd.period_range(\"1/1/2011\", freq=\"M\", periods=3)) 0 2011-01 1 2011-02 2 2011-03 dtype: period[M] pd.Series([pd.DateOffset(1), pd.DateOffset(2)]) 0 <DateOffset> 1 <2 * DateOffsets> dtype: object pd.Series(pd.date_range(\"1/1/2011\", freq=\"ME\", periods=3)) 0 2011-01-31 1 2011-02-28 2 2011-03-31 dtype: datetime64[ns] Lastly, pandas represents null date times, time deltas, and time spans as NaT which is useful for representing missing or null date like values and behaves similar as np.nan does for float data. pd.Timestamp(pd.NaT) NaT pd.Timedelta(pd.NaT) NaT pd.Period(pd.NaT)", "prev_chunk_id": "chunk_690", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_692", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Overview#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "NaT # Equality acts as np.nan would pd.NaT == pd.NaT False", "prev_chunk_id": "chunk_691", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_693", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Timestamps vs. time spans#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Timestamps vs. time spans#", "content": "Timestamps vs. time spans# Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time. import datetime pd.Timestamp(datetime.datetime(2012, 5, 1)) Timestamp('2012-05-01 00:00:00') pd.Timestamp(\"2012-05-01\") Timestamp('2012-05-01 00:00:00') pd.Timestamp(2012, 5, 1) Timestamp('2012-05-01 00:00:00') However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by Period can be specified explicitly, or inferred from datetime string format. For example: pd.Period(\"2011-01\") Period('2011-01', 'M') pd.Period(\"2012-05\", freq=\"D\") Period('2012-05-01', 'D') Timestamp and Period can serve as an index. Lists of Timestamp and Period are automatically coerced to DatetimeIndex and PeriodIndex respectively. dates = [ ....: pd.Timestamp(\"2012-05-01\"), ....: pd.Timestamp(\"2012-05-02\"), ....: pd.Timestamp(\"2012-05-03\"), ....: ] ....: ts = pd.Series(np.random.randn(3), dates) type(ts.index) pandas.core.indexes.datetimes.DatetimeIndex ts.index DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) ts 2012-05-01 0.469112 2012-05-02 -0.282863 2012-05-03 -1.509059 dtype: float64 periods = [pd.Period(\"2012-01\"), pd.Period(\"2012-02\"), pd.Period(\"2012-03\")] ts = pd.Series(np.random.randn(3), periods) type(ts.index) pandas.core.indexes.period.PeriodIndex ts.index PeriodIndex(['2012-01', '2012-02', '2012-03'], dtype='period[M]') ts 2012-01 -1.135632 2012-02 1.212112 2012-03 -0.173215 Freq: M, dtype: float64 pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of Timestamp and sequences of timestamps using instances of DatetimeIndex. For regular time spans, pandas uses Period objects for scalar values and PeriodIndex for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases.", "prev_chunk_id": "chunk_692", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_694", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Converting to timestamps#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Converting to timestamps#", "content": "Converting to timestamps# To convert a Series or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the to_datetime function. When passed a Series, this returns a Series (with the same index), while a list-like is converted to a DatetimeIndex: pd.to_datetime(pd.Series([\"Jul 31, 2009\", \"Jan 10, 2010\", None])) 0 2009-07-31 1 2010-01-10 2 NaT dtype: datetime64[ns] pd.to_datetime([\"2005/11/23\", \"2010/12/31\"]) DatetimeIndex(['2005-11-23', '2010-12-31'], dtype='datetime64[ns]', freq=None) If you use dates which start with the day first (i.e. European style), you can pass the dayfirst flag: pd.to_datetime([\"04-01-2012 10:00\"], dayfirst=True) DatetimeIndex(['2012-01-04 10:00:00'], dtype='datetime64[ns]', freq=None) pd.to_datetime([\"04-14-2012 10:00\"], dayfirst=True) DatetimeIndex(['2012-04-14 10:00:00'], dtype='datetime64[ns]', freq=None) If you pass a single string to to_datetime, it returns a single Timestamp. Timestamp can also accept string input, but it doesn’t accept string parsing options like dayfirst or format, so use to_datetime if these are required. pd.to_datetime(\"2010/11/12\") Timestamp('2010-11-12 00:00:00') pd.Timestamp(\"2010/11/12\") Timestamp('2010-11-12 00:00:00') You can also use the DatetimeIndex constructor directly: pd.DatetimeIndex([\"2018-01-01\", \"2018-01-03\", \"2018-01-05\"]) DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq=None) The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation: pd.DatetimeIndex([\"2018-01-01\", \"2018-01-03\", \"2018-01-05\"], freq=\"infer\") DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq='2D')", "prev_chunk_id": "chunk_693", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_695", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Providing a format argument#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Providing a format argument#", "content": "Providing a format argument# In addition to the required datetime string, a format argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably. pd.to_datetime(\"2010/11/12\", format=\"%Y/%m/%d\") Timestamp('2010-11-12 00:00:00') pd.to_datetime(\"12-11-2010 00:00\", format=\"%d-%m-%Y %H:%M\") Timestamp('2010-11-12 00:00:00') For more information on the choices available when specifying the format option, see the Python datetime documentation.", "prev_chunk_id": "chunk_694", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_696", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Assembling datetime from multiple DataFrame columns#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Assembling datetime from multiple DataFrame columns#", "content": "Assembling datetime from multiple DataFrame columns# You can also pass a DataFrame of integer or string columns to assemble into a Series of Timestamps. df = pd.DataFrame( ....: {\"year\": [2015, 2016], \"month\": [2, 3], \"day\": [4, 5], \"hour\": [2, 3]} ....: ) ....: pd.to_datetime(df) 0 2015-02-04 02:00:00 1 2016-03-05 03:00:00 dtype: datetime64[ns] You can pass only the columns that you need to assemble. pd.to_datetime(df[[\"year\", \"month\", \"day\"]]) 0 2015-02-04 1 2016-03-05 dtype: datetime64[ns] pd.to_datetime looks for standard designations of the datetime component in the column names, including: - required:year,month,day - optional:hour,minute,second,millisecond,microsecond,nanosecond", "prev_chunk_id": "chunk_695", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_697", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Invalid data#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Invalid data#", "content": "Invalid data# The default behavior, errors='raise', is to raise when unparsable: pd.to_datetime(['2009/07/31', 'asd'], errors='raise') --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[57], line 1 ----> 1 pd.to_datetime(['2009/07/31', 'asd'], errors='raise') File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:1104, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache) 1102 result = _convert_and_box_cache(argc, cache_array) 1103 else: -> 1104 result = convert_listlike(argc, format) 1105 else: 1106 result = convert_listlike(np.array([arg]), format)[0] File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:435, in _convert_listlike_datetimes(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact) 433 # `format` could be inferred, or user didn't ask for mixed-format parsing. 434 if format is not None and format != \"mixed\": --> 435 return _array_strptime_with_fallback(arg, name, utc, format, exact, errors) 437 result, tz_parsed = objects_to_datetime64( 438 arg, 439 dayfirst=dayfirst, (...) 443 allow_object=True, 444 ) 446 if tz_parsed is not None: 447 # We can take a shortcut since the datetime64 numpy array 448 # is in UTC File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:469, in _array_strptime_with_fallback(arg, name, utc, fmt, exact, errors) 458 def _array_strptime_with_fallback( 459 arg, 460 name, (...) 464 errors: str, 465 ) -> Index: 466 \"\"\" 467 Call array_strptime, with fallback behavior depending on 'errors'. 468 \"\"\" --> 469 result, tz_out = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc) 470 if tz_out is not None: 471 unit = np.datetime_data(result.dtype)[0] File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:501, in pandas._libs.tslibs.strptime.array_strptime() File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:451, in pandas._libs.tslibs.strptime.array_strptime() File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:583, in pandas._libs.tslibs.strptime._parse_with_format() ValueError: time data \"asd\" doesn't match format \"%Y/%m/%d\", at position 1. You might want to try: - passing `format` if your strings have a consistent format; - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format; - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this. Pass errors='coerce' to convert unparsable data to NaT (not a time): pd.to_datetime([\"2009/07/31\", \"asd\"], errors=\"coerce\") DatetimeIndex(['2009-07-31', 'NaT'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_696", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_698", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Epoch timestamps#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Epoch timestamps#", "content": "Epoch timestamps# pandas supports converting integer or float epoch times to Timestamp and DatetimeIndex. The default unit is nanoseconds, since that is how Timestamp objects are stored internally. However, epochs are often stored in another unit which can be specified. These are computed from the starting point specified by the origin parameter. pd.to_datetime( ....: [1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit=\"s\" ....: ) ....: DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05', '2012-10-10 18:15:05', '2012-10-11 18:15:05', '2012-10-12 18:15:05'], dtype='datetime64[ns]', freq=None) pd.to_datetime( ....: [1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500], ....: unit=\"ms\", ....: ) ....: DatetimeIndex(['2012-10-08 18:15:05.100000', '2012-10-08 18:15:05.200000', '2012-10-08 18:15:05.300000', '2012-10-08 18:15:05.400000', '2012-10-08 18:15:05.500000'], dtype='datetime64[ns]', freq=None) Constructing a Timestamp or DatetimeIndex with an epoch timestamp with the tz argument specified will raise a ValueError. If you have epochs in wall time in another timezone, you can read the epochs as timezone-naive timestamps and then localize to the appropriate timezone: pd.Timestamp(1262347200000000000).tz_localize(\"US/Pacific\") Timestamp('2010-01-01 12:00:00-0800', tz='US/Pacific') pd.DatetimeIndex([1262347200000000000]).tz_localize(\"US/Pacific\") DatetimeIndex(['2010-01-01 12:00:00-08:00'], dtype='datetime64[ns, US/Pacific]', freq=None)", "prev_chunk_id": "chunk_697", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_699", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "From timestamps to epoch#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "From timestamps to epoch#", "content": "From timestamps to epoch# To invert the operation from above, namely, to convert from a Timestamp to a ‘unix’ epoch: stamps = pd.date_range(\"2012-10-08 18:15:05\", periods=4, freq=\"D\") stamps DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05', '2012-10-10 18:15:05', '2012-10-11 18:15:05'], dtype='datetime64[ns]', freq='D') We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the “unit” (1 second). (stamps - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\") Index([1349720105, 1349806505, 1349892905, 1349979305], dtype='int64')", "prev_chunk_id": "chunk_698", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_700", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Using the origin parameter#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Using the origin parameter#", "content": "Using the origin parameter# Using the origin parameter, one can specify an alternative starting point for creation of a DatetimeIndex. For example, to use 1960-01-01 as the starting date: pd.to_datetime([1, 2, 3], unit=\"D\", origin=pd.Timestamp(\"1960-01-01\")) DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], dtype='datetime64[ns]', freq=None) The default is set at origin='unix', which defaults to 1970-01-01 00:00:00. Commonly called ‘unix epoch’ or POSIX time. pd.to_datetime([1, 2, 3], unit=\"D\") DatetimeIndex(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_699", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_701", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Generating ranges of timestamps#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Generating ranges of timestamps#", "content": "Generating ranges of timestamps# To generate an index with timestamps, you can use either the DatetimeIndex or Index constructor and pass in a list of datetime objects: dates = [ ....: datetime.datetime(2012, 5, 1), ....: datetime.datetime(2012, 5, 2), ....: datetime.datetime(2012, 5, 3), ....: ] ....: # Note the frequency information index = pd.DatetimeIndex(dates) index DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) # Automatically converted to DatetimeIndex index = pd.Index(dates) index DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) In practice this becomes very cumbersome because we often need a very long index with a large number of timestamps. If we need timestamps on a regular frequency, we can use the date_range() and bdate_range() functions to create a DatetimeIndex. The default frequency for date_range is a calendar day while the default for bdate_range is a business day: start = datetime.datetime(2011, 1, 1) end = datetime.datetime(2012, 1, 1) index = pd.date_range(start, end) index DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-08', '2011-01-09', '2011-01-10', ... '2011-12-23', '2011-12-24', '2011-12-25', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30', '2011-12-31', '2012-01-01'], dtype='datetime64[ns]', length=366, freq='D') index = pd.bdate_range(start, end) index DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14', ... '2011-12-19', '2011-12-20', '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30'], dtype='datetime64[ns]', length=260, freq='B') Convenience functions like date_range and bdate_range can utilize a variety of frequency aliases: pd.date_range(start, periods=1000, freq=\"ME\") DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31', '2011-06-30', '2011-07-31', '2011-08-31', '2011-09-30', '2011-10-31', ... '2093-07-31', '2093-08-31', '2093-09-30', '2093-10-31', '2093-11-30', '2093-12-31', '2094-01-31', '2094-02-28', '2094-03-31', '2094-04-30'], dtype='datetime64[ns]', length=1000, freq='ME') pd.bdate_range(start, periods=250, freq=\"BQS\") DatetimeIndex(['2011-01-03', '2011-04-01', '2011-07-01', '2011-10-03', '2012-01-02', '2012-04-02', '2012-07-02', '2012-10-01', '2013-01-01', '2013-04-01', ... '2071-01-01', '2071-04-01', '2071-07-01', '2071-10-01', '2072-01-01', '2072-04-01', '2072-07-01', '2072-10-03', '2073-01-02', '2073-04-03'], dtype='datetime64[ns]', length=250, freq='BQS-JAN') date_range and bdate_range make it easy to generate a range of dates using various combinations of parameters like start, end, periods, and freq. The start and end dates are strictly inclusive, so dates outside", "prev_chunk_id": "chunk_700", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_702", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Generating ranges of timestamps#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Generating ranges of timestamps#", "content": "of those specified will not be generated: pd.date_range(start, end, freq=\"BME\") DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31', '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30'], dtype='datetime64[ns]', freq='BME') pd.date_range(start, end, freq=\"W\") DatetimeIndex(['2011-01-02', '2011-01-09', '2011-01-16', '2011-01-23', '2011-01-30', '2011-02-06', '2011-02-13', '2011-02-20', '2011-02-27', '2011-03-06', '2011-03-13', '2011-03-20', '2011-03-27', '2011-04-03', '2011-04-10', '2011-04-17', '2011-04-24', '2011-05-01', '2011-05-08', '2011-05-15', '2011-05-22', '2011-05-29', '2011-06-05', '2011-06-12', '2011-06-19', '2011-06-26', '2011-07-03', '2011-07-10', '2011-07-17', '2011-07-24', '2011-07-31', '2011-08-07', '2011-08-14', '2011-08-21', '2011-08-28', '2011-09-04', '2011-09-11', '2011-09-18', '2011-09-25', '2011-10-02', '2011-10-09', '2011-10-16', '2011-10-23', '2011-10-30', '2011-11-06', '2011-11-13', '2011-11-20', '2011-11-27', '2011-12-04', '2011-12-11', '2011-12-18', '2011-12-25', '2012-01-01'], dtype='datetime64[ns]', freq='W-SUN') pd.bdate_range(end=end, periods=20) DatetimeIndex(['2011-12-05', '2011-12-06', '2011-12-07', '2011-12-08', '2011-12-09', '2011-12-12', '2011-12-13', '2011-12-14', '2011-12-15', '2011-12-16', '2011-12-19', '2011-12-20', '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30'], dtype='datetime64[ns]', freq='B') pd.bdate_range(start=start, periods=20) DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14', '2011-01-17', '2011-01-18', '2011-01-19', '2011-01-20', '2011-01-21', '2011-01-24', '2011-01-25', '2011-01-26', '2011-01-27', '2011-01-28'], dtype='datetime64[ns]', freq='B') Specifying start, end, and periods will generate a range of evenly spaced dates from start to end inclusively, with periods number of elements in the resulting DatetimeIndex: pd.date_range(\"2018-01-01\", \"2018-01-05\", periods=5) DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05'], dtype='datetime64[ns]', freq=None) pd.date_range(\"2018-01-01\", \"2018-01-05\", periods=10) DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 10:40:00', '2018-01-01 21:20:00', '2018-01-02 08:00:00', '2018-01-02 18:40:00', '2018-01-03 05:20:00', '2018-01-03 16:00:00', '2018-01-04 02:40:00', '2018-01-04 13:20:00', '2018-01-05 00:00:00'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_701", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_703", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Custom frequency ranges#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Custom frequency ranges#", "content": "Custom frequency ranges# bdate_range can also generate a range of custom frequency dates by using the weekmask and holidays parameters. These parameters will only be used if a custom frequency string is passed. weekmask = \"Mon Wed Fri\" holidays = [datetime.datetime(2011, 1, 5), datetime.datetime(2011, 3, 14)] pd.bdate_range(start, end, freq=\"C\", weekmask=weekmask, holidays=holidays) DatetimeIndex(['2011-01-03', '2011-01-07', '2011-01-10', '2011-01-12', '2011-01-14', '2011-01-17', '2011-01-19', '2011-01-21', '2011-01-24', '2011-01-26', ... '2011-12-09', '2011-12-12', '2011-12-14', '2011-12-16', '2011-12-19', '2011-12-21', '2011-12-23', '2011-12-26', '2011-12-28', '2011-12-30'], dtype='datetime64[ns]', length=154, freq='C') pd.bdate_range(start, end, freq=\"CBMS\", weekmask=weekmask) DatetimeIndex(['2011-01-03', '2011-02-02', '2011-03-02', '2011-04-01', '2011-05-02', '2011-06-01', '2011-07-01', '2011-08-01', '2011-09-02', '2011-10-03', '2011-11-02', '2011-12-02'], dtype='datetime64[ns]', freq='CBMS')", "prev_chunk_id": "chunk_702", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_704", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Timestamp limitations#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Timestamp limitations#", "content": "Timestamp limitations# The limits of timestamp representation depend on the chosen resolution. For nanosecond resolution, the time span that can be represented using a 64-bit integer is limited to approximately 584 years: pd.Timestamp.min Timestamp('1677-09-21 00:12:43.145224193') pd.Timestamp.max Timestamp('2262-04-11 23:47:16.854775807') When choosing second-resolution, the available range grows to +/- 2.9e11 years. Different resolutions can be converted to each other through as_unit.", "prev_chunk_id": "chunk_703", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_705", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Indexing#", "content": "Indexing# One of the main uses for DatetimeIndex is as an index for pandas objects. The DatetimeIndex class contains many time series related optimizations: - A large range of dates for various offsets are pre-computed and cached under the hood in order to make generating subsequent date ranges very fast (just have to grab a slice). - Fast shifting using theshiftmethod on pandas objects. - Unioning of overlappingDatetimeIndexobjects with the same frequency is very fast (important for fast data alignment). - Quick access to date fields via properties such asyear,month, etc. - Regularization functions likesnapand very fastasoflogic. DatetimeIndex objects have all the basic functionality of regular Index objects, and a smorgasbord of advanced time series specific methods for easy frequency processing. DatetimeIndex can be used like a regular index and offers all of its intelligent functionality like selection, slicing, etc. rng = pd.date_range(start, end, freq=\"BME\") ts = pd.Series(np.random.randn(len(rng)), index=rng) ts.index DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31', '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30'], dtype='datetime64[ns]', freq='BME') ts[:5].index DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31'], dtype='datetime64[ns]', freq='BME') ts[::2].index DatetimeIndex(['2011-01-31', '2011-03-31', '2011-05-31', '2011-07-29', '2011-09-30', '2011-11-30'], dtype='datetime64[ns]', freq='2BME')", "prev_chunk_id": "chunk_704", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_706", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Partial string indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Partial string indexing#", "content": "Partial string indexing# Dates and strings that parse to timestamps can be passed as indexing parameters: ts[\"1/31/2011\"] 0.11920871129693428 ts[datetime.datetime(2011, 12, 25):] 2011-12-30 0.56702 Freq: BME, dtype: float64 ts[\"10/31/2011\":\"12/31/2011\"] 2011-10-31 0.271860 2011-11-30 -0.424972 2011-12-30 0.567020 Freq: BME, dtype: float64 To provide convenience for accessing longer time series, you can also pass in the year or year and month as strings: ts[\"2011\"] 2011-01-31 0.119209 2011-02-28 -1.044236 2011-03-31 -0.861849 2011-04-29 -2.104569 2011-05-31 -0.494929 2011-06-30 1.071804 2011-07-29 0.721555 2011-08-31 -0.706771 2011-09-30 -1.039575 2011-10-31 0.271860 2011-11-30 -0.424972 2011-12-30 0.567020 Freq: BME, dtype: float64 ts[\"2011-6\"] 2011-06-30 1.071804 Freq: BME, dtype: float64 This type of slicing will work on a DataFrame with a DatetimeIndex as well. Since the partial string selection is a form of label slicing, the endpoints will be included. This would include matching times on an included date: dft = pd.DataFrame( .....: np.random.randn(100000, 1), .....: columns=[\"A\"], .....: index=pd.date_range(\"20130101\", periods=100000, freq=\"min\"), .....: ) .....: dft A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-03-11 10:35:00 -0.747967 2013-03-11 10:36:00 -0.034523 2013-03-11 10:37:00 -0.201754 2013-03-11 10:38:00 -1.509067 2013-03-11 10:39:00 -1.693043 [100000 rows x 1 columns] dft.loc[\"2013\"] A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-03-11 10:35:00 -0.747967 2013-03-11 10:36:00 -0.034523 2013-03-11 10:37:00 -0.201754 2013-03-11 10:38:00 -1.509067 2013-03-11 10:39:00 -1.693043 [100000 rows x 1 columns] This starts on the very first time in the month, and includes the last date and time for the month: dft[\"2013-1\":\"2013-2\"] A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-28 23:55:00 0.850929 2013-02-28 23:56:00 0.976712 2013-02-28 23:57:00 -2.693884 2013-02-28 23:58:00 -1.575535 2013-02-28 23:59:00 -1.573517 [84960 rows x 1 columns] This specifies a stop time that includes all of the times on the last", "prev_chunk_id": "chunk_705", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_707", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Partial string indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Partial string indexing#", "content": "day: dft[\"2013-1\":\"2013-2-28\"] A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-28 23:55:00 0.850929 2013-02-28 23:56:00 0.976712 2013-02-28 23:57:00 -2.693884 2013-02-28 23:58:00 -1.575535 2013-02-28 23:59:00 -1.573517 [84960 rows x 1 columns] This specifies an exact stop time (and is not the same as the above): dft[\"2013-1\":\"2013-2-28 00:00:00\"] A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-27 23:56:00 1.197749 2013-02-27 23:57:00 0.720521 2013-02-27 23:58:00 -0.072718 2013-02-27 23:59:00 -0.681192 2013-02-28 00:00:00 -0.557501 [83521 rows x 1 columns] We are stopping on the included end-point as it is part of the index: dft[\"2013-1-15\":\"2013-1-15 12:30:00\"] A 2013-01-15 00:00:00 -0.984810 2013-01-15 00:01:00 0.941451 2013-01-15 00:02:00 1.559365 2013-01-15 00:03:00 1.034374 2013-01-15 00:04:00 -1.480656 ... ... 2013-01-15 12:26:00 0.371454 2013-01-15 12:27:00 -0.930806 2013-01-15 12:28:00 -0.069177 2013-01-15 12:29:00 0.066510 2013-01-15 12:30:00 -0.003945 [751 rows x 1 columns] DatetimeIndex partial string indexing also works on a DataFrame with a MultiIndex: dft2 = pd.DataFrame( .....: np.random.randn(20, 1), .....: columns=[\"A\"], .....: index=pd.MultiIndex.from_product( .....: [pd.date_range(\"20130101\", periods=10, freq=\"12h\"), [\"a\", \"b\"]] .....: ), .....: ) .....: dft2 A 2013-01-01 00:00:00 a -0.298694 b 0.823553 2013-01-01 12:00:00 a 0.943285 b -1.479399 2013-01-02 00:00:00 a -1.643342 ... ... 2013-01-04 12:00:00 b 0.069036 2013-01-05 00:00:00 a 0.122297 b 1.422060 2013-01-05 12:00:00 a 0.370079 b 1.016331 [20 rows x 1 columns] dft2.loc[\"2013-01-05\"] A 2013-01-05 00:00:00 a 0.122297 b 1.422060 2013-01-05 12:00:00 a 0.370079 b 1.016331 idx = pd.IndexSlice dft2 = dft2.swaplevel(0, 1).sort_index() dft2.loc[idx[:, \"2013-01-05\"], :] A a 2013-01-05 00:00:00 0.122297 2013-01-05 12:00:00 0.370079 b 2013-01-05 00:00:00 1.422060 2013-01-05 12:00:00 1.016331 Slicing with string indexing also honors UTC offset. df = pd.DataFrame([0], index=pd.DatetimeIndex([\"2019-01-01\"], tz=\"US/Pacific\")) df 0 2019-01-01 00:00:00-08:00 0 df[\"2019-01-01 12:00:00+04:00\":\"2019-01-01 13:00:00+04:00\"] 0 2019-01-01 00:00:00-08:00 0", "prev_chunk_id": "chunk_706", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_708", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Slice vs. exact match#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Slice vs. exact match#", "content": "Slice vs. exact match# The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match. Consider a Series object with a minute resolution index: series_minute = pd.Series( .....: [1, 2, 3], .....: pd.DatetimeIndex( .....: [\"2011-12-31 23:59:00\", \"2012-01-01 00:00:00\", \"2012-01-01 00:02:00\"] .....: ), .....: ) .....: series_minute.index.resolution 'minute' A timestamp string less accurate than a minute gives a Series object. series_minute[\"2011-12-31 23\"] 2011-12-31 23:59:00 1 dtype: int64 A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice. series_minute[\"2011-12-31 23:59\"] 1 series_minute[\"2011-12-31 23:59:00\"] 1 If index resolution is second, then the minute-accurate timestamp gives a Series. series_second = pd.Series( .....: [1, 2, 3], .....: pd.DatetimeIndex( .....: [\"2011-12-31 23:59:59\", \"2012-01-01 00:00:00\", \"2012-01-01 00:00:01\"] .....: ), .....: ) .....: series_second.index.resolution 'second' series_second[\"2011-12-31 23:59\"] 2011-12-31 23:59:59 1 dtype: int64 If the timestamp string is treated as a slice, it can be used to index DataFrame with .loc[] as well. dft_minute = pd.DataFrame( .....: {\"a\": [1, 2, 3], \"b\": [4, 5, 6]}, index=series_minute.index .....: ) .....: dft_minute.loc[\"2011-12-31 23\"] a b 2011-12-31 23:59:00 1 4 Note also that DatetimeIndex resolution cannot be less precise than day. series_monthly = pd.Series( .....: [1, 2, 3], pd.DatetimeIndex([\"2011-12\", \"2012-01\", \"2012-02\"]) .....: ) .....: series_monthly.index.resolution 'day' series_monthly[\"2011-12\"] # returns Series 2011-12-01 1 dtype: int64", "prev_chunk_id": "chunk_707", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_709", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Exact indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Exact indexing#", "content": "Exact indexing# As discussed in previous section, indexing a DatetimeIndex with a partial string depends on the “accuracy” of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with Timestamp or datetime objects is exact, because the objects have exact meaning. These also follow the semantics of including both endpoints. These Timestamp and datetime objects have exact hours, minutes, and seconds, even though they were not explicitly specified (they are 0). dft[datetime.datetime(2013, 1, 1): datetime.datetime(2013, 2, 28)] A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-27 23:56:00 1.197749 2013-02-27 23:57:00 0.720521 2013-02-27 23:58:00 -0.072718 2013-02-27 23:59:00 -0.681192 2013-02-28 00:00:00 -0.557501 [83521 rows x 1 columns] With no defaults. dft[ .....: datetime.datetime(2013, 1, 1, 10, 12, 0): datetime.datetime( .....: 2013, 2, 28, 10, 12, 0 .....: ) .....: ] .....: A 2013-01-01 10:12:00 0.565375 2013-01-01 10:13:00 0.068184 2013-01-01 10:14:00 0.788871 2013-01-01 10:15:00 -0.280343 2013-01-01 10:16:00 0.931536 ... ... 2013-02-28 10:08:00 0.148098 2013-02-28 10:09:00 -0.388138 2013-02-28 10:10:00 0.139348 2013-02-28 10:11:00 0.085288 2013-02-28 10:12:00 0.950146 [83521 rows x 1 columns]", "prev_chunk_id": "chunk_708", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_710", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Truncating & fancy indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Truncating & fancy indexing#", "content": "Truncating & fancy indexing# A truncate() convenience function is provided that is similar to slicing. Note that truncate assumes a 0 value for any unspecified date component in a DatetimeIndex in contrast to slicing which returns any partially matching dates: rng2 = pd.date_range(\"2011-01-01\", \"2012-01-01\", freq=\"W\") ts2 = pd.Series(np.random.randn(len(rng2)), index=rng2) ts2.truncate(before=\"2011-11\", after=\"2011-12\") 2011-11-06 0.437823 2011-11-13 -0.293083 2011-11-20 -0.059881 2011-11-27 1.252450 Freq: W-SUN, dtype: float64 ts2[\"2011-11\":\"2011-12\"] 2011-11-06 0.437823 2011-11-13 -0.293083 2011-11-20 -0.059881 2011-11-27 1.252450 2011-12-04 0.046611 2011-12-11 0.059478 2011-12-18 -0.286539 2011-12-25 0.841669 Freq: W-SUN, dtype: float64 Even complicated fancy indexing that breaks the DatetimeIndex frequency regularity will result in a DatetimeIndex, although frequency is lost: ts2.iloc[[0, 2, 6]].index DatetimeIndex(['2011-01-02', '2011-01-16', '2011-02-13'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_709", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_711", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time/date components#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time/date components#", "content": "Time/date components# There are several time/date properties that one can access from Timestamp or a collection of timestamps like a DatetimeIndex. Property | Description year | The year of the datetime month | The month of the datetime day | The days of the datetime hour | The hour of the datetime minute | The minutes of the datetime second | The seconds of the datetime microsecond | The microseconds of the datetime nanosecond | The nanoseconds of the datetime date | Returns datetime.date (does not contain timezone information) time | Returns datetime.time (does not contain timezone information) timetz | Returns datetime.time as local time with timezone information dayofyear | The ordinal day of year day_of_year | The ordinal day of year weekofyear | The week ordinal of the year week | The week ordinal of the year dayofweek | The number of the day of the week with Monday=0, Sunday=6 day_of_week | The number of the day of the week with Monday=0, Sunday=6 weekday | The number of the day of the week with Monday=0, Sunday=6 quarter | Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc. days_in_month | The number of days in the month of the datetime is_month_start | Logical indicating if first day of month (defined by frequency) is_month_end | Logical indicating if last day of month (defined by frequency) is_quarter_start | Logical indicating if first day of quarter (defined by frequency) is_quarter_end | Logical indicating if last day of quarter (defined by frequency) is_year_start | Logical indicating if first day of year (defined by frequency) is_year_end | Logical indicating if last day of year (defined by frequency) is_leap_year | Logical indicating if the date belongs to a leap year Furthermore, if you have a Series with datetimelike values, then you can access these", "prev_chunk_id": "chunk_710", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_712", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time/date components#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time/date components#", "content": "properties via the .dt accessor, as detailed in the section on .dt accessors. You may obtain the year, week and day components of the ISO year from the ISO 8601 standard: idx = pd.date_range(start=\"2019-12-29\", freq=\"D\", periods=4) idx.isocalendar() year week day 2019-12-29 2019 52 7 2019-12-30 2020 1 1 2019-12-31 2020 1 2 2020-01-01 2020 1 3 idx.to_series().dt.isocalendar() year week day 2019-12-29 2019 52 7 2019-12-30 2020 1 1 2019-12-31 2020 1 2 2020-01-01 2020 1 3", "prev_chunk_id": "chunk_711", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_713", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "DateOffset objects#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "DateOffset objects#", "content": "DateOffset objects# In the preceding examples, frequency strings (e.g. 'D') were used to specify a frequency that defined: - how the date times inDatetimeIndexwere spaced when usingdate_range() - the frequency of aPeriodorPeriodIndex These frequency strings map to a DateOffset object and its subclasses. A DateOffset is similar to a Timedelta that represents a duration of time but follows specific calendar duration rules. For example, a Timedelta day will always increment datetimes by 24 hours, while a DateOffset day will increment datetimes to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight savings time. However, all DateOffset subclasses that are an hour or smaller (Hour, Minute, Second, Milli, Micro, Nano) behave like Timedelta and respect absolute time. The basic DateOffset acts similar to dateutil.relativedelta (relativedelta documentation) that shifts a date time by the corresponding calendar duration specified. The arithmetic operator (+) can be used to perform the shift. # This particular day contains a day light savings time transition ts = pd.Timestamp(\"2016-10-30 00:00:00\", tz=\"Europe/Helsinki\") # Respects absolute time ts + pd.Timedelta(days=1) Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki') # Respects calendar time ts + pd.DateOffset(days=1) Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki') friday = pd.Timestamp(\"2018-01-05\") friday.day_name() 'Friday' # Add 2 business days (Friday --> Tuesday) two_business_days = 2 * pd.offsets.BDay() friday + two_business_days Timestamp('2018-01-09 00:00:00') (friday + two_business_days).day_name() 'Tuesday' Most DateOffsets have associated frequencies strings, or offset aliases, that can be passed into freq keyword arguments. The available date offsets and associated frequency strings can be found below: Date Offset | Frequency String | Description DateOffset | None | Generic offset class, defaults to absolute 24 hours BDay or BusinessDay | 'B' | business day (weekday) CDay or CustomBusinessDay | 'C' | custom business day Week | 'W' | one week, optionally anchored on a day of the week WeekOfMonth", "prev_chunk_id": "chunk_712", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_714", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "DateOffset objects#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "DateOffset objects#", "content": "| 'WOM' | the x-th day of the y-th week of each month LastWeekOfMonth | 'LWOM' | the x-th day of the last week of each month MonthEnd | 'ME' | calendar month end MonthBegin | 'MS' | calendar month begin BMonthEnd or BusinessMonthEnd | 'BME' | business month end BMonthBegin or BusinessMonthBegin | 'BMS' | business month begin CBMonthEnd or CustomBusinessMonthEnd | 'CBME' | custom business month end CBMonthBegin or CustomBusinessMonthBegin | 'CBMS' | custom business month begin SemiMonthEnd | 'SME' | 15th (or other day_of_month) and calendar month end SemiMonthBegin | 'SMS' | 15th (or other day_of_month) and calendar month begin QuarterEnd | 'QE' | calendar quarter end QuarterBegin | 'QS' | calendar quarter begin BQuarterEnd | 'BQE | business quarter end BQuarterBegin | 'BQS' | business quarter begin FY5253Quarter | 'REQ' | retail (aka 52-53 week) quarter YearEnd | 'YE' | calendar year end YearBegin | 'YS' or 'BYS' | calendar year begin BYearEnd | 'BYE' | business year end BYearBegin | 'BYS' | business year begin FY5253 | 'RE' | retail (aka 52-53 week) year Easter | None | Easter holiday BusinessHour | 'bh' | business hour CustomBusinessHour | 'cbh' | custom business hour Day | 'D' | one absolute day Hour | 'h' | one hour Minute | 'min' | one minute Second | 's' | one second Milli | 'ms' | one millisecond Micro | 'us' | one microsecond Nano | 'ns' | one nanosecond DateOffsets additionally have rollforward() and rollback() methods for moving a date forward or backward respectively to a valid offset date relative to the offset. For example, business offsets will roll dates that land on the weekends (Saturday and Sunday) forward to Monday since business offsets operate on the weekdays. ts = pd.Timestamp(\"2018-01-06 00:00:00\") ts.day_name() 'Saturday' # BusinessHour's valid offset", "prev_chunk_id": "chunk_713", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_715", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "DateOffset objects#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "DateOffset objects#", "content": "dates are Monday through Friday offset = pd.offsets.BusinessHour(start=\"09:00\") # Bring the date to the closest offset date (Monday) offset.rollforward(ts) Timestamp('2018-01-08 09:00:00') # Date is brought to the closest offset date first and then the hour is added ts + offset Timestamp('2018-01-08 10:00:00') These operations preserve time (hour, minute, etc) information by default. To reset time to midnight, use normalize() before or after applying the operation (depending on whether you want the time information included in the operation). ts = pd.Timestamp(\"2014-01-01 09:00\") day = pd.offsets.Day() day + ts Timestamp('2014-01-02 09:00:00') (day + ts).normalize() Timestamp('2014-01-02 00:00:00') ts = pd.Timestamp(\"2014-01-01 22:00\") hour = pd.offsets.Hour() hour + ts Timestamp('2014-01-01 23:00:00') (hour + ts).normalize() Timestamp('2014-01-01 00:00:00') (hour + pd.Timestamp(\"2014-01-01 23:30\")).normalize() Timestamp('2014-01-02 00:00:00')", "prev_chunk_id": "chunk_714", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_716", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Parametric offsets#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Parametric offsets#", "content": "Parametric offsets# Some of the offsets can be “parameterized” when created to result in different behaviors. For example, the Week offset for generating weekly data accepts a weekday parameter which results in the generated dates always lying on a particular day of the week: d = datetime.datetime(2008, 8, 18, 9, 0) d datetime.datetime(2008, 8, 18, 9, 0) d + pd.offsets.Week() Timestamp('2008-08-25 09:00:00') d + pd.offsets.Week(weekday=4) Timestamp('2008-08-22 09:00:00') (d + pd.offsets.Week(weekday=4)).weekday() 4 d - pd.offsets.Week() Timestamp('2008-08-11 09:00:00') The normalize option will be effective for addition and subtraction. d + pd.offsets.Week(normalize=True) Timestamp('2008-08-25 00:00:00') d - pd.offsets.Week(normalize=True) Timestamp('2008-08-11 00:00:00') Another example is parameterizing YearEnd with the specific ending month: d + pd.offsets.YearEnd() Timestamp('2008-12-31 09:00:00') d + pd.offsets.YearEnd(month=6) Timestamp('2009-06-30 09:00:00')", "prev_chunk_id": "chunk_715", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_717", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Using offsets with Series / DatetimeIndex#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Using offsets with Series / DatetimeIndex#", "content": "Using offsets with Series / DatetimeIndex# Offsets can be used with either a Series or DatetimeIndex to apply the offset to each element. rng = pd.date_range(\"2012-01-01\", \"2012-01-03\") s = pd.Series(rng) rng DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03'], dtype='datetime64[ns]', freq='D') rng + pd.DateOffset(months=2) DatetimeIndex(['2012-03-01', '2012-03-02', '2012-03-03'], dtype='datetime64[ns]', freq=None) s + pd.DateOffset(months=2) 0 2012-03-01 1 2012-03-02 2 2012-03-03 dtype: datetime64[ns] s - pd.DateOffset(months=2) 0 2011-11-01 1 2011-11-02 2 2011-11-03 dtype: datetime64[ns] If the offset class maps directly to a Timedelta (Day, Hour, Minute, Second, Micro, Milli, Nano) it can be used exactly like a Timedelta - see the Timedelta section for more examples. s - pd.offsets.Day(2) 0 2011-12-30 1 2011-12-31 2 2012-01-01 dtype: datetime64[ns] td = s - pd.Series(pd.date_range(\"2011-12-29\", \"2011-12-31\")) td 0 3 days 1 3 days 2 3 days dtype: timedelta64[ns] td + pd.offsets.Minute(15) 0 3 days 00:15:00 1 3 days 00:15:00 2 3 days 00:15:00 dtype: timedelta64[ns] Note that some offsets (such as BQuarterEnd) do not have a vectorized implementation. They can still be used but may calculate significantly slower and will show a PerformanceWarning rng + pd.offsets.BQuarterEnd() DatetimeIndex(['2012-03-30', '2012-03-30', '2012-03-30'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_716", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_718", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Custom business days#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Custom business days#", "content": "Custom business days# The CDay or CustomBusinessDay class provides a parametric BusinessDay class which can be used to create customized business day calendars which account for local holidays and local weekend conventions. As an interesting example, let’s look at Egypt where a Friday-Saturday weekend is observed. weekmask_egypt = \"Sun Mon Tue Wed Thu\" # They also observe International Workers' Day so let's # add that for a couple of years holidays = [ .....: \"2012-05-01\", .....: datetime.datetime(2013, 5, 1), .....: np.datetime64(\"2014-05-01\"), .....: ] .....: bday_egypt = pd.offsets.CustomBusinessDay( .....: holidays=holidays, .....: weekmask=weekmask_egypt, .....: ) .....: dt = datetime.datetime(2013, 4, 30) dt + 2 * bday_egypt Timestamp('2013-05-05 00:00:00') Let’s map to the weekday names: dts = pd.date_range(dt, periods=5, freq=bday_egypt) pd.Series(dts.weekday, dts).map(pd.Series(\"Mon Tue Wed Thu Fri Sat Sun\".split())) 2013-04-30 Tue 2013-05-02 Thu 2013-05-05 Sun 2013-05-06 Mon 2013-05-07 Tue Freq: C, dtype: object Holiday calendars can be used to provide the list of holidays. See the holiday calendar section for more information. from pandas.tseries.holiday import USFederalHolidayCalendar bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar()) # Friday before MLK Day dt = datetime.datetime(2014, 1, 17) # Tuesday after MLK Day (Monday is skipped because it's a holiday) dt + bday_us Timestamp('2014-01-21 00:00:00') Monthly offsets that respect a certain holiday calendar can be defined in the usual way. bmth_us = pd.offsets.CustomBusinessMonthBegin(calendar=USFederalHolidayCalendar()) # Skip new years dt = datetime.datetime(2013, 12, 17) dt + bmth_us Timestamp('2014-01-02 00:00:00') # Define date index with custom offset pd.date_range(start=\"20100101\", end=\"20120101\", freq=bmth_us) DatetimeIndex(['2010-01-04', '2010-02-01', '2010-03-01', '2010-04-01', '2010-05-03', '2010-06-01', '2010-07-01', '2010-08-02', '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01', '2011-01-03', '2011-02-01', '2011-03-01', '2011-04-01', '2011-05-02', '2011-06-01', '2011-07-01', '2011-08-01', '2011-09-01', '2011-10-03', '2011-11-01', '2011-12-01'], dtype='datetime64[ns]', freq='CBMS')", "prev_chunk_id": "chunk_717", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_719", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Business hour#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Business hour#", "content": "Business hour# The BusinessHour class provides a business hour representation on BusinessDay, allowing to use specific start and end times. By default, BusinessHour uses 9:00 - 17:00 as business hours. Adding BusinessHour will increment Timestamp by hourly frequency. If target Timestamp is out of business hours, move to the next business hour then increment it. If the result exceeds the business hours end, the remaining hours are added to the next business day. bh = pd.offsets.BusinessHour() bh <BusinessHour: bh=09:00-17:00> # 2014-08-01 is Friday pd.Timestamp(\"2014-08-01 10:00\").weekday() 4 pd.Timestamp(\"2014-08-01 10:00\") + bh Timestamp('2014-08-01 11:00:00') # Below example is the same as: pd.Timestamp('2014-08-01 09:00') + bh pd.Timestamp(\"2014-08-01 08:00\") + bh Timestamp('2014-08-01 10:00:00') # If the results is on the end time, move to the next business day pd.Timestamp(\"2014-08-01 16:00\") + bh Timestamp('2014-08-04 09:00:00') # Remainings are added to the next day pd.Timestamp(\"2014-08-01 16:30\") + bh Timestamp('2014-08-04 09:30:00') # Adding 2 business hours pd.Timestamp(\"2014-08-01 10:00\") + pd.offsets.BusinessHour(2) Timestamp('2014-08-01 12:00:00') # Subtracting 3 business hours pd.Timestamp(\"2014-08-01 10:00\") + pd.offsets.BusinessHour(-3) Timestamp('2014-07-31 15:00:00') You can also specify start and end time by keywords. The argument must be a str with an hour:minute representation or a datetime.time instance. Specifying seconds, microseconds and nanoseconds as business hour results in ValueError. bh = pd.offsets.BusinessHour(start=\"11:00\", end=datetime.time(20, 0)) bh <BusinessHour: bh=11:00-20:00> pd.Timestamp(\"2014-08-01 13:00\") + bh Timestamp('2014-08-01 14:00:00') pd.Timestamp(\"2014-08-01 09:00\") + bh Timestamp('2014-08-01 12:00:00') pd.Timestamp(\"2014-08-01 18:00\") + bh Timestamp('2014-08-01 19:00:00') Passing start time later than end represents midnight business hour. In this case, business hour exceeds midnight and overlap to the next day. Valid business hours are distinguished by whether it started from valid BusinessDay. bh = pd.offsets.BusinessHour(start=\"17:00\", end=\"09:00\") bh <BusinessHour: bh=17:00-09:00> pd.Timestamp(\"2014-08-01 17:00\") + bh Timestamp('2014-08-01 18:00:00') pd.Timestamp(\"2014-08-01 23:00\") + bh Timestamp('2014-08-02 00:00:00') # Although 2014-08-02 is Saturday, # it is valid because it starts from 08-01 (Friday). pd.Timestamp(\"2014-08-02 04:00\") + bh", "prev_chunk_id": "chunk_718", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_720", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Business hour#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Business hour#", "content": "Timestamp('2014-08-02 05:00:00') # Although 2014-08-04 is Monday, # it is out of business hours because it starts from 08-03 (Sunday). pd.Timestamp(\"2014-08-04 04:00\") + bh Timestamp('2014-08-04 18:00:00') Applying BusinessHour.rollforward and rollback to out of business hours results in the next business hour start or previous day’s end. Different from other offsets, BusinessHour.rollforward may output different results from apply by definition. This is because one day’s business hour end is equal to next day’s business hour start. For example, under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between 2014-08-01 17:00 and 2014-08-04 09:00. # This adjusts a Timestamp to business hour edge pd.offsets.BusinessHour().rollback(pd.Timestamp(\"2014-08-02 15:00\")) Timestamp('2014-08-01 17:00:00') pd.offsets.BusinessHour().rollforward(pd.Timestamp(\"2014-08-02 15:00\")) Timestamp('2014-08-04 09:00:00') # It is the same as BusinessHour() + pd.Timestamp('2014-08-01 17:00'). # And it is the same as BusinessHour() + pd.Timestamp('2014-08-04 09:00') pd.offsets.BusinessHour() + pd.Timestamp(\"2014-08-02 15:00\") Timestamp('2014-08-04 10:00:00') # BusinessDay results (for reference) pd.offsets.BusinessHour().rollforward(pd.Timestamp(\"2014-08-02\")) Timestamp('2014-08-04 09:00:00') # It is the same as BusinessDay() + pd.Timestamp('2014-08-01') # The result is the same as rollworward because BusinessDay never overlap. pd.offsets.BusinessHour() + pd.Timestamp(\"2014-08-02\") Timestamp('2014-08-04 10:00:00') BusinessHour regards Saturday and Sunday as holidays. To use arbitrary holidays, you can use CustomBusinessHour offset, as explained in the following subsection.", "prev_chunk_id": "chunk_719", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_721", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Custom business hour#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Custom business hour#", "content": "Custom business hour# The CustomBusinessHour is a mixture of BusinessHour and CustomBusinessDay which allows you to specify arbitrary holidays. CustomBusinessHour works as the same as BusinessHour except that it skips specified custom holidays. from pandas.tseries.holiday import USFederalHolidayCalendar bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar()) # Friday before MLK Day dt = datetime.datetime(2014, 1, 17, 15) dt + bhour_us Timestamp('2014-01-17 16:00:00') # Tuesday after MLK Day (Monday is skipped because it's a holiday) dt + bhour_us * 2 Timestamp('2014-01-21 09:00:00') You can use keyword arguments supported by either BusinessHour and CustomBusinessDay. bhour_mon = pd.offsets.CustomBusinessHour(start=\"10:00\", weekmask=\"Tue Wed Thu Fri\") # Monday is skipped because it's a holiday, business hour starts from 10:00 dt + bhour_mon * 2 Timestamp('2014-01-21 10:00:00')", "prev_chunk_id": "chunk_720", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_722", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Offset aliases#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Offset aliases#", "content": "Offset aliases# A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as offset aliases. Alias | Description B | business day frequency C | custom business day frequency D | calendar day frequency W | weekly frequency ME | month end frequency SME | semi-month end frequency (15th and end of month) BME | business month end frequency CBME | custom business month end frequency MS | month start frequency SMS | semi-month start frequency (1st and 15th) BMS | business month start frequency CBMS | custom business month start frequency QE | quarter end frequency BQE | business quarter end frequency QS | quarter start frequency BQS | business quarter start frequency YE | year end frequency BYE | business year end frequency YS | year start frequency BYS | business year start frequency h | hourly frequency bh | business hour frequency cbh | custom business hour frequency min | minutely frequency s | secondly frequency ms | milliseconds us | microseconds ns | nanoseconds", "prev_chunk_id": "chunk_721", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_723", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Period aliases#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Period aliases#", "content": "Period aliases# A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as period aliases. Alias | Description B | business day frequency D | calendar day frequency W | weekly frequency M | monthly frequency Q | quarterly frequency Y | yearly frequency h | hourly frequency min | minutely frequency s | secondly frequency ms | milliseconds us | microseconds ns | nanoseconds", "prev_chunk_id": "chunk_722", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_724", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Combining aliases#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Combining aliases#", "content": "Combining aliases# As we have seen previously, the alias and the offset instance are fungible in most functions: pd.date_range(start, periods=5, freq=\"B\") DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B') pd.date_range(start, periods=5, freq=pd.offsets.BDay()) DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B') You can combine together day and intraday offsets: pd.date_range(start, periods=10, freq=\"2h20min\") DatetimeIndex(['2011-01-01 00:00:00', '2011-01-01 02:20:00', '2011-01-01 04:40:00', '2011-01-01 07:00:00', '2011-01-01 09:20:00', '2011-01-01 11:40:00', '2011-01-01 14:00:00', '2011-01-01 16:20:00', '2011-01-01 18:40:00', '2011-01-01 21:00:00'], dtype='datetime64[ns]', freq='140min') pd.date_range(start, periods=10, freq=\"1D10us\") DatetimeIndex([ '2011-01-01 00:00:00', '2011-01-02 00:00:00.000010', '2011-01-03 00:00:00.000020', '2011-01-04 00:00:00.000030', '2011-01-05 00:00:00.000040', '2011-01-06 00:00:00.000050', '2011-01-07 00:00:00.000060', '2011-01-08 00:00:00.000070', '2011-01-09 00:00:00.000080', '2011-01-10 00:00:00.000090'], dtype='datetime64[ns]', freq='86400000010us')", "prev_chunk_id": "chunk_723", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_725", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Anchored offsets#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Anchored offsets#", "content": "Anchored offsets# For some frequencies you can specify an anchoring suffix: Alias | Description W-SUN | weekly frequency (Sundays). Same as ‘W’ W-MON | weekly frequency (Mondays) W-TUE | weekly frequency (Tuesdays) W-WED | weekly frequency (Wednesdays) W-THU | weekly frequency (Thursdays) W-FRI | weekly frequency (Fridays) W-SAT | weekly frequency (Saturdays) (B)Q(E)(S)-DEC | quarterly frequency, year ends in December. Same as ‘QE’ (B)Q(E)(S)-JAN | quarterly frequency, year ends in January (B)Q(E)(S)-FEB | quarterly frequency, year ends in February (B)Q(E)(S)-MAR | quarterly frequency, year ends in March (B)Q(E)(S)-APR | quarterly frequency, year ends in April (B)Q(E)(S)-MAY | quarterly frequency, year ends in May (B)Q(E)(S)-JUN | quarterly frequency, year ends in June (B)Q(E)(S)-JUL | quarterly frequency, year ends in July (B)Q(E)(S)-AUG | quarterly frequency, year ends in August (B)Q(E)(S)-SEP | quarterly frequency, year ends in September (B)Q(E)(S)-OCT | quarterly frequency, year ends in October (B)Q(E)(S)-NOV | quarterly frequency, year ends in November (B)Y(E)(S)-DEC | annual frequency, anchored end of December. Same as ‘YE’ (B)Y(E)(S)-JAN | annual frequency, anchored end of January (B)Y(E)(S)-FEB | annual frequency, anchored end of February (B)Y(E)(S)-MAR | annual frequency, anchored end of March (B)Y(E)(S)-APR | annual frequency, anchored end of April (B)Y(E)(S)-MAY | annual frequency, anchored end of May (B)Y(E)(S)-JUN | annual frequency, anchored end of June (B)Y(E)(S)-JUL | annual frequency, anchored end of July (B)Y(E)(S)-AUG | annual frequency, anchored end of August (B)Y(E)(S)-SEP | annual frequency, anchored end of September (B)Y(E)(S)-OCT | annual frequency, anchored end of October (B)Y(E)(S)-NOV | annual frequency, anchored end of November These can be used as arguments to date_range, bdate_range, constructors for DatetimeIndex, as well as various other timeseries-related functions in pandas.", "prev_chunk_id": "chunk_724", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_726", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Anchored offset semantics#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Anchored offset semantics#", "content": "Anchored offset semantics# For those offsets that are anchored to the start or end of specific frequency (MonthEnd, MonthBegin, WeekEnd, etc), the following rules apply to rolling forward and backwards. When n is not 0, if the given date is not on an anchor point, it snapped to the next(previous) anchor point, and moved |n|-1 additional steps forwards or backwards. pd.Timestamp(\"2014-01-02\") + pd.offsets.MonthBegin(n=1) Timestamp('2014-02-01 00:00:00') pd.Timestamp(\"2014-01-02\") + pd.offsets.MonthEnd(n=1) Timestamp('2014-01-31 00:00:00') pd.Timestamp(\"2014-01-02\") - pd.offsets.MonthBegin(n=1) Timestamp('2014-01-01 00:00:00') pd.Timestamp(\"2014-01-02\") - pd.offsets.MonthEnd(n=1) Timestamp('2013-12-31 00:00:00') pd.Timestamp(\"2014-01-02\") + pd.offsets.MonthBegin(n=4) Timestamp('2014-05-01 00:00:00') pd.Timestamp(\"2014-01-02\") - pd.offsets.MonthBegin(n=4) Timestamp('2013-10-01 00:00:00') If the given date is on an anchor point, it is moved |n| points forwards or backwards. pd.Timestamp(\"2014-01-01\") + pd.offsets.MonthBegin(n=1) Timestamp('2014-02-01 00:00:00') pd.Timestamp(\"2014-01-31\") + pd.offsets.MonthEnd(n=1) Timestamp('2014-02-28 00:00:00') pd.Timestamp(\"2014-01-01\") - pd.offsets.MonthBegin(n=1) Timestamp('2013-12-01 00:00:00') pd.Timestamp(\"2014-01-31\") - pd.offsets.MonthEnd(n=1) Timestamp('2013-12-31 00:00:00') pd.Timestamp(\"2014-01-01\") + pd.offsets.MonthBegin(n=4) Timestamp('2014-05-01 00:00:00') pd.Timestamp(\"2014-01-31\") - pd.offsets.MonthBegin(n=4) Timestamp('2013-10-01 00:00:00') For the case when n=0, the date is not moved if on an anchor point, otherwise it is rolled forward to the next anchor point. pd.Timestamp(\"2014-01-02\") + pd.offsets.MonthBegin(n=0) Timestamp('2014-02-01 00:00:00') pd.Timestamp(\"2014-01-02\") + pd.offsets.MonthEnd(n=0) Timestamp('2014-01-31 00:00:00') pd.Timestamp(\"2014-01-01\") + pd.offsets.MonthBegin(n=0) Timestamp('2014-01-01 00:00:00') pd.Timestamp(\"2014-01-31\") + pd.offsets.MonthEnd(n=0) Timestamp('2014-01-31 00:00:00')", "prev_chunk_id": "chunk_725", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_727", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Holidays / holiday calendars#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Holidays / holiday calendars#", "content": "Holidays / holiday calendars# Holidays and calendars provide a simple way to define holiday rules to be used with CustomBusinessDay or in other analysis that requires a predefined set of holidays. The AbstractHolidayCalendar class provides all the necessary methods to return a list of holidays and only rules need to be defined in a specific holiday calendar class. Furthermore, the start_date and end_date class attributes determine over what date range holidays are generated. These should be overwritten on the AbstractHolidayCalendar class to have the range apply to all calendar subclasses. USFederalHolidayCalendar is the only calendar that exists and primarily serves as an example for developing other calendars. For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an observance rule determines when that holiday is observed if it falls on a weekend or some other non-observed day. Defined observance rules are: Rule | Description nearest_workday | move Saturday to Friday and Sunday to Monday sunday_to_monday | move Sunday to following Monday next_monday_or_tuesday | move Saturday to Monday and Sunday/Monday to Tuesday previous_friday | move Saturday and Sunday to previous Friday” next_monday | move Saturday and Sunday to following Monday An example of how holidays and holiday calendars are defined: from pandas.tseries.holiday import ( .....: Holiday, .....: USMemorialDay, .....: AbstractHolidayCalendar, .....: nearest_workday, .....: MO, .....: ) .....: class ExampleCalendar(AbstractHolidayCalendar): .....: rules = [ .....: USMemorialDay, .....: Holiday(\"July 4th\", month=7, day=4, observance=nearest_workday), .....: Holiday( .....: \"Columbus Day\", .....: month=10, .....: day=1, .....: offset=pd.DateOffset(weekday=MO(2)), .....: ), .....: ] .....: cal = ExampleCalendar() cal.holidays(datetime.datetime(2012, 1, 1), datetime.datetime(2012, 12, 31)) DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype='datetime64[ns]', freq=None) Using this calendar, creating an index or doing offset arithmetic skips weekends and holidays (i.e., Memorial Day/July 4th). For example, the below defines a custom business day offset using the ExampleCalendar. Like any other offset, it", "prev_chunk_id": "chunk_726", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_728", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Holidays / holiday calendars#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Holidays / holiday calendars#", "content": "can be used to create a DatetimeIndex or added to datetime or Timestamp objects. pd.date_range( .....: start=\"7/1/2012\", end=\"7/10/2012\", freq=pd.offsets.CDay(calendar=cal) .....: ).to_pydatetime() .....: array([datetime.datetime(2012, 7, 2, 0, 0), datetime.datetime(2012, 7, 3, 0, 0), datetime.datetime(2012, 7, 5, 0, 0), datetime.datetime(2012, 7, 6, 0, 0), datetime.datetime(2012, 7, 9, 0, 0), datetime.datetime(2012, 7, 10, 0, 0)], dtype=object) offset = pd.offsets.CustomBusinessDay(calendar=cal) datetime.datetime(2012, 5, 25) + offset Timestamp('2012-05-29 00:00:00') datetime.datetime(2012, 7, 3) + offset Timestamp('2012-07-05 00:00:00') datetime.datetime(2012, 7, 3) + 2 * offset Timestamp('2012-07-06 00:00:00') datetime.datetime(2012, 7, 6) + offset Timestamp('2012-07-09 00:00:00') Ranges are defined by the start_date and end_date class attributes of AbstractHolidayCalendar. The defaults are shown below. AbstractHolidayCalendar.start_date Timestamp('1970-01-01 00:00:00') AbstractHolidayCalendar.end_date Timestamp('2200-12-31 00:00:00') These dates can be overwritten by setting the attributes as datetime/Timestamp/string. AbstractHolidayCalendar.start_date = datetime.datetime(2012, 1, 1) AbstractHolidayCalendar.end_date = datetime.datetime(2012, 12, 31) cal.holidays() DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype='datetime64[ns]', freq=None) Every calendar class is accessible by name using the get_calendar function which returns a holiday class instance. Any imported calendar class will automatically be available by this function. Also, HolidayCalendarFactory provides an easy interface to create calendars that are combinations of calendars or calendars with additional rules. from pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, USLaborDay cal = get_calendar(\"ExampleCalendar\") cal.rules [Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>), Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at 0x7f10467548b0>), Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)] new_cal = HolidayCalendarFactory(\"NewExampleCalendar\", cal, USLaborDay) new_cal.rules [Holiday: Labor Day (month=9, day=1, offset=<DateOffset: weekday=MO(+1)>), Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>), Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at 0x7f10467548b0>), Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)]", "prev_chunk_id": "chunk_727", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_729", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Shifting / lagging#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Shifting / lagging#", "content": "Shifting / lagging# One may want to shift or lag the values in a time series back and forward in time. The method for this is shift(), which is available on all of the pandas objects. ts = pd.Series(range(len(rng)), index=rng) ts = ts[:5] ts.shift(1) 2012-01-01 NaN 2012-01-02 0.0 2012-01-03 1.0 Freq: D, dtype: float64 The shift method accepts an freq argument which can accept a DateOffset class or other timedelta-like object or also an offset alias. When freq is specified, shift method changes all the dates in the index rather than changing the alignment of the data and the index: ts.shift(5, freq=\"D\") 2012-01-06 0 2012-01-07 1 2012-01-08 2 Freq: D, dtype: int64 ts.shift(5, freq=pd.offsets.BDay()) 2012-01-06 0 2012-01-09 1 2012-01-10 2 dtype: int64 ts.shift(5, freq=\"BME\") 2012-05-31 0 2012-05-31 1 2012-05-31 2 dtype: int64 Note that with when freq is specified, the leading entry is no longer NaN because the data is not being realigned.", "prev_chunk_id": "chunk_728", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_730", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Frequency conversion#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Frequency conversion#", "content": "Frequency conversion# The primary function for changing frequencies is the asfreq() method. For a DatetimeIndex, this is basically just a thin, but convenient wrapper around reindex() which generates a date_range and calls reindex. dr = pd.date_range(\"1/1/2010\", periods=3, freq=3 * pd.offsets.BDay()) ts = pd.Series(np.random.randn(3), index=dr) ts 2010-01-01 1.494522 2010-01-06 -0.778425 2010-01-11 -0.253355 Freq: 3B, dtype: float64 ts.asfreq(pd.offsets.BDay()) 2010-01-01 1.494522 2010-01-04 NaN 2010-01-05 NaN 2010-01-06 -0.778425 2010-01-07 NaN 2010-01-08 NaN 2010-01-11 -0.253355 Freq: B, dtype: float64 asfreq provides a further convenience so you can specify an interpolation method for any gaps that may appear after the frequency conversion. ts.asfreq(pd.offsets.BDay(), method=\"pad\") 2010-01-01 1.494522 2010-01-04 1.494522 2010-01-05 1.494522 2010-01-06 -0.778425 2010-01-07 -0.778425 2010-01-08 -0.778425 2010-01-11 -0.253355 Freq: B, dtype: float64", "prev_chunk_id": "chunk_729", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_731", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Filling forward / backward#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Filling forward / backward#", "content": "Filling forward / backward# Related to asfreq and reindex is fillna(), which is documented in the missing data section.", "prev_chunk_id": "chunk_730", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_732", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Converting to Python datetimes#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Converting to Python datetimes#", "content": "Converting to Python datetimes# DatetimeIndex can be converted to an array of Python native datetime.datetime objects using the to_pydatetime method.", "prev_chunk_id": "chunk_731", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_733", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Resampling#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Resampling#", "content": "Resampling# pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. resample() is a time-based groupby, followed by a reduction method on each of its groups. See some cookbook examples for some advanced strategies. The resample() method can be used directly from DataFrameGroupBy objects, see the groupby docs.", "prev_chunk_id": "chunk_732", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_734", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Basics#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Basics#", "content": "Basics# rng = pd.date_range(\"1/1/2012\", periods=100, freq=\"s\") ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample(\"5Min\").sum() 2012-01-01 25103 Freq: 5min, dtype: int64 The resample function is very flexible and allows you to specify many different parameters to control the frequency conversion and resampling operation. Any built-in method available via GroupBy is available as a method of the returned object, including sum, mean, std, sem, max, min, median, first, last, ohlc: ts.resample(\"5Min\").mean() 2012-01-01 251.03 Freq: 5min, dtype: float64 ts.resample(\"5Min\").ohlc() open high low close 2012-01-01 308 460 9 205 ts.resample(\"5Min\").max() 2012-01-01 460 Freq: 5min, dtype: int64 For downsampling, closed can be set to ‘left’ or ‘right’ to specify which end of the interval is closed: ts.resample(\"5Min\", closed=\"right\").mean() 2011-12-31 23:55:00 308.000000 2012-01-01 00:00:00 250.454545 Freq: 5min, dtype: float64 ts.resample(\"5Min\", closed=\"left\").mean() 2012-01-01 251.03 Freq: 5min, dtype: float64 Parameters like label are used to manipulate the resulting labels. label specifies whether the result is labeled with the beginning or the end of the interval. ts.resample(\"5Min\").mean() # by default label='left' 2012-01-01 251.03 Freq: 5min, dtype: float64 ts.resample(\"5Min\", label=\"left\").mean() 2012-01-01 251.03 Freq: 5min, dtype: float64 The axis parameter can be set to 0 or 1 and allows you to resample the specified axis for a DataFrame. kind can be set to ‘timestamp’ or ‘period’ to convert the resulting index to/from timestamp and time span representations. By default resample retains the input representation. convention can be set to ‘start’ or ‘end’ when resampling period data (detail below). It specifies how low frequency periods are converted to higher frequency periods.", "prev_chunk_id": "chunk_733", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_735", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Upsampling#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Upsampling#", "content": "Upsampling# For upsampling, you can specify a way to upsample and the limit parameter to interpolate over the gaps that are created: # from secondly to every 250 milliseconds ts[:2].resample(\"250ms\").asfreq() 2012-01-01 00:00:00.000 308.0 2012-01-01 00:00:00.250 NaN 2012-01-01 00:00:00.500 NaN 2012-01-01 00:00:00.750 NaN 2012-01-01 00:00:01.000 204.0 Freq: 250ms, dtype: float64 ts[:2].resample(\"250ms\").ffill() 2012-01-01 00:00:00.000 308 2012-01-01 00:00:00.250 308 2012-01-01 00:00:00.500 308 2012-01-01 00:00:00.750 308 2012-01-01 00:00:01.000 204 Freq: 250ms, dtype: int64 ts[:2].resample(\"250ms\").ffill(limit=2) 2012-01-01 00:00:00.000 308.0 2012-01-01 00:00:00.250 308.0 2012-01-01 00:00:00.500 308.0 2012-01-01 00:00:00.750 NaN 2012-01-01 00:00:01.000 204.0 Freq: 250ms, dtype: float64", "prev_chunk_id": "chunk_734", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_736", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Sparse resampling#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Sparse resampling#", "content": "Sparse resampling# Sparse timeseries are the ones where you have a lot fewer points relative to the amount of time you are looking to resample. Naively upsampling a sparse series can potentially generate lots of intermediate values. When you don’t want to use a method to fill these values, e.g. fill_method is None, then intermediate values will be filled with NaN. Since resample is a time-based groupby, the following is a method to efficiently resample only the groups that are not all NaN. rng = pd.date_range(\"2014-1-1\", periods=100, freq=\"D\") + pd.Timedelta(\"1s\") ts = pd.Series(range(100), index=rng) If we want to resample to the full range of the series: ts.resample(\"3min\").sum() 2014-01-01 00:00:00 0 2014-01-01 00:03:00 0 2014-01-01 00:06:00 0 2014-01-01 00:09:00 0 2014-01-01 00:12:00 0 .. 2014-04-09 23:48:00 0 2014-04-09 23:51:00 0 2014-04-09 23:54:00 0 2014-04-09 23:57:00 0 2014-04-10 00:00:00 99 Freq: 3min, Length: 47521, dtype: int64 We can instead only resample those groups where we have points as follows: from functools import partial from pandas.tseries.frequencies import to_offset def round(t, freq): .....: freq = to_offset(freq) .....: td = pd.Timedelta(freq) .....: return pd.Timestamp((t.value // td.value) * td.value) .....: ts.groupby(partial(round, freq=\"3min\")).sum() 2014-01-01 0 2014-01-02 1 2014-01-03 2 2014-01-04 3 2014-01-05 4 .. 2014-04-06 95 2014-04-07 96 2014-04-08 97 2014-04-09 98 2014-04-10 99 Length: 100, dtype: int64", "prev_chunk_id": "chunk_735", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_737", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Aggregation#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aggregation#", "content": "Aggregation# The resample() method returns a pandas.api.typing.Resampler instance. Similar to the aggregating API, groupby API, and the window API, a Resampler can be selectively resampled. Resampling a DataFrame, the default will be to act on all columns with the same function. df = pd.DataFrame( .....: np.random.randn(1000, 3), .....: index=pd.date_range(\"1/1/2012\", freq=\"s\", periods=1000), .....: columns=[\"A\", \"B\", \"C\"], .....: ) .....: r = df.resample(\"3min\") r.mean() A B C 2012-01-01 00:00:00 -0.033823 -0.121514 -0.081447 2012-01-01 00:03:00 0.056909 0.146731 -0.024320 2012-01-01 00:06:00 -0.058837 0.047046 -0.052021 2012-01-01 00:09:00 0.063123 -0.026158 -0.066533 2012-01-01 00:12:00 0.186340 -0.003144 0.074752 2012-01-01 00:15:00 -0.085954 -0.016287 -0.050046 We can select a specific column or columns using standard getitem. r[\"A\"].mean() 2012-01-01 00:00:00 -0.033823 2012-01-01 00:03:00 0.056909 2012-01-01 00:06:00 -0.058837 2012-01-01 00:09:00 0.063123 2012-01-01 00:12:00 0.186340 2012-01-01 00:15:00 -0.085954 Freq: 3min, Name: A, dtype: float64 r[[\"A\", \"B\"]].mean() A B 2012-01-01 00:00:00 -0.033823 -0.121514 2012-01-01 00:03:00 0.056909 0.146731 2012-01-01 00:06:00 -0.058837 0.047046 2012-01-01 00:09:00 0.063123 -0.026158 2012-01-01 00:12:00 0.186340 -0.003144 2012-01-01 00:15:00 -0.085954 -0.016287 You can pass a list or dict of functions to do aggregation with, outputting a DataFrame: r[\"A\"].agg([\"sum\", \"mean\", \"std\"]) sum mean std 2012-01-01 00:00:00 -6.088060 -0.033823 1.043263 2012-01-01 00:03:00 10.243678 0.056909 1.058534 2012-01-01 00:06:00 -10.590584 -0.058837 0.949264 2012-01-01 00:09:00 11.362228 0.063123 1.028096 2012-01-01 00:12:00 33.541257 0.186340 0.884586 2012-01-01 00:15:00 -8.595393 -0.085954 1.035476 On a resampled DataFrame, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index: r.agg([\"sum\", \"mean\"]) A ... C sum mean ... sum mean 2012-01-01 00:00:00 -6.088060 -0.033823 ... -14.660515 -0.081447 2012-01-01 00:03:00 10.243678 0.056909 ... -4.377642 -0.024320 2012-01-01 00:06:00 -10.590584 -0.058837 ... -9.363825 -0.052021 2012-01-01 00:09:00 11.362228 0.063123 ... -11.975895 -0.066533 2012-01-01 00:12:00 33.541257 0.186340 ... 13.455299 0.074752 2012-01-01 00:15:00 -8.595393 -0.085954 ... -5.004580 -0.050046 [6 rows x 6 columns] By passing a dict to aggregate you", "prev_chunk_id": "chunk_736", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_738", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Aggregation#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Aggregation#", "content": "can apply a different aggregation to the columns of a DataFrame: r.agg({\"A\": \"sum\", \"B\": lambda x: np.std(x, ddof=1)}) A B 2012-01-01 00:00:00 -6.088060 1.001294 2012-01-01 00:03:00 10.243678 1.074597 2012-01-01 00:06:00 -10.590584 0.987309 2012-01-01 00:09:00 11.362228 0.944953 2012-01-01 00:12:00 33.541257 1.095025 2012-01-01 00:15:00 -8.595393 1.035312 The function names can also be strings. In order for a string to be valid it must be implemented on the resampled object: r.agg({\"A\": \"sum\", \"B\": \"std\"}) A B 2012-01-01 00:00:00 -6.088060 1.001294 2012-01-01 00:03:00 10.243678 1.074597 2012-01-01 00:06:00 -10.590584 0.987309 2012-01-01 00:09:00 11.362228 0.944953 2012-01-01 00:12:00 33.541257 1.095025 2012-01-01 00:15:00 -8.595393 1.035312 Furthermore, you can also specify multiple aggregation functions for each column separately. r.agg({\"A\": [\"sum\", \"std\"], \"B\": [\"mean\", \"std\"]}) A B sum std mean std 2012-01-01 00:00:00 -6.088060 1.043263 -0.121514 1.001294 2012-01-01 00:03:00 10.243678 1.058534 0.146731 1.074597 2012-01-01 00:06:00 -10.590584 0.949264 0.047046 0.987309 2012-01-01 00:09:00 11.362228 1.028096 -0.026158 0.944953 2012-01-01 00:12:00 33.541257 0.884586 -0.003144 1.095025 2012-01-01 00:15:00 -8.595393 1.035476 -0.016287 1.035312 If a DataFrame does not have a datetimelike index, but instead you want to resample based on datetimelike column in the frame, it can passed to the on keyword. df = pd.DataFrame( .....: {\"date\": pd.date_range(\"2015-01-01\", freq=\"W\", periods=5), \"a\": np.arange(5)}, .....: index=pd.MultiIndex.from_arrays( .....: [[1, 2, 3, 4, 5], pd.date_range(\"2015-01-01\", freq=\"W\", periods=5)], .....: names=[\"v\", \"d\"], .....: ), .....: ) .....: df date a v d 1 2015-01-04 2015-01-04 0 2 2015-01-11 2015-01-11 1 3 2015-01-18 2015-01-18 2 4 2015-01-25 2015-01-25 3 5 2015-02-01 2015-02-01 4 df.resample(\"ME\", on=\"date\")[[\"a\"]].sum() a date 2015-01-31 6 2015-02-28 4 Similarly, if you instead want to resample by a datetimelike level of MultiIndex, its name or location can be passed to the level keyword. df.resample(\"ME\", level=\"d\")[[\"a\"]].sum() a d 2015-01-31 6 2015-02-28 4", "prev_chunk_id": "chunk_737", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_739", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Iterating through groups#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Iterating through groups#", "content": "Iterating through groups# With the Resampler object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby(): small = pd.Series( .....: range(6), .....: index=pd.to_datetime( .....: [ .....: \"2017-01-01T00:00:00\", .....: \"2017-01-01T00:30:00\", .....: \"2017-01-01T00:31:00\", .....: \"2017-01-01T01:00:00\", .....: \"2017-01-01T03:00:00\", .....: \"2017-01-01T03:05:00\", .....: ] .....: ), .....: ) .....: resampled = small.resample(\"h\") for name, group in resampled: .....: print(\"Group: \", name) .....: print(\"-\" * 27) .....: print(group, end=\"\\n\\n\") .....: Group: 2017-01-01 00:00:00 --------------------------- 2017-01-01 00:00:00 0 2017-01-01 00:30:00 1 2017-01-01 00:31:00 2 dtype: int64 Group: 2017-01-01 01:00:00 --------------------------- 2017-01-01 01:00:00 3 dtype: int64 Group: 2017-01-01 02:00:00 --------------------------- Series([], dtype: int64) Group: 2017-01-01 03:00:00 --------------------------- 2017-01-01 03:00:00 4 2017-01-01 03:05:00 5 dtype: int64 See Iterating through groups or Resampler.__iter__ for more.", "prev_chunk_id": "chunk_738", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_740", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Use origin or offset to adjust the start of the bins#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Use origin or offset to adjust the start of the bins#", "content": "Use origin or offset to adjust the start of the bins# The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like 30D) or that divide a day evenly (like 90s or 1min). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument origin. For example: start, end = \"2000-10-01 23:30:00\", \"2000-10-02 00:30:00\" middle = \"2000-10-02 00:00:00\" rng = pd.date_range(start, end, freq=\"7min\") ts = pd.Series(np.arange(len(rng)) * 3, index=rng) ts 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7min, dtype: int64 Here we can see that, when using origin with its default value ('start_day'), the result after '2000-10-02 00:00:00' are not identical depending on the start of time series: ts.resample(\"17min\", origin=\"start_day\").sum() 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17min, dtype: int64 ts[middle:end].resample(\"17min\", origin=\"start_day\").sum() 2000-10-02 00:00:00 33 2000-10-02 00:17:00 45 Freq: 17min, dtype: int64 Here we can see that, when setting origin to 'epoch', the result after '2000-10-02 00:00:00' are identical depending on the start of time series: ts.resample(\"17min\", origin=\"epoch\").sum() 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17min, dtype: int64 ts[middle:end].resample(\"17min\", origin=\"epoch\").sum() 2000-10-01 23:52:00 15 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17min, dtype: int64 If needed you can use a custom timestamp for origin: ts.resample(\"17min\", origin=\"2001-01-01\").sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 ts[middle:end].resample(\"17min\", origin=pd.Timestamp(\"2001-01-01\")).sum() 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 If needed", "prev_chunk_id": "chunk_739", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_741", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Use origin or offset to adjust the start of the bins#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Use origin or offset to adjust the start of the bins#", "content": "you can just adjust the bins with an offset Timedelta that would be added to the default origin. Those two examples are equivalent for this time series: ts.resample(\"17min\", origin=\"start\").sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 ts.resample(\"17min\", offset=\"23h30min\").sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 Note the use of 'start' for origin on the last example. In that case, origin will be set to the first value of the timeseries.", "prev_chunk_id": "chunk_740", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_742", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Backward resample#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Backward resample#", "content": "Backward resample# Instead of adjusting the beginning of bins, sometimes we need to fix the end of the bins to make a backward resample with a given freq. The backward resample sets closed to 'right' by default since the last value should be considered as the edge point for the last bin. We can set origin to 'end'. The value for a specific Timestamp index stands for the resample result from the current Timestamp minus freq to the current Timestamp with a right close. ts.resample('17min', origin='end').sum() 2000-10-01 23:35:00 0 2000-10-01 23:52:00 18 2000-10-02 00:09:00 27 2000-10-02 00:26:00 63 Freq: 17min, dtype: int64 Besides, in contrast with the 'start_day' option, end_day is supported. This will set the origin as the ceiling midnight of the largest Timestamp. ts.resample('17min', origin='end_day').sum() 2000-10-01 23:38:00 3 2000-10-01 23:55:00 15 2000-10-02 00:12:00 45 2000-10-02 00:29:00 45 Freq: 17min, dtype: int64 The above result uses 2000-10-02 00:29:00 as the last bin’s right edge since the following computation. ceil_mid = rng.max().ceil('D') freq = pd.offsets.Minute(17) bin_res = ceil_mid - freq * ((ceil_mid - rng.max()) // freq) bin_res Timestamp('2000-10-02 00:29:00')", "prev_chunk_id": "chunk_741", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_743", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time span representation#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time span representation#", "content": "Time span representation# Regular intervals of time are represented by Period objects in pandas while sequences of Period objects are collected in a PeriodIndex, which can be created with the convenience function period_range.", "prev_chunk_id": "chunk_742", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_744", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Period#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Period#", "content": "Period# A Period represents a span of time (e.g., a day, a month, a quarter, etc). You can specify the span via freq keyword using a frequency alias like below. Because freq represents a span of Period, it cannot be negative like “-3D”. pd.Period(\"2012\", freq=\"Y-DEC\") Period('2012', 'Y-DEC') pd.Period(\"2012-1-1\", freq=\"D\") Period('2012-01-01', 'D') pd.Period(\"2012-1-1 19:00\", freq=\"h\") Period('2012-01-01 19:00', 'h') pd.Period(\"2012-1-1 19:00\", freq=\"5h\") Period('2012-01-01 19:00', '5h') Adding and subtracting integers from periods shifts the period by its own frequency. Arithmetic is not allowed between Period with different freq (span). p = pd.Period(\"2012\", freq=\"Y-DEC\") p + 1 Period('2013', 'Y-DEC') p - 3 Period('2009', 'Y-DEC') p = pd.Period(\"2012-01\", freq=\"2M\") p + 2 Period('2012-05', '2M') p - 1 Period('2011-11', '2M') p == pd.Period(\"2012-01\", freq=\"3M\") False If Period freq is daily or higher (D, h, min, s, ms, us, and ns), offsets and timedelta-like can be added if the result can have the same freq. Otherwise, ValueError will be raised. p = pd.Period(\"2014-07-01 09:00\", freq=\"h\") p + pd.offsets.Hour(2) Period('2014-07-01 11:00', 'h') p + datetime.timedelta(minutes=120) Period('2014-07-01 11:00', 'h') p + np.timedelta64(7200, \"s\") Period('2014-07-01 11:00', 'h') p + pd.offsets.Minute(5) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1824, in pandas._libs.tslibs.period._Period._add_timedeltalike_scalar() File ~/work/pandas/pandas/pandas/_libs/tslibs/timedeltas.pyx:278, in pandas._libs.tslibs.timedeltas.delta_to_nanoseconds() File ~/work/pandas/pandas/pandas/_libs/tslibs/np_datetime.pyx:661, in pandas._libs.tslibs.np_datetime.convert_reso() ValueError: Cannot losslessly convert units The above exception was the direct cause of the following exception: IncompatibleFrequency Traceback (most recent call last) Cell In[366], line 1 ----> 1 p + pd.offsets.Minute(5) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1845, in pandas._libs.tslibs.period._Period.__add__() File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1826, in pandas._libs.tslibs.period._Period._add_timedeltalike_scalar() IncompatibleFrequency: Input cannot be converted to Period(freq=h) If Period has other frequencies, only the same offsets can be added. Otherwise, ValueError will be raised. p = pd.Period(\"2014-07\", freq=\"M\") p + pd.offsets.MonthEnd(3) Period('2014-10', 'M') p + pd.offsets.MonthBegin(3) --------------------------------------------------------------------------- IncompatibleFrequency Traceback (most recent call last) Cell In[369], line 1 ----> 1 p + pd.offsets.MonthBegin(3) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1847, in pandas._libs.tslibs.period._Period.__add__() File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1837, in pandas._libs.tslibs.period._Period._add_offset() File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1732,", "prev_chunk_id": "chunk_743", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_745", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Period#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Period#", "content": "in pandas._libs.tslibs.period.PeriodMixin._require_matching_freq() IncompatibleFrequency: Input has different freq=3M from Period(freq=M) Taking the difference of Period instances with the same frequency will return the number of frequency units between them: pd.Period(\"2012\", freq=\"Y-DEC\") - pd.Period(\"2002\", freq=\"Y-DEC\") <10 * YearEnds: month=12>", "prev_chunk_id": "chunk_744", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_746", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "PeriodIndex and period_range#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "PeriodIndex and period_range#", "content": "PeriodIndex and period_range# Regular sequences of Period objects can be collected in a PeriodIndex, which can be constructed using the period_range convenience function: prng = pd.period_range(\"1/1/2011\", \"1/1/2012\", freq=\"M\") prng PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01'], dtype='period[M]') The PeriodIndex constructor can also be used directly: pd.PeriodIndex([\"2011-1\", \"2011-2\", \"2011-3\"], freq=\"M\") PeriodIndex(['2011-01', '2011-02', '2011-03'], dtype='period[M]') Passing multiplied frequency outputs a sequence of Period which has multiplied span. pd.period_range(start=\"2014-01\", freq=\"3M\", periods=4) PeriodIndex(['2014-01', '2014-04', '2014-07', '2014-10'], dtype='period[3M]') If start or end are Period objects, they will be used as anchor endpoints for a PeriodIndex with frequency matching that of the PeriodIndex constructor. pd.period_range( .....: start=pd.Period(\"2017Q1\", freq=\"Q\"), end=pd.Period(\"2017Q2\", freq=\"Q\"), freq=\"M\" .....: ) .....: PeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'], dtype='period[M]') Just like DatetimeIndex, a PeriodIndex can also be used to index pandas objects: ps = pd.Series(np.random.randn(len(prng)), prng) ps 2011-01 -2.916901 2011-02 0.514474 2011-03 1.346470 2011-04 0.816397 2011-05 2.258648 2011-06 0.494789 2011-07 0.301239 2011-08 0.464776 2011-09 -1.393581 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 2012-01 -0.329583 Freq: M, dtype: float64 PeriodIndex supports addition and subtraction with the same rule as Period. idx = pd.period_range(\"2014-07-01 09:00\", periods=5, freq=\"h\") idx PeriodIndex(['2014-07-01 09:00', '2014-07-01 10:00', '2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00'], dtype='period[h]') idx + pd.offsets.Hour(2) PeriodIndex(['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00', '2014-07-01 14:00', '2014-07-01 15:00'], dtype='period[h]') idx = pd.period_range(\"2014-07\", periods=5, freq=\"M\") idx PeriodIndex(['2014-07', '2014-08', '2014-09', '2014-10', '2014-11'], dtype='period[M]') idx + pd.offsets.MonthEnd(3) PeriodIndex(['2014-10', '2014-11', '2014-12', '2015-01', '2015-02'], dtype='period[M]') PeriodIndex has its own dtype named period, refer to Period Dtypes.", "prev_chunk_id": "chunk_745", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_747", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Period dtypes#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Period dtypes#", "content": "Period dtypes# PeriodIndex has a custom period dtype. This is a pandas extension dtype similar to the timezone aware dtype (datetime64[ns, tz]). The period dtype holds the freq attribute and is represented with period[freq] like period[D] or period[M], using frequency strings. pi = pd.period_range(\"2016-01-01\", periods=3, freq=\"M\") pi PeriodIndex(['2016-01', '2016-02', '2016-03'], dtype='period[M]') pi.dtype period[M] The period dtype can be used in .astype(...). It allows one to change the freq of a PeriodIndex like .asfreq() and convert a DatetimeIndex to PeriodIndex like to_period(): # change monthly freq to daily freq pi.astype(\"period[D]\") PeriodIndex(['2016-01-31', '2016-02-29', '2016-03-31'], dtype='period[D]') # convert to DatetimeIndex pi.astype(\"datetime64[ns]\") DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01'], dtype='datetime64[ns]', freq='MS') # convert to PeriodIndex dti = pd.date_range(\"2011-01-01\", freq=\"ME\", periods=3) dti DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31'], dtype='datetime64[ns]', freq='ME') dti.astype(\"period[M]\") PeriodIndex(['2011-01', '2011-02', '2011-03'], dtype='period[M]')", "prev_chunk_id": "chunk_746", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_748", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "PeriodIndex partial string indexing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "PeriodIndex partial string indexing#", "content": "PeriodIndex partial string indexing# PeriodIndex now supports partial string slicing with non-monotonic indexes. You can pass in dates and strings to Series and DataFrame with PeriodIndex, in the same manner as DatetimeIndex. For details, refer to DatetimeIndex Partial String Indexing. ps[\"2011-01\"] -2.9169013294054507 ps[datetime.datetime(2011, 12, 25):] 2011-12 2.261385 2012-01 -0.329583 Freq: M, dtype: float64 ps[\"10/31/2011\":\"12/31/2011\"] 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 Freq: M, dtype: float64 Passing a string representing a lower frequency than PeriodIndex returns partial sliced data. ps[\"2011\"] 2011-01 -2.916901 2011-02 0.514474 2011-03 1.346470 2011-04 0.816397 2011-05 2.258648 2011-06 0.494789 2011-07 0.301239 2011-08 0.464776 2011-09 -1.393581 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 Freq: M, dtype: float64 dfp = pd.DataFrame( .....: np.random.randn(600, 1), .....: columns=[\"A\"], .....: index=pd.period_range(\"2013-01-01 9:00\", periods=600, freq=\"min\"), .....: ) .....: dfp A 2013-01-01 09:00 -0.538468 2013-01-01 09:01 -1.365819 2013-01-01 09:02 -0.969051 2013-01-01 09:03 -0.331152 2013-01-01 09:04 -0.245334 ... ... 2013-01-01 18:55 0.522460 2013-01-01 18:56 0.118710 2013-01-01 18:57 0.167517 2013-01-01 18:58 0.922883 2013-01-01 18:59 1.721104 [600 rows x 1 columns] dfp.loc[\"2013-01-01 10h\"] A 2013-01-01 10:00 -0.308975 2013-01-01 10:01 0.542520 2013-01-01 10:02 1.061068 2013-01-01 10:03 0.754005 2013-01-01 10:04 0.352933 ... ... 2013-01-01 10:55 -0.865621 2013-01-01 10:56 -1.167818 2013-01-01 10:57 -2.081748 2013-01-01 10:58 -0.527146 2013-01-01 10:59 0.802298 [60 rows x 1 columns] As with DatetimeIndex, the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59. dfp[\"2013-01-01 10h\":\"2013-01-01 11h\"] A 2013-01-01 10:00 -0.308975 2013-01-01 10:01 0.542520 2013-01-01 10:02 1.061068 2013-01-01 10:03 0.754005 2013-01-01 10:04 0.352933 ... ... 2013-01-01 11:55 -0.590204 2013-01-01 11:56 1.539990 2013-01-01 11:57 -1.224826 2013-01-01 11:58 0.578798 2013-01-01 11:59 -0.685496 [120 rows x 1 columns]", "prev_chunk_id": "chunk_747", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_749", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Frequency conversion and resampling with PeriodIndex#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Frequency conversion and resampling with PeriodIndex#", "content": "Frequency conversion and resampling with PeriodIndex# The frequency of Period and PeriodIndex can be converted via the asfreq method. Let’s start with the fiscal year 2011, ending in December: p = pd.Period(\"2011\", freq=\"Y-DEC\") p Period('2011', 'Y-DEC') We can convert it to a monthly frequency. Using the how parameter, we can specify whether to return the starting or ending month: p.asfreq(\"M\", how=\"start\") Period('2011-01', 'M') p.asfreq(\"M\", how=\"end\") Period('2011-12', 'M') The shorthands ‘s’ and ‘e’ are provided for convenience: p.asfreq(\"M\", \"s\") Period('2011-01', 'M') p.asfreq(\"M\", \"e\") Period('2011-12', 'M') Converting to a “super-period” (e.g., annual frequency is a super-period of quarterly frequency) automatically returns the super-period that includes the input period: p = pd.Period(\"2011-12\", freq=\"M\") p.asfreq(\"Y-NOV\") Period('2012', 'Y-NOV') Note that since we converted to an annual frequency that ends the year in November, the monthly period of December 2011 is actually in the 2012 Y-NOV period. Period conversions with anchored frequencies are particularly useful for working with various quarterly data common to economics, business, and other fields. Many organizations define quarters relative to the month in which their fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or a few months into 2011. Via anchored frequencies, pandas works for all quarterly frequencies Q-JAN through Q-DEC. Q-DEC define regular calendar quarters: p = pd.Period(\"2012Q1\", freq=\"Q-DEC\") p.asfreq(\"D\", \"s\") Period('2012-01-01', 'D') p.asfreq(\"D\", \"e\") Period('2012-03-31', 'D') Q-MAR defines fiscal year end in March: p = pd.Period(\"2011Q4\", freq=\"Q-MAR\") p.asfreq(\"D\", \"s\") Period('2011-01-01', 'D') p.asfreq(\"D\", \"e\") Period('2011-03-31', 'D')", "prev_chunk_id": "chunk_748", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_750", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Converting between representations#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Converting between representations#", "content": "Converting between representations# Timestamped data can be converted to PeriodIndex-ed data using to_period and vice-versa using to_timestamp: rng = pd.date_range(\"1/1/2012\", periods=5, freq=\"ME\") ts = pd.Series(np.random.randn(len(rng)), index=rng) ts 2012-01-31 1.931253 2012-02-29 -0.184594 2012-03-31 0.249656 2012-04-30 -0.978151 2012-05-31 -0.873389 Freq: ME, dtype: float64 ps = ts.to_period() ps 2012-01 1.931253 2012-02 -0.184594 2012-03 0.249656 2012-04 -0.978151 2012-05 -0.873389 Freq: M, dtype: float64 ps.to_timestamp() 2012-01-01 1.931253 2012-02-01 -0.184594 2012-03-01 0.249656 2012-04-01 -0.978151 2012-05-01 -0.873389 Freq: MS, dtype: float64 Remember that ‘s’ and ‘e’ can be used to return the timestamps at the start or end of the period: ps.to_timestamp(\"D\", how=\"s\") 2012-01-01 1.931253 2012-02-01 -0.184594 2012-03-01 0.249656 2012-04-01 -0.978151 2012-05-01 -0.873389 Freq: MS, dtype: float64 Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end: prng = pd.period_range(\"1990Q1\", \"2000Q4\", freq=\"Q-NOV\") ts = pd.Series(np.random.randn(len(prng)), prng) ts.index = (prng.asfreq(\"M\", \"e\") + 1).asfreq(\"h\", \"s\") + 9 ts.head() 1990-03-01 09:00 -0.109291 1990-06-01 09:00 -0.637235 1990-09-01 09:00 -1.735925 1990-12-01 09:00 2.096946 1991-03-01 09:00 -1.039926 Freq: h, dtype: float64", "prev_chunk_id": "chunk_749", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_751", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Representing out-of-bounds spans#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Representing out-of-bounds spans#", "content": "Representing out-of-bounds spans# If you have data that is outside of the Timestamp bounds, see Timestamp limitations, then you can use a PeriodIndex and/or Series of Periods to do computations. span = pd.period_range(\"1215-01-01\", \"1381-01-01\", freq=\"D\") span PeriodIndex(['1215-01-01', '1215-01-02', '1215-01-03', '1215-01-04', '1215-01-05', '1215-01-06', '1215-01-07', '1215-01-08', '1215-01-09', '1215-01-10', ... '1380-12-23', '1380-12-24', '1380-12-25', '1380-12-26', '1380-12-27', '1380-12-28', '1380-12-29', '1380-12-30', '1380-12-31', '1381-01-01'], dtype='period[D]', length=60632) To convert from an int64 based YYYYMMDD representation. s = pd.Series([20121231, 20141130, 99991231]) s 0 20121231 1 20141130 2 99991231 dtype: int64 def conv(x): .....: return pd.Period(year=x // 10000, month=x // 100 % 100, day=x % 100, freq=\"D\") .....: s.apply(conv) 0 2012-12-31 1 2014-11-30 2 9999-12-31 dtype: period[D] s.apply(conv)[2] Period('9999-12-31', 'D') These can easily be converted to a PeriodIndex: span = pd.PeriodIndex(s.apply(conv)) span PeriodIndex(['2012-12-31', '2014-11-30', '9999-12-31'], dtype='period[D]')", "prev_chunk_id": "chunk_750", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_752", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time zone handling#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time zone handling#", "content": "Time zone handling# pandas provides rich support for working with timestamps in different time zones using the pytz and dateutil libraries or datetime.timezone objects from the standard library.", "prev_chunk_id": "chunk_751", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_753", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Working with time zones#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Working with time zones#", "content": "Working with time zones# By default, pandas objects are time zone unaware: rng = pd.date_range(\"3/6/2012 00:00\", periods=15, freq=\"D\") rng.tz is None True To localize these dates to a time zone (assign a particular time zone to a naive date), you can use the tz_localize method or the tz keyword argument in date_range(), Timestamp, or DatetimeIndex. You can either pass pytz or dateutil time zone objects or Olson time zone database strings. Olson time zone strings will return pytz time zone objects by default. To return dateutil time zone objects, append dateutil/ before the string. - Inpytzyou can find a list of common (and less common) time zones usingfrompytzimportcommon_timezones,all_timezones. - dateutiluses the OS time zones so there isn’t a fixed list available. For common zones, the names are the same aspytz. import dateutil # pytz rng_pytz = pd.date_range(\"3/6/2012 00:00\", periods=3, freq=\"D\", tz=\"Europe/London\") rng_pytz.tz <DstTzInfo 'Europe/London' LMT-1 day, 23:59:00 STD> # dateutil rng_dateutil = pd.date_range(\"3/6/2012 00:00\", periods=3, freq=\"D\") rng_dateutil = rng_dateutil.tz_localize(\"dateutil/Europe/London\") rng_dateutil.tz tzfile('/usr/share/zoneinfo/Europe/London') # dateutil - utc special case rng_utc = pd.date_range( .....: \"3/6/2012 00:00\", .....: periods=3, .....: freq=\"D\", .....: tz=dateutil.tz.tzutc(), .....: ) .....: rng_utc.tz tzutc() # datetime.timezone rng_utc = pd.date_range( .....: \"3/6/2012 00:00\", .....: periods=3, .....: freq=\"D\", .....: tz=datetime.timezone.utc, .....: ) .....: rng_utc.tz datetime.timezone.utc Note that the UTC time zone is a special case in dateutil and should be constructed explicitly as an instance of dateutil.tz.tzutc. You can also construct other time zones objects explicitly first. import pytz # pytz tz_pytz = pytz.timezone(\"Europe/London\") rng_pytz = pd.date_range(\"3/6/2012 00:00\", periods=3, freq=\"D\") rng_pytz = rng_pytz.tz_localize(tz_pytz) rng_pytz.tz == tz_pytz True # dateutil tz_dateutil = dateutil.tz.gettz(\"Europe/London\") rng_dateutil = pd.date_range(\"3/6/2012 00:00\", periods=3, freq=\"D\", tz=tz_dateutil) rng_dateutil.tz == tz_dateutil True To convert a time zone aware pandas object from one time zone to another, you can use the tz_convert method. rng_pytz.tz_convert(\"US/Eastern\") DatetimeIndex(['2012-03-05 19:00:00-05:00', '2012-03-06 19:00:00-05:00', '2012-03-07 19:00:00-05:00'], dtype='datetime64[ns, US/Eastern]',", "prev_chunk_id": "chunk_752", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_754", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Working with time zones#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Working with time zones#", "content": "freq=None) Under the hood, all timestamps are stored in UTC. Values from a time zone aware DatetimeIndex or Timestamp will have their fields (day, hour, minute, etc.) localized to the time zone. However, timestamps with the same UTC value are still considered to be equal even if they are in different time zones: rng_eastern = rng_utc.tz_convert(\"US/Eastern\") rng_berlin = rng_utc.tz_convert(\"Europe/Berlin\") rng_eastern[2] Timestamp('2012-03-07 19:00:00-0500', tz='US/Eastern') rng_berlin[2] Timestamp('2012-03-08 01:00:00+0100', tz='Europe/Berlin') rng_eastern[2] == rng_berlin[2] True Operations between Series in different time zones will yield UTC Series, aligning the data on the UTC timestamps: ts_utc = pd.Series(range(3), pd.date_range(\"20130101\", periods=3, tz=\"UTC\")) eastern = ts_utc.tz_convert(\"US/Eastern\") berlin = ts_utc.tz_convert(\"Europe/Berlin\") result = eastern + berlin result 2013-01-01 00:00:00+00:00 0 2013-01-02 00:00:00+00:00 2 2013-01-03 00:00:00+00:00 4 Freq: D, dtype: int64 result.index DatetimeIndex(['2013-01-01 00:00:00+00:00', '2013-01-02 00:00:00+00:00', '2013-01-03 00:00:00+00:00'], dtype='datetime64[ns, UTC]', freq='D') To remove time zone information, use tz_localize(None) or tz_convert(None). tz_localize(None) will remove the time zone yielding the local time representation. tz_convert(None) will remove the time zone after converting to UTC time. didx = pd.date_range(start=\"2014-08-01 09:00\", freq=\"h\", periods=3, tz=\"US/Eastern\") didx DatetimeIndex(['2014-08-01 09:00:00-04:00', '2014-08-01 10:00:00-04:00', '2014-08-01 11:00:00-04:00'], dtype='datetime64[ns, US/Eastern]', freq='h') didx.tz_localize(None) DatetimeIndex(['2014-08-01 09:00:00', '2014-08-01 10:00:00', '2014-08-01 11:00:00'], dtype='datetime64[ns]', freq=None) didx.tz_convert(None) DatetimeIndex(['2014-08-01 13:00:00', '2014-08-01 14:00:00', '2014-08-01 15:00:00'], dtype='datetime64[ns]', freq='h') # tz_convert(None) is identical to tz_convert('UTC').tz_localize(None) didx.tz_convert(\"UTC\").tz_localize(None) DatetimeIndex(['2014-08-01 13:00:00', '2014-08-01 14:00:00', '2014-08-01 15:00:00'], dtype='datetime64[ns]', freq=None)", "prev_chunk_id": "chunk_753", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_755", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Fold#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Fold#", "content": "Fold# For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument. Due to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time. Fold is supported only for constructing from naive datetime.datetime (see datetime documentation for details) or from Timestamp or for constructing from components (see below). Only dateutil timezones are supported (see dateutil documentation for dateutil methods that deal with ambiguous datetimes) as pytz timezones do not support fold (see pytz documentation for details on how pytz deals with ambiguous datetimes). To localize an ambiguous datetime with pytz, please use Timestamp.tz_localize(). In general, we recommend to rely on Timestamp.tz_localize() when localizing ambiguous datetimes if you need direct control over how they are handled. pd.Timestamp( .....: datetime.datetime(2019, 10, 27, 1, 30, 0, 0), .....: tz=\"dateutil/Europe/London\", .....: fold=0, .....: ) .....: Timestamp('2019-10-27 01:30:00+0100', tz='dateutil//usr/share/zoneinfo/Europe/London') pd.Timestamp( .....: year=2019, .....: month=10, .....: day=27, .....: hour=1, .....: minute=30, .....: tz=\"dateutil/Europe/London\", .....: fold=1, .....: ) .....: Timestamp('2019-10-27 01:30:00+0000', tz='dateutil//usr/share/zoneinfo/Europe/London')", "prev_chunk_id": "chunk_754", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_756", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Ambiguous times when localizing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Ambiguous times when localizing#", "content": "Ambiguous times when localizing# tz_localize may not be able to determine the UTC offset of a timestamp because daylight savings time (DST) in a local time zone causes some times to occur twice within one day (“clocks fall back”). The following options are available: - 'raise': Raises apytz.AmbiguousTimeError(the default behavior) - 'infer': Attempt to determine the correct offset base on the monotonicity of the timestamps - 'NaT': Replaces ambiguous times withNaT - bool:Truerepresents a DST time,Falserepresents non-DST time. An array-like ofboolvalues is supported for a sequence of times. rng_hourly = pd.DatetimeIndex( .....: [\"11/06/2011 00:00\", \"11/06/2011 01:00\", \"11/06/2011 01:00\", \"11/06/2011 02:00\"] .....: ) .....: This will fail as there are ambiguous times ('11/06/2011 01:00') rng_hourly.tz_localize('US/Eastern') --------------------------------------------------------------------------- AmbiguousTimeError Traceback (most recent call last) Cell In[483], line 1 ----> 1 rng_hourly.tz_localize('US/Eastern') File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:293, in DatetimeIndex.tz_localize(self, tz, ambiguous, nonexistent) 286 @doc(DatetimeArray.tz_localize) 287 def tz_localize( 288 self, (...) 291 nonexistent: TimeNonexistent = \"raise\", 292 ) -> Self: --> 293 arr = self._data.tz_localize(tz, ambiguous, nonexistent) 294 return type(self)._simple_new(arr, name=self.name) File ~/work/pandas/pandas/pandas/core/arrays/_mixins.py:81, in ravel_compat.<locals>.method(self, *args, **kwargs) 78 @wraps(meth) 79 def method(self, *args, **kwargs): 80 if self.ndim == 1: ---> 81 return meth(self, *args, **kwargs) 83 flags = self._ndarray.flags 84 flat = self.ravel(\"K\") File ~/work/pandas/pandas/pandas/core/arrays/datetimes.py:1090, in DatetimeArray.tz_localize(self, tz, ambiguous, nonexistent) 1087 tz = timezones.maybe_get_tz(tz) 1088 # Convert to UTC -> 1090 new_dates = tzconversion.tz_localize_to_utc( 1091 self.asi8, 1092 tz, 1093 ambiguous=ambiguous, 1094 nonexistent=nonexistent, 1095 creso=self._creso, 1096 ) 1097 new_dates_dt64 = new_dates.view(f\"M8[{self.unit}]\") 1098 dtype = tz_to_dtype(tz, unit=self.unit) File ~/work/pandas/pandas/pandas/_libs/tslibs/tzconversion.pyx:371, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc() AmbiguousTimeError: Cannot infer dst time from 2011-11-06 01:00:00, try using the 'ambiguous' argument Handle these ambiguous times by specifying the following. rng_hourly.tz_localize(\"US/Eastern\", ambiguous=\"infer\") DatetimeIndex(['2011-11-06 00:00:00-04:00', '2011-11-06 01:00:00-04:00', '2011-11-06 01:00:00-05:00', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) rng_hourly.tz_localize(\"US/Eastern\", ambiguous=\"NaT\") DatetimeIndex(['2011-11-06 00:00:00-04:00', 'NaT', 'NaT', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) rng_hourly.tz_localize(\"US/Eastern\", ambiguous=[True, True, False, False]) DatetimeIndex(['2011-11-06 00:00:00-04:00', '2011-11-06 01:00:00-04:00', '2011-11-06 01:00:00-05:00', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns,", "prev_chunk_id": "chunk_755", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_757", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Ambiguous times when localizing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Ambiguous times when localizing#", "content": "US/Eastern]', freq=None)", "prev_chunk_id": "chunk_756", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_758", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Nonexistent times when localizing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Nonexistent times when localizing#", "content": "Nonexistent times when localizing# A DST transition may also shift the local time ahead by 1 hour creating nonexistent local times (“clocks spring forward”). The behavior of localizing a timeseries with nonexistent times can be controlled by the nonexistent argument. The following options are available: - 'raise': Raises apytz.NonExistentTimeError(the default behavior) - 'NaT': Replaces nonexistent times withNaT - 'shift_forward': Shifts nonexistent times forward to the closest real time - 'shift_backward': Shifts nonexistent times backward to the closest real time - timedelta object: Shifts nonexistent times by the timedelta duration dti = pd.date_range(start=\"2015-03-29 02:30:00\", periods=3, freq=\"h\") # 2:30 is a nonexistent time Localization of nonexistent times will raise an error by default. dti.tz_localize('Europe/Warsaw') --------------------------------------------------------------------------- NonExistentTimeError Traceback (most recent call last) Cell In[488], line 1 ----> 1 dti.tz_localize('Europe/Warsaw') File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:293, in DatetimeIndex.tz_localize(self, tz, ambiguous, nonexistent) 286 @doc(DatetimeArray.tz_localize) 287 def tz_localize( 288 self, (...) 291 nonexistent: TimeNonexistent = \"raise\", 292 ) -> Self: --> 293 arr = self._data.tz_localize(tz, ambiguous, nonexistent) 294 return type(self)._simple_new(arr, name=self.name) File ~/work/pandas/pandas/pandas/core/arrays/_mixins.py:81, in ravel_compat.<locals>.method(self, *args, **kwargs) 78 @wraps(meth) 79 def method(self, *args, **kwargs): 80 if self.ndim == 1: ---> 81 return meth(self, *args, **kwargs) 83 flags = self._ndarray.flags 84 flat = self.ravel(\"K\") File ~/work/pandas/pandas/pandas/core/arrays/datetimes.py:1090, in DatetimeArray.tz_localize(self, tz, ambiguous, nonexistent) 1087 tz = timezones.maybe_get_tz(tz) 1088 # Convert to UTC -> 1090 new_dates = tzconversion.tz_localize_to_utc( 1091 self.asi8, 1092 tz, 1093 ambiguous=ambiguous, 1094 nonexistent=nonexistent, 1095 creso=self._creso, 1096 ) 1097 new_dates_dt64 = new_dates.view(f\"M8[{self.unit}]\") 1098 dtype = tz_to_dtype(tz, unit=self.unit) File ~/work/pandas/pandas/pandas/_libs/tslibs/tzconversion.pyx:431, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc() NonExistentTimeError: 2015-03-29 02:30:00 Transform nonexistent times to NaT or shift the times. dti DatetimeIndex(['2015-03-29 02:30:00', '2015-03-29 03:30:00', '2015-03-29 04:30:00'], dtype='datetime64[ns]', freq='h') dti.tz_localize(\"Europe/Warsaw\", nonexistent=\"shift_forward\") DatetimeIndex(['2015-03-29 03:00:00+02:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) dti.tz_localize(\"Europe/Warsaw\", nonexistent=\"shift_backward\") DatetimeIndex(['2015-03-29 01:59:59.999999999+01:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) dti.tz_localize(\"Europe/Warsaw\", nonexistent=pd.Timedelta(1, unit=\"h\")) DatetimeIndex(['2015-03-29 03:30:00+02:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) dti.tz_localize(\"Europe/Warsaw\", nonexistent=\"NaT\") DatetimeIndex(['NaT', '2015-03-29 03:30:00+02:00', '2015-03-29", "prev_chunk_id": "chunk_757", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_759", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Nonexistent times when localizing#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Nonexistent times when localizing#", "content": "04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None)", "prev_chunk_id": "chunk_758", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_760", "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html", "title": "Time zone Series operations#", "page_title": "Time series / date functionality — pandas 2.3.1 documentation", "breadcrumbs": "Time zone Series operations#", "content": "Time zone Series operations# A Series with time zone naive values is represented with a dtype of datetime64[ns]. s_naive = pd.Series(pd.date_range(\"20130101\", periods=3)) s_naive 0 2013-01-01 1 2013-01-02 2 2013-01-03 dtype: datetime64[ns] A Series with a time zone aware values is represented with a dtype of datetime64[ns, tz] where tz is the time zone s_aware = pd.Series(pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")) s_aware 0 2013-01-01 00:00:00-05:00 1 2013-01-02 00:00:00-05:00 2 2013-01-03 00:00:00-05:00 dtype: datetime64[ns, US/Eastern] Both of these Series time zone information can be manipulated via the .dt accessor, see the dt accessor section. For example, to localize and convert a naive stamp to time zone aware. s_naive.dt.tz_localize(\"UTC\").dt.tz_convert(\"US/Eastern\") 0 2012-12-31 19:00:00-05:00 1 2013-01-01 19:00:00-05:00 2 2013-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] Time zone information can also be manipulated using the astype method. This method can convert between different timezone-aware dtypes. # convert to a new time zone s_aware.astype(\"datetime64[ns, CET]\") 0 2013-01-01 06:00:00+01:00 1 2013-01-02 06:00:00+01:00 2 2013-01-03 06:00:00+01:00 dtype: datetime64[ns, CET]", "prev_chunk_id": "chunk_759", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_761", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Windowing operations#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Windowing operations#", "content": "Windowing operations# pandas contains a compact set of APIs for performing windowing operations - an operation that performs an aggregation over a sliding partition of values. The API functions similarly to the groupby API in that Series and DataFrame call the windowing method with necessary parameters and then subsequently call the aggregation function. s = pd.Series(range(5)) s.rolling(window=2).sum() 0 NaN 1 1.0 2 3.0 3 5.0 4 7.0 dtype: float64 The windows are comprised by looking back the length of the window from the current observation. The result above can be derived by taking the sum of the following windowed partitions of data: for window in s.rolling(window=2): ...: print(window) ...: 0 0 dtype: int64 0 0 1 1 dtype: int64 1 1 2 2 dtype: int64 2 2 3 3 dtype: int64 3 3 4 4 dtype: int64", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_762", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Overview#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "Overview# pandas supports 4 types of windowing operations: - Rolling window: Generic fixed or variable sliding window over the values. - Weighted window: Weighted, non-rectangular window supplied by thescipy.signallibrary. - Expanding window: Accumulating window over the values. - Exponentially Weighted window: Accumulating and exponentially weighted window over the values. Concept | Method | Returned Object | Supports time-based windows | Supports chained groupby | Supports table method | Supports online operations Rolling window | rolling | pandas.typing.api.Rolling | Yes | Yes | Yes (as of version 1.3) | No Weighted window | rolling | pandas.typing.api.Window | No | No | No | No Expanding window | expanding | pandas.typing.api.Expanding | No | Yes | Yes (as of version 1.3) | No Exponentially Weighted window | ewm | pandas.typing.api.ExponentialMovingWindow | No | Yes (as of version 1.2) | No | Yes (as of version 1.3) As noted above, some operations support specifying a window based on a time offset: s = pd.Series(range(5), index=pd.date_range('2020-01-01', periods=5, freq='1D')) s.rolling(window='2D').sum() 2020-01-01 0.0 2020-01-02 1.0 2020-01-03 3.0 2020-01-04 5.0 2020-01-05 7.0 Freq: D, dtype: float64 Additionally, some methods support chaining a groupby operation with a windowing operation which will first group the data by the specified keys and then perform a windowing operation per group. df = pd.DataFrame({'A': ['a', 'b', 'a', 'b', 'a'], 'B': range(5)}) df.groupby('A').expanding().sum() B A a 0 0.0 2 2.0 4 6.0 b 1 1.0 3 4.0 Some windowing operations also support the method='table' option in the constructor which performs the windowing operation over an entire DataFrame instead of a single column or row at a time. This can provide a useful performance benefit for a DataFrame with many columns or rows (with the corresponding axis argument) or the ability to utilize other columns during the windowing operation. The method='table' option can only", "prev_chunk_id": "chunk_761", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_763", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Overview#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "be used if engine='numba' is specified in the corresponding method call. For example, a weighted mean calculation can be calculated with apply() by specifying a separate column of weights. def weighted_mean(x): ...: arr = np.ones((1, x.shape[1])) ...: arr[:, :2] = (x[:, :2] * x[:, 2]).sum(axis=0) / x[:, 2].sum() ...: return arr ...: df = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]]) df.rolling(2, method=\"table\", min_periods=0).apply(weighted_mean, raw=True, engine=\"numba\") # noqa: E501 0 1 2 0 1.000000 2.000000 1.0 1 1.800000 2.000000 1.0 2 3.333333 2.333333 1.0 3 1.555556 7.000000 1.0 Some windowing operations also support an online method after constructing a windowing object which returns a new object that supports passing in new DataFrame or Series objects to continue the windowing calculation with the new values (i.e. online calculations). The methods on this new windowing objects must call the aggregation method first to “prime” the initial state of the online calculation. Then, new DataFrame or Series objects can be passed in the update argument to continue the windowing calculation. df = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]]) df.ewm(0.5).mean() 0 1 2 0 1.000000 2.000000 0.600000 1 1.750000 2.750000 0.450000 2 2.615385 3.615385 0.276923 3 3.550000 4.550000 0.562500 online_ewm = df.head(2).ewm(0.5).online() online_ewm.mean() 0 1 2 0 1.00 2.00 0.60 1 1.75 2.75 0.45 online_ewm.mean(update=df.tail(1)) 0 1 2 3 3.307692 4.307692 0.623077 All windowing operations support a min_periods argument that dictates the minimum amount of non-np.nan values a window must have; otherwise, the resulting value is np.nan. min_periods defaults to 1 for time-based windows and window for fixed windows s = pd.Series([np.nan, 1, 2, np.nan, np.nan, 3]) s.rolling(window=3, min_periods=1).sum() 0 NaN 1 1.0 2 3.0 3 3.0 4 2.0 5 3.0 dtype: float64 s.rolling(window=3, min_periods=2).sum() 0 NaN 1 NaN 2 3.0 3 3.0", "prev_chunk_id": "chunk_762", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_764", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Overview#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Overview#", "content": "4 NaN 5 NaN dtype: float64 # Equivalent to min_periods=3 s.rolling(window=3, min_periods=None).sum() 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN dtype: float64 Additionally, all windowing operations supports the aggregate method for returning a result of multiple aggregations applied to a window. df = pd.DataFrame({\"A\": range(5), \"B\": range(10, 15)}) df.expanding().agg([\"sum\", \"mean\", \"std\"]) A B sum mean std sum mean std 0 0.0 0.0 NaN 10.0 10.0 NaN 1 1.0 0.5 0.707107 21.0 10.5 0.707107 2 3.0 1.0 1.000000 33.0 11.0 1.000000 3 6.0 1.5 1.290994 46.0 11.5 1.290994 4 10.0 2.0 1.581139 60.0 12.0 1.581139", "prev_chunk_id": "chunk_763", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_765", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Rolling window#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Rolling window#", "content": "Rolling window# Generic rolling windows support specifying windows as a fixed number of observations or variable number of observations based on an offset. If a time based offset is provided, the corresponding time based index must be monotonic. times = ['2020-01-01', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-29'] s = pd.Series(range(5), index=pd.DatetimeIndex(times)) s 2020-01-01 0 2020-01-03 1 2020-01-04 2 2020-01-05 3 2020-01-29 4 dtype: int64 # Window with 2 observations s.rolling(window=2).sum() 2020-01-01 NaN 2020-01-03 1.0 2020-01-04 3.0 2020-01-05 5.0 2020-01-29 7.0 dtype: float64 # Window with 2 days worth of observations s.rolling(window='2D').sum() 2020-01-01 0.0 2020-01-03 1.0 2020-01-04 3.0 2020-01-05 5.0 2020-01-29 4.0 dtype: float64 For all supported aggregation functions, see Rolling window functions.", "prev_chunk_id": "chunk_764", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_766", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Centering windows#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Centering windows#", "content": "Centering windows# By default the labels are set to the right edge of the window, but a center keyword is available so the labels can be set at the center. s = pd.Series(range(10)) s.rolling(window=5).mean() 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 s.rolling(window=5, center=True).mean() 0 NaN 1 NaN 2 2.0 3 3.0 4 4.0 5 5.0 6 6.0 7 7.0 8 NaN 9 NaN dtype: float64 This can also be applied to datetime-like indices. df = pd.DataFrame( ....: {\"A\": [0, 1, 2, 3, 4]}, index=pd.date_range(\"2020\", periods=5, freq=\"1D\") ....: ) ....: df A 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 df.rolling(\"2D\", center=False).mean() A 2020-01-01 0.0 2020-01-02 0.5 2020-01-03 1.5 2020-01-04 2.5 2020-01-05 3.5 df.rolling(\"2D\", center=True).mean() A 2020-01-01 0.5 2020-01-02 1.5 2020-01-03 2.5 2020-01-04 3.5 2020-01-05 4.0", "prev_chunk_id": "chunk_765", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_767", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Rolling window endpoints#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Rolling window endpoints#", "content": "Rolling window endpoints# The inclusion of the interval endpoints in rolling window calculations can be specified with the closed parameter: Value | Behavior 'right' | close right endpoint 'left' | close left endpoint 'both' | close both endpoints 'neither' | open endpoints For example, having the right endpoint open is useful in many problems that require that there is no contamination from present information back to past information. This allows the rolling window to compute statistics “up to that point in time”, but not including that point in time. df = pd.DataFrame( ....: {\"x\": 1}, ....: index=[ ....: pd.Timestamp(\"20130101 09:00:01\"), ....: pd.Timestamp(\"20130101 09:00:02\"), ....: pd.Timestamp(\"20130101 09:00:03\"), ....: pd.Timestamp(\"20130101 09:00:04\"), ....: pd.Timestamp(\"20130101 09:00:06\"), ....: ], ....: ) ....: df[\"right\"] = df.rolling(\"2s\", closed=\"right\").x.sum() # default df[\"both\"] = df.rolling(\"2s\", closed=\"both\").x.sum() df[\"left\"] = df.rolling(\"2s\", closed=\"left\").x.sum() df[\"neither\"] = df.rolling(\"2s\", closed=\"neither\").x.sum() df x right both left neither 2013-01-01 09:00:01 1 1.0 1.0 NaN NaN 2013-01-01 09:00:02 1 2.0 2.0 1.0 1.0 2013-01-01 09:00:03 1 2.0 3.0 2.0 1.0 2013-01-01 09:00:04 1 2.0 3.0 2.0 1.0 2013-01-01 09:00:06 1 1.0 2.0 1.0 NaN", "prev_chunk_id": "chunk_766", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_768", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Custom window rolling#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Custom window rolling#", "content": "Custom window rolling# In addition to accepting an integer or offset as a window argument, rolling also accepts a BaseIndexer subclass that allows a user to define a custom method for calculating window bounds. The BaseIndexer subclass will need to define a get_window_bounds method that returns a tuple of two arrays, the first being the starting indices of the windows and second being the ending indices of the windows. Additionally, num_values, min_periods, center, closed and step will automatically be passed to get_window_bounds and the defined method must always accept these arguments. For example, if we have the following DataFrame use_expanding = [True, False, True, False, True] use_expanding [True, False, True, False, True] df = pd.DataFrame({\"values\": range(5)}) df values 0 0 1 1 2 2 3 3 4 4 and we want to use an expanding window where use_expanding is True otherwise a window of size 1, we can create the following BaseIndexer subclass: from pandas.api.indexers import BaseIndexer class CustomIndexer(BaseIndexer): ....: def get_window_bounds(self, num_values, min_periods, center, closed, step): ....: start = np.empty(num_values, dtype=np.int64) ....: end = np.empty(num_values, dtype=np.int64) ....: for i in range(num_values): ....: if self.use_expanding[i]: ....: start[i] = 0 ....: end[i] = i + 1 ....: else: ....: start[i] = i ....: end[i] = i + self.window_size ....: return start, end ....: indexer = CustomIndexer(window_size=1, use_expanding=use_expanding) df.rolling(indexer).sum() values 0 0.0 1 1.0 2 3.0 3 3.0 4 10.0 You can view other examples of BaseIndexer subclasses here One subclass of note within those examples is the VariableOffsetWindowIndexer that allows rolling operations over a non-fixed offset like a BusinessDay. from pandas.api.indexers import VariableOffsetWindowIndexer df = pd.DataFrame(range(10), index=pd.date_range(\"2020\", periods=10)) offset = pd.offsets.BDay(1) indexer = VariableOffsetWindowIndexer(index=df.index, offset=offset) df 0 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 2020-01-06 5 2020-01-07 6 2020-01-08 7 2020-01-09 8 2020-01-10 9 df.rolling(indexer).sum() 0 2020-01-01 0.0", "prev_chunk_id": "chunk_767", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_769", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Custom window rolling#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Custom window rolling#", "content": "2020-01-02 1.0 2020-01-03 2.0 2020-01-04 3.0 2020-01-05 7.0 2020-01-06 12.0 2020-01-07 6.0 2020-01-08 7.0 2020-01-09 8.0 2020-01-10 9.0 For some problems knowledge of the future is available for analysis. For example, this occurs when each data point is a full time series read from an experiment, and the task is to extract underlying conditions. In these cases it can be useful to perform forward-looking rolling window computations. FixedForwardWindowIndexer class is available for this purpose. This BaseIndexer subclass implements a closed fixed-width forward-looking rolling window, and we can use it as follows: from pandas.api.indexers import FixedForwardWindowIndexer indexer = FixedForwardWindowIndexer(window_size=2) df.rolling(indexer, min_periods=1).sum() 0 2020-01-01 1.0 2020-01-02 3.0 2020-01-03 5.0 2020-01-04 7.0 2020-01-05 9.0 2020-01-06 11.0 2020-01-07 13.0 2020-01-08 15.0 2020-01-09 17.0 2020-01-10 9.0 We can also achieve this by using slicing, applying rolling aggregation, and then flipping the result as shown in example below: df = pd.DataFrame( ....: data=[ ....: [pd.Timestamp(\"2018-01-01 00:00:00\"), 100], ....: [pd.Timestamp(\"2018-01-01 00:00:01\"), 101], ....: [pd.Timestamp(\"2018-01-01 00:00:03\"), 103], ....: [pd.Timestamp(\"2018-01-01 00:00:04\"), 111], ....: ], ....: columns=[\"time\", \"value\"], ....: ).set_index(\"time\") ....: df value time 2018-01-01 00:00:00 100 2018-01-01 00:00:01 101 2018-01-01 00:00:03 103 2018-01-01 00:00:04 111 reversed_df = df[::-1].rolling(\"2s\").sum()[::-1] reversed_df value time 2018-01-01 00:00:00 201.0 2018-01-01 00:00:01 101.0 2018-01-01 00:00:03 214.0 2018-01-01 00:00:04 111.0", "prev_chunk_id": "chunk_768", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_770", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Rolling apply#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Rolling apply#", "content": "Rolling apply# The apply() function takes an extra func argument and performs generic rolling computations. The func argument should be a single function that produces a single value from an ndarray input. raw specifies whether the windows are cast as Series objects (raw=False) or ndarray objects (raw=True). def mad(x): ....: return np.fabs(x - x.mean()).mean() ....: s = pd.Series(range(10)) s.rolling(window=4).apply(mad, raw=True) 0 NaN 1 NaN 2 NaN 3 1.0 4 1.0 5 1.0 6 1.0 7 1.0 8 1.0 9 1.0 dtype: float64", "prev_chunk_id": "chunk_769", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_771", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Numba engine#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Numba engine#", "content": "Numba engine# Additionally, apply() can leverage Numba if installed as an optional dependency. The apply aggregation can be executed using Numba by specifying engine='numba' and engine_kwargs arguments (raw must also be set to True). See enhancing performance with Numba for general usage of the arguments and performance considerations. Numba will be applied in potentially two routines: - Iffuncis a standard Python function, the engine willJITthe passed function.funccan also be a JITed function in which case the engine will not JIT the function again. - The engine will JIT the for loop where the apply function is applied to each window. The engine_kwargs argument is a dictionary of keyword arguments that will be passed into the numba.jit decorator. These keyword arguments will be applied to both the passed function (if a standard Python function) and the apply for loop over each window. mean, median, max, min, and sum also support the engine and engine_kwargs arguments.", "prev_chunk_id": "chunk_770", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_772", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Binary window functions#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Binary window functions#", "content": "Binary window functions# cov() and corr() can compute moving window statistics about two Series or any combination of DataFrame/Series or DataFrame/DataFrame. Here is the behavior in each case: - twoSeries: compute the statistic for the pairing. - DataFrame/Series: compute the statistics for each column of the DataFrame with the passed Series, thus returning a DataFrame. - DataFrame/DataFrame: by default compute the statistic for matching column names, returning a DataFrame. If the keyword argumentpairwise=Trueis passed then computes the statistic for each pair of columns, returning aDataFramewith aMultiIndexwhose values are the dates in question (seethe next section). For example: df = pd.DataFrame( ....: np.random.randn(10, 4), ....: index=pd.date_range(\"2020-01-01\", periods=10), ....: columns=[\"A\", \"B\", \"C\", \"D\"], ....: ) ....: df = df.cumsum() df2 = df[:4] df2.rolling(window=2).corr(df2[\"B\"]) A B C D 2020-01-01 NaN NaN NaN NaN 2020-01-02 -1.0 1.0 -1.0 1.0 2020-01-03 1.0 1.0 1.0 -1.0 2020-01-04 -1.0 1.0 1.0 -1.0", "prev_chunk_id": "chunk_771", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_773", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Computing rolling pairwise covariances and correlations#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Computing rolling pairwise covariances and correlations#", "content": "Computing rolling pairwise covariances and correlations# In financial data analysis and other fields it’s common to compute covariance and correlation matrices for a collection of time series. Often one is also interested in moving-window covariance and correlation matrices. This can be done by passing the pairwise keyword argument, which in the case of DataFrame inputs will yield a MultiIndexed DataFrame whose index are the dates in question. In the case of a single DataFrame argument the pairwise argument can even be omitted: covs = ( ....: df[[\"B\", \"C\", \"D\"]] ....: .rolling(window=4) ....: .cov(df[[\"A\", \"B\", \"C\"]], pairwise=True) ....: ) ....: covs B C D 2020-01-01 A NaN NaN NaN B NaN NaN NaN C NaN NaN NaN 2020-01-02 A NaN NaN NaN B NaN NaN NaN ... ... ... ... 2020-01-09 B 0.342006 0.230190 0.052849 C 0.230190 1.575251 0.082901 2020-01-10 A -0.333945 0.006871 -0.655514 B 0.649711 0.430860 0.469271 C 0.430860 0.829721 0.055300 [30 rows x 3 columns]", "prev_chunk_id": "chunk_772", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_774", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Weighted window#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Weighted window#", "content": "Weighted window# The win_type argument in .rolling generates a weighted windows that are commonly used in filtering and spectral estimation. win_type must be string that corresponds to a scipy.signal window function. Scipy must be installed in order to use these windows, and supplementary arguments that the Scipy window methods take must be specified in the aggregation function. s = pd.Series(range(10)) s.rolling(window=5).mean() 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 s.rolling(window=5, win_type=\"triang\").mean() 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 # Supplementary Scipy arguments passed in the aggregation function s.rolling(window=5, win_type=\"gaussian\").mean(std=0.1) 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 For all supported aggregation functions, see Weighted window functions.", "prev_chunk_id": "chunk_773", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_775", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Expanding window#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Expanding window#", "content": "Expanding window# An expanding window yields the value of an aggregation statistic with all the data available up to that point in time. Since these calculations are a special case of rolling statistics, they are implemented in pandas such that the following two calls are equivalent: df = pd.DataFrame(range(5)) df.rolling(window=len(df), min_periods=1).mean() 0 0 0.0 1 0.5 2 1.0 3 1.5 4 2.0 df.expanding(min_periods=1).mean() 0 0 0.0 1 0.5 2 1.0 3 1.5 4 2.0 For all supported aggregation functions, see Expanding window functions.", "prev_chunk_id": "chunk_774", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_776", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Exponentially weighted window#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Exponentially weighted window#", "content": "Exponentially weighted window# An exponentially weighted window is similar to an expanding window but with each prior point being exponentially weighted down relative to the current point. In general, a weighted moving average is calculated as where \\(x_t\\) is the input, \\(y_t\\) is the result and the \\(w_i\\) are the weights. For all supported aggregation functions, see Exponentially-weighted window functions. The EW functions support two variants of exponential weights. The default, adjust=True, uses the weights \\(w_i = (1 - \\alpha)^i\\) which gives When adjust=False is specified, moving averages are calculated as which is equivalent to using weights The difference between the above two variants arises because we are dealing with series which have finite history. Consider a series of infinite history, with adjust=True: Noting that the denominator is a geometric series with initial term equal to 1 and a ratio of \\(1 - \\alpha\\) we have which is the same expression as adjust=False above and therefore shows the equivalence of the two variants for infinite series. When adjust=False, we have \\(y_0 = x_0\\) and \\(y_t = \\alpha x_t + (1 - \\alpha) y_{t-1}\\). Therefore, there is an assumption that \\(x_0\\) is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point. One must have \\(0 < \\alpha \\leq 1\\), and while it is possible to pass \\(\\alpha\\) directly, it’s often easier to think about either the span, center of mass (com) or half-life of an EW moment: One must specify precisely one of span, center of mass, half-life and alpha to the EW functions: - Spancorresponds to what is commonly called an “N-day EW moving average”. - Center of masshas a more physical interpretation and can be thought of in terms of span:\\(c = (s - 1) / 2\\). - Half-lifeis the", "prev_chunk_id": "chunk_775", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_777", "url": "https://pandas.pydata.org/docs/user_guide/window.html", "title": "Exponentially weighted window#", "page_title": "Windowing operations — pandas 2.3.1 documentation", "breadcrumbs": "Exponentially weighted window#", "content": "period of time for the exponential weight to reduce to one half. - Alphaspecifies the smoothing factor directly. You can also specify halflife in terms of a timedelta convertible unit to specify the amount of time it takes for an observation to decay to half its value when also specifying a sequence of times. df = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 times = [\"2020-01-01\", \"2020-01-03\", \"2020-01-10\", \"2020-01-15\", \"2020-01-17\"] df.ewm(halflife=\"4 days\", times=pd.DatetimeIndex(times)).mean() B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 The following formula is used to compute exponentially weighted mean with an input vector of times: ExponentialMovingWindow also has an ignore_na argument, which determines how intermediate null values affect the calculation of the weights. When ignore_na=False (the default), weights are calculated based on absolute positions, so that intermediate null values affect the result. When ignore_na=True, weights are calculated by ignoring intermediate null values. For example, assuming adjust=True, if ignore_na=False, the weighted average of 3, NaN, 5 would be calculated as Whereas if ignore_na=True, the weighted average would be calculated as The var(), std(), and cov() functions have a bias argument, specifying whether the result should contain biased or unbiased statistics. For example, if bias=True, ewmvar(x) is calculated as ewmvar(x) = ewma(x**2) - ewma(x)**2; whereas if bias=False (the default), the biased variance statistics are scaled by debiasing factors (For \\(w_i = 1\\), this reduces to the usual \\(N / (N - 1)\\) factor, with \\(N = t + 1\\).) See Weighted Sample Variance on Wikipedia for further details.", "prev_chunk_id": "chunk_776", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_778", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Group by: split-apply-combine#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Group by: split-apply-combine#", "content": "Group by: split-apply-combine# By “group by” we are referring to a process involving one or more of the following steps: - Splittingthe data into groups based on some criteria. - Applyinga function to each group independently. - Combiningthe results into a data structure. Out of these, the split step is the most straightforward. In the apply step, we might wish to do one of the following: - Aggregation: compute a summary statistic (or statistics) for each group. Some examples:Compute group sums or means.Compute group sizes / counts. - Transformation: perform some group-specific computations and return a like-indexed object. Some examples:Standardize data (zscore) within a group.Filling NAs within groups with a value derived from each group. - Filtration: discard some groups, according to a group-wise computation that evaluates to True or False. Some examples:Discard data that belong to groups with only a few members.Filter out data based on the group sum or mean. Many of these operations are defined on GroupBy objects. These operations are similar to those of the aggregating API, window API, and resample API. It is possible that a given operation does not fall into one of these categories or is some combination of them. In such a case, it may be possible to compute the operation using GroupBy’s apply method. This method will examine the results of the apply step and try to sensibly combine them into a single result if it doesn’t fit into either of the above three categories. The name GroupBy should be quite familiar to those who have used a SQL-based tool (or itertools), in which you can write code like: SELECT Column1, Column2, mean(Column3), sum(Column4) FROM SomeTable GROUP BY Column1, Column2 We aim to make operations like this natural and easy to express using pandas. We’ll address each area of GroupBy functionality,", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_779", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Group by: split-apply-combine#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Group by: split-apply-combine#", "content": "then provide some non-trivial examples / use cases. See the cookbook for some advanced strategies.", "prev_chunk_id": "chunk_778", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_780", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Splitting an object into groups#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Splitting an object into groups#", "content": "Splitting an object into groups# The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following: speeds = pd.DataFrame( ...: [ ...: (\"bird\", \"Falconiformes\", 389.0), ...: (\"bird\", \"Psittaciformes\", 24.0), ...: (\"mammal\", \"Carnivora\", 80.2), ...: (\"mammal\", \"Primates\", np.nan), ...: (\"mammal\", \"Carnivora\", 58), ...: ], ...: index=[\"falcon\", \"parrot\", \"lion\", \"monkey\", \"leopard\"], ...: columns=(\"class\", \"order\", \"max_speed\"), ...: ) ...: speeds class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 grouped = speeds.groupby(\"class\") grouped = speeds.groupby([\"class\", \"order\"]) The mapping can be specified many different ways: - A Python function, to be called on each of the index labels. - A list or NumPy array of the same length as the index. - A dict orSeries, providing alabel->groupnamemapping. - ForDataFrameobjects, a string indicating either a column name or an index level name to be used to group. - A list of any of the above things. Collectively we refer to the grouping objects as the keys. For example, consider the following DataFrame: df = pd.DataFrame( ...: { ...: \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"], ...: \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"], ...: \"C\": np.random.randn(8), ...: \"D\": np.random.randn(8), ...: } ...: ) ...: df A B C D 0 foo one 0.469112 -0.861849 1 bar one -0.282863 -2.104569 2 foo two -1.509059 -0.494929 3 bar three -1.135632 1.071804 4 foo two 1.212112 0.721555 5 bar two -0.173215 -0.706771 6 foo one 0.119209 -1.039575 7 foo three -1.044236 0.271860 On a DataFrame, we obtain a GroupBy object by calling groupby(). This method returns a pandas.api.typing.DataFrameGroupBy instance. We could naturally group by either the A or", "prev_chunk_id": "chunk_779", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_781", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Splitting an object into groups#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Splitting an object into groups#", "content": "B columns, or both: grouped = df.groupby(\"A\") grouped = df.groupby(\"B\") grouped = df.groupby([\"A\", \"B\"]) If we also have a MultiIndex on columns A and B, we can group by all the columns except the one we specify: df2 = df.set_index([\"A\", \"B\"]) grouped = df2.groupby(level=df2.index.names.difference([\"B\"])) grouped.sum() C D A bar -1.591710 -1.739537 foo -0.752861 -1.402938 The above GroupBy will split the DataFrame on its index (rows). To split by columns, first do a transpose: def get_letter_type(letter): ....: if letter.lower() in 'aeiou': ....: return 'vowel' ....: else: ....: return 'consonant' ....: grouped = df.T.groupby(get_letter_type) pandas Index objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values: index = [1, 2, 3, 1, 2, 3] s = pd.Series([1, 2, 3, 10, 20, 30], index=index) s 1 1 2 2 3 3 1 10 2 20 3 30 dtype: int64 grouped = s.groupby(level=0) grouped.first() 1 1 2 2 3 3 dtype: int64 grouped.last() 1 10 2 20 3 30 dtype: int64 grouped.sum() 1 11 2 22 3 33 dtype: int64 Note that no splitting occurs until it’s needed. Creating the GroupBy object only verifies that you’ve passed a valid mapping.", "prev_chunk_id": "chunk_780", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_782", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "GroupBy sorting#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "GroupBy sorting#", "content": "GroupBy sorting# By default the group keys are sorted during the groupby operation. You may however pass sort=False for potential speedups. With sort=False the order among group-keys follows the order of appearance of the keys in the original dataframe: df2 = pd.DataFrame({\"X\": [\"B\", \"B\", \"A\", \"A\"], \"Y\": [1, 2, 3, 4]}) df2.groupby([\"X\"]).sum() Y X A 7 B 3 df2.groupby([\"X\"], sort=False).sum() Y X B 3 A 7 Note that groupby will preserve the order in which observations are sorted within each group. For example, the groups created by groupby() below are in the order they appeared in the original DataFrame: df3 = pd.DataFrame({\"X\": [\"A\", \"B\", \"A\", \"B\"], \"Y\": [1, 4, 3, 2]}) df3.groupby(\"X\").get_group(\"A\") X Y 0 A 1 2 A 3 df3.groupby([\"X\"]).get_group((\"B\",)) X Y 1 B 4 3 B 2", "prev_chunk_id": "chunk_781", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_783", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "GroupBy dropna#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "GroupBy dropna#", "content": "GroupBy dropna# By default NA values are excluded from group keys during the groupby operation. However, in case you want to include NA values in group keys, you could pass dropna=False to achieve it. df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]] df_dropna = pd.DataFrame(df_list, columns=[\"a\", \"b\", \"c\"]) df_dropna a b c 0 1 2.0 3 1 1 NaN 4 2 2 1.0 3 3 1 2.0 2 # Default ``dropna`` is set to True, which will exclude NaNs in keys df_dropna.groupby(by=[\"b\"], dropna=True).sum() a c b 1.0 2 3 2.0 2 5 # In order to allow NaN in keys, set ``dropna`` to False df_dropna.groupby(by=[\"b\"], dropna=False).sum() a c b 1.0 2 3 2.0 2 5 NaN 1 4 The default setting of dropna argument is True which means NA are not included in group keys.", "prev_chunk_id": "chunk_782", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_784", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "GroupBy object attributes#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "GroupBy object attributes#", "content": "GroupBy object attributes# The groups attribute is a dictionary whose keys are the computed unique groups and corresponding values are the axis labels belonging to each group. In the above example we have: df.groupby(\"A\").groups {'bar': [1, 3, 5], 'foo': [0, 2, 4, 6, 7]} df.T.groupby(get_letter_type).groups {'consonant': ['B', 'C', 'D'], 'vowel': ['A']} Calling the standard Python len function on the GroupBy object returns the number of groups, which is the same as the length of the groups dictionary: grouped = df.groupby([\"A\", \"B\"]) grouped.groups {('bar', 'one'): [1], ('bar', 'three'): [3], ('bar', 'two'): [5], ('foo', 'one'): [0, 6], ('foo', 'three'): [7], ('foo', 'two'): [2, 4]} len(grouped) 6 GroupBy will tab complete column names, GroupBy operations, and other attributes: n = 10 weight = np.random.normal(166, 20, size=n) height = np.random.normal(60, 10, size=n) time = pd.date_range(\"1/1/2000\", periods=n) gender = np.random.choice([\"male\", \"female\"], size=n) df = pd.DataFrame( ....: {\"height\": height, \"weight\": weight, \"gender\": gender}, index=time ....: ) ....: df height weight gender 2000-01-01 42.849980 157.500553 male 2000-01-02 49.607315 177.340407 male 2000-01-03 56.293531 171.524640 male 2000-01-04 48.421077 144.251986 female 2000-01-05 46.556882 152.526206 male 2000-01-06 68.448851 168.272968 female 2000-01-07 70.757698 136.431469 male 2000-01-08 58.909500 176.499753 female 2000-01-09 76.435631 174.094104 female 2000-01-10 45.306120 177.540920 male gb = df.groupby(\"gender\") gb.<TAB> # noqa: E225, E999 gb.agg gb.boxplot gb.cummin gb.describe gb.filter gb.get_group gb.height gb.last gb.median gb.ngroups gb.plot gb.rank gb.std gb.transform gb.aggregate gb.count gb.cumprod gb.dtype gb.first gb.groups gb.hist gb.max gb.min gb.nth gb.prod gb.resample gb.sum gb.var gb.apply gb.cummax gb.cumsum gb.fillna gb.gender gb.head gb.indices gb.mean gb.name gb.ohlc gb.quantile gb.size gb.tail gb.weight", "prev_chunk_id": "chunk_783", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_785", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "GroupBy with MultiIndex#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "GroupBy with MultiIndex#", "content": "GroupBy with MultiIndex# With hierarchically-indexed data, it’s quite natural to group by one of the levels of the hierarchy. Let’s create a Series with a two-level MultiIndex. arrays = [ ....: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ....: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ....: ] ....: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"]) s = pd.Series(np.random.randn(8), index=index) s first second bar one -0.919854 two -0.042379 baz one 1.247642 two -0.009920 foo one 0.290213 two 0.495767 qux one 0.362949 two 1.548106 dtype: float64 We can then group by one of the levels in s. grouped = s.groupby(level=0) grouped.sum() first bar -0.962232 baz 1.237723 foo 0.785980 qux 1.911055 dtype: float64 If the MultiIndex has names specified, these can be passed instead of the level number: s.groupby(level=\"second\").sum() second one 0.980950 two 1.991575 dtype: float64 Grouping with multiple levels is supported. arrays = [ ....: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ....: [\"doo\", \"doo\", \"bee\", \"bee\", \"bop\", \"bop\", \"bop\", \"bop\"], ....: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ....: ] ....: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\", \"third\"]) s = pd.Series(np.random.randn(8), index=index) s first second third bar doo one -1.131345 two -0.089329 baz bee one 0.337863 two -0.945867 foo bop one -0.932132 two 1.956030 qux bop one 0.017587 two -0.016692 dtype: float64 s.groupby(level=[\"first\", \"second\"]).sum() first second bar doo -1.220674 baz bee -0.608004 foo bop 1.023898 qux bop 0.000895 dtype: float64 Index level names may be supplied as keys. s.groupby([\"first\", \"second\"]).sum() first second bar doo -1.220674 baz bee -0.608004 foo bop 1.023898 qux bop 0.000895 dtype: float64 More on the sum function and aggregation later.", "prev_chunk_id": "chunk_784", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_786", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Grouping DataFrame with Index levels and columns#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Grouping DataFrame with Index levels and columns#", "content": "Grouping DataFrame with Index levels and columns# A DataFrame may be grouped by a combination of columns and index levels. You can specify both column and index names, or use a Grouper. Let’s first create a DataFrame with a MultiIndex: arrays = [ ....: [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"], ....: [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"], ....: ] ....: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"]) df = pd.DataFrame({\"A\": [1, 1, 1, 1, 2, 2, 3, 3], \"B\": np.arange(8)}, index=index) df A B first second bar one 1 0 two 1 1 baz one 1 2 two 1 3 foo one 2 4 two 2 5 qux one 3 6 two 3 7 Then we group df by the second index level and the A column. df.groupby([pd.Grouper(level=1), \"A\"]).sum() B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7 Index levels may also be specified by name. df.groupby([pd.Grouper(level=\"second\"), \"A\"]).sum() B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7 Index level names may be specified as keys directly to groupby. df.groupby([\"second\", \"A\"]).sum() B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7", "prev_chunk_id": "chunk_785", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_787", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "DataFrame column selection in GroupBy#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "DataFrame column selection in GroupBy#", "content": "DataFrame column selection in GroupBy# Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, by using [] on the GroupBy object in a similar way as the one used to get a column from a DataFrame, you can do: df = pd.DataFrame( ....: { ....: \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"], ....: \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"], ....: \"C\": np.random.randn(8), ....: \"D\": np.random.randn(8), ....: } ....: ) ....: df A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 grouped = df.groupby([\"A\"]) grouped_C = grouped[\"C\"] grouped_D = grouped[\"D\"] This is mainly syntactic sugar for the alternative, which is much more verbose: df[\"C\"].groupby(df[\"A\"]) <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f10570765f0> Additionally, this method avoids recomputing the internal grouping information derived from the passed key. You can also include the grouping columns if you want to operate on them. grouped[[\"A\", \"B\"]].sum() A B A bar barbarbar onethreetwo foo foofoofoofoofoo onetwotwoonethree", "prev_chunk_id": "chunk_786", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_788", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Iterating through groups#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Iterating through groups#", "content": "Iterating through groups# With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby(): grouped = df.groupby('A') for name, group in grouped: ....: print(name) ....: print(group) ....: bar A B C D 1 bar one 0.254161 1.511763 3 bar three 0.215897 -0.990582 5 bar two -0.077118 1.211526 foo A B C D 0 foo one -0.575247 1.346061 2 foo two -1.143704 1.627081 4 foo two 1.193555 -0.441652 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 In the case of grouping by multiple keys, the group name will be a tuple: for name, group in df.groupby(['A', 'B']): ....: print(name) ....: print(group) ....: ('bar', 'one') A B C D 1 bar one 0.254161 1.511763 ('bar', 'three') A B C D 3 bar three 0.215897 -0.990582 ('bar', 'two') A B C D 5 bar two -0.077118 1.211526 ('foo', 'one') A B C D 0 foo one -0.575247 1.346061 6 foo one -0.408530 0.268520 ('foo', 'three') A B C D 7 foo three -0.862495 0.02458 ('foo', 'two') A B C D 2 foo two -1.143704 1.627081 4 foo two 1.193555 -0.441652 See Iterating through groups.", "prev_chunk_id": "chunk_787", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_789", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Selecting a group#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Selecting a group#", "content": "Selecting a group# A single group can be selected using DataFrameGroupBy.get_group(): grouped.get_group(\"bar\") A B C D 1 bar one 0.254161 1.511763 3 bar three 0.215897 -0.990582 5 bar two -0.077118 1.211526 Or for an object grouped on multiple columns: df.groupby([\"A\", \"B\"]).get_group((\"bar\", \"one\")) A B C D 1 bar one 0.254161 1.511763", "prev_chunk_id": "chunk_788", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_790", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Aggregation#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Aggregation#", "content": "Aggregation# An aggregation is a GroupBy operation that reduces the dimension of the grouping object. The result of an aggregation is, or at least is treated as, a scalar value for each column in a group. For example, producing the sum of each column in a group of values. animals = pd.DataFrame( ....: { ....: \"kind\": [\"cat\", \"dog\", \"cat\", \"dog\"], ....: \"height\": [9.1, 6.0, 9.5, 34.0], ....: \"weight\": [7.9, 7.5, 9.9, 198.0], ....: } ....: ) ....: animals kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 animals.groupby(\"kind\").sum() height weight kind cat 18.6 17.8 dog 40.0 205.5 In the result, the keys of the groups appear in the index by default. They can be instead included in the columns by passing as_index=False. animals.groupby(\"kind\", as_index=False).sum() kind height weight 0 cat 18.6 17.8 1 dog 40.0 205.5", "prev_chunk_id": "chunk_789", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_791", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Built-in aggregation methods#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Built-in aggregation methods#", "content": "Built-in aggregation methods# Many common aggregations are built-in to GroupBy objects as methods. Of the methods listed below, those with a * do not have an efficient, GroupBy-specific, implementation. Method | Description any() | Compute whether any of the values in the groups are truthy all() | Compute whether all of the values in the groups are truthy count() | Compute the number of non-NA values in the groups cov() * | Compute the covariance of the groups first() | Compute the first occurring value in each group idxmax() | Compute the index of the maximum value in each group idxmin() | Compute the index of the minimum value in each group last() | Compute the last occurring value in each group max() | Compute the maximum value in each group mean() | Compute the mean of each group median() | Compute the median of each group min() | Compute the minimum value in each group nunique() | Compute the number of unique values in each group prod() | Compute the product of the values in each group quantile() | Compute a given quantile of the values in each group sem() | Compute the standard error of the mean of the values in each group size() | Compute the number of values in each group skew() * | Compute the skew of the values in each group std() | Compute the standard deviation of the values in each group sum() | Compute the sum of the values in each group var() | Compute the variance of the values in each group Some examples: df.groupby(\"A\")[[\"C\", \"D\"]].max() C D A bar 0.254161 1.511763 foo 1.193555 1.627081 df.groupby([\"A\", \"B\"]).mean() C D A B bar one 0.254161 1.511763 three 0.215897 -0.990582 two -0.077118 1.211526 foo one -0.491888 0.807291 three -0.862495 0.024580 two 0.024925 0.592714", "prev_chunk_id": "chunk_790", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_792", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Built-in aggregation methods#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Built-in aggregation methods#", "content": "Another aggregation example is to compute the size of each group. This is included in GroupBy as the size method. It returns a Series whose index consists of the group names and the values are the sizes of each group. grouped = df.groupby([\"A\", \"B\"]) grouped.size() A B bar one 1 three 1 two 1 foo one 2 three 1 two 2 dtype: int64 While the DataFrameGroupBy.describe() method is not itself a reducer, it can be used to conveniently produce a collection of summary statistics about each of the groups. grouped.describe() C ... D count mean std ... 50% 75% max A B ... bar one 1.0 0.254161 NaN ... 1.511763 1.511763 1.511763 three 1.0 0.215897 NaN ... -0.990582 -0.990582 -0.990582 two 1.0 -0.077118 NaN ... 1.211526 1.211526 1.211526 foo one 2.0 -0.491888 0.117887 ... 0.807291 1.076676 1.346061 three 1.0 -0.862495 NaN ... 0.024580 0.024580 0.024580 two 2.0 0.024925 1.652692 ... 0.592714 1.109898 1.627081 [6 rows x 16 columns] Another aggregation example is to compute the number of unique values of each group. This is similar to the DataFrameGroupBy.value_counts() function, except that it only counts the number of unique values. ll = [['foo', 1], ['foo', 2], ['foo', 2], ['bar', 1], ['bar', 1]] df4 = pd.DataFrame(ll, columns=[\"A\", \"B\"]) df4 A B 0 foo 1 1 foo 2 2 foo 2 3 bar 1 4 bar 1 df4.groupby(\"A\")[\"B\"].nunique() A bar 1 foo 2 Name: B, dtype: int64", "prev_chunk_id": "chunk_791", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_793", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "The aggregate() method#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "The aggregate() method#", "content": "The aggregate() method# Any reduction method that pandas implements can be passed as a string to aggregate(). Users are encouraged to use the shorthand, agg. It will operate as if the corresponding method was called. grouped = df.groupby(\"A\") grouped[[\"C\", \"D\"]].aggregate(\"sum\") C D A bar 0.392940 1.732707 foo -1.796421 2.824590 grouped = df.groupby([\"A\", \"B\"]) grouped.agg(\"sum\") C D A B bar one 0.254161 1.511763 three 0.215897 -0.990582 two -0.077118 1.211526 foo one -0.983776 1.614581 three -0.862495 0.024580 two 0.049851 1.185429 The result of the aggregation will have the group names as the new index. In the case of multiple keys, the result is a MultiIndex by default. As mentioned above, this can be changed by using the as_index option: grouped = df.groupby([\"A\", \"B\"], as_index=False) grouped.agg(\"sum\") A B C D 0 bar one 0.254161 1.511763 1 bar three 0.215897 -0.990582 2 bar two -0.077118 1.211526 3 foo one -0.983776 1.614581 4 foo three -0.862495 0.024580 5 foo two 0.049851 1.185429 df.groupby(\"A\", as_index=False)[[\"C\", \"D\"]].agg(\"sum\") A C D 0 bar 0.392940 1.732707 1 foo -1.796421 2.824590 Note that you could use the DataFrame.reset_index() DataFrame function to achieve the same result as the column names are stored in the resulting MultiIndex, although this will make an extra copy. df.groupby([\"A\", \"B\"]).agg(\"sum\").reset_index() A B C D 0 bar one 0.254161 1.511763 1 bar three 0.215897 -0.990582 2 bar two -0.077118 1.211526 3 foo one -0.983776 1.614581 4 foo three -0.862495 0.024580 5 foo two 0.049851 1.185429", "prev_chunk_id": "chunk_792", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_794", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Aggregation with User-Defined Functions#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Aggregation with User-Defined Functions#", "content": "Aggregation with User-Defined Functions# Users can also provide their own User-Defined Functions (UDFs) for custom aggregations. animals kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 animals.groupby(\"kind\")[[\"height\"]].agg(lambda x: set(x)) height kind cat {9.1, 9.5} dog {34.0, 6.0} The resulting dtype will reflect that of the aggregating function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. animals.groupby(\"kind\")[[\"height\"]].agg(lambda x: x.astype(int).sum()) height kind cat 18 dog 40", "prev_chunk_id": "chunk_793", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_795", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Applying multiple functions at once#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Applying multiple functions at once#", "content": "Applying multiple functions at once# On a grouped Series, you can pass a list or dict of functions to SeriesGroupBy.agg(), outputting a DataFrame: grouped = df.groupby(\"A\") grouped[\"C\"].agg([\"sum\", \"mean\", \"std\"]) sum mean std A bar 0.392940 0.130980 0.181231 foo -1.796421 -0.359284 0.912265 On a grouped DataFrame, you can pass a list of functions to DataFrameGroupBy.agg() to aggregate each column, which produces an aggregated result with a hierarchical column index: grouped[[\"C\", \"D\"]].agg([\"sum\", \"mean\", \"std\"]) C D sum mean std sum mean std A bar 0.392940 0.130980 0.181231 1.732707 0.577569 1.366330 foo -1.796421 -0.359284 0.912265 2.824590 0.564918 0.884785 The resulting aggregations are named after the functions themselves. If you need to rename, then you can add in a chained operation for a Series like this: ( .....: grouped[\"C\"] .....: .agg([\"sum\", \"mean\", \"std\"]) .....: .rename(columns={\"sum\": \"foo\", \"mean\": \"bar\", \"std\": \"baz\"}) .....: ) .....: foo bar baz A bar 0.392940 0.130980 0.181231 foo -1.796421 -0.359284 0.912265 For a grouped DataFrame, you can rename in a similar manner: ( .....: grouped[[\"C\", \"D\"]].agg([\"sum\", \"mean\", \"std\"]).rename( .....: columns={\"sum\": \"foo\", \"mean\": \"bar\", \"std\": \"baz\"} .....: ) .....: ) .....: C D foo bar baz foo bar baz A bar 0.392940 0.130980 0.181231 1.732707 0.577569 1.366330 foo -1.796421 -0.359284 0.912265 2.824590 0.564918 0.884785", "prev_chunk_id": "chunk_794", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_796", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Named aggregation#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Named aggregation#", "content": "Named aggregation# To support column-specific aggregation with control over the output column names, pandas accepts the special syntax in DataFrameGroupBy.agg() and SeriesGroupBy.agg(), known as “named aggregation”, where - The keywords are theoutputcolumn names - The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides theNamedAggnamedtuple with the fields['column','aggfunc']to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias. animals kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 animals.groupby(\"kind\").agg( .....: min_height=pd.NamedAgg(column=\"height\", aggfunc=\"min\"), .....: max_height=pd.NamedAgg(column=\"height\", aggfunc=\"max\"), .....: average_weight=pd.NamedAgg(column=\"weight\", aggfunc=\"mean\"), .....: ) .....: min_height max_height average_weight kind cat 9.1 9.5 8.90 dog 6.0 34.0 102.75 NamedAgg is just a namedtuple. Plain tuples are allowed as well. animals.groupby(\"kind\").agg( .....: min_height=(\"height\", \"min\"), .....: max_height=(\"height\", \"max\"), .....: average_weight=(\"weight\", \"mean\"), .....: ) .....: min_height max_height average_weight kind cat 9.1 9.5 8.90 dog 6.0 34.0 102.75 If the column names you want are not valid Python keywords, construct a dictionary and unpack the keyword arguments animals.groupby(\"kind\").agg( .....: **{ .....: \"total weight\": pd.NamedAgg(column=\"weight\", aggfunc=\"sum\") .....: } .....: ) .....: total weight kind cat 17.8 dog 205.5 When using named aggregation, additional keyword arguments are not passed through to the aggregation functions; only pairs of (column, aggfunc) should be passed as **kwargs. If your aggregation functions require additional arguments, apply them partially with functools.partial(). Named aggregation is also valid for Series groupby aggregations. In this case there’s no column selection, so the values are just the functions. animals.groupby(\"kind\").height.agg( .....: min_height=\"min\", .....: max_height=\"max\", .....: ) .....: min_height max_height kind cat 9.1 9.5 dog 6.0 34.0", "prev_chunk_id": "chunk_795", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_797", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Applying different functions to DataFrame columns#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Applying different functions to DataFrame columns#", "content": "Applying different functions to DataFrame columns# By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame: grouped.agg({\"C\": \"sum\", \"D\": lambda x: np.std(x, ddof=1)}) C D A bar 0.392940 1.366330 foo -1.796421 0.884785 The function names can also be strings. In order for a string to be valid it must be implemented on GroupBy: grouped.agg({\"C\": \"sum\", \"D\": \"std\"}) C D A bar 0.392940 1.366330 foo -1.796421 0.884785", "prev_chunk_id": "chunk_796", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_798", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Transformation#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Transformation#", "content": "Transformation# A transformation is a GroupBy operation whose result is indexed the same as the one being grouped. Common examples include cumsum() and diff(). speeds class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 grouped = speeds.groupby(\"class\")[\"max_speed\"] grouped.cumsum() falcon 389.0 parrot 413.0 lion 80.2 monkey NaN leopard 138.2 Name: max_speed, dtype: float64 grouped.diff() falcon NaN parrot -365.0 lion NaN monkey NaN leopard NaN Name: max_speed, dtype: float64 Unlike aggregations, the groupings that are used to split the original object are not included in the result. A common use of a transformation is to add the result back into the original DataFrame. result = speeds.copy() result[\"cumsum\"] = grouped.cumsum() result[\"diff\"] = grouped.diff() result class order max_speed cumsum diff falcon bird Falconiformes 389.0 389.0 NaN parrot bird Psittaciformes 24.0 413.0 -365.0 lion mammal Carnivora 80.2 80.2 NaN monkey mammal Primates NaN NaN NaN leopard mammal Carnivora 58.0 138.2 NaN", "prev_chunk_id": "chunk_797", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_799", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Built-in transformation methods#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Built-in transformation methods#", "content": "Built-in transformation methods# The following methods on GroupBy act as transformations. Method | Description bfill() | Back fill NA values within each group cumcount() | Compute the cumulative count within each group cummax() | Compute the cumulative max within each group cummin() | Compute the cumulative min within each group cumprod() | Compute the cumulative product within each group cumsum() | Compute the cumulative sum within each group diff() | Compute the difference between adjacent values within each group ffill() | Forward fill NA values within each group pct_change() | Compute the percent change between adjacent values within each group rank() | Compute the rank of each value within each group shift() | Shift values up or down within each group In addition, passing any built-in aggregation method as a string to transform() (see the next section) will broadcast the result across the group, producing a transformed result. If the aggregation method has an efficient implementation, this will be performant as well.", "prev_chunk_id": "chunk_798", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_800", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "The transform() method#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "The transform() method#", "content": "The transform() method# Similar to the aggregation method, the transform() method can accept string aliases to the built-in transformation methods in the previous section. It can also accept string aliases to the built-in aggregation methods. When an aggregation method is provided, the result will be broadcast across the group. speeds class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 grouped = speeds.groupby(\"class\")[[\"max_speed\"]] grouped.transform(\"cumsum\") max_speed falcon 389.0 parrot 413.0 lion 80.2 monkey NaN leopard 138.2 grouped.transform(\"sum\") max_speed falcon 413.0 parrot 413.0 lion 138.2 monkey 138.2 leopard 138.2 In addition to string aliases, the transform() method can also accept User-Defined Functions (UDFs). The UDF must: - Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g., a scalar,grouped.transform(lambdax:x.iloc[-1])). - Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply. - Not perform in-place operations on the group chunk. Group chunks should be treated as immutable, and changes to a group chunk may produce unexpected results. SeeMutating with User Defined Function (UDF) methodsfor more information. - (Optionally) operates on all columns of the entire group chunk at once. If this is supported, a fast path is used starting from thesecondchunk. Similar to The aggregate() method, the resulting dtype will reflect that of the transformation function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. Suppose we wish to standardize the data within each group: index = pd.date_range(\"10/1/1999\", periods=1100) ts = pd.Series(np.random.normal(0.5, 2, 1100), index) ts = ts.rolling(window=100, min_periods=100).mean().dropna() ts.head() 2000-01-08 0.779333 2000-01-09 0.778852 2000-01-10 0.786476 2000-01-11 0.782797 2000-01-12 0.798110 Freq: D, dtype: float64 ts.tail() 2002-09-30", "prev_chunk_id": "chunk_799", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_801", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "The transform() method#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "The transform() method#", "content": "0.660294 2002-10-01 0.631095 2002-10-02 0.673601 2002-10-03 0.709213 2002-10-04 0.719369 Freq: D, dtype: float64 transformed = ts.groupby(lambda x: x.year).transform( .....: lambda x: (x - x.mean()) / x.std() .....: ) .....: We would expect the result to now have mean 0 and standard deviation 1 within each group (up to floating-point error), which we can easily check: # Original Data grouped = ts.groupby(lambda x: x.year) grouped.mean() 2000 0.442441 2001 0.526246 2002 0.459365 dtype: float64 grouped.std() 2000 0.131752 2001 0.210945 2002 0.128753 dtype: float64 # Transformed Data grouped_trans = transformed.groupby(lambda x: x.year) grouped_trans.mean() 2000 -4.870756e-16 2001 -1.545187e-16 2002 4.136282e-16 dtype: float64 grouped_trans.std() 2000 1.0 2001 1.0 2002 1.0 dtype: float64 We can also visually compare the original and transformed data sets. compare = pd.DataFrame({\"Original\": ts, \"Transformed\": transformed}) compare.plot() <Axes: > Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array. ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min()) 2000-01-08 0.623893 2000-01-09 0.623893 2000-01-10 0.623893 2000-01-11 0.623893 2000-01-12 0.623893 ... 2002-09-30 0.558275 2002-10-01 0.558275 2002-10-02 0.558275 2002-10-03 0.558275 2002-10-04 0.558275 Freq: D, Length: 1001, dtype: float64 Another common data transform is to replace missing data with the group mean. cols = [\"A\", \"B\", \"C\"] values = np.random.randn(1000, 3) values[np.random.randint(0, 1000, 100), 0] = np.nan values[np.random.randint(0, 1000, 50), 1] = np.nan values[np.random.randint(0, 1000, 200), 2] = np.nan data_df = pd.DataFrame(values, columns=cols) data_df A B C 0 1.539708 -1.166480 0.533026 1 1.302092 -0.505754 NaN 2 -0.371983 1.104803 -0.651520 3 -1.309622 1.118697 -1.161657 4 -1.924296 0.396437 0.812436 .. ... ... ... 995 -0.093110 0.683847 -0.774753 996 -0.185043 1.438572 NaN 997 -0.394469 -0.642343 0.011374 998 -1.174126 1.857148 NaN 999 0.234564 0.517098 0.393534 [1000 rows x 3 columns] countries = np.array([\"US\", \"UK\", \"GR\", \"JP\"]) key = countries[np.random.randint(0, 4, 1000)] grouped = data_df.groupby(key) # Non-NA count in each group grouped.count() A B C GR", "prev_chunk_id": "chunk_800", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_802", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "The transform() method#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "The transform() method#", "content": "209 217 189 JP 240 255 217 UK 216 231 193 US 239 250 217 transformed = grouped.transform(lambda x: x.fillna(x.mean())) We can verify that the group means have not changed in the transformed data, and that the transformed data contains no NAs. grouped_trans = transformed.groupby(key) grouped.mean() # original group means A B C GR -0.098371 -0.015420 0.068053 JP 0.069025 0.023100 -0.077324 UK 0.034069 -0.052580 -0.116525 US 0.058664 -0.020399 0.028603 grouped_trans.mean() # transformation did not change group means A B C GR -0.098371 -0.015420 0.068053 JP 0.069025 0.023100 -0.077324 UK 0.034069 -0.052580 -0.116525 US 0.058664 -0.020399 0.028603 grouped.count() # original has some missing data points A B C GR 209 217 189 JP 240 255 217 UK 216 231 193 US 239 250 217 grouped_trans.count() # counts after transformation A B C GR 228 228 228 JP 267 267 267 UK 247 247 247 US 258 258 258 grouped_trans.size() # Verify non-NA count equals group size GR 228 JP 267 UK 247 US 258 dtype: int64 As mentioned in the note above, each of the examples in this section can be computed more efficiently using built-in methods. In the code below, the inefficient way using a UDF is commented out and the faster alternative appears below. # result = ts.groupby(lambda x: x.year).transform( # lambda x: (x - x.mean()) / x.std() # ) grouped = ts.groupby(lambda x: x.year) result = (ts - grouped.transform(\"mean\")) / grouped.transform(\"std\") # result = ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min()) grouped = ts.groupby(lambda x: x.year) result = grouped.transform(\"max\") - grouped.transform(\"min\") # grouped = data_df.groupby(key) # result = grouped.transform(lambda x: x.fillna(x.mean())) grouped = data_df.groupby(key) result = data_df.fillna(grouped.transform(\"mean\"))", "prev_chunk_id": "chunk_801", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_803", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Window and resample operations#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Window and resample operations#", "content": "Window and resample operations# It is possible to use resample(), expanding() and rolling() as methods on groupbys. The example below will apply the rolling() method on the samples of the column B, based on the groups of column A. df_re = pd.DataFrame({\"A\": [1] * 10 + [5] * 10, \"B\": np.arange(20)}) df_re A B 0 1 0 1 1 1 2 1 2 3 1 3 4 1 4 .. .. .. 15 5 15 16 5 16 17 5 17 18 5 18 19 5 19 [20 rows x 2 columns] df_re.groupby(\"A\").rolling(4).B.mean() A 1 0 NaN 1 NaN 2 NaN 3 1.5 4 2.5 ... 5 15 13.5 16 14.5 17 15.5 18 16.5 19 17.5 Name: B, Length: 20, dtype: float64 The expanding() method will accumulate a given operation (sum() in the example) for all the members of each particular group. df_re.groupby(\"A\").expanding().sum() B A 1 0 0.0 1 1.0 2 3.0 3 6.0 4 10.0 ... ... 5 15 75.0 16 91.0 17 108.0 18 126.0 19 145.0 [20 rows x 1 columns] Suppose you want to use the resample() method to get a daily frequency in each group of your dataframe, and wish to complete the missing values with the ffill() method. df_re = pd.DataFrame( .....: { .....: \"date\": pd.date_range(start=\"2016-01-01\", periods=4, freq=\"W\"), .....: \"group\": [1, 1, 2, 2], .....: \"val\": [5, 6, 7, 8], .....: } .....: ).set_index(\"date\") .....: df_re group val date 2016-01-03 1 5 2016-01-10 1 6 2016-01-17 2 7 2016-01-24 2 8 df_re.groupby(\"group\").resample(\"1D\", include_groups=False).ffill() val group date 1 2016-01-03 5 2016-01-04 5 2016-01-05 5 2016-01-06 5 2016-01-07 5 ... ... 2 2016-01-20 7 2016-01-21 7 2016-01-22 7 2016-01-23 7 2016-01-24 8 [16 rows x 1 columns]", "prev_chunk_id": "chunk_802", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_804", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Filtration#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Filtration#", "content": "Filtration# A filtration is a GroupBy operation that subsets the original grouping object. It may either filter out entire groups, part of groups, or both. Filtrations return a filtered version of the calling object, including the grouping columns when provided. In the following example, class is included in the result. speeds class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 speeds.groupby(\"class\").nth(1) class order max_speed parrot bird Psittaciformes 24.0 monkey mammal Primates NaN Filtrations will respect subsetting the columns of the GroupBy object. speeds.groupby(\"class\")[[\"order\", \"max_speed\"]].nth(1) order max_speed parrot Psittaciformes 24.0 monkey Primates NaN", "prev_chunk_id": "chunk_803", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_805", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Built-in filtrations#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Built-in filtrations#", "content": "Built-in filtrations# The following methods on GroupBy act as filtrations. All these methods have an efficient, GroupBy-specific, implementation. Method | Description head() | Select the top row(s) of each group nth() | Select the nth row(s) of each group tail() | Select the bottom row(s) of each group Users can also use transformations along with Boolean indexing to construct complex filtrations within groups. For example, suppose we are given groups of products and their volumes, and we wish to subset the data to only the largest products capturing no more than 90% of the total volume within each group. product_volumes = pd.DataFrame( .....: { .....: \"group\": list(\"xxxxyyy\"), .....: \"product\": list(\"abcdefg\"), .....: \"volume\": [10, 30, 20, 15, 40, 10, 20], .....: } .....: ) .....: product_volumes group product volume 0 x a 10 1 x b 30 2 x c 20 3 x d 15 4 y e 40 5 y f 10 6 y g 20 # Sort by volume to select the largest products first product_volumes = product_volumes.sort_values(\"volume\", ascending=False) grouped = product_volumes.groupby(\"group\")[\"volume\"] cumpct = grouped.cumsum() / grouped.transform(\"sum\") cumpct 4 0.571429 1 0.400000 2 0.666667 6 0.857143 3 0.866667 0 1.000000 5 1.000000 Name: volume, dtype: float64 significant_products = product_volumes[cumpct <= 0.9] significant_products.sort_values([\"group\", \"product\"]) group product volume 1 x b 30 2 x c 20 3 x d 15 4 y e 40 6 y g 20", "prev_chunk_id": "chunk_804", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_806", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "The filter method#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "The filter method#", "content": "The filter method# The filter method takes a User-Defined Function (UDF) that, when applied to an entire group, returns either True or False. The result of the filter method is then the subset of groups for which the UDF returned True. Suppose we want to take only elements that belong to groups with a group sum greater than 2. sf = pd.Series([1, 1, 2, 3, 3, 3]) sf.groupby(sf).filter(lambda x: x.sum() > 2) 3 3 4 3 5 3 dtype: int64 Another useful operation is filtering out elements that belong to groups with only a couple members. dff = pd.DataFrame({\"A\": np.arange(8), \"B\": list(\"aabbbbcc\")}) dff.groupby(\"B\").filter(lambda x: len(x) > 2) A B 2 2 b 3 3 b 4 4 b 5 5 b Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs. dff.groupby(\"B\").filter(lambda x: len(x) > 2, dropna=False) A B 0 NaN NaN 1 NaN NaN 2 2.0 b 3 3.0 b 4 4.0 b 5 5.0 b 6 NaN NaN 7 NaN NaN For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion. dff[\"C\"] = np.arange(8) dff.groupby(\"B\").filter(lambda x: len(x[\"C\"]) > 2) A B C 2 2 b 2 3 3 b 3 4 4 b 4 5 5 b 5", "prev_chunk_id": "chunk_805", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_807", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Flexible apply#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Flexible apply#", "content": "Flexible apply# Some operations on the grouped data might not fit into the aggregation, transformation, or filtration categories. For these, you can use the apply function. df A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 grouped = df.groupby(\"A\") # could also just call .describe() grouped[\"C\"].apply(lambda x: x.describe()) A bar count 3.000000 mean 0.130980 std 0.181231 min -0.077118 25% 0.069390 ... foo min -1.143704 25% -0.862495 50% -0.575247 75% -0.408530 max 1.193555 Name: C, Length: 16, dtype: float64 The dimension of the returned result can also change: grouped = df.groupby('A')['C'] def f(group): .....: return pd.DataFrame({'original': group, .....: 'demeaned': group - group.mean()}) .....: grouped.apply(f) original demeaned A bar 1 0.254161 0.123181 3 0.215897 0.084917 5 -0.077118 -0.208098 foo 0 -0.575247 -0.215962 2 -1.143704 -0.784420 4 1.193555 1.552839 6 -0.408530 -0.049245 7 -0.862495 -0.503211 apply on a Series can operate on a returned value from the applied function that is itself a series, and possibly upcast the result to a DataFrame: def f(x): .....: return pd.Series([x, x ** 2], index=[\"x\", \"x^2\"]) .....: s = pd.Series(np.random.rand(5)) s 0 0.582898 1 0.098352 2 0.001438 3 0.009420 4 0.815826 dtype: float64 s.apply(f) x x^2 0 0.582898 0.339770 1 0.098352 0.009673 2 0.001438 0.000002 3 0.009420 0.000089 4 0.815826 0.665572 Similar to The aggregate() method, the resulting dtype will reflect that of the apply function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction.", "prev_chunk_id": "chunk_806", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_808", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Control grouped column(s) placement with group_keys#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Control grouped column(s) placement with group_keys#", "content": "Control grouped column(s) placement with group_keys# To control whether the grouped column(s) are included in the indices, you can use the argument group_keys which defaults to True. Compare df.groupby(\"A\", group_keys=True).apply(lambda x: x, include_groups=False) B C D A bar 1 one 0.254161 1.511763 3 three 0.215897 -0.990582 5 two -0.077118 1.211526 foo 0 one -0.575247 1.346061 2 two -1.143704 1.627081 4 two 1.193555 -0.441652 6 one -0.408530 0.268520 7 three -0.862495 0.024580 with df.groupby(\"A\", group_keys=False).apply(lambda x: x, include_groups=False) B C D 0 one -0.575247 1.346061 1 one 0.254161 1.511763 2 two -1.143704 1.627081 3 three 0.215897 -0.990582 4 two 1.193555 -0.441652 5 two -0.077118 1.211526 6 one -0.408530 0.268520 7 three -0.862495 0.024580", "prev_chunk_id": "chunk_807", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_809", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Numba Accelerated Routines#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Numba Accelerated Routines#", "content": "Numba Accelerated Routines# If Numba is installed as an optional dependency, the transform and aggregate methods support engine='numba' and engine_kwargs arguments. See enhancing performance with Numba for general usage of the arguments and performance considerations. The function signature must start with values, index exactly as the data belonging to each group will be passed into values, and the group index will be passed into index.", "prev_chunk_id": "chunk_808", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_810", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Exclusion of non-numeric columns#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Exclusion of non-numeric columns#", "content": "Exclusion of non-numeric columns# Again consider the example DataFrame we’ve been looking at: df A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 Suppose we wish to compute the standard deviation grouped by the A column. There is a slight problem, namely that we don’t care about the data in column B because it is not numeric. You can avoid non-numeric columns by specifying numeric_only=True: df.groupby(\"A\").std(numeric_only=True) C D A bar 0.181231 1.366330 foo 0.912265 0.884785 Note that df.groupby('A').colname.std(). is more efficient than df.groupby('A').std().colname. So if the result of an aggregation function is only needed over one column (here colname), it may be filtered before applying the aggregation function. from decimal import Decimal df_dec = pd.DataFrame( .....: { .....: \"id\": [1, 2, 1, 2], .....: \"int_column\": [1, 2, 3, 4], .....: \"dec_column\": [ .....: Decimal(\"0.50\"), .....: Decimal(\"0.15\"), .....: Decimal(\"0.25\"), .....: Decimal(\"0.40\"), .....: ], .....: } .....: ) .....: df_dec.groupby([\"id\"])[[\"dec_column\"]].sum() dec_column id 1 0.75 2 0.55", "prev_chunk_id": "chunk_809", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_811", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Handling of (un)observed Categorical values#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Handling of (un)observed Categorical values#", "content": "Handling of (un)observed Categorical values# When using a Categorical grouper (as a single grouper, or as part of multiple groupers), the observed keyword controls whether to return a cartesian product of all possible groupers values (observed=False) or only those that are observed groupers (observed=True). Show all values: pd.Series([1, 1, 1]).groupby( .....: pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=False .....: ).count() .....: a 3 b 0 dtype: int64 Show only the observed values: pd.Series([1, 1, 1]).groupby( .....: pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=True .....: ).count() .....: a 3 dtype: int64 The returned dtype of the grouped will always include all of the categories that were grouped. s = ( .....: pd.Series([1, 1, 1]) .....: .groupby(pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=True) .....: .count() .....: ) .....: s.index.dtype CategoricalDtype(categories=['a', 'b'], ordered=False, categories_dtype=object)", "prev_chunk_id": "chunk_810", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_812", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "NA group handling#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "NA group handling#", "content": "NA group handling# By NA, we are referring to any NA values, including NA, NaN, NaT, and None. If there are any NA values in the grouping key, by default these will be excluded. In other words, any “NA group” will be dropped. You can include NA groups by specifying dropna=False. df = pd.DataFrame({\"key\": [1.0, 1.0, np.nan, 2.0, np.nan], \"A\": [1, 2, 3, 4, 5]}) df key A 0 1.0 1 1 1.0 2 2 NaN 3 3 2.0 4 4 NaN 5 df.groupby(\"key\", dropna=True).sum() A key 1.0 3 2.0 4 df.groupby(\"key\", dropna=False).sum() A key 1.0 3 2.0 4 NaN 8", "prev_chunk_id": "chunk_811", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_813", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Grouping with ordered factors#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Grouping with ordered factors#", "content": "Grouping with ordered factors# Categorical variables represented as instances of pandas’s Categorical class can be used as group keys. If so, the order of the levels will be preserved. When observed=False and sort=False, any unobserved categories will be at the end of the result in order. days = pd.Categorical( .....: values=[\"Wed\", \"Mon\", \"Thu\", \"Mon\", \"Wed\", \"Sat\"], .....: categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"], .....: ) .....: data = pd.DataFrame( .....: { .....: \"day\": days, .....: \"workers\": [3, 4, 1, 4, 2, 2], .....: } .....: ) .....: data day workers 0 Wed 3 1 Mon 4 2 Thu 1 3 Mon 4 4 Wed 2 5 Sat 2 data.groupby(\"day\", observed=False, sort=True).sum() workers day Mon 8 Tue 0 Wed 5 Thu 1 Fri 0 Sat 2 Sun 0 data.groupby(\"day\", observed=False, sort=False).sum() workers day Wed 5 Mon 8 Thu 1 Sat 2 Tue 0 Fri 0 Sun 0", "prev_chunk_id": "chunk_812", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_814", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Grouping with a grouper specification#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Grouping with a grouper specification#", "content": "Grouping with a grouper specification# You may need to specify a bit more data to properly group. You can use the pd.Grouper to provide this local control. import datetime df = pd.DataFrame( .....: { .....: \"Branch\": \"A A A A A A A B\".split(), .....: \"Buyer\": \"Carl Mark Carl Carl Joe Joe Joe Carl\".split(), .....: \"Quantity\": [1, 3, 5, 1, 8, 1, 9, 3], .....: \"Date\": [ .....: datetime.datetime(2013, 1, 1, 13, 0), .....: datetime.datetime(2013, 1, 1, 13, 5), .....: datetime.datetime(2013, 10, 1, 20, 0), .....: datetime.datetime(2013, 10, 2, 10, 0), .....: datetime.datetime(2013, 10, 1, 20, 0), .....: datetime.datetime(2013, 10, 2, 10, 0), .....: datetime.datetime(2013, 12, 2, 12, 0), .....: datetime.datetime(2013, 12, 2, 14, 0), .....: ], .....: } .....: ) .....: df Branch Buyer Quantity Date 0 A Carl 1 2013-01-01 13:00:00 1 A Mark 3 2013-01-01 13:05:00 2 A Carl 5 2013-10-01 20:00:00 3 A Carl 1 2013-10-02 10:00:00 4 A Joe 8 2013-10-01 20:00:00 5 A Joe 1 2013-10-02 10:00:00 6 A Joe 9 2013-12-02 12:00:00 7 B Carl 3 2013-12-02 14:00:00 Groupby a specific column with the desired frequency. This is like resampling. df.groupby([pd.Grouper(freq=\"1ME\", key=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum() Quantity Date Buyer 2013-01-31 Carl 1 Mark 3 2013-10-31 Carl 6 Joe 9 2013-12-31 Carl 3 Joe 9 When freq is specified, the object returned by pd.Grouper will be an instance of pandas.api.typing.TimeGrouper. When there is a column and index with the same name, you can use key to group by the column and level to group by the index. df = df.set_index(\"Date\") df[\"Date\"] = df.index + pd.offsets.MonthEnd(2) df.groupby([pd.Grouper(freq=\"6ME\", key=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum() Quantity Date Buyer 2013-02-28 Carl 1 Mark 3 2014-02-28 Carl 9 Joe 18 df.groupby([pd.Grouper(freq=\"6ME\", level=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum() Quantity Date Buyer 2013-01-31 Carl 1 Mark 3 2014-01-31 Carl 9 Joe 18", "prev_chunk_id": "chunk_813", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_815", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Taking the first rows of each group#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Taking the first rows of each group#", "content": "Taking the first rows of each group# Just like for a DataFrame or Series you can call head and tail on a groupby: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=[\"A\", \"B\"]) df A B 0 1 2 1 1 4 2 5 6 g = df.groupby(\"A\") g.head(1) A B 0 1 2 2 5 6 g.tail(1) A B 1 1 4 2 5 6 This shows the first or last n rows from each group.", "prev_chunk_id": "chunk_814", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_816", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Taking the nth row of each group#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Taking the nth row of each group#", "content": "Taking the nth row of each group# To select the nth item from each group, use DataFrameGroupBy.nth() or SeriesGroupBy.nth(). Arguments supplied can be any integer, lists of integers, slices, or lists of slices; see below for examples. When the nth element of a group does not exist an error is not raised; instead no corresponding rows are returned. In general this operation acts as a filtration. In certain cases it will also return one row per group, making it also a reduction. However because in general it can return zero or multiple rows per group, pandas treats it as a filtration in all cases. df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=[\"A\", \"B\"]) g = df.groupby(\"A\") g.nth(0) A B 0 1 NaN 2 5 6.0 g.nth(-1) A B 1 1 4.0 2 5 6.0 g.nth(1) A B 1 1 4.0 If the nth element of a group does not exist, then no corresponding row is included in the result. In particular, if the specified n is larger than any group, the result will be an empty DataFrame. g.nth(5) Empty DataFrame Columns: [A, B] Index: [] If you want to select the nth not-null item, use the dropna kwarg. For a DataFrame this should be either 'any' or 'all' just like you would pass to dropna: # nth(0) is the same as g.first() g.nth(0, dropna=\"any\") A B 1 1 4.0 2 5 6.0 g.first() B A 1 4.0 5 6.0 # nth(-1) is the same as g.last() g.nth(-1, dropna=\"any\") A B 1 1 4.0 2 5 6.0 g.last() B A 1 4.0 5 6.0 g.B.nth(0, dropna=\"all\") 1 4.0 2 6.0 Name: B, dtype: float64 You can also select multiple rows from each group by specifying multiple nth values as a list of ints. business_dates = pd.date_range(start=\"4/1/2014\", end=\"6/30/2014\", freq=\"B\") df =", "prev_chunk_id": "chunk_815", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_817", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Taking the nth row of each group#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Taking the nth row of each group#", "content": "pd.DataFrame(1, index=business_dates, columns=[\"a\", \"b\"]) # get the first, 4th, and last date index for each month df.groupby([df.index.year, df.index.month]).nth([0, 3, -1]) a b 2014-04-01 1 1 2014-04-04 1 1 2014-04-30 1 1 2014-05-01 1 1 2014-05-06 1 1 2014-05-30 1 1 2014-06-02 1 1 2014-06-05 1 1 2014-06-30 1 1 You may also use slices or lists of slices. df.groupby([df.index.year, df.index.month]).nth[1:] a b 2014-04-02 1 1 2014-04-03 1 1 2014-04-04 1 1 2014-04-07 1 1 2014-04-08 1 1 ... .. .. 2014-06-24 1 1 2014-06-25 1 1 2014-06-26 1 1 2014-06-27 1 1 2014-06-30 1 1 [62 rows x 2 columns] df.groupby([df.index.year, df.index.month]).nth[1:, :-1] a b 2014-04-01 1 1 2014-04-02 1 1 2014-04-03 1 1 2014-04-04 1 1 2014-04-07 1 1 ... .. .. 2014-06-24 1 1 2014-06-25 1 1 2014-06-26 1 1 2014-06-27 1 1 2014-06-30 1 1 [65 rows x 2 columns]", "prev_chunk_id": "chunk_816", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_818", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Enumerate group items#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Enumerate group items#", "content": "Enumerate group items# To see the order in which each row appears within its group, use the cumcount method: dfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"]) dfg A 0 a 1 a 2 a 3 b 4 b 5 a dfg.groupby(\"A\").cumcount() 0 0 1 1 2 2 3 0 4 1 5 3 dtype: int64 dfg.groupby(\"A\").cumcount(ascending=False) 0 3 1 2 2 1 3 1 4 0 5 0 dtype: int64", "prev_chunk_id": "chunk_817", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_819", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Enumerate groups#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Enumerate groups#", "content": "Enumerate groups# To see the ordering of the groups (as opposed to the order of rows within a group given by cumcount) you can use DataFrameGroupBy.ngroup(). Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed. dfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"]) dfg A 0 a 1 a 2 a 3 b 4 b 5 a dfg.groupby(\"A\").ngroup() 0 0 1 0 2 0 3 1 4 1 5 0 dtype: int64 dfg.groupby(\"A\").ngroup(ascending=False) 0 1 1 1 2 1 3 0 4 0 5 1 dtype: int64", "prev_chunk_id": "chunk_818", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_820", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Plotting#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Plotting#", "content": "Plotting# Groupby also works with some plotting methods. In this case, suppose we suspect that the values in column 1 are 3 times higher on average in group “B”. np.random.seed(1234) df = pd.DataFrame(np.random.randn(50, 2)) df[\"g\"] = np.random.choice([\"A\", \"B\"], size=50) df.loc[df[\"g\"] == \"B\", 1] += 3 We can easily visualize this with a boxplot: df.groupby(\"g\").boxplot() A Axes(0.1,0.15;0.363636x0.75) B Axes(0.536364,0.15;0.363636x0.75) dtype: object The result of calling boxplot is a dictionary whose keys are the values of our grouping column g (“A” and “B”). The values of the resulting dictionary can be controlled by the return_type keyword of boxplot. See the visualization documentation for more.", "prev_chunk_id": "chunk_819", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_821", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Piping function calls#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Piping function calls#", "content": "Piping function calls# Similar to the functionality provided by DataFrame and Series, functions that take GroupBy objects can be chained together using a pipe method to allow for a cleaner, more readable syntax. To read about .pipe in general terms, see here. Combining .groupby and .pipe is often useful when you need to reuse GroupBy objects. As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. We’d like to do a groupwise calculation of prices (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data: n = 1000 df = pd.DataFrame( .....: { .....: \"Store\": np.random.choice([\"Store_1\", \"Store_2\"], n), .....: \"Product\": np.random.choice([\"Product_1\", \"Product_2\"], n), .....: \"Revenue\": (np.random.random(n) * 50 + 10).round(2), .....: \"Quantity\": np.random.randint(1, 10, size=n), .....: } .....: ) .....: df.head(2) Store Product Revenue Quantity 0 Store_2 Product_1 26.12 1 1 Store_2 Product_1 28.86 1 We now find the prices per store/product. ( .....: df.groupby([\"Store\", \"Product\"]) .....: .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum()) .....: .unstack() .....: .round(2) .....: ) .....: Product Product_1 Product_2 Store Store_1 6.82 7.05 Store_2 6.30 6.64 Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example: def mean(groupby): .....: return groupby.mean() .....: df.groupby([\"Store\", \"Product\"]).pipe(mean) Revenue Quantity Store Product Store_1 Product_1 34.622727 5.075758 Product_2 35.482815 5.029630 Store_2 Product_1 32.972837 5.237589 Product_2 34.684360 5.224000 Here mean takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The mean function can be any function that takes in a GroupBy object; the .pipe will pass the GroupBy object as a parameter into the function you specify.", "prev_chunk_id": "chunk_820", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_822", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Multi-column factorization#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Multi-column factorization#", "content": "Multi-column factorization# By using DataFrameGroupBy.ngroup(), we can extract information about the groups in a way similar to factorize() (as described further in the reshaping API) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding. (For more information about support in pandas for full categorical data, see the Categorical introduction and the API documentation.) dfg = pd.DataFrame({\"A\": [1, 1, 2, 3, 2], \"B\": list(\"aaaba\")}) dfg A B 0 1 a 1 1 a 2 2 a 3 3 b 4 2 a dfg.groupby([\"A\", \"B\"]).ngroup() 0 0 1 0 2 1 3 2 4 1 dtype: int64 dfg.groupby([\"A\", [0, 0, 0, 1, 1]]).ngroup() 0 0 1 0 2 1 3 3 4 2 dtype: int64", "prev_chunk_id": "chunk_821", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_823", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Groupby by indexer to ‘resample’ data#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Groupby by indexer to ‘resample’ data#", "content": "Groupby by indexer to ‘resample’ data# Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples. In order for resample to work on indices that are non-datetimelike, the following procedure can be utilized. In the following examples, df.index // 5 returns an integer array which is used to determine what gets selected for the groupby operation. df = pd.DataFrame(np.random.randn(10, 2)) df 0 1 0 -0.793893 0.321153 1 0.342250 1.618906 2 -0.975807 1.918201 3 -0.810847 -1.405919 4 -1.977759 0.461659 5 0.730057 -1.316938 6 -0.751328 0.528290 7 -0.257759 -1.081009 8 0.505895 -1.701948 9 -1.006349 0.020208 df.index // 5 Index([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype='int64') df.groupby(df.index // 5).std() 0 1 0 0.823647 1.312912 1 0.760109 0.942941", "prev_chunk_id": "chunk_822", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_824", "url": "https://pandas.pydata.org/docs/user_guide/groupby.html", "title": "Returning a Series to propagate names#", "page_title": "Group by: split-apply-combine — pandas 2.3.1 documentation", "breadcrumbs": "Returning a Series to propagate names#", "content": "Returning a Series to propagate names# Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking, in which the column index name will be used as the name of the inserted column: df = pd.DataFrame( .....: { .....: \"a\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2], .....: \"b\": [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1], .....: \"c\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], .....: \"d\": [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], .....: } .....: ) .....: def compute_metrics(x): .....: result = {\"b_sum\": x[\"b\"].sum(), \"c_mean\": x[\"c\"].mean()} .....: return pd.Series(result, name=\"metrics\") .....: result = df.groupby(\"a\").apply(compute_metrics, include_groups=False) result metrics b_sum c_mean a 0 2.0 0.5 1 2.0 0.5 2 2.0 0.5 result.stack(future_stack=True) a metrics 0 b_sum 2.0 c_mean 0.5 1 b_sum 2.0 c_mean 0.5 2 b_sum 2.0 c_mean 0.5 dtype: float64", "prev_chunk_id": "chunk_823", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_825", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Table Visualization#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Table Visualization#", "content": "Table Visualization# This section demonstrates visualization of tabular data using the Styler class. For information on visualization with charting please see Chart Visualization. This document is written as a Jupyter Notebook, and can be viewed or downloaded here.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_826", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Styler Object and Customising the Display#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Styler Object and Customising the Display#", "content": "Styler Object and Customising the Display# Styling and output display customisation should be performed after the data in a DataFrame has been processed. The Styler is not dynamically updated if further changes to the DataFrame are made. The DataFrame.style attribute is a property that returns a Styler object. It has a _repr_html_ method defined on it so it is rendered automatically in Jupyter Notebook. The Styler, which can be used for large data but is primarily designed for small data, currently has the ability to output to these formats: - HTML - LaTeX - String (and CSV by extension) - Excel - (JSON is not currently available) The first three of these have display customisation methods designed to format and customise the output. These include: - Formatting values, the index and columns headers, using.format()and.format_index(), - Renaming the index or column header labels, using.relabel_index() - Hiding certain columns, the index and/or column headers, or index names, using.hide() - Concatenating similar DataFrames, using.concat()", "prev_chunk_id": "chunk_825", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_827", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Formatting Values#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Formatting Values#", "content": "Formatting Values# The Styler distinguishes the display value from the actual value, in both data values and index or columns headers. To control the display value, the text is printed in each cell as a string, and we can use the .format() and .format_index() methods to manipulate this according to a format spec string or a callable that takes a single value and returns a string. It is possible to define this for the whole table, or index, or for individual columns, or MultiIndex levels. We can also overwrite index names. Additionally, the format function has a precision argument to specifically help format floats, as well as decimal and thousands separators to support other locales, an na_rep argument to display missing data, and an escape and hyperlinks arguments to help displaying safe-HTML or safe-LaTeX. The default formatter is configured to adopt pandas’ global options such as styler.format.precision option, controllable using with pd.option_context('format.precision', 2): Using Styler to manipulate the display is a useful feature because maintaining the indexing and data values for other purposes gives greater control. You do not have to overwrite your DataFrame to display it how you like. Here is a more comprehensive example of using the formatting functions whilst still relying on the underlying data for indexing and calculations.", "prev_chunk_id": "chunk_826", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_828", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Hiding Data#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Hiding Data#", "content": "Hiding Data# The index and column headers can be completely hidden, as well subselecting rows or columns that one wishes to exclude. Both these options are performed using the same methods. The index can be hidden from rendering by calling .hide() without any arguments, which might be useful if your index is integer based. Similarly column headers can be hidden by calling .hide(axis=”columns”) without any further arguments. Specific rows or columns can be hidden from rendering by calling the same .hide() method and passing in a row/column label, a list-like or a slice of row/column labels to for the subset argument. Hiding does not change the integer arrangement of CSS classes, e.g. hiding the first two columns of a DataFrame means the column class indexing will still start at col2, since col0 and col1 are simply ignored. To invert the function to a show functionality it is best practice to compose a list of hidden items.", "prev_chunk_id": "chunk_827", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_829", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Concatenating DataFrame Outputs#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Concatenating DataFrame Outputs#", "content": "Concatenating DataFrame Outputs# Two or more Stylers can be concatenated together provided they share the same columns. This is very useful for showing summary statistics for a DataFrame, and is often used in combination with DataFrame.agg. Since the objects concatenated are Stylers they can independently be styled as will be shown below and their concatenation preserves those styles.", "prev_chunk_id": "chunk_828", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_830", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Styler Object and HTML#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Styler Object and HTML#", "content": "Styler Object and HTML# The Styler was originally constructed to support the wide array of HTML formatting options. Its HTML output creates an HTML <table> and leverages CSS styling language to manipulate many parameters including colors, fonts, borders, background, etc. See here for more information on styling HTML tables. This allows a lot of flexibility out of the box, and even enables web developers to integrate DataFrames into their exiting user interface designs. Below we demonstrate the default output, which looks very similar to the standard DataFrame HTML representation. But the HTML here has already attached some CSS classes to each cell, even if we haven’t yet created any styles. We can view these by calling the .to_html() method, which returns the raw HTML as string, which is useful for further processing or adding to a file - read on in More about CSS and HTML. This section will also provide a walkthrough for how to convert this default output to represent a DataFrame output that is more communicative. For example how we can build s: The first step we have taken is the create the Styler object from the DataFrame and then select the range of interest by hiding unwanted columns with .hide().", "prev_chunk_id": "chunk_829", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_831", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Methods to Add Styles#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Methods to Add Styles#", "content": "Methods to Add Styles# There are 3 primary methods of adding custom CSS styles to Styler: - Using.set_table_styles()to control broader areas of the table with specified internal CSS. Although table styles allow the flexibility to add CSS selectors and properties controlling all individual parts of the table, they are unwieldy for individual cell specifications. Also, note that table styles cannot be exported to Excel. - Using.set_td_classes()to directly link either external CSS classes to your data cells or link the internal CSS classes created by.set_table_styles(). Seehere. These cannot be used on column header rows or indexes, and also won’t export to Excel. - Using the.apply()and.map()functions to add direct internal CSS to specific data cells. Seehere. As of v1.4.0 there are also methods that work directly on column header rows or indexes;.apply_index()and.map_index(). Note that only these methods add styles that will export to Excel. These methods work in a similar way toDataFrame.apply()andDataFrame.map().", "prev_chunk_id": "chunk_830", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_832", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Table Styles#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Table Styles#", "content": "Table Styles# Table styles are flexible enough to control all individual parts of the table, including column headers and indexes. However, they can be unwieldy to type for individual data cells or for any kind of conditional formatting, so we recommend that table styles are used for broad styling, such as entire rows or columns at a time. Table styles are also used to control features which can apply to the whole table at once such as creating a generic hover functionality. The :hover pseudo-selector, as well as other pseudo-selectors, can only be used this way. To replicate the normal format of CSS selectors and properties (attribute value pairs), e.g. tr:hover { background-color: #ffff99; } the necessary format to pass styles to .set_table_styles() is as a list of dicts, each with a CSS-selector tag and CSS-properties. Properties can either be a list of 2-tuples, or a regular CSS-string, for example: Next we just add a couple more styling artifacts targeting specific parts of the table. Be careful here, since we are chaining methods we need to explicitly instruct the method not to overwrite the existing styles. As a convenience method (since version 1.2.0) we can also pass a dict to .set_table_styles() which contains row or column keys. Behind the scenes Styler just indexes the keys and adds relevant .col<m> or .row<n> classes as necessary to the given CSS selectors.", "prev_chunk_id": "chunk_831", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_833", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Setting Classes and Linking to External CSS#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Setting Classes and Linking to External CSS#", "content": "Setting Classes and Linking to External CSS# If you have designed a website then it is likely you will already have an external CSS file that controls the styling of table and cell objects within it. You may want to use these native files rather than duplicate all the CSS in python (and duplicate any maintenance work).", "prev_chunk_id": "chunk_832", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_834", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Table Attributes#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Table Attributes#", "content": "Table Attributes# It is very easy to add a class to the main <table> using .set_table_attributes(). This method can also attach inline styles - read more in CSS Hierarchies.", "prev_chunk_id": "chunk_833", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_835", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Data Cell CSS Classes#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Data Cell CSS Classes#", "content": "Data Cell CSS Classes# New in version 1.2.0 The .set_td_classes() method accepts a DataFrame with matching indices and columns to the underlying Styler’s DataFrame. That DataFrame will contain strings as css-classes to add to individual data cells: the <td> elements of the <table>. Rather than use external CSS we will create our classes internally and add them to table style. We will save adding the borders until the section on tooltips.", "prev_chunk_id": "chunk_834", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_836", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Acting on Data#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Acting on Data#", "content": "Acting on Data# We use the following methods to pass your style functions. Both of those methods take a function (and some other keyword arguments) and apply it to the DataFrame in a certain way, rendering CSS styles. - .map()(elementwise): accepts a function that takes a single value and returns a string with the CSS attribute-value pair. - .apply()(column-/row-/table-wise): accepts a function that takes a Series or DataFrame and returns a Series, DataFrame, or numpy array with an identical shape where each element is a string with a CSS attribute-value pair. This method passes each column or row of your DataFrame one-at-a-time or the entire table at once, depending on theaxiskeyword argument. For columnwise useaxis=0, rowwise useaxis=1, and for the entire table at once useaxis=None. This method is powerful for applying multiple, complex logic to data cells. We create a new DataFrame to demonstrate this. For example we can build a function that colors text if it is negative, and chain this with a function that partially fades cells of negligible value. Since this looks at each element in turn we use map. We can also build a function that highlights the maximum value across rows, cols, and the DataFrame all at once. In this case we use apply. Below we highlight the maximum in a column. We can use the same function across the different axes, highlighting here the DataFrame maximum in purple, and row maximums in pink. This last example shows how some styles have been overwritten by others. In general the most recent style applied is active but you can read more in the section on CSS hierarchies. You can also apply these styles to more granular parts of the DataFrame - read more in section on subset slicing. It is possible to replicate some of this", "prev_chunk_id": "chunk_835", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_837", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Acting on Data#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Acting on Data#", "content": "functionality using just classes but it can be more cumbersome. See item 3) of Optimization", "prev_chunk_id": "chunk_836", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_838", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Acting on the Index and Column Headers#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Acting on the Index and Column Headers#", "content": "Acting on the Index and Column Headers# Similar application is achieved for headers by using: - .map_index()(elementwise): accepts a function that takes a single value and returns a string with the CSS attribute-value pair. - .apply_index()(level-wise): accepts a function that takes a Series and returns a Series, or numpy array with an identical shape where each element is a string with a CSS attribute-value pair. This method passes each level of your Index one-at-a-time. To style the index useaxis=0and to style the column headers useaxis=1. You can select a level of a MultiIndex but currently no similar subset application is available for these methods.", "prev_chunk_id": "chunk_837", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_839", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Tooltips and Captions#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Tooltips and Captions#", "content": "Tooltips and Captions# Table captions can be added with the .set_caption() method. You can use table styles to control the CSS relevant to the caption. Adding tooltips (since version 1.3.0) can be done using the .set_tooltips() method in the same way you can add CSS classes to data cells by providing a string based DataFrame with intersecting indices and columns. You don’t have to specify a css_class name or any css props for the tooltips, since there are standard defaults, but the option is there if you want more visual control. The only thing left to do for our table is to add the highlighting borders to draw the audience attention to the tooltips. We will create internal CSS classes as before using table styles. Setting classes always overwrites so we need to make sure we add the previous classes.", "prev_chunk_id": "chunk_838", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_840", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Finer Control with Slicing#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Finer Control with Slicing#", "content": "Finer Control with Slicing# The examples we have shown so far for the Styler.apply and Styler.map functions have not demonstrated the use of the subset argument. This is a useful argument which permits a lot of flexibility: it allows you to apply styles to specific rows or columns, without having to code that logic into your style function. The value passed to subset behaves similar to slicing a DataFrame; - A scalar is treated as a column label - A list (or Series or NumPy array) is treated as multiple column labels - A tuple is treated as(row_indexer,column_indexer) Consider using pd.IndexSlice to construct the tuple for the last one. We will create a MultiIndexed DataFrame to demonstrate the functionality. We will use subset to highlight the maximum in the third and fourth columns with red text. We will highlight the subset sliced region in yellow. If combined with the IndexSlice as suggested then it can index across both dimensions with greater flexibility. This also provides the flexibility to sub select rows when used with the axis=1. There is also scope to provide conditional filtering. Suppose we want to highlight the maximum across columns 2 and 4 only in the case that the sum of columns 1 and 3 is less than -2.0 (essentially excluding rows (:,'r2')). Only label-based slicing is supported right now, not positional, and not callables. If your style function uses a subset or axis keyword argument, consider wrapping your function in a functools.partial, partialing out that keyword. my_func2 = functools.partial(my_func, subset=42)", "prev_chunk_id": "chunk_839", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_841", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Optimization#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Optimization#", "content": "Optimization# Generally, for smaller tables and most cases, the rendered HTML does not need to be optimized, and we don’t really recommend it. There are two cases where it is worth considering: - If you are rendering and styling a very large HTML table, certain browsers have performance issues. - If you are usingStylerto dynamically create part of online user interfaces and want to improve network performance. Here we recommend the following steps to implement:", "prev_chunk_id": "chunk_840", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_842", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "1. Remove UUID and cell_ids#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "1. Remove UUID and cell_ids#", "content": "1. Remove UUID and cell_ids# Ignore the uuid and set cell_ids to False. This will prevent unnecessary HTML.", "prev_chunk_id": "chunk_841", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_843", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "2. Use table styles#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "2. Use table styles#", "content": "2. Use table styles# Use table styles where possible (e.g. for all cells or rows or columns at a time) since the CSS is nearly always more efficient than other formats.", "prev_chunk_id": "chunk_842", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_844", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "3. Set classes instead of using Styler functions#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "3. Set classes instead of using Styler functions#", "content": "3. Set classes instead of using Styler functions# For large DataFrames where the same style is applied to many cells it can be more efficient to declare the styles as classes and then apply those classes to data cells, rather than directly applying styles to cells. It is, however, probably still easier to use the Styler function api when you are not concerned about optimization.", "prev_chunk_id": "chunk_843", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_845", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "4. Don’t use tooltips#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "4. Don’t use tooltips#", "content": "4. Don’t use tooltips# Tooltips require cell_ids to work and they generate extra HTML elements for every data cell.", "prev_chunk_id": "chunk_844", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_846", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "5. If every byte counts use string replacement#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "5. If every byte counts use string replacement#", "content": "5. If every byte counts use string replacement# You can remove unnecessary HTML, or shorten the default class names by replacing the default css dict. You can read a little more about CSS below.", "prev_chunk_id": "chunk_845", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_847", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Builtin Styles#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Builtin Styles#", "content": "Builtin Styles# Some styling functions are common enough that we’ve “built them in” to the Styler, so you don’t have to write them and apply them yourself. The current list of such functions is: - .highlight_null: for use with identifying missing data. - .highlight_minand.highlight_max: for use with identifying extremeties in data. - .highlight_betweenand.highlight_quantile: for use with identifying classes within data. - .background_gradient: a flexible method for highlighting cells based on their, or other, values on a numeric scale. - .text_gradient: similar method for highlighting text based on their, or other, values on a numeric scale. - .bar: to display mini-charts within cell backgrounds. The individual documentation on each function often gives more examples of their arguments.", "prev_chunk_id": "chunk_846", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_848", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Highlight Between#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Highlight Between#", "content": "Highlight Between# This method accepts ranges as float, or NumPy arrays or Series provided the indexes match.", "prev_chunk_id": "chunk_847", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_849", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Highlight Quantile#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Highlight Quantile#", "content": "Highlight Quantile# Useful for detecting the highest or lowest percentile values", "prev_chunk_id": "chunk_848", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_850", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Background Gradient and Text Gradient#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Background Gradient and Text Gradient#", "content": "Background Gradient and Text Gradient# You can create “heatmaps” with the background_gradient and text_gradient methods. These require matplotlib, and we’ll use Seaborn to get a nice colormap. .background_gradient and .text_gradient have a number of keyword arguments to customise the gradients and colors. See the documentation.", "prev_chunk_id": "chunk_849", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_851", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Set properties#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Set properties#", "content": "Set properties# Use Styler.set_properties when the style doesn’t actually depend on the values. This is just a simple wrapper for .map where the function returns the same properties for all cells.", "prev_chunk_id": "chunk_850", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_852", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Bar charts#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Bar charts#", "content": "Bar charts# You can include “bar charts” in your DataFrame. Additional keyword arguments give more control on centering and positioning, and you can pass a list of [color_negative, color_positive] to highlight lower and higher values or a matplotlib colormap. To showcase an example here’s how you can change the above with the new align option, combined with setting vmin and vmax limits, the width of the figure, and underlying css props of cells, leaving space to display the text and the bars. We also use text_gradient to color the text the same as the bars using a matplotlib colormap (although in this case the visualization is probably better without this additional effect). The following example aims to give a highlight of the behavior of the new align options:", "prev_chunk_id": "chunk_851", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_853", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Sharing styles#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Sharing styles#", "content": "Sharing styles# Say you have a lovely style built up for a DataFrame, and now you want to apply the same style to a second DataFrame. Export the style with df1.style.export, and import it on the second DataFrame with df1.style.set Notice that you’re able to share the styles even though they’re data aware. The styles are re-evaluated on the new DataFrame they’ve been used upon.", "prev_chunk_id": "chunk_852", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_854", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Limitations#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Limitations#", "content": "Limitations# - DataFrame only (useSeries.to_frame().style) - The index and columns do not need to be unique, but certain styling functions can only work with unique indexes. - No large repr, and construction performance isn’t great; although we have someHTML optimizations - You can only apply styles, you can’t insert new HTML entities, except via subclassing.", "prev_chunk_id": "chunk_853", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_855", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Other Fun and Useful Stuff#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Other Fun and Useful Stuff#", "content": "Other Fun and Useful Stuff# Here are a few interesting examples.", "prev_chunk_id": "chunk_854", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_856", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Widgets#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Widgets#", "content": "Widgets# Styler interacts pretty well with widgets. If you’re viewing this online instead of running the notebook yourself, you’re missing out on interactively adjusting the color palette.", "prev_chunk_id": "chunk_855", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_857", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Sticky Headers#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Sticky Headers#", "content": "Sticky Headers# If you display a large matrix or DataFrame in a notebook, but you want to always see the column and row headers you can use the .set_sticky method which manipulates the table styles CSS. It is also possible to stick MultiIndexes and even only specific levels.", "prev_chunk_id": "chunk_856", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_858", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "HTML Escaping#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "HTML Escaping#", "content": "HTML Escaping# Suppose you have to display HTML within HTML, that can be a bit of pain when the renderer can’t distinguish. You can use the escape formatting option to handle this, and even use it within a formatter that contains HTML itself.", "prev_chunk_id": "chunk_857", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_859", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Export to Excel#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Export to Excel#", "content": "Export to Excel# Some support (since version 0.20.0) is available for exporting styled DataFrames to Excel worksheets using the OpenPyXL or XlsxWriter engines. CSS2.2 properties handled include: - background-color - border-styleproperties - border-widthproperties - border-colorproperties - color - font-family - font-style - font-weight - text-align - text-decoration - vertical-align - white-space:nowrap - Shorthand and side-specific border properties are supported (e.g.border-styleandborder-left-style) as well as thebordershorthands for all sides (border:1pxsolidgreen) or specified sides (border-left:1pxsolidgreen). Using abordershorthand will override any border properties set before it (SeeCSS Working Groupfor more details) - Only CSS2 named colors and hex colors of the form#rgbor#rrggbbare currently supported. - The following pseudo CSS properties are also available to set Excel specific style properties:number-formatborder-style(for Excel-specific styles: “hair”, “mediumDashDot”, “dashDotDot”, “mediumDashDotDot”, “dashDot”, “slantDashDot”, or “mediumDashed”) Table level styles, and data cell CSS-classes are not included in the export to Excel: individual cells must have their properties mapped by the Styler.apply and/or Styler.map methods. A screenshot of the output:", "prev_chunk_id": "chunk_858", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_860", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Export to LaTeX#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Export to LaTeX#", "content": "Export to LaTeX# There is support (since version 1.3.0) to export Styler to LaTeX. The documentation for the .to_latex method gives further detail and numerous examples.", "prev_chunk_id": "chunk_859", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_861", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "More About CSS and HTML#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "More About CSS and HTML#", "content": "More About CSS and HTML# Cascading Style Sheet (CSS) language, which is designed to influence how a browser renders HTML elements, has its own peculiarities. It never reports errors: it just silently ignores them and doesn’t render your objects how you intend so can sometimes be frustrating. Here is a very brief primer on how Styler creates HTML and interacts with CSS, with advice on common pitfalls to avoid.", "prev_chunk_id": "chunk_860", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_862", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "CSS Classes and Ids#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "CSS Classes and Ids#", "content": "CSS Classes and Ids# The precise structure of the CSS class attached to each cell is as follows. - Cells with Index and Column names includeindex_nameandlevel<k>wherekis its level in a MultiIndex - Index label cells includerow_headinglevel<k>wherekis the level in a MultiIndexrow<m>wheremis the numeric position of the row - Column label cells includecol_headinglevel<k>wherekis the level in a MultiIndexcol<n>wherenis the numeric position of the column - Data cells includedatarow<m>, wheremis the numeric position of the cell.col<n>, wherenis the numeric position of the cell. - Blank cells includeblank - Trimmed cells includecol_trimorrow_trim The structure of the id is T_uuid_level<k>_row<m>_col<n> where level<k> is used only on headings, and headings will only have either row<m> or col<n> whichever is needed. By default we’ve also prepended each row/column identifier with a UUID unique to each DataFrame so that the style from one doesn’t collide with the styling from another within the same notebook or page. You can read more about the use of UUIDs in Optimization. We can see example of the HTML by calling the .to_html() method.", "prev_chunk_id": "chunk_861", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_863", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "CSS Hierarchies#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "CSS Hierarchies#", "content": "CSS Hierarchies# The examples have shown that when CSS styles overlap, the one that comes last in the HTML render, takes precedence. So the following yield different results: This is only true for CSS rules that are equivalent in hierarchy, or importance. You can read more about CSS specificity here but for our purposes it suffices to summarize the key points: A CSS importance score for each HTML element is derived by starting at zero and adding: - 1000 for an inline style attribute - 100 for each ID - 10 for each attribute, class or pseudo-class - 1 for each element name or pseudo-element Let’s use this to describe the action of the following configurations This text is red because the generated selector #T_a_ td is worth 101 (ID plus element), whereas #T_a_row0_col0 is only worth 100 (ID), so is considered inferior even though in the HTML it comes after the previous. In the above case the text is blue because the selector #T_b_ .cls-1 is worth 110 (ID plus class), which takes precedence. Now we have created another table style this time the selector T_c_ td.data (ID plus element plus class) gets bumped up to 111. If your style fails to be applied, and its really frustrating, try the !important trump card. Finally got that green text after all!", "prev_chunk_id": "chunk_862", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_864", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Extensibility#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Extensibility#", "content": "Extensibility# The core of pandas is, and will remain, its “high-performance, easy-to-use data structures”. With that in mind, we hope that DataFrame.style accomplishes two goals - Provide an API that is pleasing to use interactively and is “good enough” for many tasks - Provide the foundations for dedicated libraries to build on If you build a great library on top of this, let us know and we’ll link to it.", "prev_chunk_id": "chunk_863", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_865", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Subclassing#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Subclassing#", "content": "Subclassing# If the default template doesn’t quite suit your needs, you can subclass Styler and extend or override the template. We’ll show an example of extending the default template to insert a custom header before each table. We’ll use the following template: Now that we’ve created a template, we need to set up a subclass of Styler that knows about it. Notice that we include the original loader in our environment’s loader. That’s because we extend the original template, so the Jinja environment needs to be able to find it. Now we can use that custom styler. It’s __init__ takes a DataFrame. Our custom template accepts a table_title keyword. We can provide the value in the .to_html method. For convenience, we provide the Styler.from_custom_template method that does the same as the custom subclass.", "prev_chunk_id": "chunk_864", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_866", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "My Table", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "My Table", "content": "My Table | | c1 | c2 | c3 | c4 A | r1 | -1.048553 | -1.420018 | -1.706270 | 1.950775 r2 | -0.509652 | -0.438074 | -1.252795 | 0.777490 B | r1 | -1.613898 | -0.212740 | -0.895467 | 0.386902 r2 | -0.510805 | -1.180632 | -0.028182 | 0.428332", "prev_chunk_id": "chunk_865", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_867", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Extending Example", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Extending Example", "content": "Extending Example | | c1 | c2 | c3 | c4 A | r1 | -1.048553 | -1.420018 | -1.706270 | 1.950775 r2 | -0.509652 | -0.438074 | -1.252795 | 0.777490 B | r1 | -1.613898 | -0.212740 | -0.895467 | 0.386902 r2 | -0.510805 | -1.180632 | -0.028182 | 0.428332", "prev_chunk_id": "chunk_866", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_868", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Another Title", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Another Title", "content": "Another Title | | c1 | c2 | c3 | c4 A | r1 | -1.048553 | -1.420018 | -1.706270 | 1.950775 r2 | -0.509652 | -0.438074 | -1.252795 | 0.777490 B | r1 | -1.613898 | -0.212740 | -0.895467 | 0.386902 r2 | -0.510805 | -1.180632 | -0.028182 | 0.428332", "prev_chunk_id": "chunk_867", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_869", "url": "https://pandas.pydata.org/docs/user_guide/style.html", "title": "Template Structure#", "page_title": "Table Visualization — pandas 2.3.1 documentation", "breadcrumbs": "Template Structure#", "content": "Template Structure# Here’s the template structure for the both the style generation template and the table generation template: Style template: Table template: See the template in the GitHub repo for more details.", "prev_chunk_id": "chunk_868", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_870", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Chart visualization#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Chart visualization#", "content": "Chart visualization# This section demonstrates visualization through charting. For information on visualization of tabular data please see the section on Table Visualization. We use the standard convention for referencing the matplotlib API: import matplotlib.pyplot as plt plt.close(\"all\") We provide the basics in pandas to easily create decent looking plots. See the ecosystem page for visualization libraries that go beyond the basics documented here.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_871", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Basic plotting: plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Basic plotting: plot#", "content": "Basic plotting: plot# We will demonstrate the basics, see the cookbook for some advanced strategies. The plot method on Series and DataFrame is just a simple wrapper around plt.plot(): np.random.seed(123456) ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000)) ts = ts.cumsum() ts.plot(); If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis nicely as per above. On DataFrame, plot() is a convenience to plot all of the columns with labels: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\")) df = df.cumsum() plt.figure(); df.plot(); You can plot one column versus another using the x and y keywords in plot(): df3 = pd.DataFrame(np.random.randn(1000, 2), columns=[\"B\", \"C\"]).cumsum() df3[\"A\"] = pd.Series(list(range(len(df)))) df3.plot(x=\"A\", y=\"B\");", "prev_chunk_id": "chunk_870", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_872", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Other plots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Other plots#", "content": "Other plots# Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the kind keyword argument to plot(), and include: - ‘bar’or‘barh’for bar plots - ‘hist’for histogram - ‘box’for boxplot - ‘kde’or‘density’for density plots - ‘area’for area plots - ‘scatter’for scatter plots - ‘hexbin’for hexagonal bin plots - ‘pie’for pie plots For example, a bar plot can be created the following way: plt.figure(); df.iloc[5].plot(kind=\"bar\"); You can also create these other plots using the methods DataFrame.plot.<kind> instead of providing the kind keyword argument. This makes it easier to discover plot methods and the specific arguments they use: df = pd.DataFrame() df.plot.<TAB> # noqa: E225, E999 df.plot.area df.plot.barh df.plot.density df.plot.hist df.plot.line df.plot.scatter df.plot.bar df.plot.box df.plot.hexbin df.plot.kde df.plot.pie In addition to these kind s, there are the DataFrame.hist(), and DataFrame.boxplot() methods, which use a separate interface. Finally, there are several plotting functions in pandas.plotting that take a Series or DataFrame as an argument. These include: - Scatter Matrix - Andrews Curves - Parallel Coordinates - Lag Plot - Autocorrelation Plot - Bootstrap Plot - RadViz Plots may also be adorned with errorbars or tables.", "prev_chunk_id": "chunk_871", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_873", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Bar plots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Bar plots#", "content": "Bar plots# For labeled, non-time series data, you may wish to produce a bar plot: plt.figure(); df.iloc[5].plot.bar(); plt.axhline(0, color=\"k\"); Calling a DataFrame’s plot.bar() method produces a multiple bar plot: df2 = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"]) df2.plot.bar(); To produce a stacked bar plot, pass stacked=True: df2.plot.bar(stacked=True); To get horizontal bar plots, use the barh method: df2.plot.barh(stacked=True);", "prev_chunk_id": "chunk_872", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_874", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Histograms#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Histograms#", "content": "Histograms# Histograms can be drawn by using the DataFrame.plot.hist() and Series.plot.hist() methods. df4 = pd.DataFrame( ....: { ....: \"a\": np.random.randn(1000) + 1, ....: \"b\": np.random.randn(1000), ....: \"c\": np.random.randn(1000) - 1, ....: }, ....: columns=[\"a\", \"b\", \"c\"], ....: ) ....: plt.figure(); df4.plot.hist(alpha=0.5); A histogram can be stacked using stacked=True. Bin size can be changed using the bins keyword. plt.figure(); df4.plot.hist(stacked=True, bins=20); You can pass other keywords supported by matplotlib hist. For example, horizontal and cumulative histograms can be drawn by orientation='horizontal' and cumulative=True. plt.figure(); df4[\"a\"].plot.hist(orientation=\"horizontal\", cumulative=True); See the hist method and the matplotlib hist documentation for more. The existing interface DataFrame.hist to plot histogram still can be used. plt.figure(); df[\"A\"].diff().hist(); DataFrame.hist() plots the histograms of the columns on multiple subplots: plt.figure(); df.diff().hist(color=\"k\", alpha=0.5, bins=50); The by keyword can be specified to plot grouped histograms: data = pd.Series(np.random.randn(1000)) data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4)); In addition, the by keyword can also be specified in DataFrame.plot.hist(). data = pd.DataFrame( ....: { ....: \"a\": np.random.choice([\"x\", \"y\", \"z\"], 1000), ....: \"b\": np.random.choice([\"e\", \"f\", \"g\"], 1000), ....: \"c\": np.random.randn(1000), ....: \"d\": np.random.randn(1000) - 1, ....: }, ....: ) ....: data.plot.hist(by=[\"a\", \"b\"], figsize=(10, 5));", "prev_chunk_id": "chunk_873", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_875", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Box plots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Box plots#", "content": "Box plots# Boxplot can be drawn calling Series.plot.box() and DataFrame.plot.box(), or DataFrame.boxplot() to visualize the distribution of values within each column. For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on [0,1). df = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"]) df.plot.box(); Boxplot can be colorized by passing color keyword. You can pass a dict whose keys are boxes, whiskers, medians and caps. If some keys are missing in the dict, default colors are used for the corresponding artists. Also, boxplot has sym keyword to specify fliers style. When you pass other type of arguments via color keyword, it will be directly passed to matplotlib for all the boxes, whiskers, medians and caps colorization. The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing return_type. color = { ....: \"boxes\": \"DarkGreen\", ....: \"whiskers\": \"DarkOrange\", ....: \"medians\": \"DarkBlue\", ....: \"caps\": \"Gray\", ....: } ....: df.plot.box(color=color, sym=\"r+\"); Also, you can pass other keywords supported by matplotlib boxplot. For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords. df.plot.box(vert=False, positions=[1, 4, 5, 6, 8]); See the boxplot method and the matplotlib boxplot documentation for more. The existing interface DataFrame.boxplot to plot boxplot still can be used. df = pd.DataFrame(np.random.rand(10, 5)) plt.figure(); bp = df.boxplot() You can create a stratified boxplot using the by keyword argument to create groupings. For instance, df = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"]) df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"]) plt.figure(); bp = df.boxplot(by=\"X\") You can also pass a subset of columns to plot, as well as group by multiple columns: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"]) df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\",", "prev_chunk_id": "chunk_874", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_876", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Box plots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Box plots#", "content": "\"B\"]) df[\"Y\"] = pd.Series([\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"]) plt.figure(); bp = df.boxplot(column=[\"Col1\", \"Col2\"], by=[\"X\", \"Y\"]) You could also create groupings with DataFrame.plot.box(), for instance: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"]) df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"]) plt.figure(); bp = df.plot.box(column=[\"Col1\", \"Col2\"], by=\"X\") In boxplot, the return type can be controlled by the return_type, keyword. The valid choices are {\"axes\", \"dict\", \"both\", None}. Faceting, created by DataFrame.boxplot with the by keyword, will affect the output type as well: return_type | Faceted | Output type None | No | axes None | Yes | 2-D ndarray of axes 'axes' | No | axes 'axes' | Yes | Series of axes 'dict' | No | dict of artists 'dict' | Yes | Series of dicts of artists 'both' | No | namedtuple 'both' | Yes | Series of namedtuples Groupby.boxplot always returns a Series of return_type. np.random.seed(1234) df_box = pd.DataFrame(np.random.randn(50, 2)) df_box[\"g\"] = np.random.choice([\"A\", \"B\"], size=50) df_box.loc[df_box[\"g\"] == \"B\", 1] += 3 bp = df_box.boxplot(by=\"g\") The subplots above are split by the numeric columns first, then the value of the g column. Below the subplots are first split by the value of g, then by the numeric columns. bp = df_box.groupby(\"g\").boxplot()", "prev_chunk_id": "chunk_875", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_877", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Area plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Area plot#", "content": "Area plot# You can create area plots with Series.plot.area() and DataFrame.plot.area(). Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values. When input data contains NaN, it will be automatically filled by 0. If you want to drop or fill by different values, use dataframe.dropna() or dataframe.fillna() before calling plot. df = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"]) df.plot.area(); To produce an unstacked plot, pass stacked=False. Alpha value is set to 0.5 unless otherwise specified: df.plot.area(stacked=False);", "prev_chunk_id": "chunk_876", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_878", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Scatter plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Scatter plot#", "content": "Scatter plot# Scatter plot can be drawn by using the DataFrame.plot.scatter() method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the x and y keywords. df = pd.DataFrame(np.random.rand(50, 4), columns=[\"a\", \"b\", \"c\", \"d\"]) df[\"species\"] = pd.Categorical( ....: [\"setosa\"] * 20 + [\"versicolor\"] * 20 + [\"virginica\"] * 10 ....: ) ....: df.plot.scatter(x=\"a\", y=\"b\"); To plot multiple column groups in a single axes, repeat plot method specifying target ax. It is recommended to specify color and label keywords to distinguish each groups. ax = df.plot.scatter(x=\"a\", y=\"b\", color=\"DarkBlue\", label=\"Group 1\") df.plot.scatter(x=\"c\", y=\"d\", color=\"DarkGreen\", label=\"Group 2\", ax=ax); The keyword c may be given as the name of a column to provide colors for each point: df.plot.scatter(x=\"a\", y=\"b\", c=\"c\", s=50); If a categorical column is passed to c, then a discrete colorbar will be produced: df.plot.scatter(x=\"a\", y=\"b\", c=\"species\", cmap=\"viridis\", s=50); You can pass other keywords supported by matplotlib scatter. The example below shows a bubble chart using a column of the DataFrame as the bubble size. df.plot.scatter(x=\"a\", y=\"b\", s=df[\"c\"] * 200); See the scatter method and the matplotlib scatter documentation for more.", "prev_chunk_id": "chunk_877", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_879", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Hexagonal bin plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Hexagonal bin plot#", "content": "Hexagonal bin plot# You can create hexagonal bin plots with DataFrame.plot.hexbin(). Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually. df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"]) df[\"b\"] = df[\"b\"] + np.arange(1000) df.plot.hexbin(x=\"a\", y=\"b\", gridsize=25); A useful keyword argument is gridsize; it controls the number of hexagons in the x-direction, and defaults to 100. A larger gridsize means more, smaller bins. By default, a histogram of the counts around each (x, y) point is computed. You can specify alternative aggregations by passing values to the C and reduce_C_function arguments. C specifies the value at each (x, y) point and reduce_C_function is a function of one argument that reduces all the values in a bin to a single number (e.g. mean, max, sum, std). In this example the positions are given by columns a and b, while the value is given by column z. The bins are aggregated with NumPy’s max function. df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"]) df[\"b\"] = df[\"b\"] + np.arange(1000) df[\"z\"] = np.random.uniform(0, 3, 1000) df.plot.hexbin(x=\"a\", y=\"b\", C=\"z\", reduce_C_function=np.max, gridsize=25); See the hexbin method and the matplotlib hexbin documentation for more.", "prev_chunk_id": "chunk_878", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_880", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Pie plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Pie plot#", "content": "Pie plot# You can create a pie plot with DataFrame.plot.pie() or Series.plot.pie(). If your data includes any NaN, they will be automatically filled with 0. A ValueError will be raised if there are any negative values in your data. series = pd.Series(3 * np.random.rand(4), index=[\"a\", \"b\", \"c\", \"d\"], name=\"series\") series.plot.pie(figsize=(6, 6)); For pie plots it’s best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling ax.set_aspect('equal') on the returned axes object. Note that pie plot with DataFrame requires that you either specify a target column by the y argument or subplots=True. When y is specified, pie plot of selected column will be drawn. If subplots=True is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify legend=False to hide it. df = pd.DataFrame( ....: 3 * np.random.rand(4, 2), index=[\"a\", \"b\", \"c\", \"d\"], columns=[\"x\", \"y\"] ....: ) ....: df.plot.pie(subplots=True, figsize=(8, 4)); You can use the labels and colors keywords to specify the labels and colors of each wedge. If you want to hide wedge labels, specify labels=None. If fontsize is specified, the value will be applied to wedge labels. Also, other keywords supported by matplotlib.pyplot.pie() can be used. series.plot.pie( ....: labels=[\"AA\", \"BB\", \"CC\", \"DD\"], ....: colors=[\"r\", \"g\", \"b\", \"c\"], ....: autopct=\"%.2f\", ....: fontsize=20, ....: figsize=(6, 6), ....: ); ....: If you pass values whose sum total is less than 1.0 they will be rescaled so that they sum to 1. series = pd.Series([0.1] * 4, index=[\"a\", \"b\", \"c\", \"d\"], name=\"series2\") series.plot.pie(figsize=(6, 6)); See the matplotlib pie documentation for more.", "prev_chunk_id": "chunk_879", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_881", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting with missing data#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting with missing data#", "content": "Plotting with missing data# pandas tries to be pragmatic about plotting DataFrames or Series that contain missing data. Missing values are dropped, left out, or filled depending on the plot type. Plot Type | NaN Handling Line | Leave gaps at NaNs Line (stacked) | Fill 0’s Bar | Fill 0’s Scatter | Drop NaNs Histogram | Drop NaNs (column-wise) Box | Drop NaNs (column-wise) Area | Fill 0’s KDE | Drop NaNs (column-wise) Hexbin | Drop NaNs Pie | Fill 0’s If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using fillna() or dropna() before plotting.", "prev_chunk_id": "chunk_880", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_882", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting tools#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting tools#", "content": "Plotting tools# These functions can be imported from pandas.plotting and take a Series or DataFrame as an argument.", "prev_chunk_id": "chunk_881", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_883", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Scatter matrix plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Scatter matrix plot#", "content": "Scatter matrix plot# You can create a scatter plot matrix using the scatter_matrix method in pandas.plotting: from pandas.plotting import scatter_matrix df = pd.DataFrame(np.random.randn(1000, 4), columns=[\"a\", \"b\", \"c\", \"d\"]) scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal=\"kde\");", "prev_chunk_id": "chunk_882", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_884", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Density plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Density plot#", "content": "Density plot# You can create density plots using the Series.plot.kde() and DataFrame.plot.kde() methods. ser = pd.Series(np.random.randn(1000)) ser.plot.kde();", "prev_chunk_id": "chunk_883", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_885", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Andrews curves#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Andrews curves#", "content": "Andrews curves# Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series, see the Wikipedia entry for more information. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures. Note: The “Iris” dataset is available here. from pandas.plotting import andrews_curves data = pd.read_csv(\"data/iris.data\") plt.figure(); andrews_curves(data, \"Name\");", "prev_chunk_id": "chunk_884", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_886", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Parallel coordinates#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Parallel coordinates#", "content": "Parallel coordinates# Parallel coordinates is a plotting technique for plotting multivariate data, see the Wikipedia entry for an introduction. Parallel coordinates allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together. from pandas.plotting import parallel_coordinates data = pd.read_csv(\"data/iris.data\") plt.figure(); parallel_coordinates(data, \"Name\");", "prev_chunk_id": "chunk_885", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_887", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Lag plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Lag plot#", "content": "Lag plot# Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:]. from pandas.plotting import lag_plot plt.figure(); spacing = np.linspace(-99 * np.pi, 99 * np.pi, num=1000) data = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(spacing)) lag_plot(data);", "prev_chunk_id": "chunk_886", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_888", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Autocorrelation plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Autocorrelation plot#", "content": "Autocorrelation plot# Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the Wikipedia entry for more about autocorrelation plots. from pandas.plotting import autocorrelation_plot plt.figure(); spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000) data = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing)) autocorrelation_plot(data);", "prev_chunk_id": "chunk_887", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_889", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Bootstrap plot#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Bootstrap plot#", "content": "Bootstrap plot# Bootstrap plots are used to visually assess the uncertainty of a statistic, such as mean, median, midrange, etc. A random subset of a specified size is selected from a data set, the statistic in question is computed for this subset and the process is repeated a specified number of times. Resulting plots and histograms are what constitutes the bootstrap plot. from pandas.plotting import bootstrap_plot data = pd.Series(np.random.rand(1000)) bootstrap_plot(data, size=50, samples=500, color=\"grey\");", "prev_chunk_id": "chunk_888", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_890", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "RadViz#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "RadViz#", "content": "RadViz# RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently. See the R package Radviz for more information. Note: The “Iris” dataset is available here. from pandas.plotting import radviz data = pd.read_csv(\"data/iris.data\") plt.figure(); radviz(data, \"Name\");", "prev_chunk_id": "chunk_889", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_891", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Setting the plot style#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Setting the plot style#", "content": "Setting the plot style# From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling matplotlib.style.use(my_plot_style) before creating your plot. For example you could write matplotlib.style.use('ggplot') for ggplot-style plots. You can see the various available style names at matplotlib.style.available and it’s very easy to try them out.", "prev_chunk_id": "chunk_890", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_892", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "General plot style arguments#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "General plot style arguments#", "content": "General plot style arguments# Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot: plt.figure(); ts.plot(style=\"k--\", label=\"Series\"); For each kind of plot (e.g. line, bar, scatter) any additional arguments keywords are passed along to the corresponding matplotlib function (ax.plot(), ax.bar(), ax.scatter()). These can be used to control additional styling, beyond what pandas provides.", "prev_chunk_id": "chunk_891", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_893", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Controlling the legend#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Controlling the legend#", "content": "Controlling the legend# You may set the legend argument to False to hide the legend, which is shown by default. df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\")) df = df.cumsum() df.plot(legend=False);", "prev_chunk_id": "chunk_892", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_894", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Controlling the labels#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Controlling the labels#", "content": "Controlling the labels# You may set the xlabel and ylabel arguments to give the plot custom labels for x and y axis. By default, pandas will pick up index name as xlabel, while leaving it empty for ylabel. df.plot(); df.plot(xlabel=\"new x\", ylabel=\"new y\");", "prev_chunk_id": "chunk_893", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_895", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Scales#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Scales#", "content": "Scales# You may pass logy to get a log-scale Y axis. ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000)) ts = np.exp(ts.cumsum()) ts.plot(logy=True); See also the logx and loglog keyword arguments.", "prev_chunk_id": "chunk_894", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_896", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting on a secondary y-axis#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting on a secondary y-axis#", "content": "Plotting on a secondary y-axis# To plot data on a secondary y-axis, use the secondary_y keyword: df[\"A\"].plot(); df[\"B\"].plot(secondary_y=True, style=\"g\"); To plot some columns in a DataFrame, give the column names to the secondary_y keyword: plt.figure(); ax = df.plot(secondary_y=[\"A\", \"B\"]) ax.set_ylabel(\"CD scale\"); ax.right_ax.set_ylabel(\"AB scale\"); Note that the columns plotted on the secondary y-axis is automatically marked with “(right)” in the legend. To turn off the automatic marking, use the mark_right=False keyword: plt.figure(); df.plot(secondary_y=[\"A\", \"B\"], mark_right=False);", "prev_chunk_id": "chunk_895", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_897", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Custom formatters for timeseries plots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Custom formatters for timeseries plots#", "content": "Custom formatters for timeseries plots# pandas provides custom formatters for timeseries plots. These change the formatting of the axis labels for dates and times. By default, the custom formatters are applied only to plots created by pandas with DataFrame.plot() or Series.plot(). To have them apply to all plots, including those made by matplotlib, set the option pd.options.plotting.matplotlib.register_converters = True or use pandas.plotting.register_matplotlib_converters().", "prev_chunk_id": "chunk_896", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_898", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Suppressing tick resolution adjustment#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Suppressing tick resolution adjustment#", "content": "Suppressing tick resolution adjustment# pandas includes automatic tick resolution adjustment for regular frequency time-series data. For limited cases where pandas cannot infer the frequency information (e.g., in an externally created twinx), you can choose to suppress this behavior for alignment purposes. Here is the default behavior, notice how the x-axis tick labeling is performed: plt.figure(); df[\"A\"].plot(); Using the x_compat parameter, you can suppress this behavior: plt.figure(); df[\"A\"].plot(x_compat=True); If you have more than one plot that needs to be suppressed, the use method in pandas.plotting.plot_params can be used in a with statement: plt.figure(); with pd.plotting.plot_params.use(\"x_compat\", True): .....: df[\"A\"].plot(color=\"r\") .....: df[\"B\"].plot(color=\"g\") .....: df[\"C\"].plot(color=\"b\") .....:", "prev_chunk_id": "chunk_897", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_899", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Automatic date tick adjustment#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Automatic date tick adjustment#", "content": "Automatic date tick adjustment# TimedeltaIndex now uses the native matplotlib tick locator methods, it is useful to call the automatic date tick adjustment from matplotlib for figures whose ticklabels overlap. See the autofmt_xdate method and the matplotlib documentation for more.", "prev_chunk_id": "chunk_898", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_900", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Subplots#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Subplots#", "content": "Subplots# Each Series in a DataFrame can be plotted on a different axis with the subplots keyword: df.plot(subplots=True, figsize=(6, 6));", "prev_chunk_id": "chunk_899", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_901", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Using layout and targeting multiple axes#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Using layout and targeting multiple axes#", "content": "Using layout and targeting multiple axes# The layout of subplots can be specified by the layout keyword. It can accept (rows, columns). The layout keyword can be used in hist and boxplot also. If the input is invalid, a ValueError will be raised. The number of axes which can be contained by rows x columns specified by layout must be larger than the number of required subplots. If layout can contain more axes than required, blank axes are not drawn. Similar to a NumPy array’s reshape method, you can use -1 for one dimension to automatically calculate the number of rows or columns needed, given the other. df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False); The above example is identical to using: df.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False); The required number of columns (3) is inferred from the number of series to plot and the given number of rows (2). You can pass multiple axes created beforehand as list-like via ax keyword. This allows more complicated layouts. The passed axes must be the same number as the subplots being drawn. When multiple axes are passed via the ax keyword, layout, sharex and sharey keywords don’t affect to the output. You should explicitly pass sharex=False and sharey=False, otherwise you will see a warning. fig, axes = plt.subplots(4, 4, figsize=(9, 9)) plt.subplots_adjust(wspace=0.5, hspace=0.5) target1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]] target2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]] df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False); (-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False); Another option is passing an ax argument to Series.plot() to plot on a particular axis: np.random.seed(123456) ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000)) ts = ts.cumsum() df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\")) df = df.cumsum() fig, axes = plt.subplots(nrows=2, ncols=2) plt.subplots_adjust(wspace=0.2, hspace=0.5) df[\"A\"].plot(ax=axes[0, 0]); axes[0, 0].set_title(\"A\"); df[\"B\"].plot(ax=axes[0, 1]); axes[0, 1].set_title(\"B\"); df[\"C\"].plot(ax=axes[1, 0]); axes[1, 0].set_title(\"C\"); df[\"D\"].plot(ax=axes[1, 1]); axes[1, 1].set_title(\"D\");", "prev_chunk_id": "chunk_900", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_902", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting with error bars#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting with error bars#", "content": "Plotting with error bars# Plotting with error bars is supported in DataFrame.plot() and Series.plot(). Horizontal and vertical error bars can be supplied to the xerr and yerr keyword arguments to plot(). The error values can be specified using a variety of formats: - As aDataFrameordictof errors with column names matching thecolumnsattribute of the plottingDataFrameor matching thenameattribute of theSeries. - As astrindicating which of the columns of plottingDataFramecontain the error values. - As raw values (list,tuple, ornp.ndarray). Must be the same length as the plottingDataFrame/Series. Here is an example of one way to easily plot group means with standard deviations from the raw data. # Generate the data ix3 = pd.MultiIndex.from_arrays( .....: [ .....: [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"], .....: [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"], .....: ], .....: names=[\"letter\", \"word\"], .....: ) .....: df3 = pd.DataFrame( .....: { .....: \"data1\": [9, 3, 2, 4, 3, 2, 4, 6, 3, 2], .....: \"data2\": [9, 6, 5, 7, 5, 4, 5, 6, 5, 1], .....: }, .....: index=ix3, .....: ) .....: # Group by index labels and take the means and standard deviations # for each group gp3 = df3.groupby(level=(\"letter\", \"word\")) means = gp3.mean() errors = gp3.std() means data1 data2 letter word a bar 3.500000 6.000000 foo 4.666667 6.666667 b bar 3.666667 4.000000 foo 3.000000 4.500000 errors data1 data2 letter word a bar 0.707107 1.414214 foo 3.785939 2.081666 b bar 2.081666 2.645751 foo 1.414214 0.707107 # Plot fig, ax = plt.subplots() means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0); Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a N length Series, a 2xN array should be provided indicating lower and upper (or left and right) errors. For a MxN DataFrame, asymmetrical errors should be in a Mx2xN array.", "prev_chunk_id": "chunk_901", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_903", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting with error bars#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting with error bars#", "content": "Here is an example of one way to plot the min/max range using asymmetrical error bars. mins = gp3.min() maxs = gp3.max() # errors should be positive, and defined in the order of lower, upper errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in df3.columns] # Plot fig, ax = plt.subplots() means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);", "prev_chunk_id": "chunk_902", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_904", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting tables#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting tables#", "content": "Plotting tables# Plotting with matplotlib table is now supported in DataFrame.plot() and Series.plot() with a table keyword. The table keyword can accept bool, DataFrame or Series. The simple way to draw a table is to specify table=True. Data will be transposed to meet matplotlib’s default layout. np.random.seed(123456) fig, ax = plt.subplots(1, 1, figsize=(7, 6.5)) df = pd.DataFrame(np.random.rand(5, 3), columns=[\"a\", \"b\", \"c\"]) ax.xaxis.tick_top() # Display x-axis ticks on top. df.plot(table=True, ax=ax); Also, you can pass a different DataFrame or Series to the table keyword. The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as seen in the example below. fig, ax = plt.subplots(1, 1, figsize=(7, 6.75)) ax.xaxis.tick_top() # Display x-axis ticks on top. df.plot(table=np.round(df.T, 2), ax=ax); There also exists a helper function pandas.plotting.table, which creates a table from DataFrame or Series, and adds it to an matplotlib.Axes instance. This function can accept keywords which the matplotlib table has. from pandas.plotting import table fig, ax = plt.subplots(1, 1) table(ax, np.round(df.describe(), 2), loc=\"upper right\", colWidths=[0.2, 0.2, 0.2]); df.plot(ax=ax, ylim=(0, 2), legend=None); Note: You can get table instances on the axes using axes.tables property for further decorations. See the matplotlib table documentation for more.", "prev_chunk_id": "chunk_903", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_905", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Colormaps#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Colormaps#", "content": "Colormaps# A potential issue when plotting a large number of columns is that it can be difficult to distinguish some series due to repetition in the default colors. To remedy this, DataFrame plotting supports the use of the colormap argument, which accepts either a Matplotlib colormap or a string that is a name of a colormap registered with Matplotlib. A visualization of the default matplotlib colormaps is available here. As matplotlib does not directly support colormaps for line-based plots, the colors are selected based on an even spacing determined by the number of columns in the DataFrame. There is no consideration made for background color, so some colormaps will produce lines that are not easily visible. To use the cubehelix colormap, we can pass colormap='cubehelix'. np.random.seed(123456) df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index) df = df.cumsum() plt.figure(); df.plot(colormap=\"cubehelix\"); Alternatively, we can pass the colormap itself: from matplotlib import cm plt.figure(); df.plot(colormap=cm.cubehelix); Colormaps can also be used other plot types, like bar charts: np.random.seed(123456) dd = pd.DataFrame(np.random.randn(10, 10)).map(abs) dd = dd.cumsum() plt.figure(); dd.plot.bar(colormap=\"Greens\"); Parallel coordinates charts: plt.figure(); parallel_coordinates(data, \"Name\", colormap=\"gist_rainbow\"); Andrews curves charts: plt.figure(); andrews_curves(data, \"Name\", colormap=\"winter\");", "prev_chunk_id": "chunk_904", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_906", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting directly with Matplotlib#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting directly with Matplotlib#", "content": "Plotting directly with Matplotlib# In some situations it may still be preferable or necessary to prepare plots directly with matplotlib, for instance when a certain type of plot or customization is not (yet) supported by pandas. Series and DataFrame objects behave like arrays and can therefore be passed directly to matplotlib functions without explicit casts. pandas also automatically registers formatters and locators that recognize date indices, thereby extending date and time support to practically all plot types available in matplotlib. Although this formatting does not provide the same level of refinement you would get when plotting via pandas, it can be faster when plotting a large number of points. np.random.seed(123456) price = pd.Series( .....: np.random.randn(150).cumsum(), .....: index=pd.date_range(\"2000-1-1\", periods=150, freq=\"B\"), .....: ) .....: ma = price.rolling(20).mean() mstd = price.rolling(20).std() plt.figure(); plt.plot(price.index, price, \"k\"); plt.plot(ma.index, ma, \"b\"); plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color=\"b\", alpha=0.2);", "prev_chunk_id": "chunk_905", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_907", "url": "https://pandas.pydata.org/docs/user_guide/visualization.html", "title": "Plotting backends#", "page_title": "Chart visualization — pandas 2.3.1 documentation", "breadcrumbs": "Plotting backends#", "content": "Plotting backends# pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib. This can be done by passing ‘backend.module’ as the argument backend in plot function. For example: >>> Series([1, 2, 3]).plot(backend=\"backend.module\") Alternatively, you can also set this option globally, do you don’t need to specify the keyword in each plot call. For example: >>> pd.set_option(\"plotting.backend\", \"backend.module\") >>> pd.Series([1, 2, 3]).plot() Or: >>> pd.options.plotting.backend = \"backend.module\" >>> pd.Series([1, 2, 3]).plot() This would be more or less equivalent to: >>> import backend.module >>> backend.module.plot(pd.Series([1, 2, 3])) The backend module can then use other visualization tools (Bokeh, Altair, hvplot,…) to generate the plots. Some libraries implementing a backend for pandas are listed on the ecosystem page. Developers guide can be found at https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends", "prev_chunk_id": "chunk_906", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_908", "url": "https://pandas.pydata.org/docs/user_guide/boolean.html", "title": "Indexing with NA values#", "page_title": "Nullable Boolean data type — pandas 2.3.1 documentation", "breadcrumbs": "Indexing with NA values#", "content": "Indexing with NA values# pandas allows indexing with NA values in a boolean array, which are treated as False. s = pd.Series([1, 2, 3]) mask = pd.array([True, False, pd.NA], dtype=\"boolean\") s[mask] 0 1 dtype: int64 If you would prefer to keep the NA values you can manually fill them with fillna(True). s[mask.fillna(True)] 0 1 2 3 dtype: int64", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_909", "url": "https://pandas.pydata.org/docs/user_guide/boolean.html", "title": "Kleene logical operations#", "page_title": "Nullable Boolean data type — pandas 2.3.1 documentation", "breadcrumbs": "Kleene logical operations#", "content": "Kleene logical operations# arrays.BooleanArray implements Kleene Logic (sometimes called three-value logic) for logical operations like & (and), | (or) and ^ (exclusive-or). This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result. Expression | Result True & True | True True & False | False True & NA | NA False & False | False False & NA | False NA & NA | NA True | True | True True | False | True True | NA | True False | False | False False | NA | NA NA | NA | NA True ^ True | False True ^ False | True True ^ NA | NA False ^ False | False False ^ NA | NA NA ^ NA | NA When an NA is present in an operation, the output value is NA only if the result cannot be determined solely based on the other input. For example, True | NA is True, because both True | True and True | False are True. In that case, we don’t actually need to consider the value of the NA. On the other hand, True & NA is NA. The result depends on whether the NA really is True or False, since True & True is True, but True & False is False, so we can’t determine the output. This differs from how np.nan behaves in logical operations. pandas treated np.nan is always false in the output. In or pd.Series([True, False, np.nan], dtype=\"object\") | True 0 True 1 True 2 False dtype: bool pd.Series([True, False, np.nan], dtype=\"boolean\") | True 0 True 1 True 2 True dtype: boolean In and pd.Series([True, False, np.nan], dtype=\"object\") & True 0 True 1 False 2 False dtype:", "prev_chunk_id": "chunk_908", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_910", "url": "https://pandas.pydata.org/docs/user_guide/boolean.html", "title": "Kleene logical operations#", "page_title": "Nullable Boolean data type — pandas 2.3.1 documentation", "breadcrumbs": "Kleene logical operations#", "content": "bool pd.Series([True, False, np.nan], dtype=\"boolean\") & True 0 True 1 False 2 <NA> dtype: boolean", "prev_chunk_id": "chunk_909", "next_chunk_id": null, "type": "section"}
]