[
{"chunk_id": "chunk_0", "url": "https://docs.pytorch.org/docs/stable/index.html", "title": "PyTorch documentation#", "page_title": "PyTorch documentation — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch documentation#", "content": "PyTorch documentation# PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. Features described in this documentation are classified by release status: Stable (API-Stable): These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time). Unstable (API-Unstable): Encompasses all features that are under active development where APIs may change based on user feedback, requisite performance improvements or because coverage across operators is not yet complete. The APIs and performance characteristics of these features may change.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1", "url": "https://docs.pytorch.org/docs/stable/index.html", "title": "Indices and tables#", "page_title": "PyTorch documentation — PyTorch 2.8 documentation", "breadcrumbs": "Indices and tables#", "content": "Indices and tables# - Index - Module Index", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_2", "url": "https://docs.pytorch.org/docs/stable/pytorch-api.html", "title": "Python API#", "page_title": "Python API — PyTorch 2.8 documentation", "breadcrumbs": "Python API#", "content": "Python API# Created On: Apr 16, 2025 | Last Updated On: Apr 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_3", "url": "https://docs.pytorch.org/docs/stable/notes.html", "title": "Developer Notes#", "page_title": "Developer Notes — PyTorch 2.8 documentation", "breadcrumbs": "Developer Notes#", "content": "Developer Notes# Created On: Apr 16, 2025 | Last Updated On: Apr 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_4", "url": "https://docs.pytorch.org/docs/stable/community/index.html", "title": "Community#", "page_title": "Community — PyTorch 2.8 documentation", "breadcrumbs": "Community#", "content": "Community# Created On: Apr 16, 2025 | Last Updated On: May 12, 2025 PyTorch is more than just a deep learning framework—it’s a vibrant ecosystem powered by a diverse global community. The PyTorch community brings together researchers, developers, students, and industry professionals who collaborate to advance the state of machine learning. Check out the resources below to learn how to contribute code to the core framework, report and fix bugs, improve documentation, and much more.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_5", "url": "https://docs.pytorch.org/docs/stable/torch.html", "title": "torch#", "page_title": "torch — PyTorch 2.8 documentation", "breadcrumbs": "torch#", "content": "torch# Created On: Dec 23, 2016 | Last Updated On: Mar 10, 2025 The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_6", "url": "https://docs.pytorch.org/docs/stable/torch.html", "title": "Accelerators#", "page_title": "torch — PyTorch 2.8 documentation", "breadcrumbs": "Accelerators#", "content": "Accelerators# Within the PyTorch repo, we define an “Accelerator” as a torch.device that is being used alongside a CPU to speed up computation. These device use an asynchronous execution scheme, using torch.Stream and torch.Event as their main way to perform synchronization. We also assume that only one such accelerator can be available at once on a given host. This allows us to use the current accelerator as the default device for relevant concepts such as pinned memory, Stream device_type, FSDP, etc. As of today, accelerator devices are (in no particular order) “CUDA”, “MTIA”, “XPU”, “MPS”, “HPU”, and PrivateUse1 (many device not in the PyTorch repo itself). Many tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading or intra-op parallelism), it is thus important to delay as much as possible any operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect. In practice, you should keep in mind that checking torch.accelerator.current_accelerator() is a compile-time check by default, it is thus always fork-safe. On the contrary, passing the check_available=True flag to this function or calling torch.accelerator.is_available() will usually prevent later fork. Some backends provide an experimental opt-in option to make the runtime availability check fork-safe. When using the CUDA device PYTORCH_NVML_BASED_CUDA_CHECK=1 can be used for example.", "prev_chunk_id": "chunk_5", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_7", "url": "https://docs.pytorch.org/docs/stable/torch.html", "title": "In-place random sampling#", "page_title": "torch — PyTorch 2.8 documentation", "breadcrumbs": "In-place random sampling#", "content": "In-place random sampling# There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: - torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() - torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution - torch.Tensor.exponential_()- numbers drawn from the exponential distribution - torch.Tensor.geometric_()- elements drawn from the geometric distribution - torch.Tensor.log_normal_()- samples from the log-normal distribution - torch.Tensor.normal_()- in-place version oftorch.normal() - torch.Tensor.random_()- numbers sampled from the discrete uniform distribution - torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution", "prev_chunk_id": "chunk_6", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_8", "url": "https://docs.pytorch.org/docs/stable/torch.html", "title": "Locally disabling gradient computation#", "page_title": "torch — PyTorch 2.8 documentation", "breadcrumbs": "Locally disabling gradient computation#", "content": "Locally disabling gradient computation# The context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled() are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the threading module, etc. Examples: >>> x = torch.zeros(1, requires_grad=True) >>> with torch.no_grad(): ... y = x * 2 >>> y.requires_grad False >>> is_train = False >>> with torch.set_grad_enabled(is_train): ... y = x * 2 >>> y.requires_grad False >>> torch.set_grad_enabled(True) # this can also be used as a function >>> y = x * 2 >>> y.requires_grad True >>> torch.set_grad_enabled(False) >>> y = x * 2 >>> y.requires_grad False", "prev_chunk_id": "chunk_7", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_9", "url": "https://docs.pytorch.org/docs/stable/torch.html", "title": "Optimizations#", "page_title": "torch — PyTorch 2.8 documentation", "breadcrumbs": "Optimizations#", "content": "Optimizations# torch.compile documentation", "prev_chunk_id": "chunk_8", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_10", "url": "https://docs.pytorch.org/docs/stable/nn.html", "title": "torch.nn#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn#", "content": "torch.nn# Created On: Dec 23, 2016 | Last Updated On: Nov 06, 2024 These are the basic building blocks for graphs:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_11", "url": "https://docs.pytorch.org/docs/stable/nn.html", "title": "Containers#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Containers#", "content": "Containers# Global Hooks For Module", "prev_chunk_id": "chunk_10", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_12", "url": "https://docs.pytorch.org/docs/stable/nn.html", "title": "Utilities#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Utilities#", "content": "Utilities# From the torch.nn.utils module: Utility functions to clip parameter gradients. Utility functions to flatten and unflatten Module parameters to and from a single vector. Utility functions to fuse Modules with BatchNorm modules. Utility functions to convert Module parameter memory formats. Utility functions to apply and remove weight normalization from Module parameters. Utility functions for initializing Module parameters. Utility classes and functions for pruning Module parameters. Parametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization(). Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Utility functions to call a given Module in a stateless manner. Utility functions in other modules", "prev_chunk_id": "chunk_11", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_13", "url": "https://docs.pytorch.org/docs/stable/nn.html", "title": "Quantized Functions#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Functions#", "content": "Quantized Functions# Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.", "prev_chunk_id": "chunk_12", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_14", "url": "https://docs.pytorch.org/docs/stable/nn.html", "title": "Aliases#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Aliases#", "content": "Aliases# The following are aliases to their counterparts in torch.nn:", "prev_chunk_id": "chunk_13", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_15", "url": "https://docs.pytorch.org/docs/stable/nn.functional.html", "title": "torch.nn.functional#", "page_title": "torch.nn.functional — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.functional#", "content": "torch.nn.functional# Created On: Jun 11, 2019 | Last Updated On: Mar 25, 2024", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_16", "url": "https://docs.pytorch.org/docs/stable/nn.functional.html", "title": "Attention Mechanisms#", "page_title": "torch.nn.functional — PyTorch 2.8 documentation", "breadcrumbs": "Attention Mechanisms#", "content": "Attention Mechanisms# The torch.nn.attention.bias module contains attention_biases that are designed to be used with scaled_dot_product_attention.", "prev_chunk_id": "chunk_15", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_17", "url": "https://docs.pytorch.org/docs/stable/tensors.html", "title": "torch.Tensor#", "page_title": "torch.Tensor — PyTorch 2.8 documentation", "breadcrumbs": "torch.Tensor#", "content": "torch.Tensor# Created On: Dec 23, 2016 | Last Updated On: Jun 11, 2024 A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_18", "url": "https://docs.pytorch.org/docs/stable/tensors.html", "title": "Data types#", "page_title": "torch.Tensor — PyTorch 2.8 documentation", "breadcrumbs": "Data types#", "content": "Data types# Torch defines tensor types with the following data types: For backwards compatibility, we support the following alternate class names for these data types: However, to construct tensors, we recommend using factory functions such as torch.empty() with the dtype argument instead. The torch.Tensor constructor is an alias for the default tensor type (torch.FloatTensor).", "prev_chunk_id": "chunk_17", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_19", "url": "https://docs.pytorch.org/docs/stable/tensors.html", "title": "Initializing and basic operations#", "page_title": "torch.Tensor — PyTorch 2.8 documentation", "breadcrumbs": "Initializing and basic operations#", "content": "Initializing and basic operations# A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]]) tensor([[ 1.0000, -1.0000], [ 1.0000, -1.0000]]) >>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]])) tensor([[ 1, 2, 3], [ 4, 5, 6]]) A tensor of specific data type can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32) tensor([[ 0, 0, 0, 0], [ 0, 0, 0, 0]], dtype=torch.int32) >>> cuda0 = torch.device('cuda:0') >>> torch.ones([2, 4], dtype=torch.float64, device=cuda0) tensor([[ 1.0000, 1.0000, 1.0000, 1.0000], [ 1.0000, 1.0000, 1.0000, 1.0000]], dtype=torch.float64, device='cuda:0') For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]]) >>> print(x[1][2]) tensor(6) >>> x[0][1] = 8 >>> print(x) tensor([[ 1, 8, 3], [ 4, 5, 6]]) Use torch.Tensor.item() to get a Python number from a tensor containing a single value: >>> x = torch.tensor([[1]]) >>> x tensor([[ 1]]) >>> x.item() 1 >>> x = torch.tensor(2.5) >>> x tensor(2.5000) >>> x.item() 2.5 For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True) >>> out = x.pow(2).sum() >>> out.backward() >>> x.grad tensor([[ 2.0000, -2.0000], [ 2.0000, 2.0000]]) Each tensor has an associated torch.Storage, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.", "prev_chunk_id": "chunk_18", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_20", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "Tensor Attributes#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "Tensor Attributes#", "content": "Tensor Attributes# Created On: Apr 21, 2018 | Last Updated On: Apr 09, 2025 Each torch.Tensor has a torch.dtype, torch.device, and torch.layout.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_21", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "torch.dtype#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "torch.dtype#", "content": "torch.dtype# A torch.dtype is an object that represents the data type of a torch.Tensor. PyTorch has several different data types: To find out if a torch.dtype is a floating point data type, the property is_floating_point can be used, which returns True if the data type is a floating point data type. To find out if a torch.dtype is a complex data type, the property is_complex can be used, which returns True if the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add, sub, div, mul) differ, we promote by finding the minimum dtype that satisfies the following rules: - If the type of a scalar operand is of a higher category than tensor operands (where complex > floating > integral > boolean), we promote to a type with sufficient size to hold all scalar operands of that category. - If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category. - If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands. A floating point scalar operand has dtype torch.get_default_dtype() and an integral non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect values when determining the minimum dtypes of an operand. Quantized and complex types are not yet supported. Promotion Examples: >>> float_tensor = torch.ones(1, dtype=torch.float) >>> double_tensor = torch.ones(1, dtype=torch.double) >>> complex_float_tensor = torch.ones(1, dtype=torch.complex64) >>> complex_double_tensor = torch.ones(1, dtype=torch.complex128) >>> int_tensor = torch.ones(1, dtype=torch.int) >>> long_tensor = torch.ones(1, dtype=torch.long) >>> uint_tensor = torch.ones(1, dtype=torch.uint8) >>> bool_tensor = torch.ones(1, dtype=torch.bool) # zero-dim tensors >>> long_zerodim = torch.tensor(1, dtype=torch.long) >>> int_zerodim = torch.tensor(1, dtype=torch.int) >>> torch.add(5, 5).dtype torch.int64", "prev_chunk_id": "chunk_20", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_22", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "torch.dtype#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "torch.dtype#", "content": "# 5 is an int64, but does not have higher category than int_tensor so is not considered. >>> (int_tensor + 5).dtype torch.int32 >>> (int_tensor + long_zerodim).dtype torch.int32 >>> (long_tensor + int_tensor).dtype torch.int64 >>> (bool_tensor + long_tensor).dtype torch.int64 >>> (bool_tensor + uint_tensor).dtype torch.uint8 >>> (float_tensor + double_tensor).dtype torch.float64 >>> (complex_float_tensor + complex_double_tensor).dtype torch.complex128 >>> (bool_tensor + int_tensor).dtype torch.int32 # Since long is a different kind than float, result dtype only needs to be large enough # to hold the float. >>> torch.add(long_tensor, float_tensor).dtype torch.float32 Casting Examples: # allowed: >>> float_tensor *= float_tensor >>> float_tensor *= int_tensor >>> float_tensor *= uint_tensor >>> float_tensor *= bool_tensor >>> float_tensor *= double_tensor >>> int_tensor *= long_tensor >>> int_tensor *= uint_tensor >>> uint_tensor *= int_tensor # disallowed (RuntimeError: result type can't be cast to the desired output type): >>> int_tensor *= float_tensor >>> bool_tensor *= int_tensor >>> bool_tensor *= uint_tensor >>> float_tensor *= complex_float_tensor", "prev_chunk_id": "chunk_21", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_23", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "torch.device#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "torch.device#", "content": "torch.device# A torch.device is an object representing the device on which a torch.Tensor is or will be allocated. The torch.device contains a device type (most commonly “cpu” or “cuda”, but also potentially “mps”, “xpu”, “xla” or “meta”) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after torch.cuda.set_device() is called; e.g., a torch.Tensor constructed with device 'cuda' is equivalent to 'cuda:X' where X is the result of torch.cuda.current_device(). A torch.Tensor’s device can be accessed via the Tensor.device property. A torch.device can be constructed via a string or via a string and device ordinal Via a string: >>> torch.device('cuda:0') device(type='cuda', index=0) >>> torch.device('cpu') device(type='cpu') >>> torch.device('mps') device(type='mps') >>> torch.device('cuda') # current cuda device device(type='cuda') Via a string and device ordinal: >>> torch.device('cuda', 0) device(type='cuda', index=0) >>> torch.device('mps', 0) device(type='mps', index=0) >>> torch.device('cpu', 0) device(type='cpu', index=0) The device object can also be used as a context manager to change the default device tensors are allocated on: >>> with torch.device('cuda:1'): ... r = torch.randn(2, 3) >>> r.device device(type='cuda', index=1) This context manager has no effect if a factory function is passed an explicit, non-None device argument. To globally change the default device, see also torch.set_default_device().", "prev_chunk_id": "chunk_22", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_24", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "torch.layout#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "torch.layout#", "content": "torch.layout# A torch.layout is an object that represents the memory layout of a torch.Tensor. Currently, we support torch.strided (dense Tensors) and have beta support for torch.sparse_coo (sparse COO Tensors). torch.strided represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated torch.Storage, which holds its data. These tensors provide multi-dimensional, strided view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently. Example: >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) >>> x.t().stride() (1, 5) For more information on torch.sparse_coo tensors, see torch.sparse.", "prev_chunk_id": "chunk_23", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_25", "url": "https://docs.pytorch.org/docs/stable/tensor_attributes.html", "title": "torch.memory_format#", "page_title": "Tensor Attributes — PyTorch 2.8 documentation", "breadcrumbs": "torch.memory_format#", "content": "torch.memory_format# A torch.memory_format is an object representing the memory format on which a torch.Tensor is or will be allocated. Possible values are: - torch.contiguous_format: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order. - torch.channels_last: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. - torch.channels_last_3d: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[4]>strides[1]==1aka NDHWC order. - torch.preserve_format: Used in functions likecloneto preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will followtorch.contiguous_format", "prev_chunk_id": "chunk_24", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_26", "url": "https://docs.pytorch.org/docs/stable/tensor_view.html", "title": "Tensor Views#", "page_title": "Tensor Views — PyTorch 2.8 documentation", "breadcrumbs": "Tensor Views#", "content": "Tensor Views# Created On: Feb 28, 2020 | Last Updated On: Feb 26, 2025 PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations. For example, to get a view of an existing tensor t, you can call t.view(...). >>> t = torch.rand(4, 4) >>> b = t.view(2, 8) >>> t.storage().data_ptr() == b.storage().data_ptr() # `t` and `b` share the same underlying data. True # Modifying view tensor changes base tensor as well. >>> b[0][0] = 3.14 >>> t[0][0] tensor(3.14) Since views share underlying data with its base tensor, if you edit the data in the view, it will be reflected in the base tensor as well. Typically a PyTorch op returns a new tensor as output, e.g. add(). But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy. No data movement occurs when creating a view, view tensor just changes the way it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor. Users should pay additional attention as contiguity might have implicit performance impact. transpose() is a common example. >>> base = torch.tensor([[0, 1],[2, 3]]) >>> base.is_contiguous() True >>> t = base.transpose(0, 1) # `t` is a view of `base`. No data movement happened here. # View tensors might be non-contiguous. >>> t.is_contiguous() False # To get a contiguous tensor, call `.contiguous()` to enforce # copying data when `t` is not contiguous. >>> c = t.contiguous() For reference, here’s a full list of view ops in PyTorch: - Basic slicing and indexing op, e.g.tensor[0,2:,1:7:2]returns a view of basetensor, see note below. - adjoint()", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_27", "url": "https://docs.pytorch.org/docs/stable/tensor_view.html", "title": "Tensor Views#", "page_title": "Tensor Views — PyTorch 2.8 documentation", "breadcrumbs": "Tensor Views#", "content": "- as_strided() - detach() - diagonal() - expand() - expand_as() - movedim() - narrow() - permute() - select() - squeeze() - transpose() - t() - T - H - mT - mH - real - imag - view_as_real() - unflatten() - unfold() - unsqueeze() - view() - view_as() - unbind() - split() - hsplit() - vsplit() - tensor_split() - split_with_sizes() - swapaxes() - swapdims() - chunk() - indices()(sparse tensor only) - values()(sparse tensor only) It’s also worth mentioning a few ops with special behaviors: - reshape(),reshape_as()andflatten()can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not. - contiguous()returnsitselfif input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data. For a more detailed walk-through of PyTorch internal implementation, please refer to ezyang’s blogpost about PyTorch Internals.", "prev_chunk_id": "chunk_26", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_28", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "Automatic Mixed Precision package - torch.amp#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "Automatic Mixed Precision package - torch.amp#", "content": "Automatic Mixed Precision package - torch.amp# Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025 torch.amp provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use lower precision floating point datatype (lower_precision_fp): torch.float16 (half) or torch.bfloat16. Some ops, like linear layers and convolutions, are much faster in lower_precision_fp. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype. Ordinarily, “automatic mixed precision training” with datatype of torch.float16 uses torch.autocast and torch.amp.GradScaler together, as shown in the Automatic Mixed Precision examples and Automatic Mixed Precision recipe. However, torch.autocast and torch.GradScaler are modular, and may be used separately if desired. As shown in the CPU example section of torch.autocast, “automatic mixed precision training/inference” on CPU with datatype of torch.bfloat16 only uses torch.autocast. torch.autocast and torch.cpu.amp.autocast are new in version 1.10.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_29", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "Gradient Scaling#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "Gradient Scaling#", "content": "Gradient Scaling# If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost. To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero. Each parameter’s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.", "prev_chunk_id": "chunk_28", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_30", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "Op Eligibility#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "Op Eligibility#", "content": "Op Eligibility# Ops that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled. Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions. Ops called with an explicit dtype=... argument are not eligible, and will produce output that respects the dtype argument.", "prev_chunk_id": "chunk_29", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_31", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CUDA Op-Specific Behavior#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Op-Specific Behavior#", "content": "CUDA Op-Specific Behavior# The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops. If an op is unlisted, we assume it’s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue.", "prev_chunk_id": "chunk_30", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_32", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CUDA Ops that can autocast to float16#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Ops that can autocast to float16#", "content": "CUDA Ops that can autocast to float16# __matmul__, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, multi_dot, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell", "prev_chunk_id": "chunk_31", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_33", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CUDA Ops that can autocast to float32#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Ops that can autocast to float32#", "content": "CUDA Ops that can autocast to float32# __pow__, __rdiv__, __rpow__, __rtruediv__, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss", "prev_chunk_id": "chunk_32", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_34", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CUDA Ops that promote to the widest input type#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Ops that promote to the widest input type#", "content": "CUDA Ops that promote to the widest input type# These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. addcdiv, addcmul, atan2, bilinear, cross, dot, grid_sample, index_put, scatter_add, tensordot Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.", "prev_chunk_id": "chunk_33", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_35", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "Prefer binary_cross_entropy_with_logits over binary_cross_entropy#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "Prefer binary_cross_entropy_with_logits over binary_cross_entropy#", "content": "Prefer binary_cross_entropy_with_logits over binary_cross_entropy# The backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it) can produce gradients that aren’t representable in float16. In autocast-enabled regions, the forward input may be float16, which means the backward gradient must be representable in float16 (autocasting float16 forward inputs to float32 doesn’t help, because that cast must be reversed in backward). Therefore, binary_cross_entropy and BCELoss raise an error in autocast-enabled regions. Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits() or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits are safe to autocast.", "prev_chunk_id": "chunk_34", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_36", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "XPU Op-Specific Behavior (Experimental)#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "XPU Op-Specific Behavior (Experimental)#", "content": "XPU Op-Specific Behavior (Experimental)# The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops. If an op is unlisted, we assume it’s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue.", "prev_chunk_id": "chunk_35", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_37", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "XPU Ops that can autocast to float16#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "XPU Ops that can autocast to float16#", "content": "XPU Ops that can autocast to float16# addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, multi_dot, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, RNNCell", "prev_chunk_id": "chunk_36", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_38", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "XPU Ops that can autocast to float32#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "XPU Ops that can autocast to float32#", "content": "XPU Ops that can autocast to float32# __pow__, __rdiv__, __rpow__, __rtruediv__, binary_cross_entropy_with_logits, cosine_embedding_loss, cosine_similarity, cumsum, dist, exp, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, margin_ranking_loss, nll_loss, normalize, poisson_nll_loss, pow, reciprocal, rsqrt, soft_margin_loss, softmax, softmin, sum, triplet_margin_loss", "prev_chunk_id": "chunk_37", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_39", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "XPU Ops that promote to the widest input type#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "XPU Ops that promote to the widest input type#", "content": "XPU Ops that promote to the widest input type# These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. bilinear, cross, grid_sample, index_put, scatter_add, tensordot Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.", "prev_chunk_id": "chunk_38", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_40", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CPU Op-Specific Behavior#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CPU Op-Specific Behavior#", "content": "CPU Op-Specific Behavior# The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops. If an op is unlisted, we assume it’s numerically stable in bfloat16. If you believe an unlisted op is numerically unstable in bfloat16, please file an issue. float16 shares the lists of bfloat16.", "prev_chunk_id": "chunk_39", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_41", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CPU Ops that can autocast to bfloat16#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CPU Ops that can autocast to bfloat16#", "content": "CPU Ops that can autocast to bfloat16# conv1d, conv2d, conv3d, bmm, mm, linalg_vecdot, baddbmm, addmm, addbmm, linear, matmul, _convolution, conv_tbc, mkldnn_rnn_layer, conv_transpose1d, conv_transpose2d, conv_transpose3d, prelu, scaled_dot_product_attention, _native_multi_head_attention", "prev_chunk_id": "chunk_40", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_42", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CPU Ops that can autocast to float32#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CPU Ops that can autocast to float32#", "content": "CPU Ops that can autocast to float32# avg_pool3d, binary_cross_entropy, grid_sampler, grid_sampler_2d, _grid_sampler_2d_cpu_fallback, grid_sampler_3d, polar, prod, quantile, nanquantile, stft, cdist, trace, view_as_complex, cholesky, cholesky_inverse, cholesky_solve, inverse, lu_solve, orgqr, inverse, ormqr, pinverse, max_pool3d, max_unpool2d, max_unpool3d, adaptive_avg_pool3d, reflection_pad1d, reflection_pad2d, replication_pad1d, replication_pad2d, replication_pad3d, mse_loss, cosine_embedding_loss, nll_loss, nll_loss2d, hinge_embedding_loss, poisson_nll_loss, cross_entropy_loss, l1_loss, huber_loss, margin_ranking_loss, soft_margin_loss, triplet_margin_loss, multi_margin_loss, ctc_loss, kl_div, multilabel_margin_loss, binary_cross_entropy_with_logits, fft_fft, fft_ifft, fft_fft2, fft_ifft2, fft_fftn, fft_ifftn, fft_rfft, fft_irfft, fft_rfft2, fft_irfft2, fft_rfftn, fft_irfftn, fft_hfft, fft_ihfft, linalg_cond, linalg_matrix_rank, linalg_solve, linalg_cholesky, linalg_svdvals, linalg_eigvals, linalg_eigvalsh, linalg_inv, linalg_householder_product, linalg_tensorinv, linalg_tensorsolve, fake_quantize_per_tensor_affine, geqrf, _lu_with_info, qr, svd, triangular_solve, fractional_max_pool2d, fractional_max_pool3d, adaptive_max_pool3d, multilabel_margin_loss_forward, linalg_qr, linalg_cholesky_ex, linalg_svd, linalg_eig, linalg_eigh, linalg_lstsq, linalg_inv_ex", "prev_chunk_id": "chunk_41", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_43", "url": "https://docs.pytorch.org/docs/stable/amp.html", "title": "CPU Ops that promote to the widest input type#", "page_title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.8 documentation", "breadcrumbs": "CPU Ops that promote to the widest input type#", "content": "CPU Ops that promote to the widest input type# These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are bfloat16, the op runs in bfloat16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. cat, stack, index_copy Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of bfloat16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.", "prev_chunk_id": "chunk_42", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_44", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Automatic differentiation package - torch.autograd#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Automatic differentiation package - torch.autograd#", "content": "Automatic differentiation package - torch.autograd# Created On: Dec 23, 2016 | Last Updated On: Jun 12, 2025 torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_45", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Forward-mode Automatic Differentiation#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Forward-mode Automatic Differentiation#", "content": "Forward-mode Automatic Differentiation# Please see the forward-mode AD tutorial for detailed steps on how to use this API.", "prev_chunk_id": "chunk_44", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_46", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Functional higher level API#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Functional higher level API#", "content": "Functional higher level API# This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).", "prev_chunk_id": "chunk_45", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_47", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Locally disabling gradient computation#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Locally disabling gradient computation#", "content": "Locally disabling gradient computation# See Locally disabling gradient computation for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see Locally disabling gradient computation for a list of functions that can be used to locally disable gradients.", "prev_chunk_id": "chunk_46", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_48", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Default gradient layouts#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Default gradient layouts#", "content": "Default gradient layouts# When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None: - Ifparam’s memory is non-overlapping and dense,.gradis created with strides matchingparam(thus matchingparam’s layout). - Otherwise,.gradis created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: - Ifcreate_graph=False,backward()accumulates into.gradin-place, which preserves its strides. - Ifcreate_graph=True,backward()replaces.gradwith a new tensor.grad+newgrad, which attempts (but does not guarantee) matching the preexisting.grad’s strides. The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grads to None before each accumulation phase, e.g.: for iterations... ... for param in model.parameters(): param.grad = None loss.backward() such that they’re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks.", "prev_chunk_id": "chunk_47", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_49", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Manual gradient layouts#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Manual gradient layouts#", "content": "Manual gradient layouts# If you need manual control over .grad’s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True.", "prev_chunk_id": "chunk_48", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_50", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "In-place operations on Tensors#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "In-place operations on Tensors#", "content": "In-place operations on Tensors# Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.", "prev_chunk_id": "chunk_49", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_51", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "In-place correctness checks#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "In-place correctness checks#", "content": "In-place correctness checks# All Tensor s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.", "prev_chunk_id": "chunk_50", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_52", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Context method mixins#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Context method mixins#", "content": "Context method mixins# When creating a new Function, the following methods are available to ctx.", "prev_chunk_id": "chunk_51", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_53", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Custom Function utilities#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Custom Function utilities#", "content": "Custom Function utilities# Decorator for backward method. Base custom Function used to build PyTorch utilities", "prev_chunk_id": "chunk_52", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_54", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Profiler#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Profiler#", "content": "Profiler# Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using profile. nvprof based (registers both CPU and GPU activity) using emit_nvtx. and vtune profiler based using emit_itt.", "prev_chunk_id": "chunk_53", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_55", "url": "https://docs.pytorch.org/docs/stable/autograd.html", "title": "Autograd graph#", "page_title": "Automatic differentiation package - torch.autograd — PyTorch 2.8 documentation", "breadcrumbs": "Autograd graph#", "content": "Autograd graph# Autograd exposes methods that allow one to inspect the graph and interpose behavior during the backward pass. The grad_fn attribute of a torch.Tensor holds a torch.autograd.graph.Node if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or None otherwise. Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the grad_fn and can be accessed. For example: >>> a = torch.tensor([0., 0., 0.], requires_grad=True) >>> b = a.exp() >>> print(isinstance(b.grad_fn, torch.autograd.graph.Node)) True >>> print(dir(b.grad_fn)) ['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad'] >>> print(torch.allclose(b.grad_fn._saved_result, b)) True You can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see Hooks for saved tensors.", "prev_chunk_id": "chunk_54", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_56", "url": "https://docs.pytorch.org/docs/stable/library.html", "title": "torch.library#", "page_title": "torch.library — PyTorch 2.8 documentation", "breadcrumbs": "torch.library#", "content": "torch.library# Created On: Jun 13, 2022 | Last Updated On: Jun 07, 2025 torch.library is a collection of APIs for extending PyTorch’s core library of operators. It contains utilities for testing custom operators, creating new custom operators, and extending operators defined with PyTorch’s C++ operator registration APIs (e.g. aten operators). For a detailed guide on effectively using these APIs, please see PyTorch Custom Operators Landing Page for more details on how to effectively use these APIs.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_57", "url": "https://docs.pytorch.org/docs/stable/library.html", "title": "Testing custom ops#", "page_title": "torch.library — PyTorch 2.8 documentation", "breadcrumbs": "Testing custom ops#", "content": "Testing custom ops# Use torch.library.opcheck() to test custom ops for incorrect usage of the Python torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports training, use torch.autograd.gradcheck() to test that the gradients are mathematically correct.", "prev_chunk_id": "chunk_56", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_58", "url": "https://docs.pytorch.org/docs/stable/library.html", "title": "Creating new custom ops in Python#", "page_title": "torch.library — PyTorch 2.8 documentation", "breadcrumbs": "Creating new custom ops in Python#", "content": "Creating new custom ops in Python# Use torch.library.custom_op() to create new custom ops.", "prev_chunk_id": "chunk_57", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_59", "url": "https://docs.pytorch.org/docs/stable/library.html", "title": "Extending custom ops (created from Python or C++)#", "page_title": "torch.library — PyTorch 2.8 documentation", "breadcrumbs": "Extending custom ops (created from Python or C++)#", "content": "Extending custom ops (created from Python or C++)# Use the register.* methods, such as torch.library.register_kernel() and torch.library.register_fake(), to add implementations for any operators (they may have been created using torch.library.custom_op() or via PyTorch’s C++ operator registration APIs).", "prev_chunk_id": "chunk_58", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_60", "url": "https://docs.pytorch.org/docs/stable/library.html", "title": "Low-level APIs#", "page_title": "torch.library — PyTorch 2.8 documentation", "breadcrumbs": "Low-level APIs#", "content": "Low-level APIs# The following APIs are direct bindings to PyTorch’s C++ low-level operator registration APIs. A tutorial that walks you through some examples on how to use this API is available on Google Colab.", "prev_chunk_id": "chunk_59", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_61", "url": "https://docs.pytorch.org/docs/stable/accelerator.html", "title": "torch.accelerator#", "page_title": "torch.accelerator — PyTorch 2.8 documentation", "breadcrumbs": "torch.accelerator#", "content": "torch.accelerator# Created On: Oct 27, 2024 | Last Updated On: Jul 10, 2025 This package introduces support for the current accelerator in python.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_62", "url": "https://docs.pytorch.org/docs/stable/cpu.html", "title": "torch.cpu#", "page_title": "torch.cpu — PyTorch 2.8 documentation", "breadcrumbs": "torch.cpu#", "content": "torch.cpu# Created On: Jul 11, 2023 | Last Updated On: Oct 11, 2023 This package implements abstractions found in torch.cuda to facilitate writing device-agnostic code.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_63", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Modules#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Modules#", "content": "Modules# Created On: Feb 04, 2021 | Last Updated On: Nov 08, 2024 PyTorch uses modules to represent neural networks. Modules are: - Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks. - Tightly integrated with PyTorch’sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update. - Easy to work with and transform.Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_64", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "A Simple Custom Module#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "A Simple Custom Module#", "content": "A Simple Custom Module# To get started, let’s look at a simpler, custom version of PyTorch’s Linear module. This module applies an affine transformation to its input. import torch from torch import nn class MyLinear(nn.Module): def __init__(self, in_features, out_features): super().__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return (input @ self.weight) + self.bias This simple module has the following fundamental characteristics of modules: - It inherits from the base Module class.All modules should subclassModulefor composability with other modules. - It defines some “state” that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine transformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls toparameters(). Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless. - It defines a forward() function that performs the computation.For this affine transformation module, the input is matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called: m = MyLinear(4, 3) sample_input = torch.randn(4) m(sample_input) : tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>) Note that the module itself is callable, and that calling it invokes its forward() function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect", "prev_chunk_id": "chunk_63", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_65", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "A Simple Custom Module#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "A Simple Custom Module#", "content": "to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a backward() function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call to parameters() or named_parameters(), where the latter includes each parameter’s name: for parameter in m.named_parameters(): print(parameter) : ('weight', Parameter containing: tensor([[ 1.0597, 1.1796, 0.8247], [-0.5080, -1.2635, -1.1045], [ 0.0593, 0.2469, -1.4299], [-0.4926, -0.5457, 0.4793]], requires_grad=True)) ('bias', Parameter containing: tensor([ 0.3634, 0.2015, -0.8525], requires_grad=True)) In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.", "prev_chunk_id": "chunk_64", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_66", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Modules as Building Blocks#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Modules as Building Blocks#", "content": "Modules as Building Blocks# Modules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the Sequential module. It allows us to chain together multiple modules: net = nn.Sequential( MyLinear(4, 3), nn.ReLU(), MyLinear(3, 1) ) sample_input = torch.randn(4) net(sample_input) : tensor([-0.6749], grad_fn=<AddBackward0>) Note that Sequential automatically feeds the output of the first MyLinear module as input into the ReLU, and the output of that as input into the second MyLinear module. As shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.l0 = MyLinear(4, 3) self.l1 = MyLinear(3, 1) def forward(self, x): x = self.l0(x) x = F.relu(x) x = self.l1(x) return x This module is composed of two “children” or “submodules” (l0 and l1) that define the layers of the neural network and are utilized for computation within the module’s forward() method. Immediate children of a module can be iterated through via a call to children() or named_children(): net = Net() for child in net.named_children(): print(child) : ('l0', MyLinear()) ('l1', MyLinear()) To go deeper than just the immediate children, modules() and named_modules() recursively iterate through a module and its child modules: class BigNet(nn.Module): def __init__(self): super().__init__() self.l1 = MyLinear(5, 4) self.net = Net() def forward(self, x): return self.net(self.l1(x)) big_net = BigNet() for module in big_net.named_modules(): print(module) : ('', BigNet( (l1): MyLinear() (net): Net( (l0): MyLinear() (l1): MyLinear() ) )) ('l1', MyLinear()) ('net', Net( (l0): MyLinear() (l1): MyLinear() )) ('net.l0',", "prev_chunk_id": "chunk_65", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_67", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Modules as Building Blocks#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Modules as Building Blocks#", "content": "MyLinear()) ('net.l1', MyLinear()) Sometimes, it’s necessary for a module to dynamically define submodules. The ModuleList and ModuleDict modules are useful here; they register submodules from a list or dict: class DynamicNet(nn.Module): def __init__(self, num_layers): super().__init__() self.linears = nn.ModuleList( [MyLinear(4, 4) for _ in range(num_layers)]) self.activations = nn.ModuleDict({ 'relu': nn.ReLU(), 'lrelu': nn.LeakyReLU() }) self.final = MyLinear(4, 1) def forward(self, x, act): for linear in self.linears: x = linear(x) x = self.activations[act](x) x = self.final(x) return x dynamic_net = DynamicNet(3) sample_input = torch.randn(4) output = dynamic_net(sample_input, 'relu') For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network: for parameter in dynamic_net.named_parameters(): print(parameter) : ('linears.0.weight', Parameter containing: tensor([[-1.2051, 0.7601, 1.1065, 0.1963], [ 3.0592, 0.4354, 1.6598, 0.9828], [-0.4446, 0.4628, 0.8774, 1.6848], [-0.1222, 1.5458, 1.1729, 1.4647]], requires_grad=True)) ('linears.0.bias', Parameter containing: tensor([ 1.5310, 1.0609, -2.0940, 1.1266], requires_grad=True)) ('linears.1.weight', Parameter containing: tensor([[ 2.1113, -0.0623, -1.0806, 0.3508], [-0.0550, 1.5317, 1.1064, -0.5562], [-0.4028, -0.6942, 1.5793, -1.0140], [-0.0329, 0.1160, -1.7183, -1.0434]], requires_grad=True)) ('linears.1.bias', Parameter containing: tensor([ 0.0361, -0.9768, -0.3889, 1.1613], requires_grad=True)) ('linears.2.weight', Parameter containing: tensor([[-2.6340, -0.3887, -0.9979, 0.0767], [-0.3526, 0.8756, -1.5847, -0.6016], [-0.3269, -0.1608, 0.2897, -2.0829], [ 2.6338, 0.9239, 0.6943, -1.5034]], requires_grad=True)) ('linears.2.bias', Parameter containing: tensor([ 1.0268, 0.4489, -0.9403, 0.1571], requires_grad=True)) ('final.weight', Parameter containing: tensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True)) ('final.bias', Parameter containing: tensor([0.3381], requires_grad=True)) It’s also easy to move all parameters to a different device or change their precision using to(): # Move all parameters to a CUDA device dynamic_net.to(device='cuda') # Change precision of all parameters dynamic_net.to(dtype=torch.float64) dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64)) : tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>) More generally, an arbitrary function can be applied to a module and its submodules recursively by using the apply() function. For", "prev_chunk_id": "chunk_66", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_68", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Modules as Building Blocks#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Modules as Building Blocks#", "content": "example, to apply custom initialization to parameters of a module and its submodules: # Define a function to initialize Linear weights. # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. @torch.no_grad() def init_weights(m): if isinstance(m, nn.Linear): nn.init.xavier_normal_(m.weight) m.bias.fill_(0.0) # Apply the function recursively on the module and its submodules. dynamic_net.apply(init_weights) These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the torch.nn namespace that perform common neural network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: - Library of PyTorch-provided modules:torch.nn - Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html", "prev_chunk_id": "chunk_67", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_69", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Neural Network Training with Modules#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Neural Network Training with Modules#", "content": "Neural Network Training with Modules# Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from torch.optim: # Create the network (from previous section) and optimizer net = Net() optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9) # Run a sample training loop that \"teaches\" the network # to output the constant zero function for _ in range(10000): input = torch.randn(4) output = net(input) loss = torch.abs(output) net.zero_grad() loss.backward() optimizer.step() # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) ... net.eval() ... In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the key parts of training are present: - A network is created. - An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it. - A training loop…acquires an input,runs the network,computes a loss,zeros the network’s parameters’ gradients,calls loss.backward() to update the parameters’ gradients,calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of l1‘s weight parameter shows that its values are now much closer to 0 (as may be expected): print(net.l1.weight) : Parameter containing: tensor([[-0.0013], [ 0.0030], [-0.0008]], requires_grad=True) Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using train() and eval(). They can behave differently depending on which mode they are in.", "prev_chunk_id": "chunk_68", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_70", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Neural Network Training with Modules#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Neural Network Training with Modules#", "content": "For example, the BatchNorm module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes: class ModalModule(nn.Module): def __init__(self): super().__init__() def forward(self, x): if self.training: # Add a constant only in training mode. return x + 1. else: return x m = ModalModule() x = torch.randn(4) print('training mode output: {}'.format(m(x))) : tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481]) m.eval() print('evaluation mode output: {}'.format(m(x))) : tensor([ 0.6614, 0.2669, 0.0617, 0.6213, -0.4519]) Training neural networks can often be tricky. For more information, check out: - Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. - Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html - Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html", "prev_chunk_id": "chunk_69", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_71", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module State#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module State#", "content": "Module State# In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. “state dictionary”): # Save the module torch.save(net.state_dict(), 'net.pt') ... # Load the module later on new_net = Net() new_net.load_state_dict(torch.load('net.pt')) : <All keys matched successfully> A module’s state_dict contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have: - Parameters: learnable aspects of computation; contained within thestate_dict - Buffers: non-learnable aspects of computationPersistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading)Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s state_dict so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use register_buffer() to accomplish this: class RunningMean(nn.Module): def __init__(self, num_features, momentum=0.9): super().__init__() self.momentum = momentum self.register_buffer('mean', torch.zeros(num_features)) def forward(self, x): self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x return self.mean Now, the current value of the running mean is considered part of the module’s state_dict and will be properly restored when loading the module from disk: m = RunningMean(4) for _ in range(10): input = torch.randn(4) m(input) print(m.state_dict()) : OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647, 0.1515]))])) # Serialized", "prev_chunk_id": "chunk_70", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_72", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module State#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module State#", "content": "form will contain the 'mean' tensor torch.save(m.state_dict(), 'mean.pt') m_loaded = RunningMean(4) m_loaded.load_state_dict(torch.load('mean.pt')) assert(torch.all(m.mean == m_loaded.mean)) As mentioned previously, buffers can be left out of the module’s state_dict by marking them as non-persistent: self.register_buffer('unserialized_thing', torch.randn(5), persistent=False) Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with to(): # Moves all module parameters and buffers to the specified device / dtype m.to(device='cuda', dtype=torch.float64) Buffers of a module can be iterated over using buffers() or named_buffers(). for buffer in m.named_buffers(): print(buffer) The following class demonstrates the various ways of registering parameters and buffers within a module: class StatefulModule(nn.Module): def __init__(self): super().__init__() # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. self.param1 = nn.Parameter(torch.randn(2)) # Alternative string-based way to register a parameter. self.register_parameter('param2', nn.Parameter(torch.randn(3))) # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything # except a parameter. \"None\" entries like this will not be present in the module's state_dict. self.register_parameter('param3', None) # Registers a list of parameters. self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)]) # Registers a dictionary of parameters. self.param_dict = nn.ParameterDict({ 'foo': nn.Parameter(torch.randn(3)), 'bar': nn.Parameter(torch.randn(4)) }) # Registers a persistent buffer (one that appears in the module's state_dict). self.register_buffer('buffer1', torch.randn(4), persistent=True) # Registers a non-persistent buffer (one that does not appear in the module's state_dict). self.register_buffer('buffer2', torch.randn(5), persistent=False) # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything # except a buffer. \"None\" entries like this will not be present in the module's state_dict. self.register_buffer('buffer3', None) # Adding a submodule registers its parameters as parameters of the module. self.linear = nn.Linear(2, 3) m = StatefulModule() # Save and load state_dict. torch.save(m.state_dict(), 'state.pt') m_loaded = StatefulModule() m_loaded.load_state_dict(torch.load('state.pt')) # Note that non-persistent buffer \"buffer2\" and reserved", "prev_chunk_id": "chunk_71", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_73", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module State#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module State#", "content": "attributes \"param3\" and \"buffer3\" do # not appear in the state_dict. print(m_loaded.state_dict()) : OrderedDict([('param1', tensor([-0.0322, 0.9066])), ('param2', tensor([-0.4472, 0.1409, 0.4852])), ('buffer1', tensor([ 0.6949, -0.1944, 1.2911, -2.1044])), ('param_list.0', tensor([ 0.4202, -0.1953])), ('param_list.1', tensor([ 1.5299, -0.8747])), ('param_list.2', tensor([-1.6289, 1.4898])), ('param_dict.bar', tensor([-0.6434, 1.5187, 0.0346, -0.4077])), ('param_dict.foo', tensor([-0.0845, -1.4324, 0.7022])), ('linear.weight', tensor([[-0.3915, -0.6176], [ 0.6062, -0.5992], [ 0.4452, -0.2843]])), ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))]) For more information, check out: - Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html - Serialization semantics:https://pytorch.org/docs/main/notes/serialization.html - What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html", "prev_chunk_id": "chunk_72", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_74", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module Initialization#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module Initialization#", "content": "Module Initialization# By default, parameters and floating-point buffers for modules provided by torch.nn are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique. Examples: # Initialize module directly onto GPU. m = nn.Linear(5, 3, device='cuda') # Initialize module with 16-bit floating point parameters. m = nn.Linear(5, 3, dtype=torch.half) # Skip default parameter initialization and perform custom (e.g. orthogonal) initialization. m = torch.nn.utils.skip_init(nn.Linear, 5, 3) nn.init.orthogonal_(m.weight) Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module: m = nn.BatchNorm2d(3, dtype=torch.half) print(m.running_mean) : tensor([0., 0., 0.], dtype=torch.float16) While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use dtype=torch.float and device='cpu' by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all torch.nn modules follow: - Provide adeviceconstructor kwarg that applies to any parameters / buffers registered by the module. - Provide adtypeconstructor kwarg that applies to any parameters / floating-point buffers registered by the module. - Only use initialization functions (i.e. functions fromtorch.nn.init) on parameters and buffers within the module’s constructor. Note that this is only required to useskip_init(); seethis pagefor an explanation. For more information, check out: - Skipping module parameter initialization:https://pytorch.org/tutorials/prototype/skip_param_init.html", "prev_chunk_id": "chunk_73", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_75", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module Hooks#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module Hooks#", "content": "Module Hooks# In Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules: - Forward hooksare called during the forward pass. They can be installed for a given module withregister_forward_pre_hook()andregister_forward_hook(). These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogousregister_module_forward_pre_hook()andregister_module_forward_hook()functions. - Backward hooksare called during the backward pass. They can be installed withregister_full_backward_pre_hook()andregister_full_backward_hook(). These hooks will be called when the backward for this Module has been computed.register_full_backward_pre_hook()will allow the user to access the gradients for outputs whileregister_full_backward_hook()will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules withregister_module_full_backward_hook()andregister_module_full_backward_pre_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s forward() function. Below is an example demonstrating usage of forward and backward hooks: torch.manual_seed(1) def forward_pre_hook(m, inputs): # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. input = inputs[0] return input + 1. def", "prev_chunk_id": "chunk_74", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_76", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Module Hooks#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Module Hooks#", "content": "forward_hook(m, inputs, output): # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # are passed as-is. # Residual computation a la ResNet. return output + inputs[0] def backward_hook(m, grad_inputs, grad_outputs): # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs] return new_grad_inputs # Create sample module & input. m = nn.Linear(3, 3) x = torch.randn(2, 3, requires_grad=True) # ==== Demonstrate forward hooks. ==== # Run input through module before and after adding hooks. print('output with no forward hooks: {}'.format(m(x))) : output with no forward hooks: tensor([[-0.5059, -0.8158, 0.2390], [-0.0043, 0.4724, -0.1714]], grad_fn=<AddmmBackward>) # Note that the modified input results in a different output. forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook) print('output with forward pre hook: {}'.format(m(x))) : output with forward pre hook: tensor([[-0.5752, -0.7421, 0.4942], [-0.0736, 0.5461, 0.0838]], grad_fn=<AddmmBackward>) # Note the modified output. forward_hook_handle = m.register_forward_hook(forward_hook) print('output with both forward hooks: {}'.format(m(x))) : output with both forward hooks: tensor([[-1.0980, 0.6396, 0.4666], [ 0.3634, 0.6538, 1.0256]], grad_fn=<AddBackward0>) # Remove hooks; note that the output here matches the output before adding hooks. forward_pre_hook_handle.remove() forward_hook_handle.remove() print('output after removing forward hooks: {}'.format(m(x))) : output after removing forward hooks: tensor([[-0.5059, -0.8158, 0.2390], [-0.0043, 0.4724, -0.1714]], grad_fn=<AddmmBackward>) # ==== Demonstrate backward hooks. ==== m(x).sum().backward() print('x.grad with no backwards hook: {}'.format(x.grad)) : x.grad with no backwards hook: tensor([[ 0.4497, -0.5046, 0.3146], [ 0.4497, -0.5046, 0.3146]]) # Clear gradients before running backward pass again. m.zero_grad() x.grad.zero_() m.register_full_backward_hook(backward_hook) m(x).sum().backward() print('x.grad with backwards hook: {}'.format(x.grad)) : x.grad with backwards hook: tensor([[42., 42., 42.], [42., 42., 42.]])", "prev_chunk_id": "chunk_75", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_77", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Advanced Features#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Advanced Features#", "content": "Advanced Features# PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities are available for custom-written modules, with the small caveat that certain features may require modules to conform to particular constraints in order to be supported. In-depth discussion of these features and the corresponding requirements can be found in the links below.", "prev_chunk_id": "chunk_76", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_78", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Distributed Training#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Training#", "content": "Distributed Training# Various methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs as well as training across multiple machines. Check out the distributed training overview page for detailed information on how to utilize these.", "prev_chunk_id": "chunk_77", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_79", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Profiling Performance#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Profiling Performance#", "content": "Profiling Performance# The PyTorch Profiler can be useful for identifying performance bottlenecks within your models. It measures and outputs performance characteristics for both memory usage and time spent.", "prev_chunk_id": "chunk_78", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_80", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Improving Performance with Quantization#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Improving Performance with Quantization#", "content": "Improving Performance with Quantization# Applying quantization techniques to modules can improve performance and memory usage by utilizing lower bitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization here.", "prev_chunk_id": "chunk_79", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_81", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Improving Memory Usage with Pruning#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Improving Memory Usage with Pruning#", "content": "Improving Memory Usage with Pruning# Large deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch provides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The Pruning tutorial describes how to utilize the pruning techniques PyTorch provides or define custom pruning techniques as necessary.", "prev_chunk_id": "chunk_80", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_82", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Parametrizations#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Parametrizations#", "content": "Parametrizations# For certain applications, it can be beneficial to constrain the parameter space during model training. For example, enforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for applying parametrizations such as this, and further allows for custom constraints to be defined.", "prev_chunk_id": "chunk_81", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_83", "url": "https://docs.pytorch.org/docs/stable/notes/modules.html", "title": "Transforming Modules with FX#", "page_title": "Modules — PyTorch 2.8 documentation", "breadcrumbs": "Transforming Modules with FX#", "content": "Transforming Modules with FX# The FX component of PyTorch provides a flexible way to transform modules by operating directly on module computation graphs. This can be used to programmatically generate or manipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for convolution + batch norm fusion and CPU performance analysis.", "prev_chunk_id": "chunk_82", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_84", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "Python Module Index | | | t | | torch | | torch.__config__ | | torch.__future__ | | torch._logging | | torch.accelerator | | torch.amp | | torch.amp.autocast_mode | | torch.amp.grad_scaler | | torch.ao | | torch.ao.nn | | torch.ao.nn.intrinsic | | torch.ao.nn.intrinsic.modules | | torch.ao.nn.intrinsic.modules.fused | | torch.ao.nn.intrinsic.qat | | torch.ao.nn.intrinsic.qat.modules | | torch.ao.nn.intrinsic.qat.modules.conv_fused | | torch.ao.nn.intrinsic.qat.modules.linear_fused | | torch.ao.nn.intrinsic.qat.modules.linear_relu | | torch.ao.nn.intrinsic.quantized | | torch.ao.nn.intrinsic.quantized.dynamic | | torch.ao.nn.intrinsic.quantized.dynamic.modules | | torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu | | torch.ao.nn.intrinsic.quantized.modules | | torch.ao.nn.intrinsic.quantized.modules.bn_relu | | torch.ao.nn.intrinsic.quantized.modules.conv_add | | torch.ao.nn.intrinsic.quantized.modules.conv_relu | | torch.ao.nn.intrinsic.quantized.modules.linear_relu | | torch.ao.nn.qat | | torch.ao.nn.qat.dynamic | | torch.ao.nn.qat.dynamic.modules | | torch.ao.nn.qat.dynamic.modules.linear | | torch.ao.nn.qat.modules | | torch.ao.nn.qat.modules.conv | | torch.ao.nn.qat.modules.embedding_ops | | torch.ao.nn.qat.modules.linear | | torch.ao.nn.quantizable | | torch.ao.nn.quantizable.modules | | torch.ao.nn.quantizable.modules.activation | | torch.ao.nn.quantizable.modules.rnn | | torch.ao.nn.quantized | | torch.ao.nn.quantized.dynamic | | torch.ao.nn.quantized.dynamic.modules | | torch.ao.nn.quantized.dynamic.modules.conv | | torch.ao.nn.quantized.dynamic.modules.linear | | torch.ao.nn.quantized.dynamic.modules.rnn | | torch.ao.nn.quantized.functional | | torch.ao.nn.quantized.modules | | torch.ao.nn.quantized.modules.activation | | torch.ao.nn.quantized.modules.batchnorm | | torch.ao.nn.quantized.modules.conv | | torch.ao.nn.quantized.modules.dropout | | torch.ao.nn.quantized.modules.embedding_ops | | torch.ao.nn.quantized.modules.functional_modules | | torch.ao.nn.quantized.modules.linear | | torch.ao.nn.quantized.modules.normalization | | torch.ao.nn.quantized.modules.rnn | | torch.ao.nn.quantized.modules.utils | | torch.ao.nn.quantized.reference | | torch.ao.nn.quantized.reference.modules | | torch.ao.nn.quantized.reference.modules.conv | | torch.ao.nn.quantized.reference.modules.linear | | torch.ao.nn.quantized.reference.modules.rnn | | torch.ao.nn.quantized.reference.modules.sparse | | torch.ao.nn.quantized.reference.modules.utils | | torch.ao.nn.sparse | | torch.ao.nn.sparse.quantized | | torch.ao.nn.sparse.quantized.dynamic | | torch.ao.nn.sparse.quantized.dynamic.linear | | torch.ao.nn.sparse.quantized.linear | | torch.ao.nn.sparse.quantized.utils | | torch.ao.ns | | torch.ao.ns._numeric_suite | | torch.ao.ns._numeric_suite_fx | | torch.ao.ns.fx | | torch.ao.ns.fx.graph_matcher | | torch.ao.ns.fx.graph_passes | | torch.ao.ns.fx.mappings | | torch.ao.ns.fx.n_shadows_utils | | torch.ao.ns.fx.ns_types | | torch.ao.ns.fx.pattern_utils | | torch.ao.ns.fx.qconfig_multi_mapping | | torch.ao.ns.fx.utils | | torch.ao.ns.fx.weight_utils | | torch.ao.pruning | | torch.ao.pruning.scheduler | | torch.ao.pruning.scheduler.base_scheduler | | torch.ao.pruning.scheduler.cubic_scheduler | | torch.ao.pruning.scheduler.lambda_scheduler | | torch.ao.pruning.sparsifier | | torch.ao.pruning.sparsifier.base_sparsifier | | torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier | | torch.ao.pruning.sparsifier.utils | | torch.ao.pruning.sparsifier.weight_norm_sparsifier | | torch.ao.quantization | | torch.ao.quantization.backend_config | | torch.ao.quantization.backend_config.backend_config | | torch.ao.quantization.backend_config.executorch | |", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_85", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.ao.quantization.backend_config.fbgemm | | torch.ao.quantization.backend_config.native | | torch.ao.quantization.backend_config.observation_type | | torch.ao.quantization.backend_config.onednn | | torch.ao.quantization.backend_config.qnnpack | | torch.ao.quantization.backend_config.tensorrt | | torch.ao.quantization.backend_config.utils | | torch.ao.quantization.backend_config.x86 | | torch.ao.quantization.fake_quantize | | torch.ao.quantization.fuse_modules | | torch.ao.quantization.fuser_method_mappings | | torch.ao.quantization.fx | | torch.ao.quantization.fx.convert | | torch.ao.quantization.fx.custom_config | | torch.ao.quantization.fx.fuse | | torch.ao.quantization.fx.fuse_handler | | torch.ao.quantization.fx.graph_module | | torch.ao.quantization.fx.lower_to_fbgemm | | torch.ao.quantization.fx.lower_to_qnnpack | | torch.ao.quantization.fx.lstm_utils | | torch.ao.quantization.fx.match_utils | | torch.ao.quantization.fx.pattern_utils | | torch.ao.quantization.fx.prepare | | torch.ao.quantization.fx.qconfig_mapping_utils | | torch.ao.quantization.fx.quantize_handler | | torch.ao.quantization.fx.tracer | | torch.ao.quantization.fx.utils | | torch.ao.quantization.observer | | torch.ao.quantization.pt2e | | torch.ao.quantization.pt2e.duplicate_dq_pass | | torch.ao.quantization.pt2e.export_utils | | torch.ao.quantization.pt2e.graph_utils | | torch.ao.quantization.pt2e.lowering | | torch.ao.quantization.pt2e.port_metadata_pass | | torch.ao.quantization.pt2e.prepare | | torch.ao.quantization.pt2e.qat_utils | | torch.ao.quantization.pt2e.representation | | torch.ao.quantization.pt2e.representation.rewrite | | torch.ao.quantization.pt2e.utils | | torch.ao.quantization.qconfig | | torch.ao.quantization.qconfig_mapping | | torch.ao.quantization.quant_type | | torch.ao.quantization.quantization_mappings | | torch.ao.quantization.quantize_fx | | torch.ao.quantization.quantize_jit | | torch.ao.quantization.quantize_pt2e | | torch.ao.quantization.quantizer | | torch.ao.quantization.quantizer.composable_quantizer | | torch.ao.quantization.quantizer.embedding_quantizer | | torch.ao.quantization.quantizer.quantizer | | torch.ao.quantization.quantizer.utils | | torch.ao.quantization.quantizer.x86_inductor_quantizer | | torch.ao.quantization.quantizer.xnnpack_quantizer | | torch.ao.quantization.quantizer.xnnpack_quantizer_utils | | torch.ao.quantization.quantizer.xpu_inductor_quantizer | | torch.ao.quantization.stubs | | torch.ao.quantization.utils | | torch.autograd | | torch.autograd.anomaly_mode | | torch.autograd.forward_ad | | torch.autograd.function | | torch.autograd.functional | | torch.autograd.grad_mode | | torch.autograd.gradcheck | | torch.autograd.graph | | torch.autograd.profiler | | torch.autograd.profiler_legacy | | torch.autograd.profiler_util | | torch.autograd.variable | | torch.backends | | torch.backends.cpu | | torch.backends.cuda | | torch.backends.cudnn | | torch.backends.cudnn.rnn | | torch.backends.cusparselt | | torch.backends.kleidiai | | torch.backends.mha | | torch.backends.mkl | | torch.backends.mkldnn | | torch.backends.mps | | torch.backends.nnpack | | torch.backends.openmp | | torch.backends.opt_einsum | | torch.backends.quantized | | torch.backends.xeon | | torch.backends.xeon.run_cpu | | torch.backends.xnnpack | | torch.compiler | | torch.compiler.config | | torch.contrib | | torch.cpu | | torch.cpu.amp | | torch.cpu.amp.autocast_mode | | torch.cpu.amp.grad_scaler | | torch.cuda | | torch.cuda._sanitizer | | torch.cuda.amp | | torch.cuda.amp.autocast_mode | | torch.cuda.amp.common | | torch.cuda.amp.grad_scaler | |", "prev_chunk_id": "chunk_84", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_86", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.cuda.comm | | torch.cuda.error | | torch.cuda.gds | | torch.cuda.graphs | | torch.cuda.jiterator | | torch.cuda.memory | | torch.cuda.nccl | | torch.cuda.nvtx | | torch.cuda.profiler | | torch.cuda.random | | torch.cuda.sparse | | torch.cuda.streams | | torch.cuda.tunable | | torch.distributed | | torch.distributed.algorithms | | torch.distributed.algorithms.ddp_comm_hooks | | torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook | | torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks | | torch.distributed.algorithms.ddp_comm_hooks.default_hooks | | torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks | | torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks | | torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook | | torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook | | torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks | | torch.distributed.algorithms.join | | torch.distributed.algorithms.model_averaging | | torch.distributed.algorithms.model_averaging.averagers | | torch.distributed.algorithms.model_averaging.hierarchical_model_averager | | torch.distributed.algorithms.model_averaging.utils | | torch.distributed.argparse_util | | torch.distributed.autograd | | torch.distributed.c10d_logger | | torch.distributed.checkpoint | | torch.distributed.checkpoint.api | | torch.distributed.checkpoint.default_planner | | torch.distributed.checkpoint.filesystem | | torch.distributed.checkpoint.format_utils | | torch.distributed.checkpoint.hf_storage | | torch.distributed.checkpoint.logger | | torch.distributed.checkpoint.logging_handlers | | torch.distributed.checkpoint.metadata | | torch.distributed.checkpoint.optimizer | | torch.distributed.checkpoint.planner | | torch.distributed.checkpoint.planner_helpers | | torch.distributed.checkpoint.resharding | | torch.distributed.checkpoint.staging | | torch.distributed.checkpoint.state_dict | | torch.distributed.checkpoint.state_dict_loader | | torch.distributed.checkpoint.state_dict_saver | | torch.distributed.checkpoint.stateful | | torch.distributed.checkpoint.storage | | torch.distributed.checkpoint.utils | | torch.distributed.collective_utils | | torch.distributed.constants | | torch.distributed.device_mesh | | torch.distributed.distributed_c10d | | torch.distributed.elastic | | torch.distributed.elastic.agent | | torch.distributed.elastic.agent.server | | torch.distributed.elastic.agent.server.api | | torch.distributed.elastic.agent.server.health_check_server | | torch.distributed.elastic.agent.server.local_elastic_agent | | torch.distributed.elastic.control_plane | | torch.distributed.elastic.events | | torch.distributed.elastic.events.api | | torch.distributed.elastic.events.handlers | | torch.distributed.elastic.metrics | | torch.distributed.elastic.metrics.api | | torch.distributed.elastic.multiprocessing | | torch.distributed.elastic.multiprocessing.api | | torch.distributed.elastic.multiprocessing.errors | | torch.distributed.elastic.multiprocessing.errors.error_handler | | torch.distributed.elastic.multiprocessing.errors.handlers | | torch.distributed.elastic.multiprocessing.redirects | | torch.distributed.elastic.multiprocessing.subprocess_handler | | torch.distributed.elastic.multiprocessing.subprocess_handler.handlers | | torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler | | torch.distributed.elastic.multiprocessing.tail_log | | torch.distributed.elastic.rendezvous | | torch.distributed.elastic.rendezvous.api | | torch.distributed.elastic.rendezvous.c10d_rendezvous_backend | | torch.distributed.elastic.rendezvous.dynamic_rendezvous | | torch.distributed.elastic.rendezvous.etcd_rendezvous | | torch.distributed.elastic.rendezvous.etcd_rendezvous_backend | | torch.distributed.elastic.rendezvous.etcd_server | | torch.distributed.elastic.rendezvous.etcd_store | | torch.distributed.elastic.rendezvous.registry | | torch.distributed.elastic.rendezvous.static_tcp_rendezvous | | torch.distributed.elastic.rendezvous.utils | | torch.distributed.elastic.timer | | torch.distributed.elastic.timer.api | | torch.distributed.elastic.timer.debug_info_logging | | torch.distributed.elastic.timer.file_based_local_timer | | torch.distributed.elastic.timer.local_timer | | torch.distributed.elastic.utils | | torch.distributed.elastic.utils.api | | torch.distributed.elastic.utils.data | | torch.distributed.elastic.utils.data.cycling_iterator | | torch.distributed.elastic.utils.data.elastic_distributed_sampler | | torch.distributed.elastic.utils.distributed | |", "prev_chunk_id": "chunk_85", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_87", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.distributed.elastic.utils.log_level | | torch.distributed.elastic.utils.logging | | torch.distributed.elastic.utils.store | | torch.distributed.fsdp | | torch.distributed.fsdp.api | | torch.distributed.fsdp.fully_sharded_data_parallel | | torch.distributed.fsdp.sharded_grad_scaler | | torch.distributed.fsdp.wrap | | torch.distributed.launch | | torch.distributed.launcher | | torch.distributed.launcher.api | | torch.distributed.logging_handlers | | torch.distributed.nn | | torch.distributed.nn.api | | torch.distributed.nn.api.remote_module | | torch.distributed.nn.functional | | torch.distributed.nn.jit | | torch.distributed.nn.jit.instantiator | | torch.distributed.nn.jit.templates | | torch.distributed.nn.jit.templates.remote_module_template | | torch.distributed.optim | | torch.distributed.optim.apply_optimizer_in_backward | | torch.distributed.optim.functional_adadelta | | torch.distributed.optim.functional_adagrad | | torch.distributed.optim.functional_adam | | torch.distributed.optim.functional_adamax | | torch.distributed.optim.functional_adamw | | torch.distributed.optim.functional_rmsprop | | torch.distributed.optim.functional_rprop | | torch.distributed.optim.functional_sgd | | torch.distributed.optim.named_optimizer | | torch.distributed.optim.optimizer | | torch.distributed.optim.post_localSGD_optimizer | | torch.distributed.optim.utils | | torch.distributed.optim.zero_redundancy_optimizer | | torch.distributed.pipelining | | torch.distributed.pipelining.microbatch | | torch.distributed.pipelining.schedules | | torch.distributed.pipelining.stage | | torch.distributed.remote_device | | torch.distributed.rendezvous | | torch.distributed.rpc | | torch.distributed.rpc.api | | torch.distributed.rpc.backend_registry | | torch.distributed.rpc.constants | | torch.distributed.rpc.functions | | torch.distributed.rpc.internal | | torch.distributed.rpc.options | | torch.distributed.rpc.rref_proxy | | torch.distributed.rpc.server_process_global_profiler | | torch.distributed.run | | torch.distributed.tensor | | torch.distributed.tensor.debug | | torch.distributed.tensor.device_mesh | | torch.distributed.tensor.experimental | | torch.distributed.tensor.parallel | | torch.distributed.tensor.parallel.api | | torch.distributed.tensor.parallel.ddp | | torch.distributed.tensor.parallel.fsdp | | torch.distributed.tensor.parallel.input_reshard | | torch.distributed.tensor.parallel.loss | | torch.distributed.tensor.parallel.style | | torch.distributed.tensor.placement_types | | torch.distributed.utils | | torch.distributions | | torch.distributions.bernoulli | | torch.distributions.beta | | torch.distributions.binomial | | torch.distributions.categorical | | torch.distributions.cauchy | | torch.distributions.chi2 | | torch.distributions.constraint_registry | | torch.distributions.constraints | | torch.distributions.continuous_bernoulli | | torch.distributions.dirichlet | | torch.distributions.distribution | | torch.distributions.exp_family | | torch.distributions.exponential | | torch.distributions.fishersnedecor | | torch.distributions.gamma | | torch.distributions.generalized_pareto | | torch.distributions.geometric | | torch.distributions.gumbel | | torch.distributions.half_cauchy | | torch.distributions.half_normal | | torch.distributions.independent | | torch.distributions.inverse_gamma | | torch.distributions.kl | | torch.distributions.kumaraswamy | | torch.distributions.laplace | | torch.distributions.lkj_cholesky | | torch.distributions.log_normal | | torch.distributions.logistic_normal | | torch.distributions.lowrank_multivariate_normal | | torch.distributions.mixture_same_family | | torch.distributions.multinomial | | torch.distributions.multivariate_normal | | torch.distributions.negative_binomial | | torch.distributions.normal | | torch.distributions.one_hot_categorical | |", "prev_chunk_id": "chunk_86", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_88", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.distributions.pareto | | torch.distributions.poisson | | torch.distributions.relaxed_bernoulli | | torch.distributions.relaxed_categorical | | torch.distributions.studentT | | torch.distributions.transformed_distribution | | torch.distributions.transforms | | torch.distributions.uniform | | torch.distributions.utils | | torch.distributions.von_mises | | torch.distributions.weibull | | torch.distributions.wishart | | torch.export | | torch.export.custom_obj | | torch.export.custom_ops | | torch.export.decomp_utils | | torch.export.dynamic_shapes | | torch.export.experimental | | torch.export.exported_program | | torch.export.graph_signature | | torch.export.passes | | torch.export.pt2_archive | | torch.export.pt2_archive.constants | | torch.export.unflatten | | torch.fft | | torch.func | | torch.functional | | torch.futures | | torch.fx | | torch.fx.annotate | | torch.fx.config | | torch.fx.experimental | | torch.fx.experimental.accelerator_partitioner | | torch.fx.experimental.const_fold | | torch.fx.experimental.debug | | torch.fx.experimental.graph_gradual_typechecker | | torch.fx.experimental.merge_matmul | | torch.fx.experimental.meta_tracer | | torch.fx.experimental.migrate_gradual_types | | torch.fx.experimental.migrate_gradual_types.constraint | | torch.fx.experimental.migrate_gradual_types.constraint_generator | | torch.fx.experimental.migrate_gradual_types.constraint_transformation | | torch.fx.experimental.migrate_gradual_types.operation | | torch.fx.experimental.migrate_gradual_types.transform_to_z3 | | torch.fx.experimental.migrate_gradual_types.util | | torch.fx.experimental.migrate_gradual_types.z3_types | | torch.fx.experimental.normalize | | torch.fx.experimental.optimization | | torch.fx.experimental.partitioner_utils | | torch.fx.experimental.proxy_tensor | | torch.fx.experimental.recording | | torch.fx.experimental.refinement_types | | torch.fx.experimental.rewriter | | torch.fx.experimental.schema_type_annotation | | torch.fx.experimental.sym_node | | torch.fx.experimental.symbolic_shapes | | torch.fx.experimental.unification | | torch.fx.experimental.unification.core | | torch.fx.experimental.unification.dispatch | | torch.fx.experimental.unification.match | | torch.fx.experimental.unification.more | | torch.fx.experimental.unification.multipledispatch | | torch.fx.experimental.unification.multipledispatch.conflict | | torch.fx.experimental.unification.multipledispatch.core | | torch.fx.experimental.unification.multipledispatch.dispatcher | | torch.fx.experimental.unification.multipledispatch.utils | | torch.fx.experimental.unification.multipledispatch.variadic | | torch.fx.experimental.unification.unification_tools | | torch.fx.experimental.unification.utils | | torch.fx.experimental.unification.variable | | torch.fx.experimental.unify_refinements | | torch.fx.experimental.validator | | torch.fx.graph | | torch.fx.graph_module | | torch.fx.immutable_collections | | torch.fx.interpreter | | torch.fx.node | | torch.fx.operator_schemas | | torch.fx.passes | | torch.fx.passes.annotate_getitem_nodes | | torch.fx.passes.backends | | torch.fx.passes.backends.cudagraphs | | torch.fx.passes.dialect | | torch.fx.passes.dialect.common | | torch.fx.passes.dialect.common.cse_pass | | torch.fx.passes.fake_tensor_prop | | torch.fx.passes.graph_drawer | | torch.fx.passes.graph_manipulation | | torch.fx.passes.graph_transform_observer | | torch.fx.passes.infra | | torch.fx.passes.infra.partitioner | | torch.fx.passes.infra.pass_base | | torch.fx.passes.infra.pass_manager | | torch.fx.passes.net_min_base | | torch.fx.passes.operator_support | | torch.fx.passes.param_fetch | | torch.fx.passes.pass_manager | | torch.fx.passes.reinplace | | torch.fx.passes.runtime_assert | | torch.fx.passes.shape_prop | |", "prev_chunk_id": "chunk_87", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_89", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.fx.passes.split_module | | torch.fx.passes.split_utils | | torch.fx.passes.splitter_base | | torch.fx.passes.tests | | torch.fx.passes.tests.test_pass_manager | | torch.fx.passes.tools_common | | torch.fx.passes.utils | | torch.fx.passes.utils.common | | torch.fx.passes.utils.fuser_utils | | torch.fx.passes.utils.matcher_utils | | torch.fx.passes.utils.matcher_with_name_node_map_utils | | torch.fx.passes.utils.source_matcher_utils | | torch.fx.proxy | | torch.fx.subgraph_rewriter | | torch.fx.tensor_type | | torch.fx.traceback | | torch.hub | | torch.jit | | torch.jit.annotations | | torch.jit.frontend | | torch.jit.generate_bytecode | | torch.jit.mobile | | torch.jit.quantized | | torch.jit.supported_ops | | torch.jit.unsupported_tensor_ops | | torch.library | | torch.linalg | | torch.masked | | torch.masked.maskedtensor | | torch.masked.maskedtensor.binary | | torch.masked.maskedtensor.core | | torch.masked.maskedtensor.creation | | torch.masked.maskedtensor.passthrough | | torch.masked.maskedtensor.reductions | | torch.masked.maskedtensor.unary | | torch.monitor | | torch.mps | | torch.mps.event | | torch.mps.profiler | | torch.mtia | | torch.mtia.memory | | torch.multiprocessing | | torch.multiprocessing.pool | | torch.multiprocessing.queue | | torch.multiprocessing.reductions | | torch.multiprocessing.spawn | | torch.nested | | torch.nn | | torch.nn.attention | | torch.nn.attention.bias | | torch.nn.attention.experimental | | torch.nn.attention.flex_attention | | torch.nn.backends | | torch.nn.backends.thnn | | torch.nn.common_types | | torch.nn.cpp | | torch.nn.functional | | torch.nn.grad | | torch.nn.init | | torch.nn.intrinsic | | torch.nn.intrinsic.modules | | torch.nn.intrinsic.modules.fused | | torch.nn.intrinsic.qat | | torch.nn.intrinsic.qat.modules | | torch.nn.intrinsic.qat.modules.conv_fused | | torch.nn.intrinsic.qat.modules.linear_fused | | torch.nn.intrinsic.qat.modules.linear_relu | | torch.nn.intrinsic.quantized | | torch.nn.intrinsic.quantized.dynamic | | torch.nn.intrinsic.quantized.dynamic.modules | | torch.nn.intrinsic.quantized.dynamic.modules.linear_relu | | torch.nn.intrinsic.quantized.modules | | torch.nn.intrinsic.quantized.modules.bn_relu | | torch.nn.intrinsic.quantized.modules.conv_relu | | torch.nn.intrinsic.quantized.modules.linear_relu | | torch.nn.modules | | torch.nn.modules.activation | | torch.nn.modules.adaptive | | torch.nn.modules.batchnorm | | torch.nn.modules.channelshuffle | | torch.nn.modules.container | | torch.nn.modules.conv | | torch.nn.modules.distance | | torch.nn.modules.dropout | | torch.nn.modules.flatten | | torch.nn.modules.fold | | torch.nn.modules.instancenorm | | torch.nn.modules.lazy | | torch.nn.modules.linear | | torch.nn.modules.loss | | torch.nn.modules.module | | torch.nn.modules.normalization | | torch.nn.modules.padding | | torch.nn.modules.pixelshuffle | | torch.nn.modules.pooling | | torch.nn.modules.rnn | | torch.nn.modules.sparse | | torch.nn.modules.transformer | | torch.nn.modules.upsampling | | torch.nn.modules.utils | |", "prev_chunk_id": "chunk_88", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_90", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.nn.parallel | | torch.nn.parallel.comm | | torch.nn.parallel.distributed | | torch.nn.parallel.parallel_apply | | torch.nn.parallel.replicate | | torch.nn.parallel.scatter_gather | | torch.nn.parameter | | torch.nn.qat | | torch.nn.qat.dynamic | | torch.nn.qat.dynamic.modules | | torch.nn.qat.dynamic.modules.linear | | torch.nn.qat.modules | | torch.nn.qat.modules.conv | | torch.nn.qat.modules.embedding_ops | | torch.nn.qat.modules.linear | | torch.nn.quantizable | | torch.nn.quantizable.modules | | torch.nn.quantizable.modules.activation | | torch.nn.quantizable.modules.rnn | | torch.nn.quantized | | torch.nn.quantized.dynamic | | torch.nn.quantized.dynamic.modules | | torch.nn.quantized.dynamic.modules.conv | | torch.nn.quantized.dynamic.modules.linear | | torch.nn.quantized.dynamic.modules.rnn | | torch.nn.quantized.functional | | torch.nn.quantized.modules | | torch.nn.quantized.modules.activation | | torch.nn.quantized.modules.batchnorm | | torch.nn.quantized.modules.conv | | torch.nn.quantized.modules.dropout | | torch.nn.quantized.modules.embedding_ops | | torch.nn.quantized.modules.functional_modules | | torch.nn.quantized.modules.linear | | torch.nn.quantized.modules.normalization | | torch.nn.quantized.modules.rnn | | torch.nn.quantized.modules.utils | | torch.nn.utils | | torch.nn.utils.clip_grad | | torch.nn.utils.convert_parameters | | torch.nn.utils.fusion | | torch.nn.utils.init | | torch.nn.utils.memory_format | | torch.nn.utils.parametrizations | | torch.nn.utils.parametrize | | torch.nn.utils.prune | | torch.nn.utils.rnn | | torch.nn.utils.stateless | | torch.onnx | | torch.onnx.errors | | torch.onnx.operators | | torch.onnx.ops | | torch.onnx.symbolic_caffe2 | | torch.onnx.symbolic_helper | | torch.onnx.symbolic_opset10 | | torch.onnx.symbolic_opset11 | | torch.onnx.symbolic_opset12 | | torch.onnx.symbolic_opset13 | | torch.onnx.symbolic_opset14 | | torch.onnx.symbolic_opset15 | | torch.onnx.symbolic_opset16 | | torch.onnx.symbolic_opset17 | | torch.onnx.symbolic_opset18 | | torch.onnx.symbolic_opset19 | | torch.onnx.symbolic_opset20 | | torch.onnx.symbolic_opset7 | | torch.onnx.symbolic_opset8 | | torch.onnx.symbolic_opset9 | | torch.onnx.utils | | torch.onnx.verification | | torch.optim | | torch.optim.adadelta | | torch.optim.adagrad | | torch.optim.adam | | torch.optim.adamax | | torch.optim.adamw | | torch.optim.asgd | | torch.optim.lbfgs | | torch.optim.lr_scheduler | | torch.optim.nadam | | torch.optim.optimizer | | torch.optim.radam | | torch.optim.rmsprop | | torch.optim.rprop | | torch.optim.sgd | | torch.optim.sparse_adam | | torch.optim.swa_utils | | torch.overrides | | torch.package | | torch.package.analyze | | torch.package.analyze.find_first_use_of_broken_modules | | torch.package.analyze.is_from_package | | torch.package.analyze.trace_dependencies | | torch.package.file_structure_representation | | torch.package.find_file_dependencies | | torch.package.glob_group | | torch.package.importer | | torch.package.package_exporter | | torch.package.package_importer | | torch.profiler | |", "prev_chunk_id": "chunk_89", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_91", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.profiler.itt | | torch.profiler.profiler | | torch.profiler.python_tracer | | torch.quantization | | torch.quantization.fake_quantize | | torch.quantization.fuse_modules | | torch.quantization.fuser_method_mappings | | torch.quantization.fx | | torch.quantization.fx.convert | | torch.quantization.fx.fuse | | torch.quantization.fx.fusion_patterns | | torch.quantization.fx.graph_module | | torch.quantization.fx.match_utils | | torch.quantization.fx.pattern_utils | | torch.quantization.fx.prepare | | torch.quantization.fx.quantization_patterns | | torch.quantization.fx.quantization_types | | torch.quantization.fx.utils | | torch.quantization.observer | | torch.quantization.qconfig | | torch.quantization.quant_type | | torch.quantization.quantization_mappings | | torch.quantization.quantize | | torch.quantization.quantize_fx | | torch.quantization.quantize_jit | | torch.quantization.stubs | | torch.quantization.utils | | torch.quasirandom | | torch.random | | torch.return_types | | torch.serialization | | torch.signal | | torch.signal.windows | | torch.signal.windows.windows | | torch.sparse | | torch.sparse.semi_structured | | torch.special | | torch.storage | | torch.testing | | torch.torch_version | | torch.types | | torch.utils | | torch.utils.backcompat | | torch.utils.backend_registration | | torch.utils.benchmark | | torch.utils.benchmark.examples | | torch.utils.benchmark.examples.compare | | torch.utils.benchmark.examples.fuzzer | | torch.utils.benchmark.examples.op_benchmark | | torch.utils.benchmark.examples.simple_timeit | | torch.utils.benchmark.examples.spectral_ops_fuzz_test | | torch.utils.benchmark.op_fuzzers | | torch.utils.benchmark.op_fuzzers.binary | | torch.utils.benchmark.op_fuzzers.sparse_binary | | torch.utils.benchmark.op_fuzzers.sparse_unary | | torch.utils.benchmark.op_fuzzers.spectral | | torch.utils.benchmark.op_fuzzers.unary | | torch.utils.benchmark.utils | | torch.utils.benchmark.utils.common | | torch.utils.benchmark.utils.compare | | torch.utils.benchmark.utils.compile | | torch.utils.benchmark.utils.cpp_jit | | torch.utils.benchmark.utils.fuzzer | | torch.utils.benchmark.utils.sparse_fuzzer | | torch.utils.benchmark.utils.timer | | torch.utils.benchmark.utils.valgrind_wrapper | | torch.utils.benchmark.utils.valgrind_wrapper.timer_interface | | torch.utils.bottleneck | | torch.utils.bundled_inputs | | torch.utils.checkpoint | | torch.utils.collect_env | | torch.utils.cpp_backtrace | | torch.utils.cpp_extension | | torch.utils.data | | torch.utils.data.backward_compatibility | | torch.utils.data.dataloader | | torch.utils.data.datapipes | | torch.utils.data.datapipes.dataframe | | torch.utils.data.datapipes.dataframe.dataframe_wrapper | | torch.utils.data.datapipes.dataframe.dataframes | | torch.utils.data.datapipes.dataframe.datapipes | | torch.utils.data.datapipes.dataframe.structures | | torch.utils.data.datapipes.datapipe | | torch.utils.data.datapipes.gen_pyi | | torch.utils.data.datapipes.iter | | torch.utils.data.datapipes.iter.callable | | torch.utils.data.datapipes.iter.combinatorics | | torch.utils.data.datapipes.iter.combining | | torch.utils.data.datapipes.iter.filelister | | torch.utils.data.datapipes.iter.fileopener | | torch.utils.data.datapipes.iter.grouping | | torch.utils.data.datapipes.iter.routeddecoder | | torch.utils.data.datapipes.iter.selecting | | torch.utils.data.datapipes.iter.sharding | | torch.utils.data.datapipes.iter.streamreader | | torch.utils.data.datapipes.iter.utils | | torch.utils.data.datapipes.map | | torch.utils.data.datapipes.map.callable | | torch.utils.data.datapipes.map.combinatorics | | torch.utils.data.datapipes.map.combining | |", "prev_chunk_id": "chunk_90", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_92", "url": "https://docs.pytorch.org/docs/stable/py-modindex.html", "title": "Python Module Index", "page_title": "Python Module Index — PyTorch 2.8 documentation", "breadcrumbs": "Python Module Index", "content": "torch.utils.data.datapipes.map.grouping | | torch.utils.data.datapipes.map.utils | | torch.utils.data.datapipes.utils | | torch.utils.data.datapipes.utils.common | | torch.utils.data.datapipes.utils.decoder | | torch.utils.data.datapipes.utils.snapshot | | torch.utils.data.dataset | | torch.utils.data.distributed | | torch.utils.data.graph | | torch.utils.data.graph_settings | | torch.utils.data.sampler | | torch.utils.deterministic | | torch.utils.dlpack | | torch.utils.file_baton | | torch.utils.flop_counter | | torch.utils.hipify | | torch.utils.hipify.constants | | torch.utils.hipify.cuda_to_hip_mappings | | torch.utils.hipify.hipify_python | | torch.utils.hipify.version | | torch.utils.hooks | | torch.utils.jit | | torch.utils.jit.log_extract | | torch.utils.mkldnn | | torch.utils.mobile_optimizer | | torch.utils.model_dump | | torch.utils.model_zoo | | torch.utils.module_tracker | | torch.utils.serialization | | torch.utils.serialization.config | | torch.utils.show_pickle | | torch.utils.tensorboard | | torch.utils.tensorboard.summary | | torch.utils.tensorboard.writer | | torch.utils.throughput_benchmark | | torch.utils.viz | | torch.utils.weak | | torch.version | | torch.xpu | | torch.xpu.memory | | torch.xpu.random | | torch.xpu.streams |", "prev_chunk_id": "chunk_91", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_93", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "PyTorch Governance | Maintainers#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Governance | Maintainers#", "content": "PyTorch Governance | Maintainers# Created On: Mar 11, 2019 | Last Updated On: Apr 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_94", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Responsibilities#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Responsibilities#", "content": "Responsibilities# - Triage and fix high priority issues assigned to the module or library - Triage, review, and land high priority pull requests assigned to the module or library - Answer module or library questions ondiscuss.pytorch.organddev-discuss.pytorch.org - Maintain public user and development documentation - Run meetings and share minutes plus roadmap on a half or quarterly basis", "prev_chunk_id": "chunk_93", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_95", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Lead Core Maintainer (BDFL)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Lead Core Maintainer (BDFL)#", "content": "Lead Core Maintainer (BDFL)# - Soumith Chintala (soumith)", "prev_chunk_id": "chunk_94", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_96", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Core Maintainers#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Core Maintainers#", "content": "Core Maintainers# - Soumith Chintala (soumith) - Edward Yang (ezyang) - Greg Chanan (gchanan) - Dmytro Dzhulgakov (dzhulgakov) - Nikita Shulga (malfet) - Alban Desmaison (albanD) - Piotr Bialecki (ptrblck)", "prev_chunk_id": "chunk_95", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_97", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "NN APIs (torch.nn)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "NN APIs (torch.nn)#", "content": "NN APIs (torch.nn)# - Mikayla Gawarecki (mikaylagawarecki) - Alban Desmaison (albanD) - Joel Schlosser (jbschlosser) - (emeritus) Greg Chanan (gchanan) - (emeritus) Soumith Chintala (soumith) - (emeritus) Sam Gross (colesbury) - (emeritus) Adam Paszke (apaszke)", "prev_chunk_id": "chunk_96", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_98", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Optimizers (torch.optim)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Optimizers (torch.optim)#", "content": "Optimizers (torch.optim)# - Jane Xu (janeyx99) - Alban Desmaison (albanD) - Joel Schlosser (jbschlosser) - (emeritus) Soumith Chintala (soumith) - (emeritus) Ilqar Ramazanli (iramazanli) - (emeritus) Vincent Quenneville-Belair (vincentqb)", "prev_chunk_id": "chunk_97", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_99", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Autograd (torch.autograd)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Autograd (torch.autograd)#", "content": "Autograd (torch.autograd)# - Jeffrey Wan (soulitzer) - Alban Desmaison (alband) - Edward Yang (ezyang) - (emeritus) Adam Paszke (apaszke)", "prev_chunk_id": "chunk_98", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_100", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchDynamo#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo#", "content": "TorchDynamo# - Animesh Jain (anijain2305) - Jason Ansel (jansel) - Edward Yang (ezyang)", "prev_chunk_id": "chunk_99", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_101", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchInductor#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchInductor#", "content": "TorchInductor# - Elias Ellison (eellison) - Horace He (Chillee) - Shunting Zhang (shunting314) - Jason Ansel (jansel) - Jiong Gong (jgong5)", "prev_chunk_id": "chunk_100", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_102", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Cudagraph Tree#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Cudagraph Tree#", "content": "Cudagraph Tree# - Elias Ellison (eellison)", "prev_chunk_id": "chunk_101", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_103", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "PT2 Dispatcher#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "PT2 Dispatcher#", "content": "PT2 Dispatcher# - Brian Hirsh (bdhirsh) - Richard Zou (zou3519) - Horace He (Chillee) - Edward Yang (ezyang)", "prev_chunk_id": "chunk_102", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_104", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "PT2 Export (torch.export)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "PT2 Export (torch.export)#", "content": "PT2 Export (torch.export)# - Avik Chaudhuri (avikchaudhuri) - Yanan Cao (gmagogsfm)", "prev_chunk_id": "chunk_103", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_105", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "AOT Inductor (AOTI) & AOTI Runtime#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "AOT Inductor (AOTI) & AOTI Runtime#", "content": "AOT Inductor (AOTI) & AOTI Runtime# - Bin Bao (desertfire) - Angela Yi (angelayi) - Yang Chen (chenyang78)", "prev_chunk_id": "chunk_104", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_106", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Compilers (JIT / TorchScript / Package / Deploy)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Compilers (JIT / TorchScript / Package / Deploy)#", "content": "Compilers (JIT / TorchScript / Package / Deploy)# - (emeritus) Elias Ellison (eellison) - (emeritus) Michael Suo (suo) - (emeritus) Yanan Cao (gmagogsfm) - (emeritus) James Reed (jamesr66a) - (emeritus) Jason Ansel (jansel) - (emeritus) Jiong Gong (jgong5) - (emeritus) Zach Devito (zdevito)", "prev_chunk_id": "chunk_105", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_107", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Distributions & RNG#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Distributions & RNG#", "content": "Distributions & RNG# - Fritz Obermeyer (fritzo) - Neeraj Pradhan (neerajprad) - Alican Bozkurt (alicanb) - (emeritus) Vishwak Srinivasan (vishwakftw)", "prev_chunk_id": "chunk_106", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_108", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Distributed#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Distributed#", "content": "Distributed# - Will Constable (wconstab) - Howard Huang (H-Huang) - Wanchao Liang (wanchaol) - Ke Wen (kwen2501) - Chien-Chin Huang (fegin) - Tristan Rice (d4l3k) - (emeritus) Shen Li (mrshenli) - (emeritus) Pritam Damania (pritamdamania87) - (emeritus) Yanli Zhao (zhaojuanmao) - (emeritus) Rohan Varma (rohan-varma) - (emeritus) Junjie Wang (fduwjj) - (emeritus) Alisson Azzolini (aazzolini) - (emeritus) James Reed (jamesr66a) - (emeritus) Kiuk Chung (kiukchung) - (emeritus) Pieter Noordhuis (pietern) - (emeritus) Mingzhe Li (mingzhe09088) - (emeritus) Omkar Salpekar (osalpekar)", "prev_chunk_id": "chunk_107", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_109", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Multiprocessing#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing#", "content": "Multiprocessing# - (emeritus) Simon Wang (SsnL) - (emeritus) Vitaly Fedyunin (VitalyFedyunin) - (emeritus) Adam Paszke (apaszke)", "prev_chunk_id": "chunk_108", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_110", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Linear Algebra (torch.linalg)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Linear Algebra (torch.linalg)#", "content": "Linear Algebra (torch.linalg)# - Mario Lezcano (lezcano) - (emeritus) Mike Ruberry (mruberry) - (emeritus) Ivan Yashchuk (IvanYashchuk) - (emeritus) Vishwak Srinivasan (vishwakftw) - (emeritus) Nikita Vedeneev (nikitaved)", "prev_chunk_id": "chunk_109", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_111", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Sparse (torch.sparse)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Sparse (torch.sparse)#", "content": "Sparse (torch.sparse)# - (emeritus) Pearu Peterson (pearu) - (emeritus) Nikita Vedeneev (nikitaved) - (emeritus) Ivan Yashchuk (IvanYashchuk) - (emeritus) Christian Puhrsch (cpuhrsch) - (emeritus) Andrew James (amjames)", "prev_chunk_id": "chunk_110", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_112", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "NestedTensor (torch.nested)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "NestedTensor (torch.nested)#", "content": "NestedTensor (torch.nested)# - Joel Schlosser (jbschlosser) - Christian Puhrsch (cpuhrsch) - Driss Guessous (drisspg) - Mikayla Gawarecki (mikaylagawarecki) - Alban Desmaison (albanD) - (emeritus) Natalia Gimelshein (ngimel)", "prev_chunk_id": "chunk_111", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_113", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "MaskedTensor (torch.masked)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "MaskedTensor (torch.masked)#", "content": "MaskedTensor (torch.masked)# - Christian Puhrsch (cpuhrsch) - (emeritus) George Qi (george-qi)", "prev_chunk_id": "chunk_112", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_114", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Fast Fourier Transform (torch.fft)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Fast Fourier Transform (torch.fft)#", "content": "Fast Fourier Transform (torch.fft)# - (emeritus) Mike Ruberry (mruberry) - (emeritus) Peter Bell (peterbell10)", "prev_chunk_id": "chunk_113", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_115", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "MKLDNN#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "MKLDNN#", "content": "MKLDNN# - Xiaobing Zhang (XiaobingSuper) - Mingfei Ma (mingfeima) - Jiong Gong (jgong5) - (emeritus) Xiaoqiang Zheng (zheng-xq) - (emeritus) Sam Gross (colesbury) - (emeritus) Christian Puhrsch (cpuhrsch) - (emeritus) Ilia Cherniavskii (ilia-cher) - (emeritus) Junjie Bai (bddppq) - (emeritus) Yinghai Lu (yinghai) - (emeritus) Vitaly Fedyunin (VitalyFedyunin) - (emeritus) Jianhui Li (Jianhui-Li)", "prev_chunk_id": "chunk_114", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_116", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "CUDA#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "CUDA#", "content": "CUDA# - Natalia Gimelshein (ngimel) - Edward Yang (ezyang) - Piotr Bialecki (ptrblck) - Christian Sarofeen (csarofeen) - (emeritus) Andrew Tulloch (ajtulloch) - (emeritus) Xiaoqiang Zheng (zheng-xq)", "prev_chunk_id": "chunk_115", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_117", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "AMD/ROCm/HIP#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "AMD/ROCm/HIP#", "content": "AMD/ROCm/HIP# - Jeff Daily (jeffdaily) - Jithun Nair (jithunnair-amd) - (emeritus) Junjie Bai (bddppq)", "prev_chunk_id": "chunk_116", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_118", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Build + CI#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Build + CI#", "content": "Build + CI# - Nikita Shulga (malfet) - Eli Uriegas (seemethere) - Alban Desmaison (alband) - Andrey Talman (atalman) - Zain Rizvi (ZainRizvi) - (emeritus) Mikey Dagitses (dagitses) - (emeritus) Omkar Salpekar (osalpekar) - (emeritus) Nirav Mehta (mehtanirav) - (emeritus) Zhuojie Zhou (zhouzhuojie) - (emeritus) Edward Yang (ezyang) - (emeritus) Karl Ostmo (kostmo)", "prev_chunk_id": "chunk_117", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_119", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Performance Tools#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Performance Tools#", "content": "Performance Tools# - Taylor Robie (robieta) - Xu Zhao (xuzhao9) - (emeritus) Victor Bittorf (bitfort) - (emeritus) Gisle Dankel (gdankel) - (emeritus) Natalia Gimelshein (ngimel) - (emeritus) Mingzhe Li (mingzhe09088)", "prev_chunk_id": "chunk_118", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_120", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "C++ API#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "C++ API#", "content": "C++ API# - (emeritus) Joel Schlosser (jbschlosser) - (emeritus) Will Feng (yf225)", "prev_chunk_id": "chunk_119", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_121", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "C10 utils and operator dispatch#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "C10 utils and operator dispatch#", "content": "C10 utils and operator dispatch# - Brian Hirsh (bdhirsh) - Edward Yang (ezyang) - (emeritus) Dmytro Dzhulgakov (dzhulgakov) - (emeritus) Sebastian Messmer (smessmer)", "prev_chunk_id": "chunk_120", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_122", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "ONNX exporter#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "ONNX exporter#", "content": "ONNX exporter# - Shubham Bhokare (shubhambhokare1) - Justin Chu (justinchuby) - Xavier Dupré (xadupre) - Titai Wang (titaiwangms) - (emeritus) Bowen Bao (BowenBao) - (emeritus) Thiago Crepaldi (thiagocrepaldi) - (emeritus) Aaron Bockover (abock) - (emeritus) Gary Miguel (garymm) - (emeritus) Lara Haidar (lara-hdr) - (emeritus) Lu Fang (houseroad) - (emeritus) Negin Raoof (neginraoof) - (emeritus) Spandan Tiwari (spandantiwari)", "prev_chunk_id": "chunk_121", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_123", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "LiteInterpreter#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "LiteInterpreter#", "content": "LiteInterpreter# - (emeritus) David Reiss (dreiss) - (emeritus) Raziel Guevara (raziel) - (emeritus) Linbin Yu (linbinyu) - (emeritus) Ivan Kobzarev (IvanKobzarev) - (emeritus) Tao Xu (xta0)", "prev_chunk_id": "chunk_122", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_124", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Quantization (torch/ao)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Quantization (torch/ao)#", "content": "Quantization (torch/ao)# - Mark Saroufim (msaroufim) - Vasiliy Kuznetsov (vkuzo) - Jerry Zhang (jerryzh168) - (emeritus) Zafar Takhirov (z-a-f) - (emeritus) Raghuraman Krishnamoorthi (raghuramank100)", "prev_chunk_id": "chunk_123", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_125", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Windows#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Windows#", "content": "Windows# - (emeritus) Guoliang Hua (nbcsm) - (emeritus) Teng Gao (gaoteng-git) - (emeritus) Peter Johnson (peterjc123)", "prev_chunk_id": "chunk_124", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_126", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Apple M1/MPS/Metal#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Apple M1/MPS/Metal#", "content": "Apple M1/MPS/Metal# - Kulin Seth (kulinseth) - Alban Desmaison (alband) - Nikita Shulga (malfet) - (emeritus) Ramin Azarmehr (razarmehr)", "prev_chunk_id": "chunk_125", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_127", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "PowerPC#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "PowerPC#", "content": "PowerPC# - (emeritus) Alfredo Mendoza (avmgithub)", "prev_chunk_id": "chunk_126", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_128", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "x86 CPU#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "x86 CPU#", "content": "x86 CPU# - Mingfei Ma (mingfeima) - Jiong Gong (jgong5)", "prev_chunk_id": "chunk_127", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_129", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "AArch64 CPU#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "AArch64 CPU#", "content": "AArch64 CPU# - Sunita Nadampalli (snadampal)", "prev_chunk_id": "chunk_128", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_130", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "Docs / Tutorials#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "Docs / Tutorials#", "content": "Docs / Tutorials# - Svetlana Karslioglu (svekars)", "prev_chunk_id": "chunk_129", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_131", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "XLA#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "XLA#", "content": "XLA# - Jack Cao (JackCaoG) - Daniel Sohn (jysohn23) - Zach Cain (zcain117) - Brian Hirsh (bdhirsh) - Gregory Chanan (gchanan) - (emeritus) Ailing Zhang (ailzhang) - (emeritus) Davide Libenzi (dlibenzi) - (emeritus) Alex Suhan (asuhan)", "prev_chunk_id": "chunk_130", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_132", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchServe#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchServe#", "content": "TorchServe# - Li Ning (lxning) - Ankith Gunapal (agunapal) - Hamid Shojanazeri (HamidShojanazeri) - (emeritus) Mark Saroufim (msaroufIm) - (emeritus) Manoj Rao (mycpuorg) - (emeritus) Vamshi Dantu (vdantu) - (emeritus) Dhanasekar Karuppasamy (dhanainme)", "prev_chunk_id": "chunk_131", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_133", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchVision#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchVision#", "content": "TorchVision# - Nicolas Hug (NicolasHug) - Philip Meier (pmeier) - Victor Fomin (vfdev-5) - (emeritus) Francisco Massa (fmassa) - (emeritus) Vasilis Vryniotis (datumbox) - (emeritus) Yosua Michael Maranatha (YosuaMichael) - (emeritus) Joao Gomes (jdsgomes)", "prev_chunk_id": "chunk_132", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_134", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchText#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchText#", "content": "TorchText# - (emeritus) Nayef Ahmed (Nayef211) - (emeritus) Parmeet Singh Bhatia (parmeet) - (emeritus) Guanheng George Zhang (zhangguanheng66) - (emeritus) Christian Puhrsch (cpuhrsch)", "prev_chunk_id": "chunk_133", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_135", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchAudio#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchAudio#", "content": "TorchAudio# - Moto Hira (mthrok) - (emeritus) Jeff Hwang (hwangjeff) - (emeritus) Caroline Chen (carolineechen) - (emeritus) Xiaohui Zhang (xiaohui-zhang) - (emeritus) Zhaoheng Ni (nateanl) - (emeritus) Christian Puhrsch (cpuhrsch) - (emeritus) Vincent QB (vincentqb)", "prev_chunk_id": "chunk_134", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_136", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchRec#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchRec#", "content": "TorchRec# - Colin Taylor (colin2328) - Paul Zhang (PaulZhang12) - (emeritus) Dmytro Ivchenko (divchenko)", "prev_chunk_id": "chunk_135", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_137", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchX#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchX#", "content": "TorchX# - (emeritus) Tristan Rice (d4l3k) - (emeritus) Kiuk Chung (kiukchung)", "prev_chunk_id": "chunk_136", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_138", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchData#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchData#", "content": "TorchData# - Andrew Ho (andrewkho) - Divyansh Khanna (divyanshk)", "prev_chunk_id": "chunk_137", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_139", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchArrow#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchArrow#", "content": "TorchArrow# - (emeritus) Wenlei Xie (wenleix) - (emeritus) Vitaly Fedyunin (VitalyFedyunin)", "prev_chunk_id": "chunk_138", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_140", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "ExecuTorch (Edge, Mobile)#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "ExecuTorch (Edge, Mobile)#", "content": "ExecuTorch (Edge, Mobile)# - Mergen Nachin (mergennachin) - Kimish Patel (kimishpatel) - Dave Bort (dbort) - Martin Yuan (iseeyuan)", "prev_chunk_id": "chunk_139", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_141", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchTune#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchTune#", "content": "TorchTune# - Kartikay Khandelwal (kartikayk) - Evan Smothers (ebsmothers) - Joe Cummings (joecummings)", "prev_chunk_id": "chunk_140", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_142", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchChat#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchChat#", "content": "TorchChat# - Jack Khuu (Jack-Khuu) - Jesse White (byjlw) - (emeritus) Michael Gschwind (mikekgfb)", "prev_chunk_id": "chunk_141", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_143", "url": "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html", "title": "TorchCodec#", "page_title": "PyTorch Governance | Maintainers — PyTorch 2.8 documentation", "breadcrumbs": "TorchCodec#", "content": "TorchCodec# - Nicolas Hug (nicolashug) - Ahmad Sharif (ahmadsharif1) - Scott Schneider (scotts)", "prev_chunk_id": "chunk_142", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_144", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "PyTorch Governance | Mechanics#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Governance | Mechanics#", "content": "PyTorch Governance | Mechanics# Created On: Mar 11, 2019 | Last Updated On: Apr 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_145", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Summary#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Summary#", "content": "Summary# PyTorch adopts a technical governance structure that is hierarchical. - A community ofcontributorswho file issues, make pull requests, and contribute to the project. - A small set ofmodule maintainersdrive each module of the PyTorch project. - They are overseen bycore maintainers, who drive the overall project direction. - The core maintainers have alead core maintainerwho is the catch-all decision maker. All maintainers are expected to have a strong bias towards PyTorch’s design philosophy. Beyond the maintainers, the community is encouraged to contribute, file issues, make proposals, review pull requests and be present in the community. Given contributions and willingness to invest, anyone can be accepted as a maintainer and provided write access or ownership of parts of the codebase. Technical governance is strictly separated from business governance. Separating technical from business governance ensures that there is no way for any person or company to “buy their way into” the technical guidance of the project. Additionally, membership in the technical governance process is for individuals, not companies. That is, there are no seats reserved for specific companies, and membership is associated with the person rather than the company employing that person.", "prev_chunk_id": "chunk_144", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_146", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Module Maintainers#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Module Maintainers#", "content": "Module Maintainers# Modules are defined as GitHub repositories within the PyTorch org, or as directories within the core repository pytorch/pytorch. Each module will have its own maintainer group. Maintainer groups are responsible for reviewing and approving commits, improving design, and changing the scope of the module. Each maintainer group may adopt its own rules and procedures for making decisions (majority vote being default). Module maintainers have the right to dispute decisions made by other module maintainers – especially if it affects them. When disputes are made, the module maintainer group should provide a reasonable and public explanation of the dispute, the relevant arguments, and the resolution. In the exceptional cases where module maintainers cannot come to a conclusion themselves, they will escalate to core maintainers for review. The escalations are resolved by the core maintainers in accordance with their rules and procedures. Each maintainer group should publish publicly available communication for their module (a vision, rough roadmap, design docs, any disputes and dispute resolutions) so that contributors and other interested parties understand the future direction of the project and can participate in discussion. Responsibilities of the maintainer includes: - Triaging high priority issues of the module - Triaging and reviewing and landing high priority pull requests of the module - Supporting public documentation related to the module - Running public developer meetings", "prev_chunk_id": "chunk_145", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_147", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Core Maintainers#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Core Maintainers#", "content": "Core Maintainers# The core maintainers are expected to have a deep understanding of the PyTorch code base and design philosophies. Their responsibilities include: - Articulating a cohesive long-term vision for the project - Negotiating and resolving contentious issues in ways acceptable to all parties involved - Receiving broad requests for changes from stakeholders of PyTorch and evaluating / accepting them (small module-level requests are handled by module maintainers) The core maintainers as a group have the power to veto any decision made at a Module maintainer level. The core maintainers have power to resolve disputes as they see fit. The core maintainers should publicly articulate their decision-making, and give a clear reasoning for their decisions, vetoes and dispute resolution. The core maintainers are admins of the PyTorch GitHub Org and are listed in Maintainers.", "prev_chunk_id": "chunk_146", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_148", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Lead Core Maintainer (BDFL)#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Lead Core Maintainer (BDFL)#", "content": "Lead Core Maintainer (BDFL)# There may be decisions in which the core maintainers cannot come to a consensus. To make such difficult decisions, the core maintainers have an assigned and publicly declared Lead Core Maintainer amongst them, also commonly known in open-source governance models as a BDFL. The Lead Core Maintainer should publicly articulate their decision-making, and give a clear reasoning for their decisions. The Lead Core Maintainer is also responsible for confirming or removing core maintainers.", "prev_chunk_id": "chunk_147", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_149", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "The Principles#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "The Principles#", "content": "The Principles# - Membership in module maintainer groups is given toindividualsonmerit basisafter they demonstrated strong expertise of the component through contributions, reviews and discussions and are aligned with how the component fits in overall PyTorch direction. - For membership in the maintainer group the individual has to demonstrate strong and continued alignment with the overall PyTorch principles. - No term limits for module maintainers or core maintainers - Light criteria of moving module maintenance to ‘emeritus’ status if they don’t actively participate over long periods of time. Each module maintainer group may define the inactive period that’s appropriate for that module. - The membership is for an individual, not a company.", "prev_chunk_id": "chunk_148", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_150", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "The Process for Nomination#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "The Process for Nomination#", "content": "The Process for Nomination# - Each module has its own process. Please contact module maintainers for more information. However, if there is no process identified, you can file a request to the core maintainers by submittingthis form. Core maintainers are meeting every three months. - If you are submitting a request to the core maintainers, the information in your request must include the following items:The nominees depth and breadth of code, review and design contributions on the moduleTestimonials (positive and negative) of the nominee’s interactions with the maintainers, users, and the communityGeneral testimonials of support from the maintainers - The core maintainers then evaluate all information and make a final decision to Confirm or Decline the nomination. The decision of the core maintainers has to be articulated well and would be public.", "prev_chunk_id": "chunk_149", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_151", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "The Process for Removal#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "The Process for Removal#", "content": "The Process for Removal# - Similar to the process for nomination, anyone in the community can nominate a person to be removed from a Module maintainer position or a Core maintainer position. - A person can also self-nominate to be removed - The core maintainers (excluding persons with conflict of interest) will request or put together more information around the following:Their activity (or lack of) on the projectTheir changing thinking of the space, which results in conflict with the overall direction of the projectOther information that makes them unfit to be a maintainer, such as Code of Conduct issues, their activity outside the scope of the project that conflicts with the project’s valuesConflicts of interest: filial or romantic relationships - The core maintainers then evaluate all information and make a final decision to Confirm or Decline the removal. The decision of the core maintainers has to be articulated well and would be public.", "prev_chunk_id": "chunk_150", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_152", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Nominating Core Maintainers#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Nominating Core Maintainers#", "content": "Nominating Core Maintainers# - Any core or module maintainer can nominate someone to become a core maintainer - The lead maintainer (BDFL) is responsible for evaluating the nomination. - The lead maintainer requests or puts together more information around the strength of the candidate to be a core maintainer:Letters of support from other core and module maintainersGeneral letters of support from stakeholders within the PyTorch communityAny new relevant information that is befitting for the candidacy - The lead maintainer evaluates all information and makes a final decision to Confirm or Decline the nomination, with a clear public articulation of their reasoning behind the decision.", "prev_chunk_id": "chunk_151", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_153", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer#", "content": "Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer# - A super-majority of core maintainers (75%) can choose to remove the Lead Core Maintainer - After a removal of the Lead Core Maintainer or in unforeseen circumstances (such as permanent unavailability of the Lead Core Maintainer), the core maintainers follow a Ranked-Choice voting method to elect a new Lead Core Maintainer.", "prev_chunk_id": "chunk_152", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_154", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Add, Remove, and Re-Scope Modules and Projects#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Add, Remove, and Re-Scope Modules and Projects#", "content": "Add, Remove, and Re-Scope Modules and Projects# The core maintainers together are responsible for taking decisions on adding, removing and re-scoping new modules in the PyTorch org, either as new repositories in the PyTorch GitHub org, or as folders in the pytorch/pytorch repository. They invite proposals from members in the community (including themselves) for such changes. The proposals are open-ended, but should have some basic ground-work to make a convincing case to make change. The following is an example approach to this process: - Interview researchers / stakeholders, talk to community, gather issues; - Read papers, attend conferences, build example pipelines based on experience; - Create a state of the world - make sure this change is necessary, for example adding a new project or module is worth the maintenance cost; or removing a project or module will not remove too much value from PyTorch; - Create a proposal; the proposal covers the maintainership, development and community plan once the proposal is approved. The core maintainers take final decisions on the proposal, articulating the reasoning behind the decision publicly.", "prev_chunk_id": "chunk_153", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_155", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Uncontroversial Changes#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Uncontroversial Changes#", "content": "Uncontroversial Changes# Primary work happens through issues and pull requests on GitHub. Maintainers should avoid pushing their changes directly to the PyTorch repository, instead relying on pull requests. Approving a pull request by a core or module maintainer allows it to be merged without further process. Core and module maintainers, as listed on the Maintainers page and within CODEOWNERS ultimately approve these changes. Notifying relevant experts about an issue or a pull request is important. Reviews from experts in the given interest area are strongly preferred, especially on pull request approvals. Failure to do so might end up with the change being reverted by the relevant expert.", "prev_chunk_id": "chunk_154", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_156", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "Controversial Decision Process#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Controversial Decision Process#", "content": "Controversial Decision Process# Substantial changes in a given interest area require a GitHub issue to be opened for discussion. This includes: - Any semantic or syntactic change to the PyTorch framework or library. - Backwards-incompatible changes to the Python or C++ API. - Additions to the core framework or library, including substantial new functionality within an existing library. - Removal of core features or platform support Core and module maintainers ultimately approve these changes.", "prev_chunk_id": "chunk_155", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_157", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "General Project Policies#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "General Project Policies#", "content": "General Project Policies# PyTorch has been established as PyTorch a Series of LF Projects, LLC. Policies applicable to PyTorch and participants in PyTorch, including guidelines on the usage of trademarks, are located at https://www.lfprojects.org/policies/. PyTorch participants acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the project. Except as described below, all code contributions to the project must be made using the 3-Clause-BSD License available here: https://opensource.org/licenses/BSD-3-Clause (the “Project License”). All outbound code will be made available under the Project License. The Maintainers may approve the use of an alternative open license or licenses for inbound or outbound contributions on an exception basis.", "prev_chunk_id": "chunk_156", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_158", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "FAQ#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "FAQ#", "content": "FAQ# Q: What if I would like to own (or partly own) a part of the project such as a feature area or domain library, for example Linear Algebra or Torch Vision ? This is absolutely possible. The first step is to start contributing to the existing project area and supporting its health and success. In addition to this, you can make a proposal through a GitHub issue for new functionality or changes to improve the project area. Q: What if I am a company looking to use PyTorch internally for development, can I be granted or purchase a board seat to drive the project direction? No, the PyTorch project is strictly driven by the a maintainer project philosophy and clearly separates technical governance from business governance. However, if you want to be involved in sponsorship and support, you can become involved in the PyTorch Foundation (PTF) and sponsorship through this. You can also have individual engineers look to become maintainers, but this is not guaranteed and is merit-based. Q: Does the PyTorch project support grants or ways to support independent developers using or contributing to the project? No, not at this point. We are however looking at ways to better support the community of independent developers around PyTorch. If you have suggestions or inputs, please reach out on the PyTorch forums to discuss. Q: How do I contribute code to the project? If the change is relatively minor, a pull request on GitHub can be opened up immediately for review and merge by the project committers. For larger changes, please open an issue to make a proposal to discuss prior. Please also see the PyTorch Contributor Wiki for contribution for a walkthrough. Q: Can I become a committer on the project? Unfortunately, the current commit process to PyTorch involves", "prev_chunk_id": "chunk_157", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_159", "url": "https://docs.pytorch.org/docs/stable/community/governance.html", "title": "FAQ#", "page_title": "PyTorch Governance | Mechanics — PyTorch 2.8 documentation", "breadcrumbs": "FAQ#", "content": "an interaction with Facebook infrastructure that can only be triggered by Facebook employees. We are however looking at ways to expand the committer base to individuals outside of Facebook and will provide an update when the tooling exists to allow this. Q: What if I would like to deliver a PyTorch tutorial at a conference or otherwise? Do I need to be ‘officially’ a committer to do this? No, we encourage community members to showcase their work wherever and whenever they can. Please reach out to marketing@pytorch.org for marketing support.", "prev_chunk_id": "chunk_158", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_160", "url": "https://docs.pytorch.org/docs/stable/community/design.html", "title": "PyTorch Design Philosophy#", "page_title": "PyTorch Design Philosophy — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Design Philosophy#", "content": "PyTorch Design Philosophy# Created On: Jun 10, 2022 | Last Updated On: Apr 16, 2025 This document is designed to help contributors and module maintainers understand the high-level design principles that have developed over time in PyTorch. These are not meant to be hard-and-fast rules, but to serve as a guide to help trade off different concerns and to resolve disagreements that may come up while developing PyTorch. For more information on contributing, module maintainership, and how to escalate a disagreement to the Core Maintainers, please see PyTorch Governance.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_161", "url": "https://docs.pytorch.org/docs/stable/community/design.html", "title": "Principle 1: Usability over Performance#", "page_title": "PyTorch Design Philosophy — PyTorch 2.8 documentation", "breadcrumbs": "Principle 1: Usability over Performance#", "content": "Principle 1: Usability over Performance# This principle may be surprising! As one Hacker News poster wrote: PyTorch is amazing! […] Although I’m confused. How can a ML framework be not obsessed with speed/performance? See Hacker News discussion on PyTorch. Soumith’s blog post on Growing the PyTorch Community goes into this in some depth, but at a high-level: - PyTorch’s primary goal is usability - A secondary goal is to havereasonableperformance We believe the ability to maintain our flexibility to support researchers who are building on top of our abstractions remains critical. We can’t see what the future of what workloads will be, but we know we want them to be built first on PyTorch and that requires flexibility. In more concrete terms, we operate in a usability-first manner and try to avoid jumping to restriction-first regimes (for example, static shapes, graph-mode only) without a clear-eyed view of the tradeoffs. Often there is a temptation to impose strict user restrictions upfront because it can simplify implementation, but this comes with risks: - The performance may not be worth the user friction, either because the performance benefit is not compelling enough or it only applies to a relatively narrow set of subproblems. - Even if the performance benefit is compelling, the restrictions can fragment the ecosystem into different sets of limitations that can quickly become incomprehensible to users. We want users to be able to seamlessly move their PyTorch code to different hardware and software platforms, to interoperate with different libraries and frameworks, and to experience the full richness of the PyTorch user experience, not a least common denominator subset.", "prev_chunk_id": "chunk_160", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_162", "url": "https://docs.pytorch.org/docs/stable/community/design.html", "title": "Principle 2: Simple Over Easy#", "page_title": "PyTorch Design Philosophy — PyTorch 2.8 documentation", "breadcrumbs": "Principle 2: Simple Over Easy#", "content": "Principle 2: Simple Over Easy# Here, we borrow from The Zen of Python: - Explicit is better than implicit - Simple is better than complex A more concise way of describing these two goals is Simple Over Easy. Let’s start with an example because simple and easy are often used interchangeably in everyday English. Consider how one may model devices in PyTorch: - Simple / Explicit (to understand, debug):every tensor is associated with a device. The user explicitly specifies tensor device movement. Operations that require cross-device movement result in an error. - Easy / Implicit (to use):the user does not have to worry about devices; the system figures out the globally optimal device placement. In this specific case, and as a general design philosophy, PyTorch favors exposing simple and explicit building blocks rather than APIs that are easy-to-use by practitioners. The simple version is immediately understandable and debuggable by a new PyTorch user: you get a clear error if you call an operator requiring cross-device movement at the point in the program where the operator is actually invoked. The easy solution may let a new user move faster initially, but debugging such a system can be complex: How did the system make its determination? What is the API for plugging into such a system and how are objects represented in its IR? Some classic arguments in favor of this sort of design come from A Note on Distributed Computation (TLDR: Do not model resources with very different performance characteristics uniformly, the details will leak) and the End-to-End Principle (TLDR: building smarts into the lower-layers of the stack can prevent building performant features at higher layers in the stack, and often doesn’t work anyway). For example, we could build operator-level or global device movement rules, but the precise choices aren’t obvious", "prev_chunk_id": "chunk_161", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_163", "url": "https://docs.pytorch.org/docs/stable/community/design.html", "title": "Principle 2: Simple Over Easy#", "page_title": "PyTorch Design Philosophy — PyTorch 2.8 documentation", "breadcrumbs": "Principle 2: Simple Over Easy#", "content": "and building an extensible mechanism has unavoidable complexity and latency costs. A caveat here is that this does not mean that higher-level “easy” APIs are not valuable; certainly there is a value in, for example, higher-levels in the stack to support efficient tensor computations across heterogeneous compute in a large cluster. Instead, what we mean is that focusing on simple lower-level building blocks helps inform the easy API while still maintaining a good experience when users need to leave the beaten path. It also allows space for innovation and the growth of more opinionated tools at a rate we cannot support in the PyTorch core library, but ultimately benefit from, as evidenced by our rich ecosystem. In other words, not automating at the start allows us to potentially reach levels of good automation faster.", "prev_chunk_id": "chunk_162", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_164", "url": "https://docs.pytorch.org/docs/stable/community/design.html", "title": "Principle 3: Python First with Best In Class Language Interoperability#", "page_title": "PyTorch Design Philosophy — PyTorch 2.8 documentation", "breadcrumbs": "Principle 3: Python First with Best In Class Language Interoperability#", "content": "Principle 3: Python First with Best In Class Language Interoperability# This principle began as Python First: One thing PyTorch has needed to deal with over the years is Python overhead: we first rewrote the autograd engine in C++, then the majority of operator definitions, then developed TorchScript and the C++ frontend. Still, working in Python provides easily the best experience for our users: it is flexible, familiar, and perhaps most importantly, has a huge ecosystem of scientific computing libraries and extensions available for use. This fact motivates a few of our most recent contributions, which attempt to hit a Pareto optimal point close to the Python usability end of the curve: - TorchDynamo, a Python frame evaluation tool capable of speeding up existing eager-mode PyTorch programs with minimal user intervention. - torch_functionandtorch_dispatchextension points, which have enabled Python-first functionality to be built on-top of C++ internals, such as thetorch.fx tracerandfunctorchrespectively. These design principles are not hard-and-fast rules, but hard won choices and anchor how we built PyTorch to be the debuggable, hackable and flexible framework it is today. As we have more contributors and maintainers, we look forward to applying these core principles with you across our libraries and ecosystem. We are also open to evolving them as we learn new things and the AI space evolves, as we know it will.", "prev_chunk_id": "chunk_163", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_165", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "Multiprocessing package - torch.multiprocessing#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing package - torch.multiprocessing#", "content": "Multiprocessing package - torch.multiprocessing# Created On: Dec 23, 2016 | Last Updated On: Jun 08, 2025 torch.multiprocessing is a wrapper around the native multiprocessing module. It registers custom reducers, that use shared memory to provide shared views on the same data in different processes. Once the tensor/storage is moved to shared_memory (see share_memory_()), it will be possible to send it to other processes without making any copies. The API is 100% compatible with the original module - it’s enough to change import multiprocessing to import torch.multiprocessing to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory. Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_166", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "Sharing CUDA tensors#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "Sharing CUDA tensors#", "content": "Sharing CUDA tensors# Sharing CUDA tensors between processes is supported only in Python 3, using a spawn or forkserver start methods. Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices. - Release memory ASAP in the consumer. ## Good x = queue.get() # do somethings with x del x ## Bad x = queue.get() # do somethings with x # do everything else (producer have to keep x in memory) - Keep producer process running until all consumers exits. This will prevent the situation when the producer process releasing memory which is still in use by the consumer. ## producer # send tensors, do something event.wait() ## consumer # receive tensors and use them event.set() - Don’t pass received tensors. # not going to work x = queue.get() queue_2.put(x) # you need to create a process-local copy x = queue.get() x_clone = x.clone() queue_2.put(x_clone) # putting and getting from the same queue in the same process will likely end up with segfault queue.put(tensor) x = queue.get()", "prev_chunk_id": "chunk_165", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_167", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "Sharing strategies#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "Sharing strategies#", "content": "Sharing strategies# This section provides a brief overview into how different sharing strategies work. Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that’s the only way they can be shared.", "prev_chunk_id": "chunk_166", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_168", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "File descriptor - file_descriptor#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "File descriptor - file_descriptor#", "content": "File descriptor - file_descriptor# This strategy will use file descriptors as shared memory handles. Whenever a storage is moved to shared memory, a file descriptor obtained from shm_open is cached with the object, and when it’s going to be sent to other processes, the file descriptor will be transferred (e.g. via UNIX sockets) to it. The receiver will also cache the file descriptor and mmap it, to obtain a shared view onto the storage data. Note that if there will be a lot of tensors shared, this strategy will keep a large number of file descriptors open most of the time. If your system has low limits for the number of open file descriptors, and you can’t raise them, you should use the file_system strategy.", "prev_chunk_id": "chunk_167", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_169", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "File system - file_system#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "File system - file_system#", "content": "File system - file_system# This strategy will use file names given to shm_open to identify the shared memory regions. This has a benefit of not requiring the implementation to cache the file descriptors obtained from it, but at the same time is prone to shared memory leaks. The file can’t be deleted right after its creation, because other processes need to access it to open their views. If the processes fatally crash, or are killed, and don’t call the storage destructors, the files will remain in the system. This is very serious, because they keep using up the memory until the system is restarted, or they’re freed manually. To counter the problem of shared memory file leaks, torch.multiprocessing will spawn a daemon named torch_shm_manager that will isolate itself from the current process group, and will keep track of all shared memory allocations. Once all processes connected to it exit, it will wait a moment to ensure there will be no new connections, and will iterate over all shared memory files allocated by the group. If it finds that any of them still exist, they will be deallocated. We’ve tested this method and it proved to be robust to various failures. Still, if your system has high enough limits, and file_descriptor is a supported strategy, we do not recommend switching to this one.", "prev_chunk_id": "chunk_168", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_170", "url": "https://docs.pytorch.org/docs/stable/multiprocessing.html", "title": "Spawning subprocesses#", "page_title": "Multiprocessing package - torch.multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "Spawning subprocesses#", "content": "Spawning subprocesses# Spawning a number of subprocesses to perform some function can be done by creating Process instances and calling join to wait for their completion. This approach works fine when dealing with a single subprocess but presents potential issues when dealing with multiple processes. Namely, joining processes sequentially implies they will terminate sequentially. If they don’t, and the first process does not terminate, the process termination will go unnoticed. Also, there are no native facilities for error propagation. The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.", "prev_chunk_id": "chunk_169", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_171", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "PyTorch Contribution Guide#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Contribution Guide#", "content": "PyTorch Contribution Guide# Created On: Mar 11, 2019 | Last Updated On: Apr 27, 2025 PyTorch is a GPU-accelerated Python tensor computation package for building deep neural networks using a tape-based autograd systems.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_172", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Contribution Process#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Contribution Process#", "content": "Contribution Process# The PyTorch organization is governed by PyTorch Governance and the technical guide to contributing can be found in CONTRIBUTING.md. The PyTorch development process involves a healthy amount of open discussions between the core development team and the community. PyTorch operates similarly to most open source projects on GitHub. However, if you’ve never contributed to an open source project before, here is the basic process. - Figure out what you’re going to work on.The majority of open source contributions come from people scratching their own itches. However, if you don’t know what you want to work on, or are just looking to get more acquainted with the project, here are some tips for how to find appropriate tasks:Look through theissue trackerand see if there are any issues you know how to fix. Issues that are confirmed by other contributors tend to be better to investigate. We also maintain some labels for issues that are likely to be good for new people, e.g.,bootcampand1hr, although these labels are less well maintained.Join us ondev discussand let us know you’re interested in getting to know PyTorch. We’re very happy to help out researchers and partners get up to speed with the codebase. - Figure out the scope of your change and reach out for design comments on a GitHub issue if it’s large.The majority of pull requests are small; in that case, no need to let us know about what you want to do, just get cracking. But if the change is going to be large, it’s usually a good idea to get some design comments about it first bysubmitting an RFC.If you don’t know how big a change is going to be, we can help you figure it out! Just post about it onissuesordev discuss.Some feature additions are very standardized; for example,", "prev_chunk_id": "chunk_171", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_173", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Contribution Process#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Contribution Process#", "content": "lots of people add new operators or optimizers to PyTorch. Design discussion in these cases boils down mostly to, “Do we want this operator/optimizer?” Giving evidence for its utility, e.g., usage in peer reviewed papers, or existence in other frameworks, helps a bit when making this case.Adding operators / algorithms from recently-released researchis generally not accepted unless there is overwhelming evidence that this newly published work has ground-breaking results and will eventually become a standard in the field. If you are not sure where your method falls, open an issue first before implementing a PR.Core changes and refactors can be quite difficult to coordinate since the pace of development on the PyTorch main branch is quite fast. Definitely reach out about fundamental or cross-cutting changes; we can often give guidance about how to stage such changes into more easily reviewable pieces. - Code it out!See theCONTRIBUTING.mdfile for advice for working with PyTorch in a technical form. - Open a pull request.If you are not ready for the pull request to be reviewed, create a draft pull request first - you can later convert it to a full PR by pressing “Ready for review” button. You can also prepend the title of the PR with “[WIP]” (“work in progress”) while it’s still in draft. We will ignore draft PRs when doing review passes. If you are working on a complex change, it’s good to start things off as a draft, because you will need to spend time looking at CI results to see if things worked out or not.Find an appropriate reviewer for your change. We have some folks who regularly go through the PR queue and try to review everything, but if you happen to know who the maintainer for a given subsystem affected by your patch is, feel free", "prev_chunk_id": "chunk_172", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_174", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Contribution Process#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Contribution Process#", "content": "to include them directly on the pull request. You can learn more aboutPersons of Interestthat could review your code. - Iterate on the pull request until it’s accepted!We’ll try our best to minimize the number of review round trips and block PRs only when there are major issues. For the most common issues in pull requests, take a look atCommon Mistakes.Once a pull request is accepted and CI is passing, there is nothing else you need to do; we will merge the PR for you.", "prev_chunk_id": "chunk_173", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_175", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Proposing New Features#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Proposing New Features#", "content": "Proposing New Features# New feature ideas are best discussed on a specific issue. Please include as much information as you can, any accompanying data, and your proposed solution. The PyTorch team and community frequently review new issues and comments where they think they can help. If you feel confident in your solution, go ahead and implement it.", "prev_chunk_id": "chunk_174", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_176", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Reporting Issues#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Reporting Issues#", "content": "Reporting Issues# If you’ve identified an issue, first search through the list of existing issues on the repo. If you are unable to find a similar issue, then create a new one. Supply as much information you can to reproduce the problematic behavior. Also, include any additional insights like the behavior you expect.", "prev_chunk_id": "chunk_175", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_177", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Implementing Features or Fixing Bugs#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Implementing Features or Fixing Bugs#", "content": "Implementing Features or Fixing Bugs# If you want to fix a specific issue, it’s best to comment on the individual issue with your intent. However, we do not lock or assign issues except in cases where we have worked with the developer before. It’s best to strike up a conversation on the issue and discuss your proposed solution. The PyTorch team can provide guidance that saves you time. Issues that are labeled first-new-issue, low, or medium priority provide the best entrance points and are great places to start.", "prev_chunk_id": "chunk_176", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_178", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Adding Tutorials#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Adding Tutorials#", "content": "Adding Tutorials# A great deal of the tutorials on pytorch.org come from the community itself and we welcome additional contributions. To learn more about how to contribute a new tutorial you can learn more here: PyTorch.org Tutorial Contribution Guide on GitHub", "prev_chunk_id": "chunk_177", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_179", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Improving Documentation & Tutorials#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Improving Documentation & Tutorials#", "content": "Improving Documentation & Tutorials# We aim to produce high quality documentation and tutorials. On rare occasions that content includes typos or bugs. If you find something you can fix, send us a pull request for consideration. Take a look at the Documentation section to learn how our system works.", "prev_chunk_id": "chunk_178", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_180", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Participating in Online Discussions#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Participating in Online Discussions#", "content": "Participating in Online Discussions# You can find active discussions happening on the PyTorch Discussion Forums for users as well as the PyTorch Dev Discussion Forums for developers and maintainers.", "prev_chunk_id": "chunk_179", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_181", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Submitting Pull Requests to Fix Open Issues#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Submitting Pull Requests to Fix Open Issues#", "content": "Submitting Pull Requests to Fix Open Issues# You can view a list of all open issues here. Commenting on an issue is a great way to get the attention of the team. From here you can share your ideas and how you plan to resolve the issue. For more challenging issues, the team will provide feedback and direction for how to best solve the issue. If you’re not able to fix the issue yourself, commenting and sharing whether you can reproduce the issue can help the team identify problem areas.", "prev_chunk_id": "chunk_180", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_182", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Reviewing Open Pull Requests#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Reviewing Open Pull Requests#", "content": "Reviewing Open Pull Requests# We appreciate your help reviewing and commenting on pull requests. Our team strives to keep the number of open pull requests at a manageable size, we respond quickly for more information if we need it, and we merge PRs that we think are useful. However, due to the high level of interest, additional eyes on the pull requests are always appreciated.", "prev_chunk_id": "chunk_181", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_183", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Improving Code Readability#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Improving Code Readability#", "content": "Improving Code Readability# Improving code readability helps everyone. It is often better to submit a small number of pull requests that touch a few files versus a large pull request that touches many files. Starting a discussion in the PyTorch forum here or on an issue related to your improvement is the best way to get started.", "prev_chunk_id": "chunk_182", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_184", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Adding Test Cases to Make the Codebase More Robust#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Adding Test Cases to Make the Codebase More Robust#", "content": "Adding Test Cases to Make the Codebase More Robust# Additional test coverage is appreciated.", "prev_chunk_id": "chunk_183", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_185", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Promoting PyTorch#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Promoting PyTorch#", "content": "Promoting PyTorch# Your use of PyTorch in your projects, research papers, write ups, blogs, or general discussions around the internet helps to raise awareness for PyTorch and our growing community. Please reach out to marketing@pytorch.org for marketing support.", "prev_chunk_id": "chunk_184", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_186", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Triaging Issues#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Triaging Issues#", "content": "Triaging Issues# If you feel that an issue could benefit from a particular tag or level of complexity, comment on the issue and share your opinion. If you feel an issue isn’t categorized properly, comment and let the team know.", "prev_chunk_id": "chunk_185", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_187", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "About Open Source Development#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "About Open Source Development#", "content": "About Open Source Development# If this is your first time contributing to an open source project, some aspects of the development process may seem unusual to you. - There is no way to “claim” issues.People often want to “claim” an issue when they decide to work on it, to ensure that there isn’t wasted work when someone else ends up working on it. This doesn’t really work too well in open source, since someone may decide to work on something, and end up not having time to do it. Feel free to give information in an advisory fashion, but at the end of the day, we will take running code and rough consensus to move forward quickly. - There is a high bar for new functionality.Unlike in a corporate environment, where the person who wrote code implicitly “owns” it and can be expected to take care of it for the code’s lifetime, once a pull request is merged into an open source project, it immediately becomes the collective responsibility of all maintainers on the project. When we merge code, we are saying that we, the maintainers, can review subsequent changes and make a bugfix to the code. This naturally leads to a higher standard of contribution.", "prev_chunk_id": "chunk_186", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_188", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Common Mistakes To Avoid#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Common Mistakes To Avoid#", "content": "Common Mistakes To Avoid# - Did you add tests?(Or if the change is hard to test, did you describe how you tested your change?)We have a few motivations for why we ask for tests:to help us tell if we break it laterto help us tell if the patch is correct in the first place (yes, we did review it, but as Knuth says, “beware of the following code, for I have not run it, merely proven it correct”)When is it OK not to add a test? Sometimes a change can’t be conveniently tested, or the change is so obviously correct (and unlikely to be broken) that it’s OK not to test it. On the contrary, if a change seems likely (or is known to be likely) to be accidentally broken, it’s important to put in the time to work out a testing strategy. - Is your PR too long?It’s easier for us to review and merge small PRs. The difficulty of reviewing a PR scales nonlinearly with its size.When is it OK to submit a large PR? It helps a lot if there was a corresponding design discussion in an issue, with sign off from the people who are going to review your diff. We can also help give advice about how to split up a large change into individually shippable parts. Similarly, it helps if there is a complete description of the contents of the PR: it’s easier to review code if we know what’s inside! - Comments for subtle things?In cases where the behavior of your code is nuanced, please include extra comments and documentation to allow us to better understand the intention of your code. - Did you add a hack?Sometimes, the right answer is a hack. But usually, we will have to discuss it. - Do", "prev_chunk_id": "chunk_187", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_189", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Common Mistakes To Avoid#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Common Mistakes To Avoid#", "content": "you want to touch a very core component?To prevent major regressions, pull requests that touch core components receive extra scrutiny. Make sure you’ve discussed your changes with the team before undertaking major changes. - Want to add a new feature?If you want to add new features, comment your intention on the related issue. Our team tries to comment on and provide feedback to the community. It’s better to have an open discussion with the team and the rest of the community before building new features. This helps us stay aware of what you’re working on and increases the chance that it’ll be merged. - Did you touch code unrelated to the PR?To aid in code review, please only include files in your pull request that are directly related to your changes.", "prev_chunk_id": "chunk_188", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_190", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Frequently Asked Questions#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# - How can I contribute as a reviewer?There is lots of value if community developers reproduce issues, try out new functionality, or otherwise help us identify or troubleshoot issues. Commenting on tasks or pull requests with your environment details is helpful and appreciated. - CI tests failed, what does it mean?Maybe your PR is based off a broken main branch? You can try to rebase your change on top of the latest main branch. You can also see the current status of main branch’s CI athttps://hud.pytorch.org/. - What are the most high risk changes?Anything that touches build configuration is a risky area. Please avoid changing these unless you’ve had a discussion with the team beforehand. - Hey, a commit showed up on my branch, what’s up with that?Sometimes another community member will provide a patch or fix to your pull request or branch. This is often needed for getting CI tests to pass.", "prev_chunk_id": "chunk_189", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_191", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Python Docs#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Python Docs#", "content": "Python Docs# PyTorch documentation is generated from python source using Sphinx. Generated HTML is copied to the docs folder in the main branch of pytorch.org/docs, and is served via GitHub pages. - Site:https://pytorch.org/docs - GitHub:pytorch/pytorch - Served from:https://pytorch.org/docs/main", "prev_chunk_id": "chunk_190", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_192", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "C++ Docs#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "C++ Docs#", "content": "C++ Docs# For C++ code we use Doxygen to generate the content files. The C++ docs are built on a special server and the resulting files are copied to the pytorch/cppdocs repo, and are served from GitHub pages. - Site:https://pytorch.org/cppdocs - GitHub:pytorch/pytorch - Served from:pytorch/cppdocs", "prev_chunk_id": "chunk_191", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_193", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Tutorials#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Tutorials#", "content": "Tutorials# PyTorch tutorials are documents used to help understand using PyTorch to accomplish specific tasks or to understand more holistic concepts. Tutorials are built using Sphinx-Gallery from executable python source files, or from restructured-text (rst) files. - Site:https://pytorch.org/tutorials - GitHub:pytorch/tutorials", "prev_chunk_id": "chunk_192", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_194", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Tutorials Build Overview#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Tutorials Build Overview#", "content": "Tutorials Build Overview# For tutorials, pull requests trigger a rebuild of the entire site using CircleCI to test the effects of the change. This build is sharded into 9 worker builds and takes around 40 minutes total. At the same time, we do a Netlify build using make html-noplot, which builds the site without rendering the notebook output into pages for quick review. After a PR is accepted, the site is rebuilt and deployed using GitHub Actions.", "prev_chunk_id": "chunk_193", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_195", "url": "https://docs.pytorch.org/docs/stable/community/contribution_guide.html", "title": "Contributing a New Tutorial#", "page_title": "PyTorch Contribution Guide — PyTorch 2.8 documentation", "breadcrumbs": "Contributing a New Tutorial#", "content": "Contributing a New Tutorial# See PyTorch.org Tutorial Contribution Guide.", "prev_chunk_id": "chunk_194", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_196", "url": "https://docs.pytorch.org/docs/stable/community/build_ci_governance.html", "title": "PyTorch Governance | Build + CI#", "page_title": "PyTorch Governance | Build + CI — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Governance | Build + CI#", "content": "PyTorch Governance | Build + CI# Created On: Aug 31, 2022 | Last Updated On: Apr 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_197", "url": "https://docs.pytorch.org/docs/stable/community/build_ci_governance.html", "title": "How to Add a New Maintainer#", "page_title": "PyTorch Governance | Build + CI — PyTorch 2.8 documentation", "breadcrumbs": "How to Add a New Maintainer#", "content": "How to Add a New Maintainer# For the person to be a maintainer, a person needs to: - Land at least six commits to the related part of the PyTorch repository - At least one of these commits must be submitted in the last six months To add a qualified person to the maintainers’ list, please create a PR that adds a person to the persons of interests page and merge_rules files. Current maintainers will cast their votes of support. Decision criteria for approving the PR: - Not earlier than two business days passed before merging (ensure the majority of the contributors have seen it) - PR has the correct label (module: ci) - There are no objections from the current maintainers - There are at least three netthumbs upfrom current maintainers (or all maintainers votethumbs upwhen the module has less than 3 maintainers).", "prev_chunk_id": "chunk_196", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_198", "url": "https://docs.pytorch.org/docs/stable/torch_nccl_environment_variables.html", "title": "PYTORCH ProcessGroupNCCL Environment Variables#", "page_title": "PYTORCH ProcessGroupNCCL Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "PYTORCH ProcessGroupNCCL Environment Variables#", "content": "PYTORCH ProcessGroupNCCL Environment Variables# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025 For more information on the environment variables, see ProcessGroupNCCL Environment Variables.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_199", "url": "https://docs.pytorch.org/docs/stable/miscellaneous_environment_variables.html", "title": "Miscellaneous Environment Variables#", "page_title": "Miscellaneous Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "Miscellaneous Environment Variables#", "content": "Miscellaneous Environment Variables# Created On: Jun 17, 2025 | Last Updated On: Jun 17, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_200", "url": "https://docs.pytorch.org/docs/stable/debugging_environment_variables.html", "title": "Debugging Environment Variables#", "page_title": "Debugging Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Environment Variables#", "content": "Debugging Environment Variables# Created On: Feb 15, 2024 | Last Updated On: Jun 06, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_201", "url": "https://docs.pytorch.org/docs/stable/mps_environment_variables.html", "title": "MPS Environment Variables#", "page_title": "MPS Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "MPS Environment Variables#", "content": "MPS Environment Variables# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025 PyTorch Environment Variables", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_202", "url": "https://docs.pytorch.org/docs/stable/cuda_environment_variables.html", "title": "CUDA Environment Variables#", "page_title": "CUDA Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Environment Variables#", "content": "CUDA Environment Variables# Created On: Feb 15, 2024 | Last Updated On: Feb 15, 2024 For more information on CUDA runtime environment variables, see CUDA Environment Variables. PyTorch Environment Variables CUDA Runtime and Libraries Environment Variables", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_203", "url": "https://docs.pytorch.org/docs/stable/threading_environment_variables.html", "title": "Threading Environment Variables#", "page_title": "Threading Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "Threading Environment Variables#", "content": "Threading Environment Variables# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_204", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Distributed Autograd Design#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Autograd Design#", "content": "Distributed Autograd Design# Created On: Nov 12, 2019 | Last Updated On: Sep 03, 2021 This note will present the detailed design for distributed autograd and walk through the internals of the same. Make sure you’re familiar with Autograd mechanics and the Distributed RPC Framework before proceeding.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_205", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Background#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Background#", "content": "Background# Let’s say you have two nodes and a very simple model partitioned across two nodes. This can be implemented using torch.distributed.rpc as follows: import torch import torch.distributed.rpc as rpc def my_add(t1, t2): return torch.add(t1, t2) # On worker 0: t1 = torch.rand((3, 3), requires_grad=True) t2 = torch.rand((3, 3), requires_grad=True) # Perform some computation remotely. t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2)) # Perform some computation locally based on remote result. t4 = torch.rand((3, 3), requires_grad=True) t5 = torch.mul(t3, t4) # Compute some loss. loss = t5.sum() The main motivation behind distributed autograd is to enable running a backward pass on such distributed models with the loss that we’ve computed and record appropriate gradients for all tensors that require gradients.", "prev_chunk_id": "chunk_204", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_206", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Autograd recording during the forward pass#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Autograd recording during the forward pass#", "content": "Autograd recording during the forward pass# PyTorch builds the autograd graph during the forward pass and this graph is used to execute the backward pass. For more details see How autograd encodes the history. For distributed autograd, we need to keep track of all RPCs during the forward pass to ensure the backward pass is executed appropriately. For this purpose, we attach send and recv functions to the autograd graph when we perform an RPC. - Thesendfunction is attached to the source of the RPC and its output edges point to the autograd function for the input tensors of the RPC. The input for this function during the backward pass is received from the destination as the output of the appropriaterecvfunction. - Therecvfunction is attached to the destination of the RPC and its inputs are retrieved from operators executed on the destination using the input tensors. The output gradients of this function are sent to the source node to the appropriatesendfunction during the backward pass. - Eachsend-recvpair is assigned a globally uniqueautograd_message_idto uniquely identify the pair. This is useful to look up the corresponding function on a remote node during the backward pass. - ForRRef, whenever we calltorch.distributed.rpc.RRef.to_here()we attach an appropriatesend-recvpair for the tensors involved. As an example, this is what the autograd graph for our example above would look like (t5.sum() excluded for simplicity):", "prev_chunk_id": "chunk_205", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_207", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Distributed Autograd Context#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Autograd Context#", "content": "Distributed Autograd Context# Each forward and backward pass that uses distributed autograd is assigned a unique torch.distributed.autograd.context and this context has a globally unique autograd_context_id. This context is created on each node as needed. This context serves the following purpose: - Multiple nodes running distributed backward passes might accumulate gradients on the same tensor and as a result the.gradfield of the tensor would have gradients from a variety of distributed backward passes before we have the opportunity to run the optimizer. This is similar to callingtorch.autograd.backward()multiple times locally. In order to provide a way of separating out the gradients for each backward pass, the gradients are accumulated in thetorch.distributed.autograd.contextfor each backward pass. - During the forward pass we store thesendandrecvfunctions for each autograd pass in this context. This ensures we hold references to the appropriate nodes in the autograd graph to keep it alive. In addition to this, it is easy to look up the appropriatesendandrecvfunctions during the backward pass. - In general we also use this context to store some metadata for each distributed autograd pass. From the user’s perspective the autograd context is setup as follows: import torch.distributed.autograd as dist_autograd with dist_autograd.context() as context_id: loss = model.forward() dist_autograd.backward(context_id, loss) It is important to note that your model’s forward pass must be invoked within the distributed autograd context manager, as a valid context is needed in order to ensure that all send and recv functions are stored properly to run the backward pass across all participating nodes.", "prev_chunk_id": "chunk_206", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_208", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Distributed Backward Pass#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Backward Pass#", "content": "Distributed Backward Pass# In this section we outline the challenge of computing dependencies accurately during a distributed backward pass and describe a couple of algorithms (with tradeoffs) on how we can execute a distributed backward pass.", "prev_chunk_id": "chunk_207", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_209", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Computing dependencies#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Computing dependencies#", "content": "Computing dependencies# Consider the following piece of code being run on a single machine import torch a = torch.rand((3, 3), requires_grad=True) b = torch.rand((3, 3), requires_grad=True) c = torch.rand((3, 3), requires_grad=True) d = a + b e = b * c d.sum.().backward() This is what the autograd graph for the code above would look like: The first step the autograd engine performs as part of the backward pass is computing the number of dependencies for each node in the autograd graph. This helps the autograd engine know when a node in the graph is ready for execution. The numbers in brackets for add(1) and mul(0) denote the number of dependencies. As you can see, this means during the backward pass the add node needs 1 input and the mul node doesn’t need any inputs (in other words doesn’t need to be executed). The local autograd engine computes these dependencies by traversing the graph from the root nodes (d in this case). The fact that certain nodes in the autograd graph might not be executed in the backward pass poses a challenge for distributed autograd. Consider this piece of code which uses RPC. import torch import torch.distributed.rpc as rpc a = torch.rand((3, 3), requires_grad=True) b = torch.rand((3, 3), requires_grad=True) c = torch.rand((3, 3), requires_grad=True) d = rpc.rpc_sync(\"worker1\", torch.add, args=(a, b)) e = rpc.rpc_sync(\"worker1\", torch.mul, args=(b, c)) loss = d.sum() The associated autograd graph for the code above would be: Computing dependencies of this distributed autograd graph is much more challenging and requires some overhead (either in terms of computation or network communication). For performance sensitive applications we can avoid a lot of overhead by assuming every send and recv function are valid as part of the backward pass (most applications don’t perform RPCs that aren’t used). This simplifies the distributed autograd", "prev_chunk_id": "chunk_208", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_210", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Computing dependencies#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Computing dependencies#", "content": "algorithm and is much more efficient, but at the cost that the application needs to be aware of the limitations. This algorithm is called the FAST mode algorithm and is described in detail below. In the general case it might not be necessary that every send and recv function is valid as part of the backward pass. To address this, we have proposed a SMART mode algorithm which is described in a later section. Please note that currently, only the FAST mode algorithm is implemented.", "prev_chunk_id": "chunk_209", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_211", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "FAST mode algorithm#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "FAST mode algorithm#", "content": "FAST mode algorithm# The key assumption of this algorithm is that each send function has a dependency of 1 when we run a backward pass. In other words, we assume we’ll receive a gradient over RPC from another node. The algorithm is as follows: - We start from the worker which has the roots for the backward pass (all roots must be local). - Lookup all thesendfunctions for the currentDistributed Autograd Context. - Compute dependencies locally starting from the provided roots and all thesendfunctions we retrieved. - After computing dependencies, kick off the local autograd engine with the provided roots. - When the autograd engine executes therecvfunction, therecvfunction sends the input gradients via RPC to the appropriate worker. Eachrecvfunction knows the destination worker id since it is recorded as part of the forward pass. Therecvfunction also sends over theautograd_context_idandautograd_message_idto the remote host. - When this request is received on the remote host, we use theautograd_context_idandautograd_message_idto look up the appropriatesendfunction. - If this is the first time a worker has received a request for the givenautograd_context_id, it will compute dependencies locally as described in points 1-3 above. - Thesendfunction retrieved in 6. is then enqueued for execution on the local autograd engine for that worker. - Finally, instead of accumulating the gradients on the.gradfield of the Tensor, we accumulate the gradients separately perDistributed Autograd Context. The gradients are stored in aDict[Tensor,Tensor], which is basically a map from Tensor to its associated gradient and this map can be retrieved using theget_gradients()API. As an example the complete code with distributed autograd would be as follows: import torch import torch.distributed.autograd as dist_autograd import torch.distributed.rpc as rpc def my_add(t1, t2): return torch.add(t1, t2) # On worker 0: # Setup the autograd context. Computations that take # part in the distributed backward pass must be within", "prev_chunk_id": "chunk_210", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_212", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "FAST mode algorithm#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "FAST mode algorithm#", "content": "# the distributed autograd context manager. with dist_autograd.context() as context_id: t1 = torch.rand((3, 3), requires_grad=True) t2 = torch.rand((3, 3), requires_grad=True) # Perform some computation remotely. t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2)) # Perform some computation locally based on remote result. t4 = torch.rand((3, 3), requires_grad=True) t5 = torch.mul(t3, t4) # Compute some loss. loss = t5.sum() # Run the backward pass. dist_autograd.backward(context_id, [loss]) # Retrieve the gradients from the context. dist_autograd.get_gradients(context_id) The distributed autograd graph with dependencies would be as follows (t5.sum() excluded for simplicity): The FAST mode algorithm applied to the above example would be as follows: - OnWorker0we start from the rootslossandsend1to compute dependencies. As a resultsend1is marked with a dependency of 1 andmulonWorker0is marked with a dependency of 1. - Now, we kickoff the local autograd engine onWorker0. We first execute themulfunction, accumulate its output in the autograd context as the gradient fort4. Then, we executerecv2which sends the gradients toWorker1. - Since this is the first timeWorker1has heard about this backward pass, it starts dependency computation and marks the dependencies forsend2,addandrecv1appropriately. - Next, we enqueuesend2on the local autograd engine ofWorker1, which in turn executesaddandrecv1. - Whenrecv1is executed it sends the gradients over toWorker0. - SinceWorker0has already computed dependencies for this backward pass, it just enqueues and executessend1locally. - Finally, gradients fort1,t2andt4are accumulated in theDistributed Autograd Context.", "prev_chunk_id": "chunk_211", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_213", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "SMART mode algorithm#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "SMART mode algorithm#", "content": "SMART mode algorithm# Full details of this algorithm are still in the works, but for the general idea you can refer to Distributed Autograd Algorithm Smart mode section in the RFC.", "prev_chunk_id": "chunk_212", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_214", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Distributed Optimizer#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Optimizer#", "content": "Distributed Optimizer# The DistributedOptimizer operates as follows: - Takes a list of remote parameters (RRef) to optimize. These could also be local parameters wrapped within a localRRef. - Takes aOptimizerclass as the local optimizer to run on all distinctRRefowners. - The distributed optimizer creates an instance of the localOptimizeron each of the worker nodes and holds anRRefto them. - Whentorch.distributed.optim.DistributedOptimizer.step()is invoked, the distributed optimizer uses RPC to remotely execute all the local optimizers on the appropriate remote workers. A distributed autogradcontext_idmust be provided as input totorch.distributed.optim.DistributedOptimizer.step(). This is used by local optimizers to apply gradients stored in the corresponding context. - If multiple concurrent distributed optimizers are updating the same parameters on a worker, these updates are serialized via a lock.", "prev_chunk_id": "chunk_213", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_215", "url": "https://docs.pytorch.org/docs/stable/rpc/distributed_autograd.html", "title": "Simple end to end example#", "page_title": "Distributed Autograd Design — PyTorch 2.8 documentation", "breadcrumbs": "Simple end to end example#", "content": "Simple end to end example# Putting it all together, the following is a simple end to end example using distributed autograd and the distributed optimizer. If the code is placed into a file called “dist_autograd_simple.py”, it can be run with the command MASTER_ADDR=\"localhost\" MASTER_PORT=29500 python dist_autograd_simple.py: import torch import torch.multiprocessing as mp import torch.distributed.autograd as dist_autograd from torch.distributed import rpc from torch import optim from torch.distributed.optim import DistributedOptimizer def random_tensor(): return torch.rand((3, 3), requires_grad=True) def _run_process(rank, dst_rank, world_size): name = \"worker{}\".format(rank) dst_name = \"worker{}\".format(dst_rank) # Initialize RPC. rpc.init_rpc( name=name, rank=rank, world_size=world_size ) # Use a distributed autograd context. with dist_autograd.context() as context_id: # Forward pass (create references on remote nodes). rref1 = rpc.remote(dst_name, random_tensor) rref2 = rpc.remote(dst_name, random_tensor) loss = rref1.to_here() + rref2.to_here() # Backward pass (run distributed autograd). dist_autograd.backward(context_id, [loss.sum()]) # Build DistributedOptimizer. dist_optim = DistributedOptimizer( optim.SGD, [rref1, rref2], lr=0.05, ) # Run the distributed optimizer step. dist_optim.step(context_id) def run_process(rank, world_size): dst_rank = (rank + 1) % world_size _run_process(rank, dst_rank, world_size) rpc.shutdown() if __name__ == '__main__': # Run world_size workers world_size = 2 mp.spawn(run_process, args=(world_size,), nprocs=world_size)", "prev_chunk_id": "chunk_214", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_216", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Remote Reference Protocol#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Remote Reference Protocol#", "content": "Remote Reference Protocol# Created On: Nov 20, 2019 | Last Updated On: Apr 27, 2025 This note describes the design details of Remote Reference protocol and walks through message flows in different scenarios. Make sure you’re familiar with the Distributed RPC Framework before proceeding.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_217", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Background#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Background#", "content": "Background# RRef stands for Remote REFerence. It is a reference of an object which is located on the local or remote worker, and transparently handles reference counting under the hood. Conceptually, it can be considered as a distributed shared pointer. Applications can create an RRef by calling remote(). Each RRef is owned by the callee worker of the remote() call (i.e., owner) and can be used by multiple users. The owner stores the real data and keeps track of the global reference count. Every RRef can be uniquely identified by a global RRefId, which is assigned at the time of creation on the caller of the remote() call. On the owner worker, there is only one OwnerRRef instance, which contains the real data, while on user workers, there can be as many UserRRefs as necessary, and UserRRef does not hold the data. All usage on the owner will retrieve the unique OwnerRRef instance using the globally unique RRefId. A UserRRef will be created when it is used as an argument or return value in rpc_sync(), rpc_async() or remote() invocation, and the owner will be notified according to update the reference count. An OwnerRRef and its data will be deleted when there is no UserRRef instances globally and there are no reference to the OwnerRRef on the owner as well.", "prev_chunk_id": "chunk_216", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_218", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Assumptions#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Assumptions#", "content": "Assumptions# RRef protocol is designed with the following assumptions. - Transient Network Failures: The RRef design handles transient network failures by retrying messages. It cannot handle node crashes or permanent network partitions. When those incidents occur, the application should take down all workers, revert to the previous checkpoint, and resume training. - Non-idempotent UDFs: We assume the user functions (UDF) provided torpc_sync(),rpc_async()orremote()are not idempotent and therefore cannot be retried. However, internal RRef control messages are idempotent and retried upon message failure. - Out of Order Message Delivery: We do not assume message delivery order between any pair of nodes, because both sender and receiver are using multiple threads. There is no guarantee on which message will be processed first.", "prev_chunk_id": "chunk_217", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_219", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "RRef Lifetime#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "RRef Lifetime#", "content": "RRef Lifetime# The goal of the protocol is to delete an OwnerRRef at an appropriate time. The right time to delete an OwnerRRef is when there are no living UserRRef instances and user code is not holding references to the OwnerRRef either. The tricky part is to determine if there are any living UserRRef instances.", "prev_chunk_id": "chunk_218", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_220", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Design Reasoning#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Design Reasoning#", "content": "Design Reasoning# A user can get a UserRRef in three situations: - Receiving aUserRReffrom the owner. - Receiving aUserRReffrom another user. - Creating a newUserRRefowned by another worker. Case 1 is the simplest where the owner passes its RRef to a user, where the owner calls rpc_sync(), rpc_async(), or remote() and uses its RRef as an argument. In this case a new UserRRef will be created on the user. As the owner is the caller, it can easily update its local reference count on the OwnerRRef. The only requirement is that any UserRRef must notify the owner upon destruction. Hence, we need the first guarantee: G1. The owner will be notified when any UserRRef is deleted. As messages might come delayed or out-of-order, we need one more guarantee to make sure the delete message is not processed too soon. If A sends a message to B that involves an RRef, we call the RRef on A (the parent RRef) and the RRef on B (the child RRef). G2. Parent RRef will NOT be deleted until the child RRef is confirmed by the owner. In cases 2 and 3, it is possible that the owner has only partial or no knowledge at all about the RRef fork graph. For example, an RRef could be constructed on a user, and before the owner receives any RPC call, the creator user might have already shared the RRef with other users, and those users could further share the RRef. One invariant is that the fork graph of any RRef is always a tree, because forking an RRef always creates a new UserRRef instance on the callee (except if the callee is the owner), and hence every RRef has a single parent. The owner’s view on any UserRRef in the tree has three stages: 1)", "prev_chunk_id": "chunk_219", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_221", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Design Reasoning#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Design Reasoning#", "content": "unknown -> 2) known -> 3) deleted. The owner’s view of the entire tree keeps changing. The owner deletes its OwnerRRef instance when it thinks there are no living UserRRef instances, i.e., when OwnerRRef is deleted, all UserRRef instances could be either indeed deleted or unknown. The dangerous case is when some forks are unknown and others are deleted. G2 trivially guarantees that no parent UserRRef can be deleted before the owner knows all of its children UserRRef instances. However, it is possible that the child UserRRef may be deleted before the owner knows its parent UserRRef. Consider the following example, where the OwnerRRef forks to A, then A forks to Y, and Y forks to Z: OwnerRRef -> A -> Y -> Z If all of Z’s messages, including the delete message, are processed by the owner before Y’s messages. the owner will learn of Z’s deletion before knowing Y exists. Nevertheless, this does not cause any problem. Because, at least one of Y’s ancestors will be alive (A) and it will prevent the owner from deleting the OwnerRRef. More specifically, if the owner does not know Y, A cannot be deleted due to G2, and the owner knows A since it is A’s parent. Things get a little trickier if the RRef is created on a user: OwnerRRef ^ | A -> Y -> Z If Z calls to_here() on the UserRRef, the owner at least knows A when Z is deleted, because otherwise, to_here() wouldn’t finish. If Z does not call to_here(), it is possible that the owner receives all messages from Z before any message from A and Y. In this case, as the real data of the OwnerRRef has not been created yet, there is nothing to be deleted either. It is the same as", "prev_chunk_id": "chunk_220", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_222", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Design Reasoning#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Design Reasoning#", "content": "Z does not exist at all. Hence, it’s still OK.", "prev_chunk_id": "chunk_221", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_223", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Implementation#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Implementation#", "content": "Implementation# G1 is implemented by sending out a delete message in UserRRef destructor. To provide G2, the parent UserRRef is put into a context whenever it is forked, indexed by the new ForkId. The parent UserRRef is only removed from the context when it receives an acknowledgement message (ACK) from the child, and the child will only send out the ACK when it is confirmed by the owner.", "prev_chunk_id": "chunk_222", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_224", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Protocol Scenarios#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Protocol Scenarios#", "content": "Protocol Scenarios# Let’s now discuss how the above designs translate to the protocol in four scenarios.", "prev_chunk_id": "chunk_223", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_225", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "User Share RRef with Owner as Return Value#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "User Share RRef with Owner as Return Value#", "content": "User Share RRef with Owner as Return Value# import torch import torch.distributed.rpc as rpc # on worker A rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1)) # say the rref has RRefId 100 and ForkId 1 rref.to_here() In this case, the UserRRef is created on the user worker A, then it is passed to the owner worker B together with the remote message, and then B creates the OwnerRRef. The method remote() returns immediately, meaning that the UserRRef can be forked/used before the owner knows about it. On the owner, when receiving the remote() call, it will create the OwnerRRef, and returns an ACK to acknowledge {100, 1} (RRefId, ForkId). Only after receiving this ACK, can A delete its UserRRef. This involves both G1 and G2. G1 is obvious. For G2, the OwnerRRef is a child of the UserRRef, and the UserRRef is not deleted until it receives the ACK from the owner. The diagram above shows the message flow, where solid arrow contains user function and dashed arrow are builtin messages. Note that the first two messages from A to B (remote() and to_here()) may arrive at B in any order, but the final delete message will only be sent out when: - B acknowledgesUserRRef{100,1}(G2), and - Python GC agrees to delete the localUserRRefinstance. This occurs when the RRef is no longer in scope and is eligible for garbage collection.", "prev_chunk_id": "chunk_224", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_226", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "User Share RRef with Owner as Argument#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "User Share RRef with Owner as Argument#", "content": "User Share RRef with Owner as Argument# import torch import torch.distributed.rpc as rpc # on worker A and worker B def func(rref): pass # on worker A rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1)) # say the rref has RRefId 100 and ForkId 1 rpc.rpc_async('B', func, args=(rref, )) In this case, after creating the UserRRef on A, A uses it as an argument in a followup RPC call to B. A will keep UserRRef {100, 1} alive until it receives the acknowledge from B (G2, not the return value of the RPC call). This is necessary because A should not send out the delete message until all previous messages are received, otherwise, the OwnerRRef could be deleted before usage as we do not guarantee message delivery order. This is done by creating a child ForkId of RRef, holding them in a map until receives the owner confirms the child ForkId. The figure below shows the message flow. Note that the UserRRef could be deleted on B before func finishes or even starts. However this is OK, as at the time B sends out ACK for the child ForkId, it already acquired the OwnerRRef instance, which would prevent it been deleted too soon.", "prev_chunk_id": "chunk_225", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_227", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "Owner Share RRef with User#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "Owner Share RRef with User#", "content": "Owner Share RRef with User# Owner to user is the simplest case, where the owner can update reference counting locally, and does not need any additional control message to notify others. Regarding G2, it is same as the parent receives the ACK from the owner immediately, as the parent is the owner. import torch import torch.distributed.rpc as RRef, rpc # on worker B and worker C def func(rref): pass # on worker B, creating a local RRef rref = RRef(\"data\") # say the rref has RRefId 100 dist.rpc_async('C', func, args=(rref, )) The figure above shows the message flow. Note that when the OwnerRRef exits scope after the rpc_async call, it will not be deleted, because internally there is a map to hold it alive if there is any known forks, in which case is UserRRef {100, 1}. (G2)", "prev_chunk_id": "chunk_226", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_228", "url": "https://docs.pytorch.org/docs/stable/rpc/rref.html", "title": "User Share RRef with User#", "page_title": "Remote Reference Protocol — PyTorch 2.8 documentation", "breadcrumbs": "User Share RRef with User#", "content": "User Share RRef with User# This is the most complicated case where caller user (parent UserRRef), callee user (child UserRRef), and the owner all need to get involved. import torch import torch.distributed.rpc as rpc # on worker A and worker C def func(rref): pass # on worker A rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1)) # say the rref has RRefId 100 and ForkId 1 rpc.rpc_async('C', func, args=(rref, )) When C receives the child UserRRef from A, it sends out a fork request to the owner B. Later, when the B confirms the UserRRef on C, C will perform two actions in parallel: 1) send out the child ACK to A ,and 2) run the user provided function. During this time, the parent (A) will hold its UserRRef {100, 1} alive to achieve G2.", "prev_chunk_id": "chunk_227", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_229", "url": "https://docs.pytorch.org/docs/stable/torch.ao.ns._numeric_suite_fx.html", "title": "torch.ao.ns._numeric_suite_fx#", "page_title": "torch.ao.ns._numeric_suite_fx — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.ns._numeric_suite_fx#", "content": "torch.ao.ns._numeric_suite_fx# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 This module contains tooling to compare weights and activations across models. Example usage: import copy import torch import torch.ao.quantization.quantize_fx as quantize_fx import torch.ao.ns._numeric_suite_fx as ns m = torch.nn.Sequential(torch.nn.Conv2d(1, 1, 1)).eval() mp = quantize_fx.prepare_fx(m, {\"\": torch.ao.quantization.default_qconfig}) # We convert a copy because we need the original prepared model # to be available for comparisons, and `quantize_fx.convert_fx` is inplace. mq = quantize_fx.convert_fx(copy.deepcopy(mp)) # # Comparing weights # # extract weight pairs weight_comparison = ns.extract_weights(\"a\", mp, \"b\", mq) # add SQNR for each comparison, inplace ns.extend_logger_results_with_comparison( weight_comparison, \"a\", \"b\", torch.ao.ns.fx.utils.compute_sqnr, \"sqnr\" ) # weight_comparison contains the weights from `mp` and `mq` stored # in pairs, and can be used for further analysis. # # Comparing activations, with error propagation # # add loggers mp_ns, mq_ns = ns.add_loggers( \"a\", copy.deepcopy(mp), \"b\", copy.deepcopy(mq), ns.OutputLogger ) # send an example datum to capture intermediate activations datum = torch.randn(1, 1, 1, 1) mp_ns(datum) mq_ns(datum) # extract intermediate activations act_comparison = ns.extract_logger_info(mp_ns, mq_ns, ns.OutputLogger, \"b\") # add SQNR for each comparison, inplace ns.extend_logger_results_with_comparison( act_comparison, \"a\", \"b\", torch.ao.ns.fx.utils.compute_sqnr, \"sqnr\" ) # act_comparison contains the activations from `mp_ns` and `mq_ns` stored # in pairs, and can be used for further analysis. # # Comparing activations, without error propagation # # create shadow model mp_shadows_mq = ns.add_shadow_loggers( \"a\", copy.deepcopy(mp), \"b\", copy.deepcopy(mq), ns.OutputLogger ) # send an example datum to capture intermediate activations datum = torch.randn(1, 1, 1, 1) mp_shadows_mq(datum) # extract intermediate activations shadow_act_comparison = ns.extract_shadow_logger_info( mp_shadows_mq, ns.OutputLogger, \"b\" ) # add SQNR for each comparison, inplace ns.extend_logger_results_with_comparison( shadow_act_comparison, \"a\", \"b\", torch.ao.ns.fx.utils.compute_sqnr, \"sqnr\" ) # shadow_act_comparison contains the activations from `mp_ns` and `mq_ns` stored # in pairs, and can be used for further analysis.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_230", "url": "https://docs.pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html", "title": "torch.ao.ns._numeric_suite#", "page_title": "torch.ao.ns._numeric_suite — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.ns._numeric_suite#", "content": "torch.ao.ns._numeric_suite# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_231", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "Quantization Accuracy Debugging#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Accuracy Debugging#", "content": "Quantization Accuracy Debugging# Created On: May 16, 2022 | Last Updated On: Jun 18, 2025 This document provides high level strategies for improving quantization accuracy. If a quantized model has error compared to the original model, we can categorize the error into: - data insensitive error- caused by intrinsic model quantization error, large portion of input data has large error - data sensitive error- caused by outlier input data, small portion of input data has large error - implementation error- quantized kernel is not matching reference implementation", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_232", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "General tips#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "General tips#", "content": "General tips# - For PTQ, ensure that the data you are calibrating with is representative of your dataset. For example, for a classification problem a general guideline is to have multiple samples in every category, and the overall number of samples should be at least 100. There is no penalty for calibrating with more data other than calibration time. - If your model has Conv-BN or Linear-BN patterns, consider fusing them. If you are using FX graph mode quantization, this is done automatically by the workflow. If you are using Eager mode quantization, you can do this manually with thetorch.ao.quantization.fuse_modulesAPI. - Increase the precision of dtype of the problematic ops. Usually, fp32 will have the highest accuracy, followed by fp16, followed by dynamically quantized int8, followed by statically quantized int8.Note: this is trading off performance for accuracy.Note: availability of kernels per dtype per op can vary by backend.Note: dtype conversions add an additional performance cost. For example,fp32_op->quant->int8_op->dequant->fp32_op->quant->int8_op->dequantwill have a performance penalty compared tofp32_op->fp32_op->quant->int8_op->int8_op->dequantbecause of a higher number of required dtype conversions. - If you are using PTQ, consider using QAT to recover some of the accuracy loss from quantization.", "prev_chunk_id": "chunk_231", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_233", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "Int8 quantization tips#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "Int8 quantization tips#", "content": "Int8 quantization tips# - If you are using per-tensor weight quantization, consider using per-channel weight quantization. - If you are doing inference onfbgemm, ensure that you set thereduce_rangeargument toFalseif your CPU is Cooperlake or newer, and toTrueotherwise. - Audit the input activation distribution variation across different samples. If this variation is high, the layer may be suitable for dynamic quantization but not static quantization.", "prev_chunk_id": "chunk_232", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_234", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "Data sensitive error#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "Data sensitive error#", "content": "Data sensitive error# If you are using static quantization and a small portion of your input data is resulting in high quantization error, you can try: - Adjust your calibration dataset to make it more representative of your inference dataset. - Manually inspect (using Numeric Suite) which layers have high quantization error. For these layers, consider leaving them in floating point or adjusting the observer settings to choose a better scale and zero_point.", "prev_chunk_id": "chunk_233", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_235", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "Implementation error#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "Implementation error#", "content": "Implementation error# If you are using PyTorch quantization with your own backend you may see differences between the reference implementation of an operation (such as dequant -> op_fp32 -> quant) and the quantized implementation (such as op_int8) of the op on the target hardware. This could mean one of two things: - the differences (usually small) are expected due to specific behavior of the target kernel on the target hardware compared to fp32/cpu. An example of this is accumulating in an integer dtype. Unless the kernel guarantees bitwise equivalency with the reference implementation, this is expected. - the kernel on the target hardware has an accuracy issue. In this case, reach out to the kernel developer.", "prev_chunk_id": "chunk_234", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_236", "url": "https://docs.pytorch.org/docs/stable/quantization-accuracy-debugging.html", "title": "Numerical Debugging Tooling (prototype)#", "page_title": "Quantization Accuracy Debugging — PyTorch 2.8 documentation", "breadcrumbs": "Numerical Debugging Tooling (prototype)#", "content": "Numerical Debugging Tooling (prototype)# - torch.ao.ns._numeric_suiteEager mode numeric suite - torch.ao.ns._numeric_suite_fxFX numeric suite", "prev_chunk_id": "chunk_235", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_237", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Quantization Backend Configuration#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Backend Configuration#", "content": "Quantization Backend Configuration# Created On: Apr 04, 2022 | Last Updated On: Jun 18, 2025 FX Graph Mode Quantization allows the user to configure various quantization behaviors of an op in order to match the expectation of their backend. In the future, this document will contain a detailed spec of these configurations.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_238", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "Default values for native configurations# Below is the output of the configuration for quantization of ops in x86 and qnnpack (PyTorch’s default quantized backends). Results: { 'pattern': <class 'torch.nn.modules.pooling.AdaptiveAvgPool1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method adaptive_avg_pool1d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function adaptive_avg_pool2d at 0x7f6a3c113160>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.AdaptiveAvgPool3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function adaptive_avg_pool3d at 0x7f6a3c1131f0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function add>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method add of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype':", "prev_chunk_id": "chunk_237", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_239", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method avg_pool1d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.AvgPool2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function avg_pool2d>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.AvgPool3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function avg_pool3d>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv1d'>, <class 'torch.nn.modules.batchnorm.BatchNorm1d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn1d'>, 'fuser_method': <function fuse_conv_bn at 0x7f6a3b7445e0>, }, { 'pattern': (<class 'torch.nn.modules.conv.ConvTranspose1d'>, <class 'torch.nn.modules.batchnorm.BatchNorm1d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose1d'>, 'fuser_method': <function fuse_convtranspose_bn at 0x7f6a3b744790>, }, { 'pattern': (<class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.batchnorm.BatchNorm1d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None,", "prev_chunk_id": "chunk_238", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_240", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.LinearBn1d'>, 'fuser_method': <function fuse_linear_bn at 0x7f6a3b744700>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn2d'>, 'fuser_method': <function fuse_conv_bn at 0x7f6a3b7445e0>, }, { 'pattern': (<class 'torch.nn.modules.conv.ConvTranspose2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose2d'>, 'fuser_method': <function fuse_convtranspose_bn at 0x7f6a3b744790>, }, { 'pattern': <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv3d'>, <class 'torch.nn.modules.batchnorm.BatchNorm3d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn3d'>, 'fuser_method': <function fuse_conv_bn at 0x7f6a3b7445e0>, }, { 'pattern': (<class 'torch.nn.modules.conv.ConvTranspose3d'>, <class 'torch.nn.modules.batchnorm.BatchNorm3d'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose3d'>, 'fuser_method': <function fuse_convtranspose_bn at 0x7f6a3b744790>, }, { 'pattern': <class 'torch.nn.modules.batchnorm.BatchNorm3d'>, 'dtype_configs': [ {", "prev_chunk_id": "chunk_239", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_241", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method cat of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method clamp of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': clamp, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': contiguous, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.conv.Conv1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'qat_module': <class 'torch.ao.nn.qat.modules.conv.Conv1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv1d'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.conv.Conv1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv1d'>, }, { 'pattern': <built-in method conv1d of type object at 0x7f6a641d2ce0>,", "prev_chunk_id": "chunk_240", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_242", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.nn.modules.conv.Conv2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'qat_module': <class 'torch.ao.nn.qat.modules.conv.Conv2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv2d'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.conv.Conv2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv2d'>, }, { 'pattern': <built-in method conv2d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.nn.modules.conv.Conv3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'qat_module': <class 'torch.ao.nn.qat.modules.conv.Conv3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv3d'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.conv.Conv3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv3d'>, }, { 'pattern': <built-in method conv3d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype':", "prev_chunk_id": "chunk_241", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_243", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBn3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn3d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBn3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv3d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None,", "prev_chunk_id": "chunk_242", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_244", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv3d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None,", "prev_chunk_id": "chunk_243", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_245", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv1d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv2d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.Conv3d'>, }, { 'pattern': <class 'torch.nn.modules.conv.ConvTranspose1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose1d'>, }, { 'pattern': <built-in method conv_transpose1d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.nn.modules.conv.ConvTranspose2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None),", "prev_chunk_id": "chunk_244", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_246", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose2d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose2d'>, }, { 'pattern': <built-in method conv_transpose2d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.nn.modules.conv.ConvTranspose3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.ConvTranspose3d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.conv.ConvTranspose3d'>, }, { 'pattern': <built-in method conv_transpose3d of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': detach, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': detach_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.dropout.Dropout'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function dropout at 0x7f6a3c113280>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.ELU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ],", "prev_chunk_id": "chunk_245", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_247", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function elu at 0x7f6a3c1138b0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.sparse.Embedding'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint4x2, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.sparse.Embedding'>, 'qat_module': <class 'torch.ao.nn.qat.modules.embedding_ops.Embedding'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.sparse.Embedding'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.embedding_ops.Embedding'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint4x2, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.sparse.Embedding'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.sparse.Embedding'>, }, { 'pattern': <class 'torch.nn.modules.sparse.EmbeddingBag'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint4x2, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.sparse.EmbeddingBag'>, 'qat_module': <class 'torch.ao.nn.qat.modules.embedding_ops.EmbeddingBag'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.sparse.EmbeddingBag'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.embedding_ops.EmbeddingBag'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.quint4x2, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None),", "prev_chunk_id": "chunk_246", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_248", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "}, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.sparse.EmbeddingBag'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.sparse.EmbeddingBag'>, }, { 'pattern': <built-in method flatten of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function floordiv>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function group_norm at 0x7f6a3c116820>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 2, 'bias': 3}, }, { 'pattern': <class 'torch.nn.modules.rnn.GRU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.rnn.GRU'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.rnn.GRU'>, }, { 'pattern': <class 'torch.nn.modules.rnn.GRUCell'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.rnn.GRUCell'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.rnn.GRUCell'>, }, { 'pattern': <class 'torch.nn.modules.activation.Hardsigmoid'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function hardsigmoid at 0x7f6a3c1160d0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8,", "prev_chunk_id": "chunk_247", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_249", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': hardsigmoid, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': hardsigmoid_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.Hardswish'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function hardswish at 0x7f6a3c116280>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.Hardtanh'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function hardtanh at 0x7f6a3c113790>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function hardtanh_>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.linear.Identity'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function instance_norm at 0x7f6a3c116670>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 3, 'bias': 4}, }, { 'pattern': <class 'torch.nn.modules.instancenorm.InstanceNorm1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None,", "prev_chunk_id": "chunk_248", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_250", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function interpolate at 0x7f6a3c02a550>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.normalization.LayerNorm'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function layer_norm at 0x7f6a3c116700>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 2, 'bias': 3}, }, { 'pattern': <class 'torch.nn.modules.activation.LeakyReLU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <function leaky_relu at 0x7f6a3c113a60>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.linear.Linear'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, },", "prev_chunk_id": "chunk_249", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_251", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "{ 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'qat_module': <class 'torch.ao.nn.qat.modules.linear.Linear'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <class 'torch.ao.nn.qat.modules.linear.Linear'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <built-in function linear>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'input_type_to_index': {'weight': 1, 'bias': 2}, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.LinearBn1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None,", "prev_chunk_id": "chunk_250", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_252", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.linear_fused.LinearBn1d'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.linear_fused.LinearBn1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.modules.fused.LinearReLU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'qat_module': <class 'torch.ao.nn.intrinsic.qat.modules.linear_relu.LinearReLU'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <class 'torch.ao.nn.intrinsic.qat.modules.linear_relu.LinearReLU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None),", "prev_chunk_id": "chunk_251", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_253", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.linear.Linear'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.linear.Linear'>, }, { 'pattern': <class 'torch.nn.modules.rnn.LSTM'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.rnn.LSTM'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.rnn.LSTM'>, }, { 'pattern': <class 'torch.nn.modules.rnn.LSTMCell'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.rnn.LSTMCell'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.rnn.LSTMCell'>, }, { 'pattern': <built-in method matmul of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': torch.nn.functional.max_pool1d, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None,", "prev_chunk_id": "chunk_252", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_254", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.MaxPool2d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': torch.nn.functional.max_pool2d, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pooling.MaxPool3d'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': torch.nn.functional.max_pool3d, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method mean of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': mean, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in function mul>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method mul of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method narrow of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, },", "prev_chunk_id": "chunk_253", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_255", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "{ 'pattern': permute, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pixelshuffle.PixelShuffle'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method pixel_shuffle of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.pixelshuffle.PixelUnshuffle'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method pixel_unshuffle of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.PReLU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv1d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU1d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a6b5ca310>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv1d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU1d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a597dc0>, }, { 'pattern': (<built-in method conv1d of type object at 0x7f6a641d2ce0>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None),", "prev_chunk_id": "chunk_254", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_256", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method conv1d of type object at 0x7f6a641d2ce0>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv1d'>, <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU1d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv1d'>, <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv1d'>, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU1d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU2d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a597e50>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv2d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU2d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a597ee0>, }, { 'pattern': (<built-in method conv2d of type object at 0x7f6a641d2ce0>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None,", "prev_chunk_id": "chunk_255", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_257", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method conv2d of type object at 0x7f6a641d2ce0>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU2d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv2d'>, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU2d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv3d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU3d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a597f70>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv3d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvReLU3d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d040>, }, { 'pattern': (<built-in method conv3d of type object at 0x7f6a641d2ce0>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None,", "prev_chunk_id": "chunk_256", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_258", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method conv3d of type object at 0x7f6a641d2ce0>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv3d'>, <class 'torch.nn.modules.batchnorm.BatchNorm3d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU3d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.conv.Conv3d'>, <class 'torch.nn.modules.batchnorm.BatchNorm3d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.conv.Conv3d'>, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.ConvBnReLU3d'>, 'fuser_method': <function fuse_conv_bn_relu at 0x7f6a3b744670>, }, { 'pattern': (<class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.LinearReLU'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d0d0>, }, { 'pattern': (<class 'torch.nn.modules.linear.Linear'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None,", "prev_chunk_id": "chunk_257", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_259", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.LinearReLU'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d160>, }, { 'pattern': (<built-in function linear>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function linear>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function add>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs':", "prev_chunk_id": "chunk_258", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_260", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "[ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function add>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function add>, <built-in method relu of type object at 0x7f6a641d2ce0>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method add of type object at 0x7f6a641d2ce0>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method add of type object at 0x7f6a641d2ce0>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method add of type object at 0x7f6a641d2ce0>, <built-in method relu of type object at 0x7f6a641d2ce0>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function mul>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, },", "prev_chunk_id": "chunk_259", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_261", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function mul>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in function mul>, <built-in method relu of type object at 0x7f6a641d2ce0>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method mul of type object at 0x7f6a641d2ce0>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method mul of type object at 0x7f6a641d2ce0>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': (<built-in method mul of type object at 0x7f6a641d2ce0>, <built-in method relu of type object at 0x7f6a641d2ce0>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'num_tensor_args_to_observation_type': { 0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, 2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.ReLU'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function relu at 0x7f6a3c113670>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': relu, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8,", "prev_chunk_id": "chunk_260", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_262", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': relu_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': (<class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU2d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d1f0>, }, { 'pattern': (<class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU2d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d280>, }, { 'pattern': (<class 'torch.nn.modules.batchnorm.BatchNorm3d'>, <class 'torch.nn.modules.activation.ReLU'>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU3d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d310>, }, { 'pattern': (<class 'torch.nn.modules.batchnorm.BatchNorm3d'>, <function relu at 0x7f6a3c113670>), 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'fused_module': <class 'torch.ao.nn.intrinsic.modules.fused.BNReLU3d'>, 'fuser_method': <function _sequential_wrapper2.<locals>.fuser_method at 0x7f6a3a59d3a0>, }, { 'pattern': <class 'torch.nn.modules.activation.ReLU6'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <function relu6 at 0x7f6a3c113820>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': repeat, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method repeat_interleave of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ {", "prev_chunk_id": "chunk_261", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_263", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': repeat_interleave, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': reshape, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': resize_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.rnn.RNNCell'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.qint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, { 'input_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.float32, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'weight_dtype': DTypeWithConstraints(dtype=torch.float16, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'bias_dtype': torch.float32, 'is_dynamic': True, }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, 'root_module': <class 'torch.nn.modules.rnn.RNNCell'>, 'reference_quantized_module_for_root': <class 'torch.ao.nn.quantized.reference.modules.rnn.RNNCell'>, }, { 'pattern': shape, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.INPUT_OUTPUT_NOT_OBSERVED, }, { 'pattern': <class 'torch.nn.modules.activation.Sigmoid'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method sigmoid of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': sigmoid, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': sigmoid_, 'dtype_configs': [ {", "prev_chunk_id": "chunk_262", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_264", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': size, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.INPUT_OUTPUT_NOT_OBSERVED, }, { 'pattern': <class 'torch.nn.modules.activation.Softmax'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.00390625, zero_point_exact_match=0), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method squeeze of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': squeeze, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': squeeze_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method stack of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <class 'torch.nn.modules.activation.Tanh'>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method tanh of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': tanh, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': tanh_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125,", "prev_chunk_id": "chunk_263", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_265", "url": "https://docs.pytorch.org/docs/stable/quantization-backend-configuration.html", "title": "Default values for native configurations#", "page_title": "Quantization Backend Configuration — PyTorch 2.8 documentation", "breadcrumbs": "Default values for native configurations#", "content": "zero_point_exact_match=128), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=0.0078125, zero_point_exact_match=128), }, ], 'observation_type': ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT, }, { 'pattern': <built-in method transpose of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': transpose, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': <built-in method unsqueeze of type object at 0x7f6a641d2ce0>, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': unsqueeze, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': unsqueeze_, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }, { 'pattern': view, 'dtype_configs': [ { 'input_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), 'output_dtype': DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None), }, ], 'observation_type': ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT, }", "prev_chunk_id": "chunk_264", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_266", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv2d", "title": "torch.nn#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn#", "content": "torch.nn# Created On: Dec 23, 2016 | Last Updated On: Nov 06, 2024 These are the basic building blocks for graphs:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_267", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv2d", "title": "Containers#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Containers#", "content": "Containers# Global Hooks For Module", "prev_chunk_id": "chunk_266", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_268", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv2d", "title": "Utilities#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Utilities#", "content": "Utilities# From the torch.nn.utils module: Utility functions to clip parameter gradients. Utility functions to flatten and unflatten Module parameters to and from a single vector. Utility functions to fuse Modules with BatchNorm modules. Utility functions to convert Module parameter memory formats. Utility functions to apply and remove weight normalization from Module parameters. Utility functions for initializing Module parameters. Utility classes and functions for pruning Module parameters. Parametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization(). Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Utility functions to call a given Module in a stateless manner. Utility functions in other modules", "prev_chunk_id": "chunk_267", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_269", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv2d", "title": "Quantized Functions#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Functions#", "content": "Quantized Functions# Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.", "prev_chunk_id": "chunk_268", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_270", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv2d", "title": "Aliases#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Aliases#", "content": "Aliases# The following are aliases to their counterparts in torch.nn:", "prev_chunk_id": "chunk_269", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_271", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv3d", "title": "torch.nn#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn#", "content": "torch.nn# Created On: Dec 23, 2016 | Last Updated On: Nov 06, 2024 These are the basic building blocks for graphs:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_272", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv3d", "title": "Containers#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Containers#", "content": "Containers# Global Hooks For Module", "prev_chunk_id": "chunk_271", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_273", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv3d", "title": "Utilities#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Utilities#", "content": "Utilities# From the torch.nn.utils module: Utility functions to clip parameter gradients. Utility functions to flatten and unflatten Module parameters to and from a single vector. Utility functions to fuse Modules with BatchNorm modules. Utility functions to convert Module parameter memory formats. Utility functions to apply and remove weight normalization from Module parameters. Utility functions for initializing Module parameters. Utility classes and functions for pruning Module parameters. Parametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization(). Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Utility functions to call a given Module in a stateless manner. Utility functions in other modules", "prev_chunk_id": "chunk_272", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_274", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv3d", "title": "Quantized Functions#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Functions#", "content": "Quantized Functions# Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.", "prev_chunk_id": "chunk_273", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_275", "url": "https://docs.pytorch.org/docs/stable/nn.html?highlight=conv3d", "title": "Aliases#", "page_title": "torch.nn — PyTorch 2.8 documentation", "breadcrumbs": "Aliases#", "content": "Aliases# The following are aliases to their counterparts in torch.nn:", "prev_chunk_id": "chunk_274", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_276", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "Quantization API Reference#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "Quantization API Reference#", "content": "Quantization API Reference# Created On: Jul 25, 2020 | Last Updated On: Jun 18, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_277", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization#", "content": "torch.ao.quantization# This module contains Eager mode quantization APIs.", "prev_chunk_id": "chunk_276", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_278", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.quantize_fx#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.quantize_fx#", "content": "torch.ao.quantization.quantize_fx# This module contains FX graph mode quantization APIs (prototype).", "prev_chunk_id": "chunk_277", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_279", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.qconfig_mapping#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.qconfig_mapping#", "content": "torch.ao.quantization.qconfig_mapping# This module contains QConfigMapping for configuring FX graph mode quantization.", "prev_chunk_id": "chunk_278", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_280", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.backend_config#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.backend_config#", "content": "torch.ao.quantization.backend_config# This module contains BackendConfig, a config object that defines how quantization is supported in a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode Quantization to work with this as well.", "prev_chunk_id": "chunk_279", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_281", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.fx.custom_config#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.fx.custom_config#", "content": "torch.ao.quantization.fx.custom_config# This module contains a few CustomConfig classes that’s used in both eager mode and FX graph mode quantization", "prev_chunk_id": "chunk_280", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_282", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch (quantization related functions)#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch (quantization related functions)#", "content": "torch (quantization related functions)# This describes the quantization related functions of the torch namespace.", "prev_chunk_id": "chunk_281", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_283", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.Tensor (quantization related methods)#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.Tensor (quantization related methods)#", "content": "torch.Tensor (quantization related methods)# Quantized Tensors support a limited subset of data manipulation methods of the regular full-precision tensor.", "prev_chunk_id": "chunk_282", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_284", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.observer#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.observer#", "content": "torch.ao.quantization.observer# This module contains observers which are used to collect statistics about the values observed during calibration (PTQ) or training (QAT).", "prev_chunk_id": "chunk_283", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_285", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.fake_quantize#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.fake_quantize#", "content": "torch.ao.quantization.fake_quantize# This module implements modules which are used to perform fake quantization during QAT.", "prev_chunk_id": "chunk_284", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_286", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.quantization.qconfig#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.quantization.qconfig#", "content": "torch.ao.quantization.qconfig# This module defines QConfig objects which are used to configure quantization settings for individual ops.", "prev_chunk_id": "chunk_285", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_287", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.intrinsic#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.intrinsic#", "content": "torch.ao.nn.intrinsic# This module implements the combined (fused) modules conv + relu which can then be quantized.", "prev_chunk_id": "chunk_286", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_288", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.intrinsic.qat#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.intrinsic.qat#", "content": "torch.ao.nn.intrinsic.qat# This module implements the versions of those fused operations needed for quantization aware training.", "prev_chunk_id": "chunk_287", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_289", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.intrinsic.quantized#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.intrinsic.quantized#", "content": "torch.ao.nn.intrinsic.quantized# This module implements the quantized implementations of fused operations like conv + relu. No BatchNorm variants as it’s usually folded into convolution for inference.", "prev_chunk_id": "chunk_288", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_290", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.intrinsic.quantized.dynamic#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.intrinsic.quantized.dynamic#", "content": "torch.ao.nn.intrinsic.quantized.dynamic# This module implements the quantized dynamic implementations of fused operations like linear + relu.", "prev_chunk_id": "chunk_289", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_291", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.qat#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.qat#", "content": "torch.ao.nn.qat# This module implements versions of the key nn modules Conv2d() and Linear() which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.", "prev_chunk_id": "chunk_290", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_292", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.qat.dynamic#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.qat.dynamic#", "content": "torch.ao.nn.qat.dynamic# This module implements versions of the key nn modules such as Linear() which run in FP32 but with rounding applied to simulate the effect of INT8 quantization and will be dynamically quantized during inference.", "prev_chunk_id": "chunk_291", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_293", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.quantized#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.quantized#", "content": "torch.ao.nn.quantized# This module implements the quantized versions of the nn layers such as ~torch.nn.Conv2d and torch.nn.ReLU.", "prev_chunk_id": "chunk_292", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_294", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.quantized.functional#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.quantized.functional#", "content": "torch.ao.nn.quantized.functional# Functional interface (quantized). This module implements the quantized versions of the functional layers such as ~torch.nn.functional.conv2d and torch.nn.functional.relu. Note: torch.nn.functional.relu~torch.nn.functional.relu torch.nn.functional.relu supports quantized inputs.", "prev_chunk_id": "chunk_293", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_295", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.quantizable#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.quantizable#", "content": "torch.ao.nn.quantizable# This module implements the quantizable versions of some of the nn layers. These modules can be used in conjunction with the custom module mechanism, by providing the custom_module_config argument to both prepare and convert.", "prev_chunk_id": "chunk_294", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_296", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "torch.ao.nn.quantized.dynamic#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.ao.nn.quantized.dynamic#", "content": "torch.ao.nn.quantized.dynamic# Dynamically quantized Linear, LSTM, LSTMCell, GRUCell, and RNNCell.", "prev_chunk_id": "chunk_295", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_297", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "Quantized dtypes and quantization schemes#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "Quantized dtypes and quantization schemes#", "content": "Quantized dtypes and quantization schemes# Note that operator implementations currently only support per channel quantization for weights of the conv and linear operators. Furthermore, the input data is mapped linearly to the quantized data and vice versa as follows: where clamp(.)\\text{clamp}(.)clamp(.) is the same as clamp() while the scale sss and zero point zzz are then computed as described in MinMaxObserver, specifically: where :math:[x_\\text{min}, x_\\text{max}] denotes the range of the input data while :math:Q_\\text{min} and :math:Q_\\text{max} are respectively the minimum and maximum values of the quantized dtype. Note that the choice of :math:s and :math:z implies that zero is represented with no quantization error whenever zero is within the range of the input data or symmetric quantization is being used. Additional data types and quantization schemes can be implemented through the custom operator mechanism <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>_. - torch.qscheme— Type to describe the quantization scheme of a tensor. Supported types:torch.per_tensor_affine— per tensor, asymmetrictorch.per_channel_affine— per channel, asymmetrictorch.per_tensor_symmetric— per tensor, symmetrictorch.per_channel_symmetric— per channel, symmetric - torch.dtype— Type to describe the data. Supported types:torch.quint8— 8-bit unsigned integertorch.qint8— 8-bit signed integertorch.qint32— 32-bit signed integer QAT Modules. This package is in the process of being deprecated. Please, use torch.ao.nn.qat.modules instead. QAT Dynamic Modules. This package is in the process of being deprecated. Please, use torch.ao.nn.qat.dynamic instead. This file is in the process of migration to torch/ao/quantization, and is kept here for compatibility while the migration process is ongoing. If you are adding a new entry/functionality, please, add it to the appropriate files under torch/ao/quantization/fx/, while adding an import statement here. QAT Dynamic Modules. This package is in the process of being deprecated. Please, use torch.ao.nn.qat.dynamic instead. Quantized Modules. Quantized Dynamic Modules. This file is in the process of migration to torch/ao/nn/quantized/dynamic, and is kept here for compatibility while the migration process is ongoing. If you are adding", "prev_chunk_id": "chunk_296", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_298", "url": "https://docs.pytorch.org/docs/stable/quantization-support.html", "title": "Quantized dtypes and quantization schemes#", "page_title": "Quantization API Reference — PyTorch 2.8 documentation", "breadcrumbs": "Quantized dtypes and quantization schemes#", "content": "a new entry/functionality, please, add it to the appropriate file under the torch/ao/nn/quantized/dynamic, while adding an import statement here.", "prev_chunk_id": "chunk_297", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_299", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "TorchScript-based ONNX Exporter#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript-based ONNX Exporter#", "content": "TorchScript-based ONNX Exporter# Created On: Aug 31, 2017 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_300", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Example: AlexNet from PyTorch to ONNX#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Example: AlexNet from PyTorch to ONNX#", "content": "Example: AlexNet from PyTorch to ONNX# Here is a simple script which exports a pretrained AlexNet to an ONNX file named alexnet.onnx. The call to torch.onnx.export runs the model once to trace its execution and then exports the traced model to the specified file: import torch import torchvision dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\") model = torchvision.models.alexnet(pretrained=True).cuda() # Providing input and output names sets the display names for values # within the model's graph. Setting these does not change the semantics # of the graph; it is only for readability. # # The inputs to the network consist of the flat list of inputs (i.e. # the values you would pass to the forward() method) followed by the # flat list of parameters. You can partially specify names, i.e. provide # a list here shorter than the number of inputs to the model, and we will # only set that subset of names, starting from the beginning. input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ] output_names = [ \"output1\" ] torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names) The resulting alexnet.onnx file contains a binary protocol buffer which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The argument verbose=True causes the exporter to print out a human-readable representation of the model: # These are the inputs and parameters to the network, which have taken on # the names we specified earlier. graph(%actual_input_1 : Float(10, 3, 224, 224) %learned_0 : Float(64, 3, 11, 11) %learned_1 : Float(64) %learned_2 : Float(192, 64, 5, 5) %learned_3 : Float(192) # ---- omitted for brevity ---- %learned_14 : Float(1000, 4096) %learned_15 : Float(1000)) { # Every statement consists of some output tensors (and their types), # the operator to be run", "prev_chunk_id": "chunk_299", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_301", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Example: AlexNet from PyTorch to ONNX#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Example: AlexNet from PyTorch to ONNX#", "content": "(with its attributes, e.g., kernels, strides, # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1) %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0] %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1] %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2] # ---- omitted for brevity ---- %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12] # Dynamic means that the shape is not known. This may be because of a # limitation of our implementation (which we would like to fix in a # future release) or shapes which are truly dynamic. %30 : Dynamic = onnx::Shape(%29), scope: AlexNet %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet # ---- omitted for brevity ---- %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6] return (%output1); } You can also verify the output using the ONNX library, which you can install using pip: pip install onnx Then, you can run: import onnx # Load the ONNX model model = onnx.load(\"alexnet.onnx\") # Check that the model is well formed onnx.checker.check_model(model) # Print a human readable representation of the graph print(onnx.helper.printable_graph(model.graph)) You can also run the exported model with one of the many runtimes that support ONNX. For example after installing ONNX Runtime, you can load and run the model: import onnxruntime as ort import numpy as np ort_session = ort.InferenceSession(\"alexnet.onnx\") outputs = ort_session.run( None, {\"actual_input_1\": np.random.randn(10, 3, 224, 224).astype(np.float32)}, ) print(outputs[0]) Here is a more involved tutorial on exporting a model and running it with ONNX Runtime.", "prev_chunk_id": "chunk_300", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_302", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Tracing vs Scripting#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Tracing vs Scripting#", "content": "Tracing vs Scripting# Internally, torch.onnx.export() requires a torch.jit.ScriptModule rather than a torch.nn.Module. If the passed-in model is not already a ScriptModule, export() will use tracing to convert it to one: - Tracing: Iftorch.onnx.export()is called with a Module that is not already aScriptModule, it first does the equivalent oftorch.jit.trace(), which executes the model once with the givenargsand records all operations that happen during that execution. This means that if your model is dynamic, e.g., changes behavior depending on input data, the exported model willnotcapture this dynamic behavior. We recommend examining the exported model and making sure the operators look reasonable. Tracing will unroll loops and if statements, exporting a static graph that is exactly the same as the traced run. If you want to export your model with dynamic control flow, you will need to usescripting. - Scripting: Compiling a model via scripting preserves dynamic control flow and is valid for inputs of different sizes. To use scripting:Usetorch.jit.script()to produce aScriptModule.Calltorch.onnx.export()with theScriptModuleas the model. Theargsare still required, but they will be used internally only to produce example outputs, so that the types and shapes of the outputs can be captured. No tracing will be performed. See Introduction to TorchScript and TorchScript for more details, including how to compose tracing and scripting to suit the particular requirements of different models.", "prev_chunk_id": "chunk_301", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_303", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Avoid NumPy and built-in Python types#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Avoid NumPy and built-in Python types#", "content": "Avoid NumPy and built-in Python types# PyTorch models can be written using NumPy or Python types and functions, but during tracing, any variables of NumPy or Python types (rather than torch.Tensor) are converted to constants, which will produce the wrong result if those values should change depending on the inputs. For example, rather than using numpy functions on numpy.ndarrays: # Bad! Will be replaced with constants during tracing. x, y = np.random.rand(1, 2), np.random.rand(1, 2) np.concatenate((x, y), axis=1) Use torch operators on torch.Tensors: # Good! Tensor operations will be captured during tracing. x, y = torch.randn(1, 2), torch.randn(1, 2) torch.cat((x, y), dim=1) And rather than use torch.Tensor.item() (which converts a Tensor to a Python built-in number): # Bad! y.item() will be replaced with a constant during tracing. def forward(self, x, y): return x.reshape(y.item(), -1) Use torch’s support for implicit casting of single-element tensors: # Good! y will be preserved as a variable during tracing. def forward(self, x, y): return x.reshape(y, -1)", "prev_chunk_id": "chunk_302", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_304", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Avoid Tensor.data#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Avoid Tensor.data#", "content": "Avoid Tensor.data# Using the Tensor.data field can produce an incorrect trace and therefore an incorrect ONNX graph. Use torch.Tensor.detach() instead. (Work is ongoing to remove Tensor.data entirely).", "prev_chunk_id": "chunk_303", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_305", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Avoid in-place operations when using tensor.shape in tracing mode#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Avoid in-place operations when using tensor.shape in tracing mode#", "content": "Avoid in-place operations when using tensor.shape in tracing mode# In tracing mode, shapes obtained from tensor.shape are traced as tensors, and share the same memory. This might cause a mismatch the final output values. As a workaround, avoid the use of inplace operations in these scenarios. For example, in the model: class Model(torch.nn.Module): def forward(self, states): batch_size, seq_length = states.shape[:2] real_seq_length = seq_length real_seq_length += 2 return real_seq_length + seq_length real_seq_length and seq_length share the same memory in tracing mode. This could be avoided by rewriting the inplace operation: real_seq_length = real_seq_length + 2", "prev_chunk_id": "chunk_304", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_306", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Types#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Types#", "content": "Types# - Onlytorch.Tensors, numeric types that can be trivially converted to torch.Tensors (e.g. float, int), and tuples and lists of those types are supported as model inputs or outputs. Dict and str inputs and outputs are accepted intracingmode, but:Any computation that depends on the value of a dict or a str inputwill be replaced with the constant valueseen during the one traced execution.Any output that is a dict will be silently replaced with aflattened sequence of its values (keys will be removed). E.g.{\"foo\":1,\"bar\":2}becomes(1,2).Any output that is a str will be silently removed. - Certain operations involving tuples and lists are not supported inscriptingmode due to limited support in ONNX for nested sequences. In particular appending a tuple to a list is not supported. In tracing mode, the nested sequences will be flattened automatically during the tracing.", "prev_chunk_id": "chunk_305", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_307", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Differences in Operator Implementations#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Differences in Operator Implementations#", "content": "Differences in Operator Implementations# Due to differences in implementations of operators, running the exported model on different runtimes may produce different results from each other or from PyTorch. Normally these differences are numerically small, so this should only be a concern if your application is sensitive to these small differences.", "prev_chunk_id": "chunk_306", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_308", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Unsupported Tensor Indexing Patterns#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Unsupported Tensor Indexing Patterns#", "content": "Unsupported Tensor Indexing Patterns# Tensor indexing patterns that cannot be exported are listed below. If you are experiencing issues exporting a model that does not include any of the unsupported patterns below, please double check that you are exporting with the latest opset_version.", "prev_chunk_id": "chunk_307", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_309", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Reads / Gets#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Reads / Gets#", "content": "Reads / Gets# When indexing into a tensor for reading, the following patterns are not supported: # Tensor indices that includes negative values. data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])] # Workarounds: use positive index values.", "prev_chunk_id": "chunk_308", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_310", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Writes / Sets#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Writes / Sets#", "content": "Writes / Sets# When indexing into a Tensor for writing, the following patterns are not supported: # Multiple tensor indices if any has rank >= 2 data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data # Workarounds: use single tensor index with rank >= 2, # or multiple consecutive tensor indices with rank == 1. # Multiple tensor indices that are not consecutive data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data # Workarounds: transpose `data` such that tensor indices are consecutive. # Tensor indices that includes negative values. data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data # Workarounds: use positive index values. # Implicit broadcasting required for new_data. data[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data # Workarounds: expand new_data explicitly. # Example: # data shape: [3, 4, 5] # new_data shape: [5] # expected new_data shape after broadcasting: [2, 2, 2, 5]", "prev_chunk_id": "chunk_309", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_311", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Adding support for operators#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Adding support for operators#", "content": "Adding support for operators# When exporting a model that includes unsupported operators, you’ll see an error message like: RuntimeError: ONNX export failed: Couldn't export operator foo When that happens, there are a few things you can do: - Change the model to not use that operator. - Create a symbolic function to convert the operator and register it as a custom symbolic function. - Contribute to PyTorch to add the same symbolic function totorch.onnxitself. If you decided to implement a symbolic function (we hope you will contribute it back to PyTorch!), here is how you can get started:", "prev_chunk_id": "chunk_310", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_312", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "ONNX exporter internals#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "ONNX exporter internals#", "content": "ONNX exporter internals# A “symbolic function” is a function that decomposes a PyTorch operator into a composition of a series of ONNX operators. During export, each node (which contains a PyTorch operator) in the TorchScript graph is visited by the exporter in topological order. Upon visiting a node, the exporter looks for a registered symbolic functions for that operator. Symbolic functions are implemented in Python. A symbolic function for an op named foo would look something like: def foo( g, input_0: torch._C.Value, input_1: torch._C.Value) -> Union[None, torch._C.Value, List[torch._C.Value]]: \"\"\" Adds the ONNX operations representing this PyTorch function by updating the graph g with `g.op()` calls. Args: g (Graph): graph to write the ONNX representation into. input_0 (Value): value representing the variables which contain the first input for this operator. input_1 (Value): value representing the variables which contain the second input for this operator. Returns: A Value or List of Values specifying the ONNX nodes that compute something equivalent to the original PyTorch operator with the given inputs. None if it cannot be converted to ONNX. \"\"\" ... The torch._C types are Python wrappers around the types defined in C++ in ir.h. The process for adding a symbolic function depends on the type of operator.", "prev_chunk_id": "chunk_311", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_313", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "ATen operators#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "ATen operators#", "content": "ATen operators# ATen is PyTorch’s built-in tensor library. If the operator is an ATen operator (shows up in the TorchScript graph with the prefix aten::), make sure it is not supported already.", "prev_chunk_id": "chunk_312", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_314", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "List of supported operators#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "List of supported operators#", "content": "List of supported operators# Visit the auto generated list of supported TorchScript operators for details on which operator are supported in each opset_version.", "prev_chunk_id": "chunk_313", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_315", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Adding support for an aten or quantized operator#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Adding support for an aten or quantized operator#", "content": "Adding support for an aten or quantized operator# If the operator is not in the list above: - Define the symbolic function intorch/onnx/symbolic_opset<version>.py, for exampletorch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen function, which may be declared intorch/_C/_VariableFunctions.pyiortorch/nn/functional.pyi(these files are generated at build time, so will not appear in your checkout until you build PyTorch). - By default, the first arg is the ONNX graph. Other arg names must EXACTLY match the names in the.pyifile, because dispatch is done with keyword arguments. - In the symbolic function, if the operator is in theONNX standard operator set, we only need to create a node to represent the ONNX operator in the graph. If not, we can compose several standard operators that have the equivalent semantics to the ATen operator. Here is an example of handling missing symbolic function for the ELU operator. If we run the following code: print( torch.jit.trace( torch.nn.ELU(), # module torch.ones(1) # example input ).graph ) We see something like: graph(%self : __torch__.torch.nn.modules.activation.___torch_mangle_0.ELU, %input : Float(1, strides=[1], requires_grad=0, device=cpu)): %4 : float = prim::Constant[value=1.]() %5 : int = prim::Constant[value=1]() %6 : int = prim::Constant[value=1]() %7 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::elu(%input, %4, %5, %6) return (%7) Since we see aten::elu in the graph, we know this is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We find a signature for elu in torch/nn/functional.pyi: def elu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor: ... We add the following lines to symbolic_opset9.py: def elu(g, input: torch.Value, alpha: torch.Value, inplace: bool = False): return g.op(\"Elu\", input, alpha_f=alpha) Now PyTorch is able to export models containing the aten::elu operator! See the torch/onnx/symbolic_opset*.py files for more examples.", "prev_chunk_id": "chunk_314", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_316", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "torch.autograd.Functions#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "torch.autograd.Functions#", "content": "torch.autograd.Functions# If the operator is a sub-class of torch.autograd.Function, there are three ways to export it.", "prev_chunk_id": "chunk_315", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_317", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Static Symbolic Method#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Static Symbolic Method#", "content": "Static Symbolic Method# You can add a static method named symbolic to your function class. It should return ONNX operators that represent the function’s behavior in ONNX. For example: class MyRelu(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -> torch.Tensor: ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def symbolic(g: torch.Graph, input: torch.Value) -> torch.Value: return g.op(\"Clip\", input, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.float)))", "prev_chunk_id": "chunk_316", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_318", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Inline Autograd Function#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Inline Autograd Function#", "content": "Inline Autograd Function# In cases where a static symbolic method is not provided for its subsequent torch.autograd.Function or where a function to register prim::PythonOp as custom symbolic functions is not provided, torch.onnx.export() tries to inline the graph that corresponds to that torch.autograd.Function such that this function is broken down into individual operators that were used within the function. The export should be successful as long as these individual operators are supported. For example: class MyLogExp(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor) -> torch.Tensor: ctx.save_for_backward(input) h = input.exp() return h.log().log() There is no static symbolic method present for this model, yet it is exported as follows: graph(%input : Float(1, strides=[1], requires_grad=0, device=cpu)): %1 : float = onnx::Exp[](%input) %2 : float = onnx::Log[](%1) %3 : float = onnx::Log[](%2) return (%3) If you need to avoid inlining of torch.autograd.Function, you should export models with operator_export_type set to ONNX_FALLTHROUGH or ONNX_ATEN_FALLBACK.", "prev_chunk_id": "chunk_317", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_319", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Custom operators#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Custom operators#", "content": "Custom operators# You can export your model with custom operators that includes a combination of many standard ONNX ops, or are driven by self-defined C++ backend.", "prev_chunk_id": "chunk_318", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_320", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "ONNX-script functions#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "ONNX-script functions#", "content": "ONNX-script functions# If an operator is not a standard ONNX op, but can be composed of multiple existing ONNX ops, you can utilize ONNX-script to create an external ONNX function to support the operator. You can export it by following this example: import onnxscript # There are three opset version needed to be aligned # This is (1) the opset version in ONNX function from onnxscript.onnx_opset import opset15 as op opset_version = 15 x = torch.randn(1, 2, 3, 4, requires_grad=True) model = torch.nn.SELU() custom_opset = onnxscript.values.Opset(domain=\"onnx-script\", version=1) @onnxscript.script(custom_opset) def Selu(X): alpha = 1.67326 # auto wrapped as Constants gamma = 1.0507 alphaX = op.CastLike(alpha, X) gammaX = op.CastLike(gamma, X) neg = gammaX * (alphaX * op.Exp(X) - alphaX) pos = gammaX * X zero = op.CastLike(0, X) return op.Where(X <= zero, neg, pos) # setType API provides shape/type to ONNX shape/type inference def custom_selu(g: jit_utils.GraphContext, X): return g.onnxscript_op(Selu, X).setType(X.type()) # Register custom symbolic function # There are three opset version needed to be aligned # This is (2) the opset version in registry torch.onnx.register_custom_op_symbolic( symbolic_name=\"aten::selu\", symbolic_fn=custom_selu, opset_version=opset_version, ) # There are three opset version needed to be aligned # This is (2) the opset version in exporter torch.onnx.export( model, x, \"model.onnx\", opset_version=opset_version, # only needed if you want to specify an opset version > 1. custom_opsets={\"onnx-script\": 2} ) The example above exports it as a custom operator in the “onnx-script” opset. When exporting a custom operator, you can specify the custom domain version using the custom_opsets dictionary at export. If not specified, the custom opset version defaults to 1. NOTE: Be careful to align the opset version mentioned in the above example, and make sure they are consumed in exporter step. The example usage of how to write a onnx-script function is a beta version in terms of the active", "prev_chunk_id": "chunk_319", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_321", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "ONNX-script functions#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "ONNX-script functions#", "content": "development on onnx-script. Please follow the latest ONNX-script", "prev_chunk_id": "chunk_320", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_322", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "C++ Operators#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "C++ Operators#", "content": "C++ Operators# If a model uses a custom operator implemented in C++ as described in Extending TorchScript with Custom C++ Operators, you can export it by following this example: from torch.onnx import symbolic_helper # Define custom symbolic function @symbolic_helper.parse_args(\"v\", \"v\", \"f\", \"i\") def symbolic_foo_forward(g, input1, input2, attr1, attr2): return g.op(\"custom_domain::Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2) # Register custom symbolic function torch.onnx.register_custom_op_symbolic(\"custom_ops::foo_forward\", symbolic_foo_forward, 9) class FooModel(torch.nn.Module): def __init__(self, attr1, attr2): super().__init__() self.attr1 = attr1 self.attr2 = attr2 def forward(self, input1, input2): # Calling custom op return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2) model = FooModel(attr1, attr2) torch.onnx.export( model, (example_input1, example_input1), \"model.onnx\", # only needed if you want to specify an opset version > 1. custom_opsets={\"custom_domain\": 2} ) The example above exports it as a custom operator in the “custom_domain” opset. When exporting a custom operator, you can specify the custom domain version using the custom_opsets dictionary at export. If not specified, the custom opset version defaults to 1. The runtime that consumes the model needs to support the custom op. See Caffe2 custom ops, ONNX Runtime custom ops, or your runtime of choice’s documentation.", "prev_chunk_id": "chunk_321", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_323", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Discovering all unconvertible ATen ops at once#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Discovering all unconvertible ATen ops at once#", "content": "Discovering all unconvertible ATen ops at once# When export fails due to an unconvertible ATen op, there may in fact be more than one such op but the error message only mentions the first. To discover all of the unconvertible ops in one go you can: # prepare model, args, opset_version ... torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops( model, args, opset_version=opset_version ) print(set(unconvertible_ops)) The set is approximated because some ops may be removed during the conversion process and don’t need to be converted. Some other ops may have partial support that will fail conversion with particular inputs, but this should give you a general idea of what ops are not supported. Please feel free to open GitHub Issues for op support requests.", "prev_chunk_id": "chunk_322", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_324", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript.html", "title": "Frequently Asked Questions#", "page_title": "TorchScript-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# Q: I have exported my LSTM model, but its input size seems to be fixed? Q: How to export models containing loops? Q: How to export models with primitive type inputs (e.g. int, float)? Q: Does ONNX support implicit scalar datatype casting? Q: Are lists of Tensors exportable to ONNX?", "prev_chunk_id": "chunk_323", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_325", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo_onnxruntime_backend.html", "title": "ONNX Backend for TorchDynamo#", "page_title": "ONNX Backend for TorchDynamo — PyTorch 2.8 documentation", "breadcrumbs": "ONNX Backend for TorchDynamo#", "content": "ONNX Backend for TorchDynamo# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025 For a quick overview of torch.compiler, see torch.compiler.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_326", "url": "https://docs.pytorch.org/docs/stable/onnx_verification.html", "title": "torch.onnx.verification#", "page_title": "torch.onnx.verification — PyTorch 2.8 documentation", "breadcrumbs": "torch.onnx.verification#", "content": "torch.onnx.verification# Created On: Mar 18, 2025 | Last Updated On: Jun 10, 2025 The ONNX verification module provides a set of tools to verify the correctness of ONNX models.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_327", "url": "https://docs.pytorch.org/docs/stable/onnx_verification.html", "title": "Deprecated#", "page_title": "torch.onnx.verification — PyTorch 2.8 documentation", "breadcrumbs": "Deprecated#", "content": "Deprecated# The following classes and functions are deprecated.", "prev_chunk_id": "chunk_326", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_328", "url": "https://docs.pytorch.org/docs/stable/onnx_ops.html", "title": "torch.onnx.ops#", "page_title": "torch.onnx.ops — PyTorch 2.8 documentation", "breadcrumbs": "torch.onnx.ops#", "content": "torch.onnx.ops# Created On: Jun 10, 2025 | Last Updated On: Jun 20, 2025 ONNX operators as native torch.fx operators. This module provides a set of functions to create ONNX operators in the FX graph which are exportable to ONNX.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_329", "url": "https://docs.pytorch.org/docs/stable/onnx_ops.html", "title": "Symbolic Operators#", "page_title": "torch.onnx.ops — PyTorch 2.8 documentation", "breadcrumbs": "Symbolic Operators#", "content": "Symbolic Operators# Operators that can be used to create any ONNX ops in the FX graph symbolically. These operators do not do actual computation. It’s recommended that you used them inside an if torch.onnx.is_in_onnx_export block.", "prev_chunk_id": "chunk_328", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_330", "url": "https://docs.pytorch.org/docs/stable/onnx_ops.html", "title": "ONNX Operators#", "page_title": "torch.onnx.ops — PyTorch 2.8 documentation", "breadcrumbs": "ONNX Operators#", "content": "ONNX Operators# The following operators are implemented as native PyTorch ops and can be exported as ONNX operators. They can be used natively in an nn.Module. For example, you can define a module: class Model(torch.nn.Module): def forward( self, input_data, cos_cache_data, sin_cache_data, position_ids_data ): return torch.onnx.ops.rotary_embedding( input_data, cos_cache_data, sin_cache_data, position_ids_data, ) and export it to ONNX using: input_data = torch.rand(2, 3, 4, 8) position_ids_data = torch.randint(0, 50, (2, 3)).long() sin_cache_data = torch.rand(50, 4) cos_cache_data = torch.rand(50, 4) dynamic_shapes = { \"input_data\": {0: torch.export.Dim.DYNAMIC}, \"cos_cache_data\": None, \"sin_cache_data\": None, \"position_ids_data\": {0: torch.export.Dim.DYNAMIC}, } onnx_program = torch.onnx.export( model, (input_data, cos_cache_data, sin_cache_data, position_ids_data), dynamic_shapes=dynamic_shapes, dynamo=True, opset_version=23, ) Printing the ONNX program will show the ONNX operators used in the graph: <...> graph( name=main_graph, inputs=( %\"input_data\"<FLOAT,[s0,3,4,8]>, %\"cos_cache_data\"<FLOAT,[50,4]>, %\"sin_cache_data\"<FLOAT,[50,4]>, %\"position_ids_data\"<INT64,[s0,3]> ), outputs=( %\"rotary_embedding\"<FLOAT,[s0,3,4,8]> ), ) { 0 | # rotary_embedding %\"rotary_embedding\"<FLOAT,[s0,3,4,8]> ⬅️ ::RotaryEmbedding(%\"input_data\", %\"cos_cache_data\", %\"sin_cache_data\", %\"position_ids_data\") return %\"rotary_embedding\"<FLOAT,[s0,3,4,8]> } with the corresponding ExportedProgram: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, input_data: \"f32[s0, 3, 4, 8]\", cos_cache_data: \"f32[50, 4]\", sin_cache_data: \"f32[50, 4]\", position_ids_data: \"i64[s0, 3]\"): rotary_embedding: \"f32[s0, 3, 4, 8]\" = torch.ops.onnx.RotaryEmbedding.opset23(input_data, cos_cache_data, sin_cache_data, position_ids_data); input_data = cos_cache_data = sin_cache_data = position_ids_data = None return (rotary_embedding,)", "prev_chunk_id": "chunk_329", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_331", "url": "https://docs.pytorch.org/docs/stable/onnx_ops.html", "title": "ONNX to ATen Decomposition Table#", "page_title": "torch.onnx.ops — PyTorch 2.8 documentation", "breadcrumbs": "ONNX to ATen Decomposition Table#", "content": "ONNX to ATen Decomposition Table# You can use torch.onnx.ops.aten_decompositions() to obtain a decomposition table to decompose ONNX operators defined above to ATen operators. class Model(torch.nn.Module): def forward( self, input_data, cos_cache_data, sin_cache_data, position_ids_data ): return torch.onnx.ops.rotary_embedding( input_data, cos_cache_data, sin_cache_data, position_ids_data, ) model = Model() ep = torch.export.export( model, (input_data, cos_cache_data, sin_cache_data, position_ids_data), ) # The program can be decomposed into aten ops ep_decomposed = ep.run_decompositions(torch.onnx.ops.aten_decompositions())", "prev_chunk_id": "chunk_330", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_332", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo_memory_usage.html", "title": "Understanding TorchDynamo-based ONNX Exporter Memory Usage#", "page_title": "Understanding TorchDynamo-based ONNX Exporter Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Understanding TorchDynamo-based ONNX Exporter Memory Usage#", "content": "Understanding TorchDynamo-based ONNX Exporter Memory Usage# Created On: Nov 06, 2024 | Last Updated On: Jun 18, 2025 The previous TorchScript-based ONNX exporter would execute the model once to trace its execution, which could cause it to run out of memory on your GPU if the model’s memory requirements exceeded the available GPU memory. This issue has been addressed with the new TorchDynamo-based ONNX exporter. The TorchDynamo-based ONNX exporter utilizes torch.export.export() function to leverage FakeTensorMode to avoid performing actual tensor computations during the export process. This approach results in significantly lower memory usage compared to the TorchScript-based ONNX exporter. Below is an example demonstrating the memory usage difference between TorchScript-based and TorchDynamo-based ONNX exporters. In this example, we use the HighResNet model from MONAI. Before proceeding, please install it from PyPI: pip install monai PyTorch offers a tool for capturing and visualizing memory usage traces. We will use this tool to record the memory usage of the two exporters during the export process and compare the results. You can find more details about this tool on Understanding CUDA Memory Usage.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_333", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo_memory_usage.html", "title": "TorchScript-based exporter#", "page_title": "Understanding TorchDynamo-based ONNX Exporter Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript-based exporter#", "content": "TorchScript-based exporter# The code below could be run to generate a snapshot file which records the state of allocated CUDA memory during the export process. import torch from monai.networks.nets import ( HighResNet, ) torch.cuda.memory._record_memory_history() model = HighResNet( spatial_dims=3, in_channels=1, out_channels=3, norm_type=\"batch\" ).eval() model = model.to(\"cuda\") data = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to(\"cuda\") with torch.no_grad(): onnx_program = torch.onnx.export( model, data, \"torchscript_exporter_highresnet.onnx\", dynamo=False, ) snapshot_name = \"torchscript_exporter_example.pickle\" print(f\"generate {snapshot_name}\") torch.cuda.memory._dump_snapshot(snapshot_name) print(\"Export is done.\") Open pytorch.org/memory_viz and drag/drop the generated pickled snapshot file into the visualizer. The memory usage is described as below: By this figure, we can see the memory usage peak is above 2.8GB.", "prev_chunk_id": "chunk_332", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_334", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo_memory_usage.html", "title": "TorchDynamo-based exporter#", "page_title": "Understanding TorchDynamo-based ONNX Exporter Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo-based exporter#", "content": "TorchDynamo-based exporter# The code below could be run to generate a snapshot file which records the state of allocated CUDA memory during the export process. import torch from monai.networks.nets import ( HighResNet, ) torch.cuda.memory._record_memory_history() model = HighResNet( spatial_dims=3, in_channels=1, out_channels=3, norm_type=\"batch\" ).eval() model = model.to(\"cuda\") data = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to(\"cuda\") with torch.no_grad(): onnx_program = torch.onnx.export( model, data, \"test_faketensor.onnx\", dynamo=True, ) snapshot_name = f\"torchdynamo_exporter_example.pickle\" print(f\"generate {snapshot_name}\") torch.cuda.memory._dump_snapshot(snapshot_name) print(f\"Export is done.\") Open pytorch.org/memory_viz and drag/drop the generated pickled snapshot file into the visualizer. The memory usage is described as below: By this figure, we can see the memory usage peak is only around 45MB. Comparing to the memory usage peak of TorchScript-based exporter, it reduces 98% memory usage.", "prev_chunk_id": "chunk_333", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_335", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "TorchDynamo-based ONNX Exporter#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo-based ONNX Exporter#", "content": "TorchDynamo-based ONNX Exporter# Created On: Jun 10, 2025 | Last Updated On: Jun 20, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_336", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Overview#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# The ONNX exporter leverages TorchDynamo engine to hook into Python’s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph. The main advantage of this approach is that the FX graph is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques. In addition, during the export process, memory usage is significantly reduced compared to the TorchScript-enabled exporter. See the memory usage documentation for more information.", "prev_chunk_id": "chunk_335", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_337", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Dependencies#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Dependencies#", "content": "Dependencies# The ONNX exporter depends on extra Python packages: - ONNX - ONNX Script They can be installed through pip: pip install --upgrade onnx onnxscript onnxruntime can then be used to execute the model on a large variety of processors.", "prev_chunk_id": "chunk_336", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_338", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "A simple example#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "A simple example#", "content": "A simple example# See below a demonstration of exporter API in action with a simple Multilayer Perceptron (MLP) as example: import torch import torch.nn as nn class MLPModel(nn.Module): def __init__(self): super().__init__() self.fc0 = nn.Linear(8, 8, bias=True) self.fc1 = nn.Linear(8, 4, bias=True) self.fc2 = nn.Linear(4, 2, bias=True) self.fc3 = nn.Linear(2, 2, bias=True) self.fc_combined = nn.Linear(8 + 8 + 8, 8, bias=True) # Combine all inputs def forward(self, tensor_x: torch.Tensor, input_dict: dict, input_list: list): \"\"\" Forward method that requires all inputs: - tensor_x: A direct tensor input. - input_dict: A dictionary containing the tensor under the key 'tensor_x'. - input_list: A list where the first element is the tensor. \"\"\" # Extract tensors from inputs dict_tensor = input_dict['tensor_x'] list_tensor = input_list[0] # Combine all inputs into a single tensor combined_tensor = torch.cat([tensor_x, dict_tensor, list_tensor], dim=1) # Process the combined tensor through the layers combined_tensor = self.fc_combined(combined_tensor) combined_tensor = torch.sigmoid(combined_tensor) combined_tensor = self.fc0(combined_tensor) combined_tensor = torch.sigmoid(combined_tensor) combined_tensor = self.fc1(combined_tensor) combined_tensor = torch.sigmoid(combined_tensor) combined_tensor = self.fc2(combined_tensor) combined_tensor = torch.sigmoid(combined_tensor) output = self.fc3(combined_tensor) return output model = MLPModel() # Example inputs tensor_input = torch.rand((97, 8), dtype=torch.float32) dict_input = {'tensor_x': torch.rand((97, 8), dtype=torch.float32)} list_input = [torch.rand((97, 8), dtype=torch.float32)] # The input_names and output_names are used to identify the inputs and outputs of the ONNX model input_names = ['tensor_input', 'tensor_x', 'list_input_index_0'] output_names = ['output'] # Exporting the model with all required inputs onnx_program = torch.onnx.export(model,(tensor_input, dict_input, list_input), dynamic_shapes=({0: \"batch_size\"},{\"tensor_x\": {0: \"batch_size\"}},[{0: \"batch_size\"}]), input_names=input_names, output_names=output_names, dynamo=True,) # Check the exported ONNX model is dynamic assert onnx_program.model.graph.inputs[0].shape == (\"batch_size\", 8) assert onnx_program.model.graph.inputs[1].shape == (\"batch_size\", 8) assert onnx_program.model.graph.inputs[2].shape == (\"batch_size\", 8) As the code above shows, all you need is to provide torch.onnx.export() with an instance of the model and its input. The exporter will then return an instance of torch.onnx.ONNXProgram that contains the exported ONNX graph along with extra", "prev_chunk_id": "chunk_337", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_339", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "A simple example#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "A simple example#", "content": "information. The in-memory model available through onnx_program.model_proto is an onnx.ModelProto object in compliance with the ONNX IR spec. The ONNX model may then be serialized into a Protobuf file using the torch.onnx.ONNXProgram.save() API. onnx_program.save(\"mlp.onnx\")", "prev_chunk_id": "chunk_338", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_340", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Use the same model to compare with the TorchScript-enabled exporter#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Use the same model to compare with the TorchScript-enabled exporter#", "content": "Use the same model to compare with the TorchScript-enabled exporter# The biggest difference between the TorchScript-enabled exporter and the TorchDynamo-based exporter is that the latter requires dynamic_shapes to be the same tree structure as the input, while the former requires the dynamic_shapes to be a single and flatten dictionary. torch.onnx.export(model,(tensor_input, dict_input, list_input), \"mlp.onnx\", dynamic_axes={\"tensor_input\":{0: \"batch_size\"}, \"tensor_x\": {0: \"batch_size\"}, \"list_input_index_0\": {0: \"batch_size\"}}, input_names=input_names, output_names=output_names)", "prev_chunk_id": "chunk_339", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_341", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Inspecting the ONNX model using GUI#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Inspecting the ONNX model using GUI#", "content": "Inspecting the ONNX model using GUI# You can view the exported model using Netron.", "prev_chunk_id": "chunk_340", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_342", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "When the conversion fails#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "When the conversion fails#", "content": "When the conversion fails# Function torch.onnx.export() should be called a second time with parameter report=True. A markdown report is generated to help the user to resolve the issue.", "prev_chunk_id": "chunk_341", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_343", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Metadata#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Metadata#", "content": "Metadata# During ONNX export, each ONNX node is annotated with metadata that helps trace its origin and context from the original PyTorch model. This metadata is useful for debugging, model inspection, and understanding the mapping between PyTorch and ONNX graphs. The following metadata fields are added to each ONNX node: - namespaceA string representing the hierarchical namespace of the node, consisting of a stack trace of modules/methods.Example:__main__.SimpleAddModel/add:aten.add.Tensor - pkg.torch.onnx.class_hierarchyA list of class names representing the hierarchy of modules leading to this node.Example:['__main__.SimpleAddModel','aten.add.Tensor'] - pkg.torch.onnx.fx_nodeThe string representation of the original FX node, including its name, number of consumers, the targeted torch op, arguments, and keyword arguments.Example:%cat:[num_users=1]=call_function[target=torch.ops.aten.cat.default](args=([%tensor_x,%input_dict_tensor_x,%input_list_0],1),kwargs={}) - pkg.torch.onnx.name_scopesA list of name scopes (methods) representing the path to this node in the PyTorch model.Example:['','add'] - pkg.torch.onnx.stack_traceThe stack trace from the original code where this node was created, if available.Example:File\"simpleadd.py\",line7,inforwardreturntorch.add(x,y) These metadata fields are stored in the metadata_props attribute of each ONNX node and can be inspected using Netron or programmatically. The overall ONNX graph has the following metadata_props: - pkg.torch.export.ExportedProgram.graph_signatureThis property contains a string representation of the graph_signature from the original PyTorch ExportedProgram. The graph signature describes the structure of the model’s inputs and outputs and how they map to the ONNX graph. The inputs are defined asInputSpecobjects, which include the kind of input (e.g.,InputKind.PARAMETERfor parameters,InputKind.USER_INPUTfor user-defined inputs), the argument name, the target (which can be a specific node in the model), and whether the input is persistent. The outputs are defined asOutputSpecobjects, which specify the kind of output (e.g.,OutputKind.USER_OUTPUT) and the argument name.To read more about the graph signature, please see thetorch.exportfor more information. - pkg.torch.export.ExportedProgram.range_constraintsThis property contains a string representation of any range constraints that were present in the original PyTorch ExportedProgram. Range constraints specify valid ranges for symbolic shapes or values in the model, which can be important for", "prev_chunk_id": "chunk_342", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_344", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Metadata#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Metadata#", "content": "models that use dynamic shapes or symbolic dimensions.Example:s0:VR[2,int_oo], which indicates that the size of the input tensor must be at least 2.To read more about range constraints, please see thetorch.exportfor more information. Each input value in the ONNX graph may have the following metadata property: - pkg.torch.export.graph_signature.InputSpec.kindThe kind of input, as defined by PyTorch’s InputKind enum.Example values:“USER_INPUT”: A user-provided input to the model.“PARAMETER”: A model parameter (e.g., weight).“BUFFER”: A model buffer (e.g., running mean in BatchNorm).“CONSTANT_TENSOR”: A constant tensor argument.“CUSTOM_OBJ”: A custom object input.“TOKEN”: A token input. - pkg.torch.export.graph_signature.InputSpec.persistentIndicates whether the input is persistent (i.e., should be saved as part of the model’s state).Example values:“True”“False” Each output value in the ONNX graph may have the following metadata property: - pkg.torch.export.graph_signature.OutputSpec.kindThe kind of input, as defined by PyTorch’s OutputKind enum.Example values:“USER_OUTPUT”: A user-visible output.“LOSS_OUTPUT”: A loss value output.“BUFFER_MUTATION”: Indicates a buffer was mutated.“GRADIENT_TO_PARAMETER”: Gradient output for a parameter.“GRADIENT_TO_USER_INPUT”: Gradient output for a user input.“USER_INPUT_MUTATION”: Indicates a user input was mutated.“TOKEN”: A token output. Each initialized value, input, output has the following metadata: - pkg.torch.onnx.original_node_nameThe original name of the node in the PyTorch FX graph that produced this value in the case where the value was renamed. This helps trace initializers back to their source in the original model.Example:fc1.weight", "prev_chunk_id": "chunk_343", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_345", "url": "https://docs.pytorch.org/docs/stable/onnx_dynamo.html", "title": "Deprecated#", "page_title": "TorchDynamo-based ONNX Exporter — PyTorch 2.8 documentation", "breadcrumbs": "Deprecated#", "content": "Deprecated# The following classes and functions are deprecated and will be removed.", "prev_chunk_id": "chunk_344", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_346", "url": "https://docs.pytorch.org/docs/stable/nn.attention.experimental.html", "title": "torch.nn.attention.experimental#", "page_title": "torch.nn.attention.experimental — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.attention.experimental#", "content": "torch.nn.attention.experimental# Created On: Oct 29, 2024 | Last Updated On: Jun 14, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_347", "url": "https://docs.pytorch.org/docs/stable/nn.attention.bias.html", "title": "torch.nn.attention.bias#", "page_title": "torch.nn.attention.bias — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.attention.bias#", "content": "torch.nn.attention.bias# Created On: Dec 05, 2023 | Last Updated On: Jun 14, 2025 Defines bias subclasses that work with scaled_dot_product_attention", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_348", "url": "https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html", "title": "torch.nn.attention.flex_attention#", "page_title": "torch.nn.attention.flex_attention — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.attention.flex_attention#", "content": "torch.nn.attention.flex_attention# Created On: Jul 16, 2024 | Last Updated On: Jun 14, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_349", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript_supported_aten_ops.html", "title": "ONNX supported TorchScript operators#", "page_title": "ONNX supported TorchScript operators — PyTorch 2.8 documentation", "breadcrumbs": "ONNX supported TorchScript operators#", "content": "ONNX supported TorchScript operators# Created On: Apr 07, 2022 | Last Updated On: Sep 08, 2023 This page lists the TorchScript operators that are supported/unsupported by ONNX export.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_350", "url": "https://docs.pytorch.org/docs/stable/onnx_torchscript_supported_aten_ops.html", "title": "Unsupported operators#", "page_title": "ONNX supported TorchScript operators — PyTorch 2.8 documentation", "breadcrumbs": "Unsupported operators#", "content": "Unsupported operators# Operators that are not yet supported", "prev_chunk_id": "chunk_349", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_351", "url": "https://docs.pytorch.org/docs/stable/jit_unsupported.html", "title": "TorchScript Unsupported PyTorch Constructs#", "page_title": "TorchScript Unsupported PyTorch Constructs — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Unsupported PyTorch Constructs#", "content": "TorchScript Unsupported PyTorch Constructs# Created On: Dec 18, 2019 | Last Updated On: Nov 21, 2023", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_352", "url": "https://docs.pytorch.org/docs/stable/jit_unsupported.html", "title": "Torch and Tensor Unsupported Attributes#", "page_title": "TorchScript Unsupported PyTorch Constructs — PyTorch 2.8 documentation", "breadcrumbs": "Torch and Tensor Unsupported Attributes#", "content": "Torch and Tensor Unsupported Attributes# TorchScript supports most methods defined on torch and torch.Tensor, but we do not have full coverage. Here are specific known ops and categories of ops which have diverging behavior between Python and TorchScript. If you encounter something else that is not supported please file a GitHub issue. Deprecated ops are not listed below.", "prev_chunk_id": "chunk_351", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_353", "url": "https://docs.pytorch.org/docs/stable/jit_unsupported.html", "title": "Functions Not Correctly Bound on Torch#", "page_title": "TorchScript Unsupported PyTorch Constructs — PyTorch 2.8 documentation", "breadcrumbs": "Functions Not Correctly Bound on Torch#", "content": "Functions Not Correctly Bound on Torch# The following functions will fail if used in TorchScript, either because they are not bound on torch or because Python expects a different schema than TorchScript.", "prev_chunk_id": "chunk_352", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_354", "url": "https://docs.pytorch.org/docs/stable/jit_unsupported.html", "title": "Ops With Divergent Schemas Between Torch & Python#", "page_title": "TorchScript Unsupported PyTorch Constructs — PyTorch 2.8 documentation", "breadcrumbs": "Ops With Divergent Schemas Between Torch & Python#", "content": "Ops With Divergent Schemas Between Torch & Python# The following categories of ops have divergent schemas: Functions which construct tensors from non-tensor inputs do not support the requires_grad argument, except for torch.tensor. This covers the following ops: The following functions require dtype, layout, device as parameters in TorchScript, but these parameters are optional in Python.", "prev_chunk_id": "chunk_353", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_355", "url": "https://docs.pytorch.org/docs/stable/jit_unsupported.html", "title": "PyTorch Unsupported Modules and Classes#", "page_title": "TorchScript Unsupported PyTorch Constructs — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Unsupported Modules and Classes#", "content": "PyTorch Unsupported Modules and Classes# TorchScript cannot currently compile a number of other commonly used PyTorch constructs. Below are listed the modules that TorchScript does not support, and an incomplete list of PyTorch classes that are not supported. For unsupported modules we suggest using torch.jit.trace().", "prev_chunk_id": "chunk_354", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_356", "url": "https://docs.pytorch.org/docs/stable/jit_python_reference.html", "title": "Python Language Reference Coverage#", "page_title": "Python Language Reference Coverage — PyTorch 2.8 documentation", "breadcrumbs": "Python Language Reference Coverage#", "content": "Python Language Reference Coverage# Created On: Dec 26, 2019 | Last Updated On: May 19, 2021 This is a 1:1 mapping of the features listed in https://docs.python.org/3/reference/ and their support in TorchScript. The categorizations are as follows:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_357", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "TorchScript Language Reference#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Language Reference#", "content": "TorchScript Language Reference# Created On: Mar 10, 2021 | Last Updated On: Jun 13, 2025 This reference manual describes the syntax and core semantics of the TorchScript language. TorchScript is a statically typed subset of the Python language. This document explains the supported features of Python in TorchScript and also how the language diverges from regular Python. Any features of Python that are not mentioned in this reference manual are not part of TorchScript. TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_358", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Terminology#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Terminology#", "content": "Terminology# This document uses the following terminologies:", "prev_chunk_id": "chunk_357", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_359", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Type System#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Type System#", "content": "Type System# TorchScript is a statically typed subset of Python. The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models.", "prev_chunk_id": "chunk_358", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_360", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "TorchScript Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Types#", "content": "TorchScript Types# The TorchScript type system consists of TSType and TSModuleType as defined below. TSAllType ::= TSType | TSModuleType TSType ::= TSMetaType | TSPrimitiveType | TSStructuralType | TSNominalType TSType represents the majority of TorchScript types that are composable and that can be used in TorchScript type annotations. TSType refers to any of the following: - Meta Types, e.g.,Any - Primitive Types, e.g.,int,float, andstr - Structural Types, e.g.,Optional[int]orList[MyClass] - Nominal Types (Python classes), e.g.,MyClass(user-defined),torch.tensor(built-in) TSModuleType represents torch.nn.Module and its subclasses. It is treated differently from TSType because its type schema is inferred partly from the object instance and partly from the class definition. As such, instances of a TSModuleType may not follow the same static type schema. TSModuleType cannot be used as a TorchScript type annotation or be composed with TSType for type safety considerations.", "prev_chunk_id": "chunk_359", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_361", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Meta Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Meta Types#", "content": "Meta Types# Meta types are so abstract that they are more like type constraints than concrete types. Currently TorchScript defines one meta-type, Any, that represents any TorchScript type.", "prev_chunk_id": "chunk_360", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_362", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Any Type#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Any Type#", "content": "Any Type# The Any type represents any TorchScript type. Any specifies no type constraints, thus there is no type-checking on Any. As such it can be bound to any Python or TorchScript data types (e.g., int, TorchScript tuple, or an arbitrary Python class that is not scripted). TSMetaType ::= \"Any\" Where: - Anyis the Python class name from the typing module. Therefore, to use theAnytype, you must import it fromtyping(e.g.,fromtypingimportAny). - SinceAnycan represent any TorchScript type, the set of operators that are allowed to operate on values of this type onAnyis limited.", "prev_chunk_id": "chunk_361", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_363", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Operators Supported for Any Type#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Operators Supported for Any Type#", "content": "Operators Supported for Any Type# - Assignment to data ofAnytype. - Binding to parameter or return ofAnytype. - xis,xisnotwherexis ofAnytype. - isinstance(x,Type)wherexis ofAnytype. - Data ofAnytype is printable. - Data ofList[Any]type may be sortable if the data is a list of values of the same typeTand thatTsupports comparison operators. Compared to Python Any is the least constrained type in the TorchScript type system. In that sense, it is quite similar to the Object class in Python. However, Any only supports a subset of the operators and methods that are supported by Object.", "prev_chunk_id": "chunk_362", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_364", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Design Notes#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Design Notes#", "content": "Design Notes# When we script a PyTorch module, we may encounter data that is not involved in the execution of the script. Nevertheless, it has to be described by a type schema. It is not only cumbersome to describe static types for unused data (in the context of the script), but also may lead to unnecessary scripting failures. Any is introduced to describe the type of the data where precise static types are not necessary for compilation. Example 1 This example illustrates how Any can be used to allow the second element of the tuple parameter to be of any type. This is possible because x[1] is not involved in any computation that requires knowing its precise type. import torch from typing import Tuple from typing import Any @torch.jit.export def inc_first_element(x: Tuple[int, Any]): return (x[0]+1, x[1]) m = torch.jit.script(inc_first_element) print(m((1,2.0))) print(m((1,(100,200)))) The example above produces the following output: (2, 2.0) (2, (100, 200)) The second element of the tuple is of Any type, thus can bind to multiple types. For example, (1, 2.0) binds a float type to Any as in Tuple[int, Any], whereas (1, (100, 200)) binds a tuple to Any in the second invocation. Example 2 This example illustrates how we can use isinstance to dynamically check the type of the data that is annotated as Any type: import torch from typing import Any def f(a:Any): print(a) return (isinstance(a, torch.Tensor)) ones = torch.ones([2]) m = torch.jit.script(f) print(m(ones)) The example above produces the following output: 1 1 [ CPUFloatType{2} ] True", "prev_chunk_id": "chunk_363", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_365", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Primitive Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Primitive Types#", "content": "Primitive Types# Primitive TorchScript types are types that represent a single type of value and go with a single pre-defined type name. TSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\"", "prev_chunk_id": "chunk_364", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_366", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Structural Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Structural Types#", "content": "Structural Types# Structural types are types that are structurally defined without a user-defined name (unlike nominal types), such as Future[int]. Structural types are composable with any TSType. TSStructuralType ::= TSTuple | TSNamedTuple | TSList | TSDict | TSOptional | TSUnion | TSFuture | TSRRef | TSAwait TSTuple ::= \"Tuple\" \"[\" (TSType \",\")* TSType \"]\" TSNamedTuple ::= \"namedtuple\" \"(\" (TSType \",\")* TSType \")\" TSList ::= \"List\" \"[\" TSType \"]\" TSOptional ::= \"Optional\" \"[\" TSType \"]\" TSUnion ::= \"Union\" \"[\" (TSType \",\")* TSType \"]\" TSFuture ::= \"Future\" \"[\" TSType \"]\" TSRRef ::= \"RRef\" \"[\" TSType \"]\" TSAwait ::= \"Await\" \"[\" TSType \"]\" TSDict ::= \"Dict\" \"[\" KeyType \",\" TSType \"]\" KeyType ::= \"str\" | \"int\" | \"float\" | \"bool\" | TensorType | \"Any\" Where: - Tuple,List,Optional,Union,Future,Dictrepresent Python type class names that are defined in the moduletyping. To use these type names, you must import them fromtyping(e.g.,fromtypingimportTuple). - namedtuplerepresents the Python classcollections.namedtupleortyping.NamedTuple. - FutureandRRefrepresent the Python classestorch.futuresandtorch.distributed.rpc. - Awaitrepresent the Python classtorch._awaits._Await Compared to Python Apart from being composable with TorchScript types, these TorchScript structural types often support a common subset of the operators and methods of their Python counterparts. Example 1 This example uses typing.NamedTuple syntax to define a tuple: import torch from typing import NamedTuple from typing import Tuple class MyTuple(NamedTuple): first: int second: int def inc(x: MyTuple) -> Tuple[int, int]: return (x.first+1, x.second+1) t = MyTuple(first=1, second=2) scripted_inc = torch.jit.script(inc) print(\"TorchScript:\", scripted_inc(t)) The example above produces the following output: TorchScript: (2, 3) Example 2 This example uses collections.namedtuple syntax to define a tuple: import torch from typing import NamedTuple from typing import Tuple from collections import namedtuple _AnnotatedNamedTuple = NamedTuple('_NamedTupleAnnotated', [('first', int), ('second', int)]) _UnannotatedNamedTuple = namedtuple('_NamedTupleAnnotated', ['first', 'second']) def inc(x: _AnnotatedNamedTuple) -> Tuple[int, int]: return (x.first+1, x.second+1) m = torch.jit.script(inc) print(inc(_UnannotatedNamedTuple(1,2))) The example above produces the following output: (2,", "prev_chunk_id": "chunk_365", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_367", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Structural Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Structural Types#", "content": "3) Example 3 This example illustrates a common mistake of annotating structural types, i.e., not importing the composite type classes from the typing module: import torch # ERROR: Tuple not recognized because not imported from typing @torch.jit.export def inc(x: Tuple[int, int]): return (x[0]+1, x[1]+1) m = torch.jit.script(inc) print(m((1,2))) Running the above code yields the following scripting error: File \"test-tuple.py\", line 5, in <module> def inc(x: Tuple[int, int]): NameError: name 'Tuple' is not defined The remedy is to add the line from typing import Tuple to the beginning of the code.", "prev_chunk_id": "chunk_366", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_368", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Nominal Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Nominal Types#", "content": "Nominal Types# Nominal TorchScript types are Python classes. These types are called nominal because they are declared with a custom name and are compared using class names. Nominal classes are further classified into the following categories: TSNominalType ::= TSBuiltinClasses | TSCustomClass | TSEnum Among them, TSCustomClass and TSEnum must be compilable to TorchScript Intermediate Representation (IR). This is enforced by the type-checker.", "prev_chunk_id": "chunk_367", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_369", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Built-in Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Built-in Class#", "content": "Built-in Class# Built-in nominal types are Python classes whose semantics are built into the TorchScript system (e.g., tensor types). TorchScript defines the semantics of these built-in nominal types, and often supports only a subset of the methods or attributes of its Python class definition. TSBuiltinClass ::= TSTensor | \"torch.device\" | \"torch.Stream\" | \"torch.dtype\" | \"torch.nn.ModuleList\" | \"torch.nn.ModuleDict\" | ... TSTensor ::= \"torch.Tensor\" | \"common.SubTensor\" | \"common.SubWithTorchFunction\" | \"torch.nn.parameter.Parameter\" | and subclasses of torch.Tensor", "prev_chunk_id": "chunk_368", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_370", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Special Note on torch.nn.ModuleList and torch.nn.ModuleDict#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Special Note on torch.nn.ModuleList and torch.nn.ModuleDict#", "content": "Special Note on torch.nn.ModuleList and torch.nn.ModuleDict# Although torch.nn.ModuleList and torch.nn.ModuleDict are defined as a list and dictionary in Python, they behave more like tuples in TorchScript: - In TorchScript, instances oftorch.nn.ModuleListortorch.nn.ModuleDictare immutable. - Code that iterates overtorch.nn.ModuleListortorch.nn.ModuleDictis completely unrolled so that elements oftorch.nn.ModuleListor keys oftorch.nn.ModuleDictcan be of different subclasses oftorch.nn.Module. Example The following example highlights the use of a few built-in Torchscript classes (torch.*): import torch @torch.jit.script class A: def __init__(self): self.x = torch.rand(3) def f(self, y: torch.device): return self.x.to(device=y) def g(): a = A() return a.f(torch.device(\"cpu\")) script_g = torch.jit.script(g) print(script_g.graph)", "prev_chunk_id": "chunk_369", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_371", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Custom Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Custom Class#", "content": "Custom Class# Unlike built-in classes, semantics of custom classes are user-defined and the entire class definition must be compilable to TorchScript IR and subject to TorchScript type-checking rules. TSClassDef ::= [ \"@torch.jit.script\" ] \"class\" ClassName [ \"(object)\" ] \":\" MethodDefinition | [ \"@torch.jit.ignore\" ] | [ \"@torch.jit.unused\" ] MethodDefinition Where: - Classes must be new-style classes. Python 3 supports only new-style classes. In Python 2.x, a new-style class is specified by subclassing from the object. - Instance data attributes are statically typed, and instance attributes must be declared by assignments inside the__init__()method. - Method overloading is not supported (i.e., you cannot have multiple methods with the same method name). - MethodDefinitionmust be compilable to TorchScript IR and adhere to TorchScript’s type-checking rules, (i.e., all methods must be valid TorchScript functions and class attribute definitions must be valid TorchScript statements). - torch.jit.ignoreandtorch.jit.unusedcan be used to ignore the method or function that is not fully torchscriptable or should be ignored by the compiler. Compared to Python TorchScript custom classes are quite limited compared to their Python counterpart. Torchscript custom classes: - Do not support class attributes. - Do not support subclassing except for subclassing an interface type or object. - Do not support method overloading. - Must initialize all its instance attributes in__init__(); this is because TorchScript constructs a static schema of the class by inferring attribute types in__init__(). - Must contain only methods that satisfy TorchScript type-checking rules and are compilable to TorchScript IRs. Example 1 Python classes can be used in TorchScript if they are annotated with @torch.jit.script, similar to how a TorchScript function would be declared: @torch.jit.script class MyClass: def __init__(self, x: int): self.x = x def inc(self, val: int): self.x += val Example 2 A TorchScript custom class type must “declare” all its instance attributes by assignments in", "prev_chunk_id": "chunk_370", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_372", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Custom Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Custom Class#", "content": "__init__(). If an instance attribute is not defined in __init__() but accessed in other methods of the class, the class cannot be compiled as a TorchScript class, as shown in the following example: import torch @torch.jit.script class foo: def __init__(self): self.y = 1 # ERROR: self.x is not defined in __init__ def assign_x(self): self.x = torch.rand(2, 3) The class will fail to compile and issue the following error: RuntimeError: Tried to set nonexistent attribute: x. Did you forget to initialize it in __init__()?: def assign_x(self): self.x = torch.rand(2, 3) ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE Example 3 In this example, a TorchScript custom class defines a class variable name, which is not allowed: import torch @torch.jit.script class MyClass(object): name = \"MyClass\" def __init__(self, x: int): self.x = x def fn(a: MyClass): return a.name It leads to the following compile-time error: RuntimeError: '__torch__.MyClass' object has no attribute or method 'name'. Did you forget to initialize an attribute in __init__()?: File \"test-class2.py\", line 10 def fn(a: MyClass): return a.name ~~~~~~ <--- HERE", "prev_chunk_id": "chunk_371", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_373", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Enum Type#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Enum Type#", "content": "Enum Type# Like custom classes, semantics of the enum type are user-defined and the entire class definition must be compilable to TorchScript IR and adhere to TorchScript type-checking rules. TSEnumDef ::= \"class\" Identifier \"(enum.Enum | TSEnumType)\" \":\" ( MemberIdentifier \"=\" Value )+ ( MethodDefinition )* Where: - Value must be a TorchScript literal of typeint,float, orstr, and must be of the same TorchScript type. - TSEnumTypeis the name of a TorchScript enumerated type. Similar to Python enum, TorchScript allows restrictedEnumsubclassing, that is, subclassing an enumerated is allowed only if it does not define any members. Compared to Python - TorchScript supports onlyenum.Enum. It does not support other variations such asenum.IntEnum,enum.Flag,enum.IntFlag, andenum.auto. - Values of TorchScript enum members must be of the same type and can only beint,float, orstrtypes, whereas Python enum members can be of any type. - Enums containing methods are ignored in TorchScript. Example 1 The following example defines the class Color as an Enum type: import torch from enum import Enum class Color(Enum): RED = 1 GREEN = 2 def enum_fn(x: Color, y: Color) -> bool: if x == Color.RED: return True return x == y m = torch.jit.script(enum_fn) print(\"Eager: \", enum_fn(Color.RED, Color.GREEN)) print(\"TorchScript: \", m(Color.RED, Color.GREEN)) Example 2 The following example shows the case of restricted enum subclassing, where BaseColor does not define any member, thus can be subclassed by Color: import torch from enum import Enum class BaseColor(Enum): def foo(self): pass class Color(BaseColor): RED = 1 GREEN = 2 def enum_fn(x: Color, y: Color) -> bool: if x == Color.RED: return True return x == y m = torch.jit.script(enum_fn) print(\"TorchScript: \", m(Color.RED, Color.GREEN)) print(\"Eager: \", enum_fn(Color.RED, Color.GREEN))", "prev_chunk_id": "chunk_372", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_374", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "TorchScript Module Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Module Class#", "content": "TorchScript Module Class# TSModuleType is a special class type that is inferred from object instances that are created outside TorchScript. TSModuleType is named by the Python class of the object instance. The __init__() method of the Python class is not considered a TorchScript method, so it does not have to comply with TorchScript’s type-checking rules. The type schema of a module instance class is constructed directly from an instance object (created outside the scope of TorchScript) rather than inferred from __init__() like custom classes. It is possible that two objects of the same instance class type follow two different type schemas. In this sense, TSModuleType is not really a static type. Therefore, for type safety considerations, TSModuleType cannot be used in a TorchScript type annotation or be composed with TSType.", "prev_chunk_id": "chunk_373", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_375", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Module Instance Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Module Instance Class#", "content": "Module Instance Class# TorchScript module type represents the type schema of a user-defined PyTorch module instance. When scripting a PyTorch module, the module object is always created outside TorchScript (i.e., passed in as parameter to forward). The Python module class is treated as a module instance class, so the __init__() method of the Python module class is not subject to the type-checking rules of TorchScript. TSModuleType ::= \"class\" Identifier \"(torch.nn.Module)\" \":\" ClassBodyDefinition Where: - forward()and other methods decorated with@torch.jit.exportmust be compilable to TorchScript IR and subject to TorchScript’s type-checking rules. Unlike custom classes, only the forward method and other methods decorated with @torch.jit.export of the module type need to be compilable. Most notably, __init__() is not considered a TorchScript method. Consequently, module type constructors cannot be invoked within the scope of TorchScript. Instead, TorchScript module objects are always constructed outside and passed into torch.jit.script(ModuleObj). Example 1 This example illustrates a few features of module types: - TheTestModuleinstance is created outside the scope of TorchScript (i.e., before invokingtorch.jit.script). - __init__()is not considered a TorchScript method, therefore, it does not have to be annotated and can contain arbitrary Python code. In addition, the__init__()method of an instance class cannot be invoked in TorchScript code. BecauseTestModuleinstances are instantiated in Python, in this example,TestModule(2.0)andTestModule(2)create two instances with different types for its data attributes.self.xis of typefloatforTestModule(2.0), whereasself.yis of typeintforTestModule(2.0). - TorchScript automatically compiles other methods (e.g.,mul()) invoked by methods annotated via@torch.jit.exportorforward()methods. - Entry-points to a TorchScript program are eitherforward()of a module type, functions annotated astorch.jit.script, or methods annotated astorch.jit.export. import torch class TestModule(torch.nn.Module): def __init__(self, v): super().__init__() self.x = v def forward(self, inc: int): return self.x + inc m = torch.jit.script(TestModule(1)) print(f\"First instance: {m(3)}\") m = torch.jit.script(TestModule(torch.ones([5]))) print(f\"Second instance: {m(3)}\") The example above produces the following output: First instance: 4 Second instance: tensor([4., 4., 4., 4.,", "prev_chunk_id": "chunk_374", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_376", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Module Instance Class#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Module Instance Class#", "content": "4.]) Example 2 The following example shows an incorrect usage of module type. Specifically, this example invokes the constructor of TestModule inside the scope of TorchScript: import torch class TestModule(torch.nn.Module): def __init__(self, v): super().__init__() self.x = v def forward(self, x: int): return self.x + x class MyModel: def __init__(self, v: int): self.val = v @torch.jit.export def doSomething(self, val: int) -> int: # error: should not invoke the constructor of module type myModel = TestModule(self.val) return myModel(val) # m = torch.jit.script(MyModel(2)) # Results in below RuntimeError # RuntimeError: Could not get name of python class object", "prev_chunk_id": "chunk_375", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_377", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Type Annotation#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Type Annotation#", "content": "Type Annotation# Since TorchScript is statically typed, programmers need to annotate types at strategic points of TorchScript code so that every local variable or instance data attribute has a static type, and every function and method has a statically typed signature.", "prev_chunk_id": "chunk_376", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_378", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "When to Annotate Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "When to Annotate Types#", "content": "When to Annotate Types# In general, type annotations are only needed in places where static types cannot be automatically inferred (e.g., parameters or sometimes return types to methods or functions). Types of local variables and data attributes are often automatically inferred from their assignment statements. Sometimes an inferred type may be too restrictive, e.g., x being inferred as NoneType through assignment x = None, whereas x is actually used as an Optional. In such cases, type annotations may be needed to overwrite auto inference, e.g., x: Optional[int] = None. Note that it is always safe to type annotate a local variable or data attribute even if its type can be automatically inferred. The annotated type must be congruent with TorchScript’s type-checking. When a parameter, local variable, or data attribute is not type annotated and its type cannot be automatically inferred, TorchScript assumes it to be a default type of TensorType, List[TensorType], or Dict[str, TensorType].", "prev_chunk_id": "chunk_377", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_379", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Annotate Function Signature#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Annotate Function Signature#", "content": "Annotate Function Signature# Since a parameter may not be automatically inferred from the body of the function (including both functions and methods), they need to be type annotated. Otherwise, they assume the default type TensorType. TorchScript supports two styles for method and function signature type annotation: - Python3-styleannotates types directly on the signature. As such, it allows individual parameters to be left unannotated (whose type will be the default type ofTensorType), or allows the return type to be left unannotated (whose type will be automatically inferred). Python3Annotation ::= \"def\" Identifier [ \"(\" ParamAnnot* \")\" ] [ReturnAnnot] \":\" FuncOrMethodBody ParamAnnot ::= Identifier [ \":\" TSType ] \",\" ReturnAnnot ::= \"->\" TSType Note that when using Python3 style, the type self is automatically inferred and should not be annotated. - Mypy styleannotates types as a comment right below the function/method declaration. In the Mypy style, since parameter names do not appear in the annotation, all parameters have to be annotated. MyPyAnnotation ::= \"# type:\" \"(\" ParamAnnot* \")\" [ ReturnAnnot ] ParamAnnot ::= TSType \",\" ReturnAnnot ::= \"->\" TSType Example 1 In this example: - ais not annotated and assumes the default type ofTensorType. - bis annotated as typeint. - The return type is not annotated and is automatically inferred as typeTensorType(based on the type of the value being returned). import torch def f(a, b: int): return a+b m = torch.jit.script(f) print(\"TorchScript:\", m(torch.ones([6]), 100)) Example 2 The following example uses Mypy style annotation. Note that parameters or return values must be annotated even if some of them assume the default type. import torch def f(a, b): # type: (torch.Tensor, int) → torch.Tensor return a+b m = torch.jit.script(f) print(\"TorchScript:\", m(torch.ones([6]), 100))", "prev_chunk_id": "chunk_378", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_380", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Annotate Variables and Data Attributes#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Annotate Variables and Data Attributes#", "content": "Annotate Variables and Data Attributes# In general, types of data attributes (including class and instance data attributes) and local variables can be automatically inferred from assignment statements. Sometimes, however, if a variable or attribute is associated with values of different types (e.g., as None or TensorType), then they may need to be explicitly type annotated as a wider type such as Optional[int] or Any.", "prev_chunk_id": "chunk_379", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_381", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Local Variables#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Local Variables#", "content": "Local Variables# Local variables can be annotated according to Python3 typing module annotation rules, i.e., LocalVarAnnotation ::= Identifier [\":\" TSType] \"=\" Expr In general, types of local variables can be automatically inferred. In some cases, however, you may need to annotate a multi-type for local variables that may be associated with different concrete types. Typical multi-types include Optional[T] and Any. Example import torch def f(a, setVal: bool): value: Optional[torch.Tensor] = None if setVal: value = a return value ones = torch.ones([6]) m = torch.jit.script(f) print(\"TorchScript:\", m(ones, True), m(ones, False))", "prev_chunk_id": "chunk_380", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_382", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Instance Data Attributes#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Instance Data Attributes#", "content": "Instance Data Attributes# For ModuleType classes, instance data attributes can be annotated according to Python3 typing module annotation rules. Instance data attributes can be annotated (optionally) as final via Final. \"class\" ClassIdentifier \"(torch.nn.Module):\" InstanceAttrIdentifier \":\" [\"Final(\"] TSType [\")\"] ... Where: - InstanceAttrIdentifieris the name of an instance attribute. - Finalindicates that the attribute cannot be re-assigned outside of__init__or overridden in subclasses. Example import torch class MyModule(torch.nn.Module): offset_: int def __init__(self, offset): self.offset_ = offset ...", "prev_chunk_id": "chunk_381", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_383", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "torch.jit.annotate(T, expr)#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.jit.annotate(T, expr)#", "content": "torch.jit.annotate(T, expr)# This API annotates type T to an expression expr. This is often used when the default type of an expression is not the type intended by the programmer. For instance, an empty list (dictionary) has the default type of List[TensorType] (Dict[TensorType, TensorType]), but sometimes it may be used to initialize a list of some other types. Another common use case is for annotating the return type of tensor.tolist(). Note, however, that it cannot be used to annotate the type of a module attribute in __init__; torch.jit.Attribute should be used for this instead. Example In this example, [] is declared as a list of integers via torch.jit.annotate (instead of assuming [] to be the default type of List[TensorType]): import torch from typing import List def g(l: List[int], val: int): l.append(val) return l def f(val: int): l = g(torch.jit.annotate(List[int], []), val) return l m = torch.jit.script(f) print(\"Eager:\", f(3)) print(\"TorchScript:\", m(3)) See torch.jit.annotate() for more information.", "prev_chunk_id": "chunk_382", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_384", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "TorchScript Type System Definition#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Type System Definition#", "content": "TorchScript Type System Definition# TSAllType ::= TSType | TSModuleType TSType ::= TSMetaType | TSPrimitiveType | TSStructuralType | TSNominalType TSMetaType ::= \"Any\" TSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\" TSStructuralType ::= TSTuple | TSNamedTuple | TSList | TSDict | TSOptional | TSUnion | TSFuture | TSRRef | TSAwait TSTuple ::= \"Tuple\" \"[\" (TSType \",\")* TSType \"]\" TSNamedTuple ::= \"namedtuple\" \"(\" (TSType \",\")* TSType \")\" TSList ::= \"List\" \"[\" TSType \"]\" TSOptional ::= \"Optional\" \"[\" TSType \"]\" TSUnion ::= \"Union\" \"[\" (TSType \",\")* TSType \"]\" TSFuture ::= \"Future\" \"[\" TSType \"]\" TSRRef ::= \"RRef\" \"[\" TSType \"]\" TSAwait ::= \"Await\" \"[\" TSType \"]\" TSDict ::= \"Dict\" \"[\" KeyType \",\" TSType \"]\" KeyType ::= \"str\" | \"int\" | \"float\" | \"bool\" | TensorType | \"Any\" TSNominalType ::= TSBuiltinClasses | TSCustomClass | TSEnum TSBuiltinClass ::= TSTensor | \"torch.device\" | \"torch.stream\"| \"torch.dtype\" | \"torch.nn.ModuleList\" | \"torch.nn.ModuleDict\" | ... TSTensor ::= \"torch.tensor\" and subclasses", "prev_chunk_id": "chunk_383", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_385", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Unsupported Typing Constructs#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Unsupported Typing Constructs#", "content": "Unsupported Typing Constructs# TorchScript does not support all features and types of the Python3 typing module. Any functionality from the typing module that is not explicitly specified in this documentation is unsupported. The following table summarizes typing constructs that are either unsupported or supported with restrictions in TorchScript.", "prev_chunk_id": "chunk_384", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_386", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Expressions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Expressions#", "content": "Expressions# The following section describes the grammar of expressions that are supported in TorchScript. It is modeled after the expressions chapter of the Python language reference.", "prev_chunk_id": "chunk_385", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_387", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Arithmetic Conversions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Arithmetic Conversions#", "content": "Arithmetic Conversions# There are a number of implicit type conversions that are performed in TorchScript: - ATensorwith afloatorintdata type can be implicitly converted to an instance ofFloatTypeorIntTypeprovided that it has a size of 0, does not haverequire_gradset toTrue, and will not require narrowing. - Instances ofStringTypecan be implicitly converted toDeviceType. - The implicit conversion rules from the two bullet points above can be applied to instances ofTupleTypeto produce instances ofListTypewith the appropriate contained type. Explicit conversions can be invoked using the float, int, bool, and str built-in functions that accept primitive data types as arguments and can accept user-defined types if they implement __bool__, __str__, etc.", "prev_chunk_id": "chunk_386", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_388", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Atoms#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Atoms#", "content": "Atoms# Atoms are the most basic elements of expressions. atom ::= identifier | literal | enclosure enclosure ::= parenth_form | list_display | dict_display", "prev_chunk_id": "chunk_387", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_389", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Identifiers#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Identifiers#", "content": "Identifiers# The rules that dictate what is a legal identifier in TorchScript are the same as their Python counterparts.", "prev_chunk_id": "chunk_388", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_390", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Literals#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Literals#", "content": "Literals# literal ::= stringliteral | integer | floatnumber Evaluation of a literal yields an object of the appropriate type with the specific value (with approximations applied as necessary for floats). Literals are immutable, and multiple evaluations of identical literals may obtain the same object or distinct objects with the same value. stringliteral, integer, and floatnumber are defined in the same way as their Python counterparts.", "prev_chunk_id": "chunk_389", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_391", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Parenthesized Forms#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Parenthesized Forms#", "content": "Parenthesized Forms# parenth_form ::= '(' [expression_list] ')' A parenthesized expression list yields whatever the expression list yields. If the list contains at least one comma, it yields a Tuple; otherwise, it yields the single expression inside the expression list. An empty pair of parentheses yields an empty Tuple object (Tuple[]).", "prev_chunk_id": "chunk_390", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_392", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "List and Dictionary Displays#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "List and Dictionary Displays#", "content": "List and Dictionary Displays# list_comprehension ::= expression comp_for comp_for ::= 'for' target_list 'in' or_expr list_display ::= '[' [expression_list | list_comprehension] ']' dict_display ::= '{' [key_datum_list | dict_comprehension] '}' key_datum_list ::= key_datum (',' key_datum)* key_datum ::= expression ':' expression dict_comprehension ::= key_datum comp_for Lists and dicts can be constructed by either listing the container contents explicitly or by providing instructions on how to compute them via a set of looping instructions (i.e. a comprehension). A comprehension is semantically equivalent to using a for loop and appending to an ongoing list. Comprehensions implicitly create their own scope to make sure that the items of the target list do not leak into the enclosing scope. In the case that container items are explicitly listed, the expressions in the expression list are evaluated left-to-right. If a key is repeated in a dict_display that has a key_datum_list, the resultant dictionary uses the value from the rightmost datum in the list that uses the repeated key.", "prev_chunk_id": "chunk_391", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_393", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Primaries#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Primaries#", "content": "Primaries# primary ::= atom | attributeref | subscription | slicing | call", "prev_chunk_id": "chunk_392", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_394", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Attribute References#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Attribute References#", "content": "Attribute References# attributeref ::= primary '.' identifier The primary must evaluate to an object of a type that supports attribute references that have an attribute named identifier.", "prev_chunk_id": "chunk_393", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_395", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Subscriptions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Subscriptions#", "content": "Subscriptions# subscription ::= primary '[' expression_list ']' The primary must evaluate to an object that supports subscription. - If the primary is aList,Tuple, orstr, the expression list must evaluate to an integer or slice. - If the primary is aDict, the expression list must evaluate to an object of the same type as the key type of theDict. - If the primary is aModuleList, the expression list must be anintegerliteral. - If the primary is aModuleDict, the expression must be astringliteral.", "prev_chunk_id": "chunk_394", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_396", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Slicings#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Slicings#", "content": "Slicings# A slicing selects a range of items in a str, Tuple, List, or Tensor. Slicings may be used as expressions or targets in assignment or del statements. slicing ::= primary '[' slice_list ']' slice_list ::= slice_item (',' slice_item)* [','] slice_item ::= expression | proper_slice proper_slice ::= [expression] ':' [expression] [':' [expression] ] Slicings with more than one slice item in their slice lists can only be used with primaries that evaluate to an object of type Tensor.", "prev_chunk_id": "chunk_395", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_397", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Calls#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Calls#", "content": "Calls# call ::= primary '(' argument_list ')' argument_list ::= args [',' kwargs] | kwargs args ::= [arg (',' arg)*] kwargs ::= [kwarg (',' kwarg)*] kwarg ::= arg '=' expression arg ::= identifier The primary must desugar or evaluate to a callable object. All argument expressions are evaluated before the call is attempted.", "prev_chunk_id": "chunk_396", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_398", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Power Operator#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Power Operator#", "content": "Power Operator# power ::= primary ['**' u_expr] The power operator has the same semantics as the built-in pow function (not supported); it computes its left argument raised to the power of its right argument. It binds more tightly than unary operators on the left, but less tightly than unary operators on the right; i.e. -2 ** -3 == -(2 ** (-3)). The left and right operands can be int, float or Tensor. Scalars are broadcast in the case of scalar-tensor/tensor-scalar exponentiation operations, and tensor-tensor exponentiation is done elementwise without any broadcasting.", "prev_chunk_id": "chunk_397", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_399", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Unary and Arithmetic Bitwise Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Unary and Arithmetic Bitwise Operations#", "content": "Unary and Arithmetic Bitwise Operations# u_expr ::= power | '-' power | '~' power The unary - operator yields the negation of its argument. The unary ~ operator yields the bitwise inversion of its argument. - can be used with int, float, and Tensor of int and float. ~ can only be used with int and Tensor of int.", "prev_chunk_id": "chunk_398", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_400", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Binary Arithmetic Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Binary Arithmetic Operations#", "content": "Binary Arithmetic Operations# m_expr ::= u_expr | m_expr '*' u_expr | m_expr '@' m_expr | m_expr '//' u_expr | m_expr '/' u_expr | m_expr '%' u_expr a_expr ::= m_expr | a_expr '+' m_expr | a_expr '-' m_expr The binary arithmetic operators can operate on Tensor, int, and float. For tensor-tensor ops, both arguments must have the same shape. For scalar-tensor or tensor-scalar ops, the scalar is usually broadcast to the size of the tensor. Division ops can only accept scalars as their right-hand side argument, and do not support broadcasting. The @ operator is for matrix multiplication and only operates on Tensor arguments. The multiplication operator (*) can be used with a list and integer in order to get a result that is the original list repeated a certain number of times.", "prev_chunk_id": "chunk_399", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_401", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Shifting Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Shifting Operations#", "content": "Shifting Operations# shift_expr ::= a_expr | shift_expr ( '<<' | '>>' ) a_expr These operators accept two int arguments, two Tensor arguments, or a Tensor argument and an int or float argument. In all cases, a right shift by n is defined as floor division by pow(2, n), and a left shift by n is defined as multiplication by pow(2, n). When both arguments are Tensors, they must have the same shape. When one is a scalar and the other is a Tensor, the scalar is logically broadcast to match the size of the Tensor.", "prev_chunk_id": "chunk_400", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_402", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Binary Bitwise Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Binary Bitwise Operations#", "content": "Binary Bitwise Operations# and_expr ::= shift_expr | and_expr '&' shift_expr xor_expr ::= and_expr | xor_expr '^' and_expr or_expr ::= xor_expr | or_expr '|' xor_expr The & operator computes the bitwise AND of its arguments, the ^ the bitwise XOR, and the | the bitwise OR. Both operands must be int or Tensor, or the left operand must be Tensor and the right operand must be int. When both operands are Tensor, they must have the same shape. When the right operand is int, and the left operand is Tensor, the right operand is logically broadcast to match the shape of the Tensor.", "prev_chunk_id": "chunk_401", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_403", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Comparisons#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Comparisons#", "content": "Comparisons# comparison ::= or_expr (comp_operator or_expr)* comp_operator ::= '<' | '>' | '==' | '>=' | '<=' | '!=' | 'is' ['not'] | ['not'] 'in' A comparison yields a boolean value (True or False), or if one of the operands is a Tensor, a boolean Tensor. Comparisons can be chained arbitrarily as long as they do not yield boolean Tensors that have more than one element. a op1 b op2 c ... is equivalent to a op1 b and b op2 c and ....", "prev_chunk_id": "chunk_402", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_404", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Value Comparisons#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Value Comparisons#", "content": "Value Comparisons# The operators <, >, ==, >=, <=, and != compare the values of two objects. The two objects generally need to be of the same type, unless there is an implicit type conversion available between the objects. User-defined types can be compared if rich comparison methods (e.g., __lt__) are defined on them. Built-in type comparison works like Python: - Numbers are compared mathematically. - Strings are compared lexicographically. - lists,tuples, anddictscan be compared only to otherlists,tuples, anddictsof the same type and are compared using the comparison operator of corresponding elements.", "prev_chunk_id": "chunk_403", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_405", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Membership Test Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Membership Test Operations#", "content": "Membership Test Operations# The operators in and not in test for membership. x in s evaluates to True if x is a member of s and False otherwise. x not in s is equivalent to not x in s. This operator is supported for lists, dicts, and tuples, and can be used with user-defined types if they implement the __contains__ method.", "prev_chunk_id": "chunk_404", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_406", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Identity Comparisons#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Identity Comparisons#", "content": "Identity Comparisons# For all types except int, double, bool, and torch.device, operators is and is not test for the object’s identity; x is y is True if and only if x and y are the same object. For all other types, is is equivalent to comparing them using ==. x is not y yields the inverse of x is y.", "prev_chunk_id": "chunk_405", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_407", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Boolean Operations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Boolean Operations#", "content": "Boolean Operations# or_test ::= and_test | or_test 'or' and_test and_test ::= not_test | and_test 'and' not_test not_test ::= 'bool' '(' or_expr ')' | comparison | 'not' not_test User-defined objects can customize their conversion to bool by implementing a __bool__ method. The operator not yields True if its operand is false, False otherwise. The expression x and y first evaluates x; if it is False, its value (False) is returned; otherwise, y is evaluated and its value is returned (False or True). The expression x or y first evaluates x; if it is True, its value (True) is returned; otherwise, y is evaluated and its value is returned (False or True).", "prev_chunk_id": "chunk_406", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_408", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Conditional Expressions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Conditional Expressions#", "content": "Conditional Expressions# conditional_expression ::= or_expr ['if' or_test 'else' conditional_expression] expression ::= conditional_expression The expression x if c else y first evaluates the condition c rather than x. If c is True, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned. As with if-statements, x and y must evaluate to a value of the same type.", "prev_chunk_id": "chunk_407", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_409", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Expression Lists#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Expression Lists#", "content": "Expression Lists# expression_list ::= expression (',' expression)* [','] starred_item ::= '*' primary A starred item can only appear on the left-hand side of an assignment statement, e.g., a, *b, c = ....", "prev_chunk_id": "chunk_408", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_410", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Simple Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Simple Statements#", "content": "Simple Statements# The following section describes the syntax of simple statements that are supported in TorchScript. It is modeled after the simple statements chapter of the Python language reference.", "prev_chunk_id": "chunk_409", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_411", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Expression Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Expression Statements#", "content": "Expression Statements# expression_stmt ::= starred_expression starred_expression ::= expression | (starred_item \",\")* [starred_item] starred_item ::= assignment_expression | \"*\" or_expr", "prev_chunk_id": "chunk_410", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_412", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Assignment Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Assignment Statements#", "content": "Assignment Statements# assignment_stmt ::= (target_list \"=\")+ (starred_expression) target_list ::= target (\",\" target)* [\",\"] target ::= identifier | \"(\" [target_list] \")\" | \"[\" [target_list] \"]\" | attributeref | subscription | slicing | \"*\" target", "prev_chunk_id": "chunk_411", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_413", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Augmented Assignment Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Augmented Assignment Statements#", "content": "Augmented Assignment Statements# augmented_assignment_stmt ::= augtarget augop (expression_list) augtarget ::= identifier | attributeref | subscription augop ::= \"+=\" | \"-=\" | \"*=\" | \"/=\" | \"//=\" | \"%=\" | \"**=\"| \">>=\" | \"<<=\" | \"&=\" | \"^=\" | \"|=\"", "prev_chunk_id": "chunk_412", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_414", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Annotated Assignment Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Annotated Assignment Statements#", "content": "Annotated Assignment Statements# annotated_assignment_stmt ::= augtarget \":\" expression [\"=\" (starred_expression)]", "prev_chunk_id": "chunk_413", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_415", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The raise Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The raise Statement#", "content": "The raise Statement# raise_stmt ::= \"raise\" [expression [\"from\" expression]] Raise statements in TorchScript do not support try\\except\\finally.", "prev_chunk_id": "chunk_414", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_416", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The assert Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The assert Statement#", "content": "The assert Statement# assert_stmt ::= \"assert\" expression [\",\" expression] Assert statements in TorchScript do not support try\\except\\finally.", "prev_chunk_id": "chunk_415", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_417", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The return Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The return Statement#", "content": "The return Statement# return_stmt ::= \"return\" [expression_list] Return statements in TorchScript do not support try\\except\\finally.", "prev_chunk_id": "chunk_416", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_418", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The del Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The del Statement#", "content": "The del Statement# del_stmt ::= \"del\" target_list", "prev_chunk_id": "chunk_417", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_419", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The pass Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The pass Statement#", "content": "The pass Statement# pass_stmt ::= \"pass\"", "prev_chunk_id": "chunk_418", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_420", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The print Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The print Statement#", "content": "The print Statement# print_stmt ::= \"print\" \"(\" expression [, expression] [.format{expression_list}] \")\"", "prev_chunk_id": "chunk_419", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_421", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The break Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The break Statement#", "content": "The break Statement# break_stmt ::= \"break\"", "prev_chunk_id": "chunk_420", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_422", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The continue Statement:#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The continue Statement:#", "content": "The continue Statement:# continue_stmt ::= \"continue\"", "prev_chunk_id": "chunk_421", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_423", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Compound Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Compound Statements#", "content": "Compound Statements# The following section describes the syntax of compound statements that are supported in TorchScript. The section also highlights how Torchscript differs from regular Python statements. It is modeled after the compound statements chapter of the Python language reference.", "prev_chunk_id": "chunk_422", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_424", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The if Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The if Statement#", "content": "The if Statement# Torchscript supports both basic if/else and ternary if/else.", "prev_chunk_id": "chunk_423", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_425", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Basic if/else Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Basic if/else Statement#", "content": "Basic if/else Statement# if_stmt ::= \"if\" assignment_expression \":\" suite (\"elif\" assignment_expression \":\" suite) [\"else\" \":\" suite] elif statements can repeat for an arbitrary number of times, but it needs to be before else statement.", "prev_chunk_id": "chunk_424", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_426", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Ternary if/else Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Ternary if/else Statement#", "content": "Ternary if/else Statement# if_stmt ::= return [expression_list] \"if\" assignment_expression \"else\" [expression_list] Example 1 A tensor with 1 dimension is promoted to bool: import torch @torch.jit.script def fn(x: torch.Tensor): if x: # The tensor gets promoted to bool return True return False print(fn(torch.rand(1))) The example above produces the following output: True Example 2 A tensor with multi dimensions are not promoted to bool: import torch # Multi dimensional Tensors error out. @torch.jit.script def fn(): if torch.rand(2): print(\"Tensor is available\") if torch.rand(4,5,6): print(\"Tensor is available\") print(fn()) Running the above code yields the following RuntimeError. RuntimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): @torch.jit.script def fn(): if torch.rand(2): ~~~~~~~~~~~~ <--- HERE print(\"Tensor is available\") RuntimeError: Boolean value of Tensor with more than one value is ambiguous If a conditional variable is annotated as final, either the true or false branch is evaluated depending on the evaluation of the conditional variable. Example 3 In this example, only the True branch is evaluated, since a is annotated as final and set to True: import torch a : torch.jit.final[Bool] = True if a: return torch.empty(2,3) else: return []", "prev_chunk_id": "chunk_425", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_427", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The while Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The while Statement#", "content": "The while Statement# while_stmt ::= \"while\" assignment_expression \":\" suite while...else statements are not supported in Torchscript. It results in a RuntimeError.", "prev_chunk_id": "chunk_426", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_428", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The for-in Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The for-in Statement#", "content": "The for-in Statement# for_stmt ::= \"for\" target_list \"in\" expression_list \":\" suite [\"else\" \":\" suite] for...else statements are not supported in Torchscript. It results in a RuntimeError. Example 1 For loops on tuples: these unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member. import torch from typing import Tuple @torch.jit.script def fn(): tup = (3, torch.ones(4)) for x in tup: print(x) fn() The example above produces the following output: 3 1 1 1 1 [ CPUFloatType{4} ] Example 2 For loops on lists: for loops over a nn.ModuleList will unroll the body of the loop at compile time, with each member of the module list. class SubModule(torch.nn.Module): def __init__(self): super().__init__() self.weight = nn.Parameter(torch.randn(2)) def forward(self, input): return self.weight + input class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.mods = torch.nn.ModuleList([SubModule() for i in range(10)]) def forward(self, v): for module in self.mods: v = module(v) return v model = torch.jit.script(MyModule())", "prev_chunk_id": "chunk_427", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_429", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The with Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The with Statement#", "content": "The with Statement# The with statement is used to wrap the execution of a block with methods defined by a context manager. with_stmt ::= \"with\" with_item (\",\" with_item) \":\" suite with_item ::= expression [\"as\" target] - If a target was included in thewithstatement, the return value from the context manager’s__enter__()is assigned to it. Unlike python, if an exception caused the suite to be exited, its type, value, and traceback are not passed as arguments to__exit__(). ThreeNonearguments are supplied. - try,except, andfinallystatements are not supported insidewithblocks. - Exceptions raised withinwithblock cannot be suppressed.", "prev_chunk_id": "chunk_428", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_430", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The tuple Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The tuple Statement#", "content": "The tuple Statement# tuple_stmt ::= tuple([iterables]) - Iterable types in TorchScript includeTensors,lists,tuples,dictionaries,strings,torch.nn.ModuleList, andtorch.nn.ModuleDict. - You cannot convert a List to Tuple by using this built-in function. Unpacking all outputs into a tuple is covered by: abc = func() # Function that returns a tuple a,b = func()", "prev_chunk_id": "chunk_429", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_431", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The getattr Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The getattr Statement#", "content": "The getattr Statement# getattr_stmt ::= getattr(object, name[, default]) - Attribute name must be a literal string. - Module type object is not supported (e.g., torch._C). - Custom class object is not supported (e.g., torch.classes.*).", "prev_chunk_id": "chunk_430", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_432", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The hasattr Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The hasattr Statement#", "content": "The hasattr Statement# hasattr_stmt ::= hasattr(object, name) - Attribute name must be a literal string. - Module type object is not supported (e.g., torch._C). - Custom class object is not supported (e.g., torch.classes.*).", "prev_chunk_id": "chunk_431", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_433", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The zip Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The zip Statement#", "content": "The zip Statement# zip_stmt ::= zip(iterable1, iterable2) - Arguments must be iterables. - Two iterables of same outer container type but different length are supported. Example 1 Both the iterables must be of the same container type: a = [1, 2] # List b = [2, 3, 4] # List zip(a, b) # works Example 2 This example fails because the iterables are of different container types: a = (1, 2) # Tuple b = [2, 3, 4] # List zip(a, b) # Runtime error Running the above code yields the following RuntimeError. RuntimeError: Can not iterate over a module list or tuple with a value that does not have a statically determinable length. Example 3 Two iterables of the same container Type but different data type is supported: a = [1.3, 2.4] b = [2, 3, 4] zip(a, b) # Works Iterable types in TorchScript include Tensors, lists, tuples, dictionaries, strings, torch.nn.ModuleList, and torch.nn.ModuleDict.", "prev_chunk_id": "chunk_432", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_434", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "The enumerate Statement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "The enumerate Statement#", "content": "The enumerate Statement# enumerate_stmt ::= enumerate([iterable]) - Arguments must be iterables. - Iterable types in TorchScript includeTensors,lists,tuples,dictionaries,strings,torch.nn.ModuleListandtorch.nn.ModuleDict.", "prev_chunk_id": "chunk_433", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_435", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Resolution Rules#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Resolution Rules#", "content": "Resolution Rules# When given a Python value, TorchScript attempts to resolve it in the following five different ways: - Compilable Python Implementation:When a Python value is backed by a Python implementation that can be compiled by TorchScript, TorchScript compiles and uses the underlying Python implementation.Example:torch.jit.Attribute - Op Python Wrapper:When a Python value is a wrapper of a native PyTorch op, TorchScript emits the corresponding operator.Example:torch.jit._logging.add_stat_value - Python Object Identity Match:For a limited set oftorch.*API calls (in the form of Python values) that TorchScript supports, TorchScript attempts to match a Python value against each item in the set.When matched, TorchScript generates a correspondingSugaredValueinstance that contains lowering logic for these values.Example:torch.jit.isinstance() - Name Match:For Python built-in functions and constants, TorchScript identifies them by name, and creates a correspondingSugaredValueinstance that implements their functionality.Example:all() - Value Snapshot:For Python values from unrecognized modules, TorchScript attempts to take a snapshot of the value and converts it to a constant in the graph of the function(s) or method(s) that are being compiled.Example:math.pi", "prev_chunk_id": "chunk_434", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_436", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Remote Procedure Calls#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Remote Procedure Calls#", "content": "Remote Procedure Calls# TorchScript supports a subset of RPC APIs that supports running a function on a specified remote worker instead of locally. Specifically, following APIs are fully supported: - torch.distributed.rpc.rpc_sync()rpc_sync()makes a blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.More details about its usage and examples can be found inrpc_sync(). - torch.distributed.rpc.rpc_async()rpc_async()makes a non-blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.More details about its usage and examples can be found inrpc_async(). - torch.distributed.rpc.remote()remote.()executes a remote call on a worker and gets a Remote ReferenceRRefas the return value.More details about its usage and examples can be found inremote().", "prev_chunk_id": "chunk_435", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_437", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Asynchronous Execution#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Asynchronous Execution#", "content": "Asynchronous Execution# TorchScript enables you to create asynchronous computation tasks to make better use of computation resources. This is done via supporting a list of APIs that are only usable within TorchScript: - torch.jit.fork()Creates an asynchronous task executing func and a reference to the value of the result of this execution. Fork will return immediately.Synonymous totorch.jit._fork(), which is only kept for backward compatibility reasons.More details about its usage and examples can be found infork(). - torch.jit.wait()Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.Synonymous totorch.jit._wait(), which is only kept for backward compatibility reasons.More details about its usage and examples can be found inwait().", "prev_chunk_id": "chunk_436", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_438", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Type Annotations#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Type Annotations#", "content": "Type Annotations# TorchScript is statically-typed. It provides and supports a set of utilities to help annotate variables and attributes: - torch.jit.annotate()Provides a type hint to TorchScript where Python 3 style type hints do not work well.One common example is to annotate type for expressions like[].[]is treated asList[torch.Tensor]by default. When a different type is needed, you can use this code to hint TorchScript:torch.jit.annotate(List[int],[]).More details can be found inannotate() - torch.jit.AttributeCommon use cases include providing type hint fortorch.nn.Moduleattributes. Because their__init__methods are not parsed by TorchScript,torch.jit.Attributeshould be used instead oftorch.jit.annotatein the module’s__init__methods.More details can be found inAttribute() - torch.jit.FinalAn alias for Python’styping.Final.torch.jit.Finalis kept only for backward compatibility reasons.", "prev_chunk_id": "chunk_437", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_439", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Meta Programming#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Meta Programming#", "content": "Meta Programming# TorchScript provides a set of utilities to facilitate meta programming: - torch.jit.is_scripting()Returns a boolean value indicating whether the current program is compiled bytorch.jit.scriptor not.When used in anassertor anifstatement, the scope or branch wheretorch.jit.is_scripting()evaluates toFalseis not compiled.Its value can be evaluated statically at compile time, thus commonly used inifstatements to stop TorchScript from compiling one of the branches.More details and examples can be found inis_scripting() - torch.jit.is_tracing()Returns a boolean value indicating whether the current program is traced bytorch.jit.trace/torch.jit.trace_moduleor not.More details can be found inis_tracing() - @torch.jit.ignoreThis decorator indicates to the compiler that a function or method should be ignored and left as a Python function.This allows you to leave code in your model that is not yet TorchScript compatible.If a function decorated by@torch.jit.ignoreis called from TorchScript, ignored functions will dispatch the call to the Python interpreter.Models with ignored functions cannot be exported.More details and examples can be found inignore() - @torch.jit.unusedThis decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.If a function decorated by@torch.jit.unusedis called from TorchScript, a runtime error will be raised.More details and examples can be found inunused()", "prev_chunk_id": "chunk_438", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_440", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference_v2.html", "title": "Type Refinement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Type Refinement#", "content": "Type Refinement# - torch.jit.isinstance()Returns a boolean indicating whether a variable is of the specified type.More details about its usage and examples can be found inisinstance().", "prev_chunk_id": "chunk_439", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_441", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "TorchScript Language Reference#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Language Reference#", "content": "TorchScript Language Reference# Created On: Dec 18, 2019 | Last Updated On: Jun 13, 2025 TorchScript is a statically typed subset of Python that can either be written directly (using the @torch.jit.script decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code. When writing TorchScript directly using @torch.jit.script decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See Builtin Functions for a complete reference of available PyTorch tensor methods, modules, and functions. As a subset of Python, any valid TorchScript function is also a valid Python function. This makes it possible to disable TorchScript and debug the function using standard Python tools like pdb. The reverse is not true: there are many valid Python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_442", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Types#", "content": "Types# The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports: Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions. Example (a type mismatch) import torch @torch.jit.script def an_error(x): if x: r = torch.rand(1) else: r = 4 return r Traceback (most recent call last): ... RuntimeError: ... Type mismatch: r is set to type Tensor in the true branch and type int in the false branch: @torch.jit.script def an_error(x): if x: ~~~~~ r = torch.rand(1) ~~~~~~~~~~~~~~~~~ else: ~~~~~ r = 4 ~~~~~ <--- HERE return r and was used here: else: r = 4 return r ~ <--- HERE...", "prev_chunk_id": "chunk_441", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_443", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Unsupported Typing Constructs#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Unsupported Typing Constructs#", "content": "Unsupported Typing Constructs# TorchScript does not support all features and types of the typing module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority. These types and features from the typing module are unavailable in TorchScript. Any other functionality from the typing module not explicitly listed in this documentation is unsupported.", "prev_chunk_id": "chunk_442", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_444", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Default Types#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Default Types#", "content": "Default Types# By default, all parameters to a TorchScript function are assumed to be Tensor. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above. import torch @torch.jit.script def foo(x, tup): # type: (int, Tuple[Tensor, Tensor]) -> Tensor t0, t1 = tup return t0 + t1 + x print(foo(3, (torch.rand(3), torch.rand(3)))) An empty list is assumed to be List[Tensor] and empty dicts Dict[str, Tensor]. To instantiate an empty list or dict of other types, use Python 3 type hints. Example (type annotations for Python 3): import torch import torch.nn as nn from typing import Dict, List, Tuple class EmptyDataStructures(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> Tuple[List[Tuple[int, float]], Dict[str, int]]: # This annotates the list to be a `List[Tuple[int, float]]` my_list: List[Tuple[int, float]] = [] for i in range(10): my_list.append((i, x.item())) my_dict: Dict[str, int] = {} return my_list, my_dict x = torch.jit.script(EmptyDataStructures())", "prev_chunk_id": "chunk_443", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_445", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Optional Type Refinement#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Optional Type Refinement#", "content": "Optional Type Refinement# TorchScript will refine the type of a variable of type Optional[T] when a comparison to None is made inside the conditional of an if-statement or checked in an assert. The compiler can reason about multiple None checks that are combined with and, or, and not. Refinement will also occur for else blocks of if-statements that are not explicitly written. The None check must be within the if-statement’s condition; assigning a None check to a variable and using it in the if-statement’s condition will not refine the types of variables in the check. Only local variables will be refined, an attribute like self.x will not and must assigned to a local variable to be refined. Example (refining types on parameters and locals): import torch import torch.nn as nn from typing import Optional class M(nn.Module): z: Optional[int] def __init__(self, z): super().__init__() # If `z` is None, its type cannot be inferred, so it must # be specified (above) self.z = z def forward(self, x, y, z): # type: (Optional[int], Optional[int], Optional[int]) -> int if x is None: x = 1 x = x + 1 # Refinement for an attribute by assigning it to a local z = self.z if y is not None and z is not None: x = y + z # Refinement via an `assert` assert z is not None x += z return x module = torch.jit.script(M(2)) module = torch.jit.script(M(None))", "prev_chunk_id": "chunk_444", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_446", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "TorchScript Classes#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Classes#", "content": "TorchScript Classes# Python classes can be used in TorchScript if they are annotated with @torch.jit.script, similar to how you would declare a TorchScript function: @torch.jit.script class Foo: def __init__(self, x, y): self.x = x def aug_add_x(self, inc): self.x += inc This subset is restricted: - All functions must be valid TorchScript functions (including__init__()). - Classes must be new-style classes, as we use__new__()to construct them with pybind11. - TorchScript classes are statically typed. Members can only be declared by assigning to self in the__init__()method.For example, assigning toselfoutside of the__init__()method:@torch.jit.scriptclassFoo:defassign_x(self):self.x=torch.rand(2,3)Will result in:RuntimeError: Tried to set nonexistent attribute: x. Did you forget to initialize it in __init__()?: def assign_x(self): self.x = torch.rand(2, 3) ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE - No expressions except method definitions are allowed in the body of the class. - No support for inheritance or any other polymorphism strategy, except for inheriting fromobjectto specify a new-style class. After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type: # Declare a TorchScript class @torch.jit.script class Pair: def __init__(self, first, second): self.first = first self.second = second @torch.jit.script def sum_pair(p): # type: (Pair) -> Tensor return p.first + p.second p = Pair(torch.rand(2, 3), torch.rand(2, 3)) print(sum_pair(p))", "prev_chunk_id": "chunk_445", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_447", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "TorchScript Enums#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Enums#", "content": "TorchScript Enums# Python enums can be used in TorchScript without any extra annotation or code: from enum import Enum class Color(Enum): RED = 1 GREEN = 2 @torch.jit.script def enum_fn(x: Color, y: Color) -> bool: if x == Color.RED: return True return x == y After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be int, float, or str. All values must be of the same type; heterogeneous types for enum values are not supported.", "prev_chunk_id": "chunk_446", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_448", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Named Tuples#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Named Tuples#", "content": "Named Tuples# Types produced by collections.namedtuple can be used in TorchScript. import torch import collections Point = collections.namedtuple('Point', ['x', 'y']) @torch.jit.script def total(point): # type: (Point) -> Tensor return point.x + point.y p = Point(x=torch.rand(3), y=torch.rand(3)) print(total(p))", "prev_chunk_id": "chunk_447", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_449", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Iterables#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Iterables#", "content": "Iterables# Some functions (for example, zip and enumerate) can only operate on iterable types. Iterable types in TorchScript include Tensors, lists, tuples, dictionaries, strings, torch.nn.ModuleList and torch.nn.ModuleDict.", "prev_chunk_id": "chunk_448", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_450", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Expressions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Expressions#", "content": "Expressions# The following Python Expressions are supported.", "prev_chunk_id": "chunk_449", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_451", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Literals#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Literals#", "content": "Literals# True False None 'string literals' \"string literals\" 3 # interpreted as int 3.4 # interpreted as a float", "prev_chunk_id": "chunk_450", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_452", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "List Construction#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "List Construction#", "content": "List Construction# An empty list is assumed have type List[Tensor]. The types of other list literals are derived from the type of the members. See [Default Types] for more details. [3, 4] [] [torch.rand(3), torch.rand(4)]", "prev_chunk_id": "chunk_451", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_453", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Tuple Construction#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Tuple Construction#", "content": "Tuple Construction# (3, 4) (3,)", "prev_chunk_id": "chunk_452", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_454", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Dict Construction#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Dict Construction#", "content": "Dict Construction# An empty dict is assumed have type Dict[str, Tensor]. The types of other dict literals are derived from the type of the members. See [Default Types] for more details. {'hello': 3} {} {'a': torch.rand(3), 'b': torch.rand(4)}", "prev_chunk_id": "chunk_453", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_455", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Variables#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Variables#", "content": "Variables# See [Variable Resolution] for how variables are resolved. my_variable_name", "prev_chunk_id": "chunk_454", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_456", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Arithmetic Operators#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Arithmetic Operators#", "content": "Arithmetic Operators# a + b a - b a * b a / b a ^ b a @ b", "prev_chunk_id": "chunk_455", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_457", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Comparison Operators#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Comparison Operators#", "content": "Comparison Operators# a == b a != b a < b a > b a <= b a >= b", "prev_chunk_id": "chunk_456", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_458", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Logical Operators#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Logical Operators#", "content": "Logical Operators# a and b a or b not b", "prev_chunk_id": "chunk_457", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_459", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Subscripts and Slicing#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Subscripts and Slicing#", "content": "Subscripts and Slicing# t[0] t[-1] t[0:2] t[1:] t[:1] t[:] t[0, 1] t[0, 1:2] t[0, :1] t[-1, 1:, 0] t[1:, -1, 0] t[i:j, i]", "prev_chunk_id": "chunk_458", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_460", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Function Calls#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Function Calls#", "content": "Function Calls# Calls to builtin functions torch.rand(3, dtype=torch.int) Calls to other script functions: import torch @torch.jit.script def foo(x): return x + 1 @torch.jit.script def bar(x): return foo(x)", "prev_chunk_id": "chunk_459", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_461", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Method Calls#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Method Calls#", "content": "Method Calls# Calls to methods of builtin types like tensor: x.mm(y) On modules, methods must be compiled before they can be called. The TorchScript compiler recursively compiles methods it sees when compiling other methods. By default, compilation starts on the forward method. Any methods called by forward will be compiled, and any methods called by those methods, and so on. To start compilation at a method other than forward, use the @torch.jit.export decorator (forward implicitly is marked @torch.jit.export). Calling a submodule directly (e.g. self.resnet(input)) is equivalent to calling its forward method (e.g. self.resnet.forward(input)). import torch import torch.nn as nn import torchvision class MyModule(nn.Module): def __init__(self): super().__init__() means = torch.tensor([103.939, 116.779, 123.68]) self.means = torch.nn.Parameter(means.resize_(1, 3, 1, 1)) resnet = torchvision.models.resnet18() self.resnet = torch.jit.trace(resnet, torch.rand(1, 3, 224, 224)) def helper(self, input): return self.resnet(input - self.means) def forward(self, input): return self.helper(input) # Since nothing in the model calls `top_level_method`, the compiler # must be explicitly told to compile this method @torch.jit.export def top_level_method(self, input): return self.other_helper(input) def other_helper(self, input): return input + 10 # `my_script_module` will have the compiled methods `forward`, `helper`, # `top_level_method`, and `other_helper` my_script_module = torch.jit.script(MyModule())", "prev_chunk_id": "chunk_460", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_462", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Ternary Expressions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Ternary Expressions#", "content": "Ternary Expressions# x if x > y else y", "prev_chunk_id": "chunk_461", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_463", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Casts#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Casts#", "content": "Casts# float(ten) int(3.5) bool(ten) str(2)``", "prev_chunk_id": "chunk_462", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_464", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Accessing Module Parameters#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Accessing Module Parameters#", "content": "Accessing Module Parameters# self.my_parameter self.my_submodule.my_parameter", "prev_chunk_id": "chunk_463", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_465", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Statements#", "content": "Statements# TorchScript supports the following types of statements:", "prev_chunk_id": "chunk_464", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_466", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Simple Assignments#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Simple Assignments#", "content": "Simple Assignments# a = b a += b # short-hand for a = a + b, does not operate in-place on a a -= b", "prev_chunk_id": "chunk_465", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_467", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Pattern Matching Assignments#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Pattern Matching Assignments#", "content": "Pattern Matching Assignments# a, b = tuple_or_list a, b, *c = a_tuple Multiple Assignments a = b, c = tup", "prev_chunk_id": "chunk_466", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_468", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Print Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Print Statements#", "content": "Print Statements# print(\"the result of an add:\", a + b)", "prev_chunk_id": "chunk_467", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_469", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "If Statements#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "If Statements#", "content": "If Statements# if a < 4: r = -a elif a < 3: r = a + a else: r = 3 * a In addition to bools, floats, ints, and Tensors can be used in a conditional and will be implicitly casted to a boolean.", "prev_chunk_id": "chunk_468", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_470", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "While Loops#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "While Loops#", "content": "While Loops# a = 0 while a < 4: print(a) a += 1", "prev_chunk_id": "chunk_469", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_471", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "For loops with range#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "For loops with range#", "content": "For loops with range# x = 0 for i in range(10): x *= i", "prev_chunk_id": "chunk_470", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_472", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "For loops over tuples#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "For loops over tuples#", "content": "For loops over tuples# These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member. tup = (3, torch.rand(4)) for x in tup: print(x)", "prev_chunk_id": "chunk_471", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_473", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "For loops over constant nn.ModuleList#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "For loops over constant nn.ModuleList#", "content": "For loops over constant nn.ModuleList# To use a nn.ModuleList inside a compiled method, it must be marked constant by adding the name of the attribute to the __constants__ list for the type. For loops over a nn.ModuleList will unroll the body of the loop at compile time, with each member of the constant module list. class SubModule(torch.nn.Module): def __init__(self): super().__init__() self.weight = nn.Parameter(torch.randn(2)) def forward(self, input): return self.weight + input class MyModule(torch.nn.Module): __constants__ = ['mods'] def __init__(self): super().__init__() self.mods = torch.nn.ModuleList([SubModule() for i in range(10)]) def forward(self, v): for module in self.mods: v = module(v) return v m = torch.jit.script(MyModule())", "prev_chunk_id": "chunk_472", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_474", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Break and Continue#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Break and Continue#", "content": "Break and Continue# for i in range(5): if i == 1: continue if i == 3: break print(i)", "prev_chunk_id": "chunk_473", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_475", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Return#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Return#", "content": "Return# return a, b", "prev_chunk_id": "chunk_474", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_476", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Variable Resolution#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Variable Resolution#", "content": "Variable Resolution# TorchScript supports a subset of Python’s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement. Similarly, a variable is not allowed to be used if it is only defined along some paths through the function. Example: @torch.jit.script def foo(x): if x < 0: y = 4 print(y) Traceback (most recent call last): ... RuntimeError: ... y is not defined in the false branch... @torch.jit.script... def foo(x): if x < 0: ~~~~~~~~~ y = 4 ~~~~~ <--- HERE print(y) and was used here: if x < 0: y = 4 print(y) ~ <--- HERE... Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in [Use of Python Values].", "prev_chunk_id": "chunk_475", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_477", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Use of Python Values#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Use of Python Values#", "content": "Use of Python Values# To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to torch, the TorchScript compiler is actually resolving it to the torch Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.", "prev_chunk_id": "chunk_476", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_478", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Functions#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Functions#", "content": "Functions# TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.", "prev_chunk_id": "chunk_477", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_479", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Attribute Lookup On Python Modules#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Attribute Lookup On Python Modules#", "content": "Attribute Lookup On Python Modules# TorchScript can lookup attributes on modules. Builtin functions like torch.add are accessed this way. This allows TorchScript to call functions defined in other modules.", "prev_chunk_id": "chunk_478", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_480", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Python-defined Constants#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Python-defined Constants#", "content": "Python-defined Constants# TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant. - Values looked up as attributes of a module are assumed to be constant: import math import torch @torch.jit.script def fn(): return math.pi - Attributes of a ScriptModule can be marked constant by annotating them withFinal[T] import torch import torch.nn as nn class Foo(nn.Module): # `Final` from the `typing_extensions` module can also be used a : torch.jit.Final[int] def __init__(self): super().__init__() self.a = 1 + 4 def forward(self, input): return self.a + input f = torch.jit.script(Foo()) Supported constant Python types are - int - float - bool - torch.device - torch.layout - torch.dtype - tuples containing supported types - torch.nn.ModuleListwhich can be used in a TorchScript for loop", "prev_chunk_id": "chunk_479", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_481", "url": "https://docs.pytorch.org/docs/stable/jit_language_reference.html", "title": "Module Attributes#", "page_title": "TorchScript Language Reference — PyTorch 2.8 documentation", "breadcrumbs": "Module Attributes#", "content": "Module Attributes# The torch.nn.Parameter wrapper and register_buffer can be used to assign tensors to a module. Other values assigned to a module that is compiled will be added to the compiled module if their types can be inferred. All [types] available in TorchScript can be used as module attributes. Tensor attributes are semantically the same as buffers. The type of empty lists and dictionaries and None values cannot be inferred and must be specified via PEP 526-style class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting ScriptModule. Example: from typing import List, Dict class Foo(nn.Module): # `words` is initialized as an empty list, so its type must be specified words: List[str] # The type could potentially be inferred if `a_dict` (below) was not # empty, but this annotation ensures `some_dict` will be made into the # proper type some_dict: Dict[str, int] def __init__(self, a_dict): super().__init__() self.words = [] self.some_dict = a_dict # `int`s can be inferred self.my_int = 10 def forward(self, input): # type: (str) -> int self.words.append(input) return self.some_dict[input] + self.my_int f = torch.jit.script(Foo({'hi': 2}))", "prev_chunk_id": "chunk_480", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_482", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtins#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtins#", "content": "TorchScript Builtins# Created On: Aug 09, 2019 | Last Updated On: Mar 30, 2021 This is a full reference of functions and Tensor methods accessible in TorchScript", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_483", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Supported Tensor Methods# Tensor.__and__(other : number) -> Tensor Tensor.__and__(other : Tensor) -> Tensor Tensor.__iand__(other : Tensor) -> Tensor Tensor.__iand__(other : number) -> Tensor Tensor.__ilshift__(other : Tensor) -> Tensor Tensor.__ilshift__(other : number) -> Tensor Tensor.__ior__(other : Tensor) -> Tensor Tensor.__ior__(other : number) -> Tensor Tensor.__irshift__(other : Tensor) -> Tensor Tensor.__irshift__(other : number) -> Tensor Tensor.__ixor__(other : Tensor) -> Tensor Tensor.__ixor__(other : number) -> Tensor Tensor.__lshift__(other : Tensor) -> Tensor Tensor.__lshift__(other : number) -> Tensor Tensor.__lshift__(other : number, out : Tensor) -> Tensor Tensor.__lshift__(other : Tensor, out : Tensor) -> Tensor Tensor.__or__(other : Tensor) -> Tensor Tensor.__or__(other : number) -> Tensor Tensor.__rshift__(other : Tensor) -> Tensor Tensor.__rshift__(other : number) -> Tensor Tensor.__rshift__(other : number, out : Tensor) -> Tensor Tensor.__rshift__(other : Tensor, out : Tensor) -> Tensor Tensor.__xor__(other : Tensor) -> Tensor Tensor.__xor__(other : number) -> Tensor Tensor.abs() -> Tensor Tensor.abs(out : Tensor) -> Tensor Tensor.abs_() -> Tensor Tensor.absolute() -> Tensor Tensor.absolute(out : Tensor) -> Tensor Tensor.absolute_() -> Tensor Tensor.acos() -> Tensor Tensor.acos(out : Tensor) -> Tensor Tensor.acos_() -> Tensor Tensor.acosh() -> Tensor Tensor.acosh(out : Tensor) -> Tensor Tensor.acosh_() -> Tensor Tensor.add(other : Tensor, alpha : number=1) -> Tensor Tensor.add(other : number, alpha : number=1) -> Tensor Tensor.add(other : Tensor, alpha : number=1, out : Tensor) -> Tensor Tensor.add(other : number, alpha : number=1, out : Tensor) -> Tensor Tensor.add_(other : Tensor, alpha : number=1) -> Tensor Tensor.add_(other : number, alpha : number=1) -> Tensor Tensor.addbmm(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addbmm(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.addbmm_(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addcdiv(tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor Tensor.addcdiv(tensor1 : Tensor, tensor2 : Tensor, value : number=1, out :", "prev_chunk_id": "chunk_482", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_484", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor) -> Tensor Tensor.addcdiv_(tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor Tensor.addcmul(tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor Tensor.addcmul(tensor1 : Tensor, tensor2 : Tensor, value : number=1, out : Tensor) -> Tensor Tensor.addcmul_(tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor Tensor.addmm(mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addmm(mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.addmm(mat1 : Tensor, mat2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.addmm(mat1 : Tensor, mat2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1) -> Tensor Tensor.addmm_(mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addmv(mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addmv(mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.addmv_(mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addr(vec1 : Tensor, vec2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.addr(vec1 : Tensor, vec2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.addr_(vec1 : Tensor, vec2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.adjoint() -> Tensor Tensor.align_as(other : Tensor) -> Tensor Tensor.align_to(names : List[str]) -> Tensor Tensor.align_to(order : List[str], ellipsis_idx : int) -> Tensor Tensor.all() -> Tensor Tensor.all(dim : int, keepdim : bool=False) -> Tensor Tensor.all(dim : Optional[List[int]], keepdim : bool=False) -> Tensor Tensor.all(dim : int, keepdim : bool=False, out : Tensor) -> Tensor Tensor.all(dim : Optional[List[int]], keepdim : bool=False, out : Tensor) -> Tensor Tensor.all(out : Tensor) -> Tensor Tensor.all(dim : str, keepdim : bool=False) -> Tensor Tensor.all(dim : str,", "prev_chunk_id": "chunk_483", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_485", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "keepdim : bool=False, out : Tensor) -> Tensor Tensor.allclose(other : Tensor, rtol : float=1e-05, atol : float=1e-08, equal_nan : bool=False) -> bool Tensor.amax(dim : List[int]=[], keepdim : bool=False) -> Tensor Tensor.amax(dim : List[int]=[], keepdim : bool=False, out : Tensor) -> Tensor Tensor.amin(dim : List[int]=[], keepdim : bool=False) -> Tensor Tensor.amin(dim : List[int]=[], keepdim : bool=False, out : Tensor) -> Tensor Tensor.aminmax(dim : Optional[int], keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.aminmax(dim : Optional[int], keepdim : bool=False, min : Tensor, max : Tensor) -> Tuple[Tensor, Tensor] Tensor.angle() -> Tensor Tensor.angle(out : Tensor) -> Tensor Tensor.any() -> Tensor Tensor.any(dim : int, keepdim : bool=False) -> Tensor Tensor.any(dim : Optional[List[int]], keepdim : bool=False) -> Tensor Tensor.any(dim : int, keepdim : bool=False, out : Tensor) -> Tensor Tensor.any(dim : Optional[List[int]], keepdim : bool=False, out : Tensor) -> Tensor Tensor.any(out : Tensor) -> Tensor Tensor.any(dim : str, keepdim : bool=False) -> Tensor Tensor.any(dim : str, keepdim : bool=False, out : Tensor) -> Tensor Tensor.arccos() -> Tensor Tensor.arccos(out : Tensor) -> Tensor Tensor.arccos_() -> Tensor Tensor.arccosh() -> Tensor Tensor.arccosh(out : Tensor) -> Tensor Tensor.arccosh_() -> Tensor Tensor.arcsin() -> Tensor Tensor.arcsin(out : Tensor) -> Tensor Tensor.arcsin_() -> Tensor Tensor.arcsinh() -> Tensor Tensor.arcsinh(out : Tensor) -> Tensor Tensor.arcsinh_() -> Tensor Tensor.arctan() -> Tensor Tensor.arctan(out : Tensor) -> Tensor Tensor.arctan2(other : Tensor) -> Tensor Tensor.arctan2(other : Tensor, out : Tensor) -> Tensor Tensor.arctan2_(other : Tensor) -> Tensor Tensor.arctan_() -> Tensor Tensor.arctanh() -> Tensor Tensor.arctanh(out : Tensor) -> Tensor Tensor.arctanh_() -> Tensor Tensor.argmax(dim : Optional[int], keepdim : bool=False) -> Tensor Tensor.argmax(dim : Optional[int], keepdim : bool=False, out : Tensor) -> Tensor Tensor.argmin(dim : Optional[int], keepdim : bool=False) -> Tensor Tensor.argmin(dim : Optional[int], keepdim : bool=False, out : Tensor) -> Tensor Tensor.argsort(dim : int=-1, descending : bool=False) -> Tensor Tensor.argsort(stable : bool, dim : int=-1, descending : bool=False) -> Tensor Tensor.argsort(stable :", "prev_chunk_id": "chunk_484", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_486", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "bool, dim : int=-1, descending : bool=False, out : Tensor) -> Tensor Tensor.argsort(dim : str, descending : bool=False) -> Tensor Tensor.argwhere() -> Tensor Tensor.as_strided(size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor Tensor.as_strided_(size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor Tensor.as_strided_scatter(src : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor Tensor.as_strided_scatter(src : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int], out : Tensor) -> Tensor Tensor.asin() -> Tensor Tensor.asin(out : Tensor) -> Tensor Tensor.asin_() -> Tensor Tensor.asinh() -> Tensor Tensor.asinh(out : Tensor) -> Tensor Tensor.asinh_() -> Tensor Tensor.atan() -> Tensor Tensor.atan(out : Tensor) -> Tensor Tensor.atan2(other : Tensor) -> Tensor Tensor.atan2(other : Tensor, out : Tensor) -> Tensor Tensor.atan2_(other : Tensor) -> Tensor Tensor.atan_() -> Tensor Tensor.atanh() -> Tensor Tensor.atanh(out : Tensor) -> Tensor Tensor.atanh_() -> Tensor Tensor.backward(gradient : Optional[Tensor], retain_graph : Optional[bool], create_graph : bool=False) -> Tuple[] Tensor.baddbmm(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.baddbmm(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.baddbmm(batch1 : Tensor, batch2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.baddbmm(batch1 : Tensor, batch2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1) -> Tensor Tensor.baddbmm_(batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.bernoulli(generator : Optional[Generator]) -> Tensor Tensor.bernoulli(generator : Optional[Generator], out : Tensor) -> Tensor Tensor.bernoulli(p : float, generator : Optional[Generator]) -> Tensor Tensor.bernoulli(p : Tensor, generator : Optional[Generator]) -> Tensor Tensor.bernoulli(p : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor Tensor.bernoulli(p : float=0.5, generator : Optional[Generator], out : Tensor) -> Tensor Tensor.bernoulli_(p : Tensor, generator : Optional[Generator]) -> Tensor Tensor.bernoulli_(p : float=0.5, generator : Optional[Generator]) -> Tensor Tensor.bincount(weights :", "prev_chunk_id": "chunk_485", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_487", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Optional[Tensor], minlength : int=0) -> Tensor Tensor.bincount(weights : Optional[Tensor], minlength : int=0, out : Tensor) -> Tensor Tensor.bitwise_and(other : Tensor) -> Tensor Tensor.bitwise_and(other : number) -> Tensor Tensor.bitwise_and(other : Tensor, out : Tensor) -> Tensor Tensor.bitwise_and(other : number, out : Tensor) -> Tensor Tensor.bitwise_and_(other : Tensor) -> Tensor Tensor.bitwise_and_(other : number) -> Tensor Tensor.bitwise_left_shift(other : Tensor) -> Tensor Tensor.bitwise_left_shift(other : number) -> Tensor Tensor.bitwise_left_shift(other : Tensor, out : Tensor) -> Tensor Tensor.bitwise_left_shift(other : number, out : Tensor) -> Tensor Tensor.bitwise_left_shift_(other : number) -> Tensor Tensor.bitwise_left_shift_(other : Tensor) -> Tensor Tensor.bitwise_not() -> Tensor Tensor.bitwise_not(out : Tensor) -> Tensor Tensor.bitwise_not_() -> Tensor Tensor.bitwise_or(other : Tensor) -> Tensor Tensor.bitwise_or(other : number) -> Tensor Tensor.bitwise_or(other : Tensor, out : Tensor) -> Tensor Tensor.bitwise_or(other : number, out : Tensor) -> Tensor Tensor.bitwise_or_(other : Tensor) -> Tensor Tensor.bitwise_or_(other : number) -> Tensor Tensor.bitwise_right_shift(other : Tensor) -> Tensor Tensor.bitwise_right_shift(other : number) -> Tensor Tensor.bitwise_right_shift(other : Tensor, out : Tensor) -> Tensor Tensor.bitwise_right_shift(other : number, out : Tensor) -> Tensor Tensor.bitwise_right_shift_(other : number) -> Tensor Tensor.bitwise_right_shift_(other : Tensor) -> Tensor Tensor.bitwise_xor(other : Tensor) -> Tensor Tensor.bitwise_xor(other : number) -> Tensor Tensor.bitwise_xor(other : Tensor, out : Tensor) -> Tensor Tensor.bitwise_xor(other : number, out : Tensor) -> Tensor Tensor.bitwise_xor_(other : Tensor) -> Tensor Tensor.bitwise_xor_(other : number) -> Tensor Tensor.bmm(mat2 : Tensor) -> Tensor Tensor.bmm(mat2 : Tensor, out : Tensor) -> Tensor Tensor.bmm(mat2 : Tensor, out_dtype : int, out : Tensor) -> Tensor Tensor.bmm(mat2 : Tensor, out_dtype : int) -> Tensor Tensor.broadcast_to(size : List[int]) -> Tensor Tensor.cauchy_(median : float=0.0, sigma : float=1.0, generator : Optional[Generator]) -> Tensor Tensor.ccol_indices() -> Tensor Tensor.ceil() -> Tensor Tensor.ceil(out : Tensor) -> Tensor Tensor.ceil_() -> Tensor Tensor.chalf(memory_format : Optional[int]) -> Tensor Tensor.cholesky(upper : bool=False) -> Tensor Tensor.cholesky(upper : bool=False, out : Tensor) -> Tensor Tensor.cholesky_inverse(upper : bool=False) -> Tensor Tensor.cholesky_inverse(upper : bool=False, out : Tensor) ->", "prev_chunk_id": "chunk_486", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_488", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor Tensor.cholesky_solve(input2 : Tensor, upper : bool=False) -> Tensor Tensor.cholesky_solve(input2 : Tensor, upper : bool=False, out : Tensor) -> Tensor Tensor.chunk(chunks : int, dim : int=0) -> List[Tensor] Tensor.clamp(min : Optional[number], max : Optional[number]) -> Tensor Tensor.clamp(min : Optional[Tensor], max : Optional[Tensor]) -> Tensor Tensor.clamp(min : Optional[number], max : Optional[number], out : Tensor) -> Tensor Tensor.clamp(min : Optional[Tensor], max : Optional[Tensor], out : Tensor) -> Tensor Tensor.clamp_(min : Optional[number], max : Optional[number]) -> Tensor Tensor.clamp_(min : Optional[Tensor], max : Optional[Tensor]) -> Tensor Tensor.clamp_max(max : number) -> Tensor Tensor.clamp_max(max : Tensor) -> Tensor Tensor.clamp_max(max : number, out : Tensor) -> Tensor Tensor.clamp_max(max : Tensor, out : Tensor) -> Tensor Tensor.clamp_max_(max : number) -> Tensor Tensor.clamp_max_(max : Tensor) -> Tensor Tensor.clamp_min(min : number) -> Tensor Tensor.clamp_min(min : Tensor) -> Tensor Tensor.clamp_min(min : number, out : Tensor) -> Tensor Tensor.clamp_min(min : Tensor, out : Tensor) -> Tensor Tensor.clamp_min_(min : number) -> Tensor Tensor.clamp_min_(min : Tensor) -> Tensor Tensor.clip(min : Optional[number], max : Optional[number]) -> Tensor Tensor.clip(min : Optional[Tensor], max : Optional[Tensor]) -> Tensor Tensor.clip(min : Optional[number], max : Optional[number], out : Tensor) -> Tensor Tensor.clip(min : Optional[Tensor], max : Optional[Tensor], out : Tensor) -> Tensor Tensor.clip_(min : Optional[number], max : Optional[number]) -> Tensor Tensor.clip_(min : Optional[Tensor], max : Optional[Tensor]) -> Tensor Tensor.clone(memory_format : Optional[int]) -> Tensor Tensor.clone(memory_format : Optional[int], out : Tensor) -> Tensor Tensor.coalesce() -> Tensor Tensor.col_indices() -> Tensor Tensor.conj() -> Tensor Tensor.conj_physical() -> Tensor Tensor.conj_physical(out : Tensor) -> Tensor Tensor.conj_physical_() -> Tensor Tensor.contiguous(memory_format : int=0) -> Tensor Tensor.copy_(src : Tensor, non_blocking : bool=False) -> Tensor Tensor.copy_(other : Tensor) -> Tensor Tensor.copy_(other : int) -> Tensor Tensor.copy_(other : float) -> Tensor Tensor.copysign(other : Tensor) -> Tensor Tensor.copysign(other : number) -> Tensor Tensor.copysign(other : Tensor, out : Tensor) -> Tensor Tensor.copysign(other : number, out : Tensor) -> Tensor Tensor.copysign_(other : Tensor) -> Tensor", "prev_chunk_id": "chunk_487", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_489", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor.copysign_(other : number) -> Tensor Tensor.corrcoef() -> Tensor Tensor.cos() -> Tensor Tensor.cos(out : Tensor) -> Tensor Tensor.cos_() -> Tensor Tensor.cosh() -> Tensor Tensor.cosh(out : Tensor) -> Tensor Tensor.cosh_() -> Tensor Tensor.count_nonzero(dim : List[int]) -> Tensor Tensor.count_nonzero(dim : List[int], out : Tensor) -> Tensor Tensor.count_nonzero(dim : Optional[int]) -> Tensor Tensor.count_nonzero(dim : Optional[int], out : Tensor) -> Tensor Tensor.cov(correction : int=1, fweights : Optional[Tensor], aweights : Optional[Tensor]) -> Tensor Tensor.cpu() -> Tensor Tensor.cross(other : Tensor, dim : Optional[int]) -> Tensor Tensor.cross(other : Tensor, dim : Optional[int], out : Tensor) -> Tensor Tensor.crow_indices() -> Tensor Tensor.cuda() -> Tensor Tensor.cummax(dim : int) -> Tuple[Tensor, Tensor] Tensor.cummax(dim : str) -> Tuple[Tensor, Tensor] Tensor.cummax(dim : str, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.cummax(dim : int, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.cummin(dim : int) -> Tuple[Tensor, Tensor] Tensor.cummin(dim : str) -> Tuple[Tensor, Tensor] Tensor.cummin(dim : str, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.cummin(dim : int, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.cumprod(dim : int, dtype : Optional[int]) -> Tensor Tensor.cumprod(dim : str, dtype : Optional[int]) -> Tensor Tensor.cumprod(dim : str, dtype : Optional[int], out : Tensor) -> Tensor Tensor.cumprod(dim : int, dtype : Optional[int], out : Tensor) -> Tensor Tensor.cumprod_(dim : int, dtype : Optional[int]) -> Tensor Tensor.cumprod_(dim : str, dtype : Optional[int]) -> Tensor Tensor.cumsum(dim : int, dtype : Optional[int]) -> Tensor Tensor.cumsum(dim : str, dtype : Optional[int]) -> Tensor Tensor.cumsum(dim : str, dtype : Optional[int], out : Tensor) -> Tensor Tensor.cumsum(dim : int, dtype : Optional[int], out : Tensor) -> Tensor Tensor.cumsum_(dim : int, dtype : Optional[int]) -> Tensor Tensor.cumsum_(dim : str, dtype : Optional[int]) -> Tensor Tensor.data() -> Tensor Tensor.deg2rad() -> Tensor Tensor.deg2rad(out : Tensor) -> Tensor Tensor.deg2rad_() -> Tensor Tensor.dense_dim() -> int Tensor.dequantize() -> Tensor Tensor.dequantize(out : Tensor) -> Tensor", "prev_chunk_id": "chunk_488", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_490", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor.det() -> Tensor Tensor.detach() -> Tensor Tensor.detach_() -> Tensor Tensor.diag(diagonal : int=0) -> Tensor Tensor.diag(diagonal : int=0, out : Tensor) -> Tensor Tensor.diag_embed(offset : int=0, dim1 : int=-2, dim2 : int=-1) -> Tensor Tensor.diag_embed(offset : int=0, dim1 : int=-2, dim2 : int=-1, out : Tensor) -> Tensor Tensor.diagflat(offset : int=0) -> Tensor Tensor.diagonal(offset : int=0, dim1 : int=0, dim2 : int=1) -> Tensor Tensor.diagonal(outdim : str, dim1 : str, dim2 : str, offset : int=0) -> Tensor Tensor.diagonal_scatter(src : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1) -> Tensor Tensor.diagonal_scatter(src : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1, out : Tensor) -> Tensor Tensor.diff(n : int=1, dim : int=-1, prepend : Optional[Tensor], append : Optional[Tensor]) -> Tensor Tensor.diff(n : int=1, dim : int=-1, prepend : Optional[Tensor], append : Optional[Tensor], out : Tensor) -> Tensor Tensor.digamma() -> Tensor Tensor.digamma(out : Tensor) -> Tensor Tensor.digamma_() -> Tensor Tensor.dim() -> int Tensor.dist(other : Tensor, p : number=2) -> Tensor Tensor.dist(other : Tensor, p : number=2, out : Tensor) -> Tensor Tensor.div(other : Tensor) -> Tensor Tensor.div(other : number) -> Tensor Tensor.div(other : Tensor, rounding_mode : Optional[str]) -> Tensor Tensor.div(other : number, rounding_mode : Optional[str]) -> Tensor Tensor.div(other : Tensor, out : Tensor) -> Tensor Tensor.div(other : Tensor, rounding_mode : Optional[str], out : Tensor) -> Tensor Tensor.div(other : number, out : Tensor) -> Tensor Tensor.div(other : number, rounding_mode : Optional[str], out : Tensor) -> Tensor Tensor.div_(other : Tensor) -> Tensor Tensor.div_(other : Tensor, rounding_mode : Optional[str]) -> Tensor Tensor.div_(other : number) -> Tensor Tensor.div_(other : number, rounding_mode : Optional[str]) -> Tensor Tensor.divide(other : Tensor) -> Tensor Tensor.divide(other : number) -> Tensor Tensor.divide(other : Tensor, rounding_mode : Optional[str]) -> Tensor Tensor.divide(other : number, rounding_mode : Optional[str]) -> Tensor Tensor.divide(other : Tensor, out : Tensor) -> Tensor Tensor.divide(other : Tensor, rounding_mode", "prev_chunk_id": "chunk_489", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_491", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": Optional[str], out : Tensor) -> Tensor Tensor.divide_(other : Tensor) -> Tensor Tensor.divide_(other : Tensor, rounding_mode : Optional[str]) -> Tensor Tensor.divide_(other : number, rounding_mode : Optional[str]) -> Tensor Tensor.divide_(other : number) -> Tensor Tensor.dot(tensor : Tensor) -> Tensor Tensor.dot(tensor : Tensor, out : Tensor) -> Tensor Tensor.dsplit(sections : int) -> List[Tensor] Tensor.dsplit(indices : List[int]) -> List[Tensor] Tensor.element_size() -> int Tensor.eq(other : Tensor) -> Tensor Tensor.eq(other : number) -> Tensor Tensor.eq(other : number, out : Tensor) -> Tensor Tensor.eq(other : Tensor, out : Tensor) -> Tensor Tensor.eq_(other : number) -> Tensor Tensor.eq_(other : Tensor) -> Tensor Tensor.equal(other : Tensor) -> bool Tensor.erf() -> Tensor Tensor.erf(out : Tensor) -> Tensor Tensor.erf_() -> Tensor Tensor.erfc() -> Tensor Tensor.erfc(out : Tensor) -> Tensor Tensor.erfc_() -> Tensor Tensor.erfinv() -> Tensor Tensor.erfinv(out : Tensor) -> Tensor Tensor.erfinv_() -> Tensor Tensor.exp() -> Tensor Tensor.exp(out : Tensor) -> Tensor Tensor.exp2() -> Tensor Tensor.exp2(out : Tensor) -> Tensor Tensor.exp2_() -> Tensor Tensor.exp_() -> Tensor Tensor.expand(size : List[int], implicit : bool=False) -> Tensor Tensor.expand_as(other : Tensor) -> Tensor Tensor.expm1() -> Tensor Tensor.expm1(out : Tensor) -> Tensor Tensor.expm1_() -> Tensor Tensor.exponential_(lambd : float=1.0, generator : Optional[Generator]) -> Tensor Tensor.fill_(value : number) -> Tensor Tensor.fill_(value : Tensor) -> Tensor Tensor.fill_diagonal_(fill_value : number, wrap : bool=False) -> Tensor Tensor.fix() -> Tensor Tensor.fix(out : Tensor) -> Tensor Tensor.fix_() -> Tensor Tensor.flatten(start_dim : int=0, end_dim : int=-1) -> Tensor Tensor.flatten(dims : List[str], out_dim : str) -> Tensor Tensor.flatten(start_dim : int, end_dim : int, out_dim : str) -> Tensor Tensor.flatten(start_dim : str, end_dim : str, out_dim : str) -> Tensor Tensor.flip(dims : List[int]) -> Tensor Tensor.flip(dims : List[int], out : Tensor) -> Tensor Tensor.fliplr() -> Tensor Tensor.flipud() -> Tensor Tensor.float_power(exponent : Tensor) -> Tensor Tensor.float_power(exponent : number) -> Tensor Tensor.float_power(exponent : Tensor, out : Tensor) -> Tensor Tensor.float_power(exponent : number, out : Tensor) -> Tensor Tensor.float_power_(exponent", "prev_chunk_id": "chunk_490", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_492", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": Tensor) -> Tensor Tensor.float_power_(exponent : number) -> Tensor Tensor.floor() -> Tensor Tensor.floor(out : Tensor) -> Tensor Tensor.floor_() -> Tensor Tensor.floor_divide(other : Tensor) -> Tensor Tensor.floor_divide(other : number) -> Tensor Tensor.floor_divide(other : Tensor, out : Tensor) -> Tensor Tensor.floor_divide(other : number, out : Tensor) -> Tensor Tensor.floor_divide_(other : number) -> Tensor Tensor.floor_divide_(other : Tensor) -> Tensor Tensor.fmax(other : Tensor) -> Tensor Tensor.fmax(other : Tensor, out : Tensor) -> Tensor Tensor.fmin(other : Tensor) -> Tensor Tensor.fmin(other : Tensor, out : Tensor) -> Tensor Tensor.fmod(other : Tensor) -> Tensor Tensor.fmod(other : number) -> Tensor Tensor.fmod(other : Tensor, out : Tensor) -> Tensor Tensor.fmod(other : number, out : Tensor) -> Tensor Tensor.fmod_(other : Tensor) -> Tensor Tensor.fmod_(other : number) -> Tensor Tensor.frac() -> Tensor Tensor.frac(out : Tensor) -> Tensor Tensor.frac_() -> Tensor Tensor.frexp() -> Tuple[Tensor, Tensor] Tensor.frexp(mantissa : Tensor, exponent : Tensor) -> Tuple[Tensor, Tensor] Tensor.gather(dim : int, index : Tensor, sparse_grad : bool=False) -> Tensor Tensor.gather(dim : int, index : Tensor, sparse_grad : bool=False, out : Tensor) -> Tensor Tensor.gather(dim : str, index : Tensor, sparse_grad : bool=False) -> Tensor Tensor.gather(dim : str, index : Tensor, sparse_grad : bool=False, out : Tensor) -> Tensor Tensor.gcd(other : Tensor) -> Tensor Tensor.gcd(other : Tensor, out : Tensor) -> Tensor Tensor.gcd_(other : Tensor) -> Tensor Tensor.ge(other : Tensor) -> Tensor Tensor.ge(other : number) -> Tensor Tensor.ge(other : number, out : Tensor) -> Tensor Tensor.ge(other : Tensor, out : Tensor) -> Tensor Tensor.ge_(other : number) -> Tensor Tensor.ge_(other : Tensor) -> Tensor Tensor.geometric_(p : float, generator : Optional[Generator]) -> Tensor Tensor.geqrf() -> Tuple[Tensor, Tensor] Tensor.geqrf(a : Tensor, tau : Tensor) -> Tuple[Tensor, Tensor] Tensor.ger(vec2 : Tensor) -> Tensor Tensor.ger(vec2 : Tensor, out : Tensor) -> Tensor Tensor.get_device() -> int Tensor.greater(other : Tensor) -> Tensor Tensor.greater(other : number) -> Tensor Tensor.greater(other : number, out : Tensor) ->", "prev_chunk_id": "chunk_491", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_493", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor Tensor.greater(other : Tensor, out : Tensor) -> Tensor Tensor.greater_(other : number) -> Tensor Tensor.greater_(other : Tensor) -> Tensor Tensor.greater_equal(other : Tensor) -> Tensor Tensor.greater_equal(other : number) -> Tensor Tensor.greater_equal(other : number, out : Tensor) -> Tensor Tensor.greater_equal(other : Tensor, out : Tensor) -> Tensor Tensor.greater_equal_(other : number) -> Tensor Tensor.greater_equal_(other : Tensor) -> Tensor Tensor.gt(other : Tensor) -> Tensor Tensor.gt(other : number) -> Tensor Tensor.gt(other : number, out : Tensor) -> Tensor Tensor.gt(other : Tensor, out : Tensor) -> Tensor Tensor.gt_(other : number) -> Tensor Tensor.gt_(other : Tensor) -> Tensor Tensor.hardshrink(lambd : number=0.5) -> Tensor Tensor.hardshrink(lambd : number=0.5, out : Tensor) -> Tensor Tensor.heaviside(values : Tensor) -> Tensor Tensor.heaviside(values : Tensor, out : Tensor) -> Tensor Tensor.heaviside_(values : Tensor) -> Tensor Tensor.histc(bins : int=100, min : number=0, max : number=0) -> Tensor Tensor.histc(bins : int=100, min : number=0, max : number=0, out : Tensor) -> Tensor Tensor.histogram(bins : Tensor, weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, Tensor] Tensor.histogram(bins : Tensor, weight : Optional[Tensor], density : bool=False, hist : Tensor, bin_edges : Tensor) -> Tuple[Tensor, Tensor] Tensor.histogram(bins : int=100, range : Optional[List[float]], weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, Tensor] Tensor.histogram(bins : int=100, range : Optional[List[float]], weight : Optional[Tensor], density : bool=False, hist : Tensor, bin_edges : Tensor) -> Tuple[Tensor, Tensor] Tensor.hsplit(sections : int) -> List[Tensor] Tensor.hsplit(indices : List[int]) -> List[Tensor] Tensor.hypot(other : Tensor) -> Tensor Tensor.hypot(other : Tensor, out : Tensor) -> Tensor Tensor.hypot_(other : Tensor) -> Tensor Tensor.i0() -> Tensor Tensor.i0(out : Tensor) -> Tensor Tensor.i0_() -> Tensor Tensor.igamma(other : Tensor) -> Tensor Tensor.igamma(other : Tensor, out : Tensor) -> Tensor Tensor.igamma_(other : Tensor) -> Tensor Tensor.igammac(other : Tensor) -> Tensor Tensor.igammac(other : Tensor, out : Tensor) -> Tensor Tensor.igammac_(other : Tensor) -> Tensor Tensor.imag() -> Tensor Tensor.index_add(dim : int, index : Tensor, source : Tensor,", "prev_chunk_id": "chunk_492", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_494", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "alpha : number=1) -> Tensor Tensor.index_add(dim : int, index : Tensor, source : Tensor, alpha : number=1, out : Tensor) -> Tensor Tensor.index_add(dim : str, index : Tensor, source : Tensor, alpha : number=1) -> Tensor Tensor.index_add_(dim : int, index : Tensor, source : Tensor, alpha : number=1) -> Tensor Tensor.index_copy(dim : int, index : Tensor, source : Tensor) -> Tensor Tensor.index_copy(dim : str, index : Tensor, source : Tensor) -> Tensor Tensor.index_copy(dim : int, index : Tensor, source : Tensor, out : Tensor) -> Tensor Tensor.index_copy_(dim : int, index : Tensor, source : Tensor) -> Tensor Tensor.index_copy_(dim : str, index : Tensor, source : Tensor) -> Tensor Tensor.index_fill(dim : int, index : Tensor, value : Tensor) -> Tensor Tensor.index_fill(dim : int, index : Tensor, value : number) -> Tensor Tensor.index_fill(dim : str, index : Tensor, value : number) -> Tensor Tensor.index_fill(dim : str, index : Tensor, value : Tensor) -> Tensor Tensor.index_fill(dim : int, index : Tensor, value : number, out : Tensor) -> Tensor Tensor.index_fill(dim : int, index : Tensor, value : Tensor, out : Tensor) -> Tensor Tensor.index_fill_(dim : int, index : Tensor, value : Tensor) -> Tensor Tensor.index_fill_(dim : int, index : Tensor, value : number) -> Tensor Tensor.index_fill_(dim : str, index : Tensor, value : number) -> Tensor Tensor.index_fill_(dim : str, index : Tensor, value : Tensor) -> Tensor Tensor.index_put(indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False) -> Tensor Tensor.index_put(indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False, out : Tensor) -> Tensor Tensor.index_put(indices : List[Tensor], values : Tensor, accumulate : bool=False) -> Tensor Tensor.index_put_(indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False) -> Tensor Tensor.index_put_(indices : List[Tensor], values : Tensor, accumulate : bool=False) -> Tensor Tensor.index_reduce(dim : int, index : Tensor, source : Tensor, reduce : str, include_self : bool=True) -> Tensor Tensor.index_reduce(dim", "prev_chunk_id": "chunk_493", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_495", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": int, index : Tensor, source : Tensor, reduce : str, include_self : bool=True, out : Tensor) -> Tensor Tensor.index_reduce_(dim : int, index : Tensor, source : Tensor, reduce : str, include_self : bool=True) -> Tensor Tensor.index_select(dim : int, index : Tensor) -> Tensor Tensor.index_select(dim : int, index : Tensor, out : Tensor) -> Tensor Tensor.index_select(dim : str, index : Tensor) -> Tensor Tensor.index_select(dim : str, index : Tensor, out : Tensor) -> Tensor Tensor.indices() -> Tensor Tensor.inner(other : Tensor) -> Tensor Tensor.inner(other : Tensor, out : Tensor) -> Tensor Tensor.int_repr(out : Tensor) -> Tensor Tensor.int_repr() -> Tensor Tensor.inverse() -> Tensor Tensor.inverse(out : Tensor) -> Tensor Tensor.is_coalesced() -> bool Tensor.is_complex() -> bool Tensor.is_conj() -> bool Tensor.is_contiguous() -> bool Tensor.is_contiguous(memory_format : int) -> bool Tensor.is_distributed() -> bool Tensor.is_floating_point() -> bool Tensor.is_inference() -> bool Tensor.is_leaf() -> bool Tensor.is_neg() -> bool Tensor.is_nonzero() -> bool Tensor.is_pinned(device : Optional[Device]) -> bool Tensor.is_same_size(other : Tensor) -> bool Tensor.is_set_to(tensor : Tensor) -> bool Tensor.is_signed() -> bool Tensor.isclose(other : Tensor, rtol : float=1e-05, atol : float=1e-08, equal_nan : bool=False) -> Tensor Tensor.isfinite() -> Tensor Tensor.isinf() -> Tensor Tensor.isinf(out : Tensor) -> Tensor Tensor.isnan() -> Tensor Tensor.isnan(out : Tensor) -> Tensor Tensor.isneginf() -> Tensor Tensor.isneginf(out : Tensor) -> Tensor Tensor.isposinf() -> Tensor Tensor.isposinf(out : Tensor) -> Tensor Tensor.isreal() -> Tensor Tensor.istft(n_fft : int, hop_length : Optional[int], win_length : Optional[int], window : Optional[Tensor], center : bool=True, normalized : bool=False, onesided : Optional[bool], length : Optional[int], return_complex : bool=False) -> Tensor Tensor.item() -> number Tensor.kron(other : Tensor) -> Tensor Tensor.kron(other : Tensor, out : Tensor) -> Tensor Tensor.kthvalue(k : int, dim : int=-1, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.kthvalue(k : int, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.kthvalue(k : int, dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.kthvalue(k :", "prev_chunk_id": "chunk_494", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_496", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "int, dim : int=-1, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.lcm(other : Tensor) -> Tensor Tensor.lcm(other : Tensor, out : Tensor) -> Tensor Tensor.lcm_(other : Tensor) -> Tensor Tensor.ldexp(other : Tensor) -> Tensor Tensor.ldexp(other : Tensor, out : Tensor) -> Tensor Tensor.ldexp_(other : Tensor) -> Tensor Tensor.le(other : Tensor) -> Tensor Tensor.le(other : number) -> Tensor Tensor.le(other : number, out : Tensor) -> Tensor Tensor.le(other : Tensor, out : Tensor) -> Tensor Tensor.le_(other : number) -> Tensor Tensor.le_(other : Tensor) -> Tensor Tensor.lerp(end : Tensor, weight : number) -> Tensor Tensor.lerp(end : Tensor, weight : Tensor) -> Tensor Tensor.lerp(end : Tensor, weight : number, out : Tensor) -> Tensor Tensor.lerp(end : Tensor, weight : Tensor, out : Tensor) -> Tensor Tensor.lerp_(end : Tensor, weight : number) -> Tensor Tensor.lerp_(end : Tensor, weight : Tensor) -> Tensor Tensor.less(other : Tensor) -> Tensor Tensor.less(other : number) -> Tensor Tensor.less(other : number, out : Tensor) -> Tensor Tensor.less(other : Tensor, out : Tensor) -> Tensor Tensor.less_(other : number) -> Tensor Tensor.less_(other : Tensor) -> Tensor Tensor.less_equal(other : Tensor) -> Tensor Tensor.less_equal(other : number) -> Tensor Tensor.less_equal(other : number, out : Tensor) -> Tensor Tensor.less_equal(other : Tensor, out : Tensor) -> Tensor Tensor.less_equal_(other : number) -> Tensor Tensor.less_equal_(other : Tensor) -> Tensor Tensor.lgamma() -> Tensor Tensor.lgamma(out : Tensor) -> Tensor Tensor.lgamma_() -> Tensor Tensor.log() -> Tensor Tensor.log(out : Tensor) -> Tensor Tensor.log10() -> Tensor Tensor.log10(out : Tensor) -> Tensor Tensor.log10_() -> Tensor Tensor.log1p() -> Tensor Tensor.log1p(out : Tensor) -> Tensor Tensor.log1p_() -> Tensor Tensor.log2() -> Tensor Tensor.log2(out : Tensor) -> Tensor Tensor.log2_() -> Tensor Tensor.log_() -> Tensor Tensor.log_normal_(mean : float=1.0, std : float=2.0, generator : Optional[Generator]) -> Tensor Tensor.log_softmax(dim : int, dtype : Optional[int]) -> Tensor Tensor.log_softmax(dim : str, dtype : Optional[int]) -> Tensor Tensor.log_softmax(dim : int, dtype", "prev_chunk_id": "chunk_495", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_497", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": Optional[int], out : Tensor) -> Tensor Tensor.logaddexp(other : Tensor) -> Tensor Tensor.logaddexp(other : Tensor, out : Tensor) -> Tensor Tensor.logaddexp2(other : Tensor) -> Tensor Tensor.logaddexp2(other : Tensor, out : Tensor) -> Tensor Tensor.logcumsumexp(dim : int) -> Tensor Tensor.logcumsumexp(dim : str) -> Tensor Tensor.logcumsumexp(dim : str, out : Tensor) -> Tensor Tensor.logcumsumexp(dim : int, out : Tensor) -> Tensor Tensor.logdet() -> Tensor Tensor.logical_and(other : Tensor) -> Tensor Tensor.logical_and(other : Tensor, out : Tensor) -> Tensor Tensor.logical_and_(other : Tensor) -> Tensor Tensor.logical_not() -> Tensor Tensor.logical_not(out : Tensor) -> Tensor Tensor.logical_not_() -> Tensor Tensor.logical_or(other : Tensor) -> Tensor Tensor.logical_or(other : Tensor, out : Tensor) -> Tensor Tensor.logical_or_(other : Tensor) -> Tensor Tensor.logical_xor(other : Tensor) -> Tensor Tensor.logical_xor(other : Tensor, out : Tensor) -> Tensor Tensor.logical_xor_(other : Tensor) -> Tensor Tensor.logit(eps : Optional[float]) -> Tensor Tensor.logit(eps : Optional[float], out : Tensor) -> Tensor Tensor.logit_(eps : Optional[float]) -> Tensor Tensor.logsumexp(dim : List[int], keepdim : bool=False) -> Tensor Tensor.logsumexp(dim : List[str], keepdim : bool=False) -> Tensor Tensor.logsumexp(dim : List[str], keepdim : bool=False, out : Tensor) -> Tensor Tensor.logsumexp(dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor Tensor.lt(other : Tensor) -> Tensor Tensor.lt(other : number) -> Tensor Tensor.lt(other : number, out : Tensor) -> Tensor Tensor.lt(other : Tensor, out : Tensor) -> Tensor Tensor.lt_(other : number) -> Tensor Tensor.lt_(other : Tensor) -> Tensor Tensor.lu_solve(LU_data : Tensor, LU_pivots : Tensor) -> Tensor Tensor.lu_solve(LU_data : Tensor, LU_pivots : Tensor, out : Tensor) -> Tensor Tensor.mH() -> Tensor Tensor.mH() -> Tensor Tensor.mT() -> Tensor Tensor.mT() -> Tensor Tensor.masked_fill(mask : Tensor, value : number) -> Tensor Tensor.masked_fill(mask : Tensor, value : Tensor) -> Tensor Tensor.masked_fill(mask : Tensor, value : number, out : Tensor) -> Tensor Tensor.masked_fill(mask : Tensor, value : Tensor, out : Tensor) -> Tensor Tensor.masked_fill_(mask : Tensor, value : number) -> Tensor Tensor.masked_fill_(mask : Tensor, value", "prev_chunk_id": "chunk_496", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_498", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": Tensor) -> Tensor Tensor.masked_scatter(mask : Tensor, source : Tensor) -> Tensor Tensor.masked_scatter(mask : Tensor, source : Tensor, out : Tensor) -> Tensor Tensor.masked_scatter_(mask : Tensor, source : Tensor) -> Tensor Tensor.masked_select(mask : Tensor) -> Tensor Tensor.masked_select(mask : Tensor, out : Tensor) -> Tensor Tensor.matmul(other : Tensor) -> Tensor Tensor.matmul(other : Tensor, out : Tensor) -> Tensor Tensor.matrix_exp() -> Tensor Tensor.matrix_power(n : int) -> Tensor Tensor.matrix_power(n : int, out : Tensor) -> Tensor Tensor.max(other : Tensor) -> Tensor Tensor.max() -> Tensor Tensor.max(dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.max(dim : int, keepdim : bool=False, max : Tensor, max_values : Tensor) -> Tuple[Tensor, Tensor] Tensor.max(dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.max(dim : str, keepdim : bool=False, max : Tensor, max_values : Tensor) -> Tuple[Tensor, Tensor] Tensor.max(out : Tensor) -> Tensor Tensor.max(other : Tensor, out : Tensor) -> Tensor Tensor.maximum(other : Tensor) -> Tensor Tensor.maximum(other : Tensor, out : Tensor) -> Tensor Tensor.mean(dtype : Optional[int]) -> Tensor Tensor.mean(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.mean(dim : List[str], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.mean(dim : List[str], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.mean(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.mean(dtype : Optional[int], out : Tensor) -> Tensor Tensor.median() -> Tensor Tensor.median(dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.median(dim : int, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.median(dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.median(dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.median(out : Tensor) -> Tensor Tensor.min(other : Tensor) -> Tensor Tensor.min() -> Tensor Tensor.min(dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.min(dim : int, keepdim : bool=False, min : Tensor,", "prev_chunk_id": "chunk_497", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_499", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "min_indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.min(dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.min(dim : str, keepdim : bool=False, min : Tensor, min_indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.min(out : Tensor) -> Tensor Tensor.min(other : Tensor, out : Tensor) -> Tensor Tensor.minimum(other : Tensor) -> Tensor Tensor.minimum(other : Tensor, out : Tensor) -> Tensor Tensor.mm(mat2 : Tensor) -> Tensor Tensor.mm(mat2 : Tensor, out : Tensor) -> Tensor Tensor.mm(mat2 : Tensor, out_dtype : int, out : Tensor) -> Tensor Tensor.mm(mat2 : Tensor, out_dtype : int) -> Tensor Tensor.mode(dim : int=-1, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.mode(dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.mode(dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.mode(dim : int=-1, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.moveaxis(source : List[int], destination : List[int]) -> Tensor Tensor.moveaxis(source : int, destination : int) -> Tensor Tensor.movedim(source : int, destination : int) -> Tensor Tensor.movedim(source : List[int], destination : List[int]) -> Tensor Tensor.msort() -> Tensor Tensor.msort(out : Tensor) -> Tensor Tensor.mul(other : Tensor) -> Tensor Tensor.mul(other : number) -> Tensor Tensor.mul(other : Tensor, out : Tensor) -> Tensor Tensor.mul(other : number, out : Tensor) -> Tensor Tensor.mul_(other : Tensor) -> Tensor Tensor.mul_(other : number) -> Tensor Tensor.multinomial(num_samples : int, replacement : bool=False, generator : Optional[Generator]) -> Tensor Tensor.multinomial(num_samples : int, replacement : bool=False, generator : Optional[Generator], out : Tensor) -> Tensor Tensor.multiply(other : Tensor) -> Tensor Tensor.multiply(other : number) -> Tensor Tensor.multiply(other : Tensor, out : Tensor) -> Tensor Tensor.multiply_(other : Tensor) -> Tensor Tensor.multiply_(other : number) -> Tensor Tensor.mv(vec : Tensor) -> Tensor Tensor.mv(vec : Tensor, out : Tensor) -> Tensor Tensor.mvlgamma(p : int) -> Tensor Tensor.mvlgamma(p : int, out : Tensor) -> Tensor Tensor.mvlgamma_(p : int) -> Tensor Tensor.nan_to_num(nan : Optional[float], posinf", "prev_chunk_id": "chunk_498", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_500", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": Optional[float], neginf : Optional[float]) -> Tensor Tensor.nan_to_num(nan : Optional[float], posinf : Optional[float], neginf : Optional[float], out : Tensor) -> Tensor Tensor.nan_to_num_(nan : Optional[float], posinf : Optional[float], neginf : Optional[float]) -> Tensor Tensor.nanmean(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.nanmean(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.nanmedian() -> Tensor Tensor.nanmedian(dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.nanmedian(dim : int, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.nanmedian(dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] Tensor.nanmedian(dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.nanmedian(out : Tensor) -> Tensor Tensor.nanquantile(q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor Tensor.nanquantile(q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor Tensor.nanquantile(q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor Tensor.nanquantile(q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor Tensor.nansum(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.nansum(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.narrow(dim : int, start : int, length : int) -> Tensor Tensor.narrow(dim : int, start : Tensor, length : int) -> Tensor Tensor.narrow_copy(dim : int, start : int, length : int) -> Tensor Tensor.narrow_copy(dim : int, start : int, length : int, out : Tensor) -> Tensor Tensor.ne(other : Tensor) -> Tensor Tensor.ne(other : number) -> Tensor Tensor.ne(other : number, out : Tensor) -> Tensor Tensor.ne(other : Tensor, out : Tensor) -> Tensor Tensor.ne_(other : number) -> Tensor Tensor.ne_(other : Tensor) -> Tensor Tensor.neg() -> Tensor Tensor.neg(out : Tensor) -> Tensor Tensor.neg_() -> Tensor Tensor.negative() -> Tensor Tensor.negative(out : Tensor)", "prev_chunk_id": "chunk_499", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_501", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "-> Tensor Tensor.negative_() -> Tensor Tensor.new_empty(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor Tensor.new_empty(size : List[int], out : Tensor) -> Tensor Tensor.new_empty_strided(size : List[int], stride : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor Tensor.new_empty_strided(size : List[int], stride : List[int], out : Tensor) -> Tensor Tensor.new_full(size : List[int], fill_value : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor Tensor.new_full(size : List[int], fill_value : number, out : Tensor) -> Tensor Tensor.new_ones(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor Tensor.new_ones(size : List[int], out : Tensor) -> Tensor Tensor.new_zeros(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor Tensor.new_zeros(size : List[int], out : Tensor) -> Tensor Tensor.nextafter(other : Tensor) -> Tensor Tensor.nextafter(other : Tensor, out : Tensor) -> Tensor Tensor.nextafter_(other : Tensor) -> Tensor Tensor.nonzero() -> Tensor Tensor.nonzero(out : Tensor) -> Tensor Tensor.nonzero_static(size : int, fill_value : int=-1) -> Tensor Tensor.nonzero_static(size : int, fill_value : int=-1, out : Tensor) -> Tensor Tensor.norm(p : number=2) -> Tensor Tensor.norm(p : Optional[number], dim : List[int], keepdim : bool=False) -> Tensor Tensor.norm(p : Optional[number], dim : List[str], keepdim : bool=False) -> Tensor Tensor.norm(p : Optional[number], dim : List[int], keepdim : bool, dtype : int) -> Tensor Tensor.norm(p : Optional[number], dim : List[int], keepdim : bool, dtype : int, out : Tensor) -> Tensor Tensor.norm(p : Optional[number], dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor Tensor.norm(p : Optional[number], dtype : int) -> Tensor Tensor.norm(p : Optional[number], dtype : int, out : Tensor) -> Tensor Tensor.norm(p : number=2, out : Tensor) -> Tensor Tensor.norm(p : Optional[number], dim : List[str], keepdim : bool, dtype : int) -> Tensor", "prev_chunk_id": "chunk_500", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_502", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor.norm(p : Optional[number], dim : List[str], keepdim : bool, dtype : int, out : Tensor) -> Tensor Tensor.norm(p : Optional[number], dim : List[str], keepdim : bool=False, out : Tensor) -> Tensor Tensor.normal_(mean : float=0.0, std : float=1.0, generator : Optional[Generator]) -> Tensor Tensor.not_equal(other : Tensor) -> Tensor Tensor.not_equal(other : number) -> Tensor Tensor.not_equal(other : number, out : Tensor) -> Tensor Tensor.not_equal(other : Tensor, out : Tensor) -> Tensor Tensor.not_equal_(other : number) -> Tensor Tensor.not_equal_(other : Tensor) -> Tensor Tensor.numel() -> int Tensor.orgqr(input2 : Tensor) -> Tensor Tensor.orgqr(input2 : Tensor, out : Tensor) -> Tensor Tensor.ormqr(input2 : Tensor, input3 : Tensor, left : bool=True, transpose : bool=False) -> Tensor Tensor.ormqr(input2 : Tensor, input3 : Tensor, left : bool=True, transpose : bool=False, out : Tensor) -> Tensor Tensor.outer(vec2 : Tensor) -> Tensor Tensor.outer(vec2 : Tensor, out : Tensor) -> Tensor Tensor.output_nr() -> int Tensor.permute(dims : List[int]) -> Tensor Tensor.pin_memory(device : Optional[Device]) -> Tensor Tensor.pinverse(rcond : float=1e-15) -> Tensor Tensor.polygamma_(n : int) -> Tensor Tensor.positive() -> Tensor Tensor.pow(exponent : Tensor) -> Tensor Tensor.pow(exponent : number) -> Tensor Tensor.pow(exponent : number, out : Tensor) -> Tensor Tensor.pow(exponent : Tensor, out : Tensor) -> Tensor Tensor.pow_(exponent : number) -> Tensor Tensor.pow_(exponent : Tensor) -> Tensor Tensor.prelu(weight : Tensor) -> Tensor Tensor.prod(dtype : Optional[int]) -> Tensor Tensor.prod(dim : int, keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.prod(dim : str, keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.prod(dim : str, keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.prod(dim : int, keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.prod(dtype : Optional[int], out : Tensor) -> Tensor Tensor.put(index : Tensor, source : Tensor, accumulate : bool=False) -> Tensor Tensor.put(index : Tensor, source : Tensor, accumulate : bool=False, out : Tensor) -> Tensor Tensor.put_(index : Tensor, source : Tensor, accumulate", "prev_chunk_id": "chunk_501", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_503", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": ": bool=False) -> Tensor Tensor.q_per_channel_axis() -> int Tensor.q_per_channel_scales(out : Tensor) -> Tensor Tensor.q_per_channel_scales() -> Tensor Tensor.q_per_channel_zero_points(out : Tensor) -> Tensor Tensor.q_per_channel_zero_points() -> Tensor Tensor.q_scale() -> float Tensor.q_zero_point() -> int Tensor.qr(some : bool=True) -> Tuple[Tensor, Tensor] Tensor.qr(some : bool=True, Q : Tensor, R : Tensor) -> Tuple[Tensor, Tensor] Tensor.qscheme() -> QScheme Tensor.quantile(q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor Tensor.quantile(q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor Tensor.quantile(q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor Tensor.quantile(q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor Tensor.rad2deg() -> Tensor Tensor.rad2deg(out : Tensor) -> Tensor Tensor.rad2deg_() -> Tensor Tensor.random_(from : int, to : Optional[int], generator : Optional[Generator]) -> Tensor Tensor.random_(to : int, generator : Optional[Generator]) -> Tensor Tensor.random_(generator : Optional[Generator]) -> Tensor Tensor.ravel() -> Tensor Tensor.real() -> Tensor Tensor.reciprocal() -> Tensor Tensor.reciprocal(out : Tensor) -> Tensor Tensor.reciprocal_() -> Tensor Tensor.record_stream(s : Stream) -> Tuple[] Tensor.refine_names(names : List[str]) -> Tensor Tensor.relu() -> Tensor Tensor.relu(out : Tensor) -> Tensor Tensor.relu_() -> Tensor Tensor.remainder(other : Tensor) -> Tensor Tensor.remainder(other : number) -> Tensor Tensor.remainder(other : Tensor, out : Tensor) -> Tensor Tensor.remainder(other : number, out : Tensor) -> Tensor Tensor.remainder_(other : Tensor) -> Tensor Tensor.remainder_(other : number) -> Tensor Tensor.rename(names : Optional[List[str]]) -> Tensor Tensor.rename_(names : Optional[List[str]]) -> Tensor Tensor.renorm(p : number, dim : int, maxnorm : number) -> Tensor Tensor.renorm(p : number, dim : int, maxnorm : number, out : Tensor) -> Tensor Tensor.renorm_(p : number, dim : int, maxnorm : number) -> Tensor Tensor.repeat(repeats : List[int]) -> Tensor Tensor.repeat(repeats : List[int], out : Tensor) -> Tensor Tensor.repeat_interleave(repeats : Tensor, dim : Optional[int], output_size : Optional[int]) -> Tensor Tensor.repeat_interleave(repeats : int, dim : Optional[int], output_size : Optional[int])", "prev_chunk_id": "chunk_502", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_504", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "-> Tensor Tensor.requires_grad_(requires_grad : bool=True) -> Tensor Tensor.reshape(shape : List[int]) -> Tensor Tensor.reshape_as(other : Tensor) -> Tensor Tensor.resize(size : List[int], memory_format : Optional[int]) -> Tensor Tensor.resize(size : List[int], memory_format : Optional[int], out : Tensor) -> Tensor Tensor.resize_(size : List[int], memory_format : Optional[int]) -> Tensor Tensor.resize_as(the_template : Tensor, memory_format : Optional[int]) -> Tensor Tensor.resize_as(the_template : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor Tensor.resize_as_(the_template : Tensor, memory_format : Optional[int]) -> Tensor Tensor.resize_as_sparse_(the_template : Tensor) -> Tensor Tensor.resolve_conj() -> Tensor Tensor.resolve_neg() -> Tensor Tensor.retain_grad() -> Tuple[] Tensor.retains_grad() -> bool Tensor.roll(shifts : List[int], dims : List[int]=[]) -> Tensor Tensor.roll(shifts : List[int], dims : List[int]=[], out : Tensor) -> Tensor Tensor.rot90(k : int=1, dims : List[int]=[0, 1]) -> Tensor Tensor.rot90(k : int=1, dims : List[int]=[0, 1], out : Tensor) -> Tensor Tensor.round() -> Tensor Tensor.round(decimals : int) -> Tensor Tensor.round(out : Tensor) -> Tensor Tensor.round(decimals : int, out : Tensor) -> Tensor Tensor.round_() -> Tensor Tensor.round_(decimals : int) -> Tensor Tensor.row_indices() -> Tensor Tensor.rsqrt() -> Tensor Tensor.rsqrt(out : Tensor) -> Tensor Tensor.rsqrt_() -> Tensor Tensor.scatter(dim : int, index : Tensor, value : number) -> Tensor Tensor.scatter(dim : int, index : Tensor, src : Tensor) -> Tensor Tensor.scatter(dim : int, index : Tensor, src : Tensor, reduce : str) -> Tensor Tensor.scatter(dim : int, index : Tensor, value : number, reduce : str) -> Tensor Tensor.scatter(dim : int, index : Tensor, src : Tensor, out : Tensor) -> Tensor Tensor.scatter(dim : int, index : Tensor, value : number, out : Tensor) -> Tensor Tensor.scatter(dim : int, index : Tensor, src : Tensor, reduce : str, out : Tensor) -> Tensor Tensor.scatter(dim : int, index : Tensor, value : number, reduce : str, out : Tensor) -> Tensor Tensor.scatter(dim : str, index : Tensor, src : Tensor) -> Tensor Tensor.scatter(dim : str, index : Tensor,", "prev_chunk_id": "chunk_503", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_505", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "value : number) -> Tensor Tensor.scatter_(dim : int, index : Tensor, src : Tensor) -> Tensor Tensor.scatter_(dim : int, index : Tensor, value : number) -> Tensor Tensor.scatter_(dim : int, index : Tensor, src : Tensor, reduce : str) -> Tensor Tensor.scatter_(dim : int, index : Tensor, value : number, reduce : str) -> Tensor Tensor.scatter_add(dim : int, index : Tensor, src : Tensor) -> Tensor Tensor.scatter_add(dim : int, index : Tensor, src : Tensor, out : Tensor) -> Tensor Tensor.scatter_add(dim : str, index : Tensor, src : Tensor) -> Tensor Tensor.scatter_add_(dim : int, index : Tensor, src : Tensor) -> Tensor Tensor.scatter_reduce(dim : int, index : Tensor, src : Tensor, reduce : str, include_self : bool=True) -> Tensor Tensor.scatter_reduce(dim : int, index : Tensor, src : Tensor, reduce : str, include_self : bool=True, out : Tensor) -> Tensor Tensor.scatter_reduce_(dim : int, index : Tensor, src : Tensor, reduce : str, include_self : bool=True) -> Tensor Tensor.select(dim : str, index : int) -> Tensor Tensor.select(dim : int, index : int) -> Tensor Tensor.select_scatter(src : Tensor, dim : int, index : int) -> Tensor Tensor.select_scatter(src : Tensor, dim : int, index : int, out : Tensor) -> Tensor Tensor.set_(source : Storage, storage_offset : int, size : List[int], stride : List[int]=[]) -> Tensor Tensor.set_(source : Tensor) -> Tensor Tensor.set_() -> Tensor Tensor.set_(source : Storage) -> Tensor Tensor.set_(source : Tensor, storage_offset : int, size : List[int], stride : List[int]=[]) -> Tensor Tensor.sgn() -> Tensor Tensor.sgn(out : Tensor) -> Tensor Tensor.sgn_() -> Tensor Tensor.sigmoid() -> Tensor Tensor.sigmoid(out : Tensor) -> Tensor Tensor.sigmoid_() -> Tensor Tensor.sign() -> Tensor Tensor.sign(out : Tensor) -> Tensor Tensor.sign_() -> Tensor Tensor.signbit() -> Tensor Tensor.signbit(out : Tensor) -> Tensor Tensor.sin() -> Tensor Tensor.sin(out : Tensor) -> Tensor Tensor.sin_() -> Tensor Tensor.sinc() -> Tensor Tensor.sinc(out : Tensor) -> Tensor Tensor.sinc_() ->", "prev_chunk_id": "chunk_504", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_506", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor Tensor.sinh() -> Tensor Tensor.sinh(out : Tensor) -> Tensor Tensor.sinh_() -> Tensor Tensor.size(dim : int) -> int Tensor.size(dim : str) -> int Tensor.size() -> List[int] Tensor.slice_inverse(src : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1) -> Tensor Tensor.slice_scatter(src : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1) -> Tensor Tensor.slice_scatter(src : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1, out : Tensor) -> Tensor Tensor.slogdet() -> Tuple[Tensor, Tensor] Tensor.slogdet(sign : Tensor, logabsdet : Tensor) -> Tuple[Tensor, Tensor] Tensor.smm(mat2 : Tensor) -> Tensor Tensor.softmax(dim : int, dtype : Optional[int]) -> Tensor Tensor.softmax(dim : str, dtype : Optional[int]) -> Tensor Tensor.softmax(dim : int, dtype : Optional[int], out : Tensor) -> Tensor Tensor.sort(dim : int=-1, descending : bool=False) -> Tuple[Tensor, Tensor] Tensor.sort(stable : Optional[bool], dim : int=-1, descending : bool=False) -> Tuple[Tensor, Tensor] Tensor.sort(stable : Optional[bool], dim : int=-1, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.sort(dim : int=-1, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.sort(dim : str, descending : bool=False) -> Tuple[Tensor, Tensor] Tensor.sort(dim : str, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.sort(stable : Optional[bool], dim : str, descending : bool=False) -> Tuple[Tensor, Tensor] Tensor.sort(stable : Optional[bool], dim : str, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.sparse_dim() -> int Tensor.sparse_mask(mask : Tensor, out : Tensor) -> Tensor Tensor.sparse_mask(mask : Tensor) -> Tensor Tensor.sparse_resize_(size : List[int], sparse_dim : int, dense_dim : int) -> Tensor Tensor.sparse_resize_and_clear_(size : List[int], sparse_dim : int, dense_dim : int) -> Tensor Tensor.split(split_size : int, dim : int=0) -> List[Tensor] Tensor.split(split_size : List[int], dim : int=0) -> List[Tensor] Tensor.split(split_sizes : List[int], dim : int=0) -> List[Tensor] Tensor.split_with_sizes(split_sizes : List[int], dim : int=0)", "prev_chunk_id": "chunk_505", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_507", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "-> List[Tensor] Tensor.sqrt() -> Tensor Tensor.sqrt(out : Tensor) -> Tensor Tensor.sqrt_() -> Tensor Tensor.square() -> Tensor Tensor.square(out : Tensor) -> Tensor Tensor.square_() -> Tensor Tensor.squeeze() -> Tensor Tensor.squeeze(dim : int) -> Tensor Tensor.squeeze(dim : List[int]) -> Tensor Tensor.squeeze(dim : str) -> Tensor Tensor.squeeze_() -> Tensor Tensor.squeeze_(dim : int) -> Tensor Tensor.squeeze_(dim : List[int]) -> Tensor Tensor.squeeze_(dim : str) -> Tensor Tensor.sspaddmm(mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor Tensor.sspaddmm(mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor Tensor.std(unbiased : bool=True) -> Tensor Tensor.std(dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tensor Tensor.std(dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False) -> Tensor Tensor.std(dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tensor Tensor.std(dim : List[str], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor Tensor.std(dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor Tensor.std(dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor Tensor.std(dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tensor Tensor.std(dim : List[str], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor Tensor.stft(n_fft : int, hop_length : Optional[int], win_length : Optional[int], window : Optional[Tensor], normalized : bool=False, onesided : Optional[bool], return_complex : Optional[bool], align_to_window : Optional[bool]) -> Tensor Tensor.stft(n_fft : int, hop_length : Optional[int], win_length : Optional[int], window : Optional[Tensor], center : bool=True, pad_mode : str=reflect, normalized : bool=False, onesided : Optional[bool], return_complex : Optional[bool], align_to_window : Optional[bool]) -> Tensor Tensor.storage_offset() -> int Tensor.stride(dim : int) -> int Tensor.stride(dim : str) -> int Tensor.stride() -> List[int] Tensor.sub(other : Tensor, alpha : number=1) -> Tensor Tensor.sub(other : number, alpha : number=1) -> Tensor Tensor.sub(other : Tensor, alpha : number=1, out : Tensor) -> Tensor Tensor.sub(other : number,", "prev_chunk_id": "chunk_506", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_508", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "alpha : number=1, out : Tensor) -> Tensor Tensor.sub_(other : Tensor, alpha : number=1) -> Tensor Tensor.sub_(other : number, alpha : number=1) -> Tensor Tensor.subtract(other : Tensor, alpha : number=1) -> Tensor Tensor.subtract(other : Tensor, alpha : number=1, out : Tensor) -> Tensor Tensor.subtract(other : number, alpha : number=1) -> Tensor Tensor.subtract_(other : Tensor, alpha : number=1) -> Tensor Tensor.subtract_(other : number, alpha : number=1) -> Tensor Tensor.sum(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.sum(dtype : Optional[int]) -> Tensor Tensor.sum(dim : List[str], keepdim : bool=False, dtype : Optional[int]) -> Tensor Tensor.sum(dim : List[str], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.sum(dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor Tensor.sum(dtype : Optional[int], out : Tensor) -> Tensor Tensor.sum_to_size(size : List[int]) -> Tensor Tensor.svd(some : bool=True, compute_uv : bool=True) -> Tuple[Tensor, Tensor, Tensor] Tensor.svd(some : bool=True, compute_uv : bool=True, U : Tensor, S : Tensor, V : Tensor) -> Tuple[Tensor, Tensor, Tensor] Tensor.swapaxes(axis0 : int, axis1 : int) -> Tensor Tensor.swapaxes_(axis0 : int, axis1 : int) -> Tensor Tensor.swapdims(dim0 : int, dim1 : int) -> Tensor Tensor.swapdims_(dim0 : int, dim1 : int) -> Tensor Tensor.t() -> Tensor Tensor.t_() -> Tensor Tensor.take(index : Tensor) -> Tensor Tensor.take(index : Tensor, out : Tensor) -> Tensor Tensor.take_along_dim(indices : Tensor, dim : Optional[int]) -> Tensor Tensor.take_along_dim(indices : Tensor, dim : Optional[int], out : Tensor) -> Tensor Tensor.tan() -> Tensor Tensor.tan(out : Tensor) -> Tensor Tensor.tan_() -> Tensor Tensor.tanh() -> Tensor Tensor.tanh(out : Tensor) -> Tensor Tensor.tanh_() -> Tensor Tensor.tensor_split(sections : int, dim : int=0) -> List[Tensor] Tensor.tensor_split(indices : List[int], dim : int=0) -> List[Tensor] Tensor.tensor_split(tensor_indices_or_sections : Tensor, dim : int=0) -> List[Tensor] Tensor.tile(dims : List[int]) -> Tensor Tensor.to(device : Device, dtype : int, non_blocking : bool=False, copy : bool=False, memory_format : Optional[int])", "prev_chunk_id": "chunk_507", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_509", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "-> Tensor Tensor.to(dtype : int, non_blocking : bool=False, copy : bool=False, memory_format : Optional[int]) -> Tensor Tensor.to(other : Tensor, non_blocking : bool=False, copy : bool=False, memory_format : Optional[int]) -> Tensor Tensor.to(dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], non_blocking : bool=False, copy : bool=False, memory_format : Optional[int]) -> Tensor Tensor.to(device : Optional[Device], dtype : Optional[int], non_blocking : bool=False, copy : bool=False) -> Tensor Tensor.to(dtype : Optional[int], non_blocking : bool=False, copy : bool=False) -> Tensor Tensor.to(non_blocking : bool=False, copy : bool=False) -> Tensor Tensor.to_dense(dtype : Optional[int], masked_grad : Optional[bool]) -> Tensor Tensor.to_mkldnn(dtype : Optional[int]) -> Tensor Tensor.to_mkldnn(dtype : Optional[int], out : Tensor) -> Tensor Tensor.to_padded_tensor(padding : float, output_size : Optional[List[int]], out : Tensor) -> Tensor Tensor.to_padded_tensor(padding : float, output_size : Optional[List[int]]) -> Tensor Tensor.to_sparse(sparse_dim : int) -> Tensor Tensor.to_sparse(layout : Optional[int], blocksize : Optional[List[int]], dense_dim : Optional[int]) -> Tensor Tensor.to_sparse_bsc(blocksize : List[int], dense_dim : Optional[int]) -> Tensor Tensor.to_sparse_bsr(blocksize : List[int], dense_dim : Optional[int]) -> Tensor Tensor.to_sparse_csc(dense_dim : Optional[int]) -> Tensor Tensor.to_sparse_csr(dense_dim : Optional[int]) -> Tensor Tensor.topk(k : int, dim : int=-1, largest : bool=True, sorted : bool=True) -> Tuple[Tensor, Tensor] Tensor.topk(k : int, dim : int=-1, largest : bool=True, sorted : bool=True, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] Tensor.trace() -> Tensor Tensor.trace(out : Tensor) -> Tensor Tensor.transpose(dim0 : int, dim1 : int) -> Tensor Tensor.transpose(dim0 : str, dim1 : str) -> Tensor Tensor.transpose_(dim0 : int, dim1 : int) -> Tensor Tensor.triangular_solve(A : Tensor, upper : bool=True, transpose : bool=False, unitriangular : bool=False) -> Tuple[Tensor, Tensor] Tensor.triangular_solve(A : Tensor, upper : bool=True, transpose : bool=False, unitriangular : bool=False, X : Tensor, M : Tensor) -> Tuple[Tensor, Tensor] Tensor.tril(diagonal : int=0) -> Tensor Tensor.tril(diagonal : int=0, out : Tensor) -> Tensor Tensor.tril_(diagonal : int=0) -> Tensor Tensor.triu(diagonal : int=0) -> Tensor Tensor.triu(diagonal : int=0, out :", "prev_chunk_id": "chunk_508", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_510", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "Tensor) -> Tensor Tensor.triu_(diagonal : int=0) -> Tensor Tensor.true_divide(other : Tensor) -> Tensor Tensor.true_divide(other : number) -> Tensor Tensor.true_divide(other : Tensor, out : Tensor) -> Tensor Tensor.true_divide_(other : Tensor) -> Tensor Tensor.true_divide_(other : number) -> Tensor Tensor.trunc() -> Tensor Tensor.trunc(out : Tensor) -> Tensor Tensor.trunc_() -> Tensor Tensor.type_as(other : Tensor) -> Tensor Tensor.unbind(dim : int=0) -> List[Tensor] Tensor.unbind(dim : str) -> List[Tensor] Tensor.unflatten(dim : int, sizes : List[int]) -> Tensor Tensor.unflatten(dim : str, sizes : List[int], names : List[str]) -> Tensor Tensor.unfold(dimension : int, size : int, step : int) -> Tensor Tensor.uniform_(from : float=0.0, to : float=1.0, generator : Optional[Generator]) -> Tensor Tensor.unique_consecutive(return_inverse : bool=False, return_counts : bool=False, dim : Optional[int]) -> Tuple[Tensor, Tensor, Tensor] Tensor.unique_consecutive(return_inverse : bool=False, return_counts : bool=False, dim : Optional[int], out0 : Tensor, out1 : Tensor, out2 : Tensor) -> Tuple[Tensor, Tensor, Tensor] Tensor.unsafe_chunk(chunks : int, dim : int=0) -> List[Tensor] Tensor.unsafe_split(split_size : int, dim : int=0) -> List[Tensor] Tensor.unsafe_split(split_size : int, dim : int=0, out : List[Tensor]) -> Tuple[] Tensor.unsafe_split_with_sizes(split_sizes : List[int], dim : int=0) -> List[Tensor] Tensor.unsafe_split_with_sizes(split_sizes : List[int], dim : int=0, out : List[Tensor]) -> Tuple[] Tensor.unsqueeze(dim : int) -> Tensor Tensor.unsqueeze_(dim : int) -> Tensor Tensor.values() -> Tensor Tensor.var(unbiased : bool=True) -> Tensor Tensor.var(dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tensor Tensor.var(dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False) -> Tensor Tensor.var(dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tensor Tensor.var(dim : List[str], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor Tensor.var(dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor Tensor.var(dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor Tensor.var(dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tensor Tensor.var(dim : List[str], correction : Optional[number], keepdim : bool=False, out : Tensor)", "prev_chunk_id": "chunk_509", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_511", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported Tensor Methods#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported Tensor Methods#", "content": "-> Tensor Tensor.vdot(other : Tensor) -> Tensor Tensor.vdot(other : Tensor, out : Tensor) -> Tensor Tensor.view(size : List[int]) -> Tensor Tensor.view(dtype : int) -> Tensor Tensor.view_as(other : Tensor) -> Tensor Tensor.vsplit(sections : int) -> List[Tensor] Tensor.vsplit(indices : List[int]) -> List[Tensor] Tensor.xlogy(other : Tensor) -> Tensor Tensor.xlogy(other : number) -> Tensor Tensor.xlogy(other : Tensor, out : Tensor) -> Tensor Tensor.xlogy(other : number, out : Tensor) -> Tensor Tensor.xlogy_(other : Tensor) -> Tensor Tensor.xlogy_(other : number) -> Tensor Tensor.zero_() -> Tensor", "prev_chunk_id": "chunk_510", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_512", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Supported PyTorch Functions# torch.nn.functional.adaptive_avg_pool2d(input : Tensor, output_size : List[int]) -> Tensor torch.nn.functional.adaptive_avg_pool3d(input : Tensor, output_size : List[int]) -> Tensor torch.nn.functional.adaptive_max_pool1d_with_indices(input : Tensor, output_size : List[int], return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.adaptive_max_pool2d_with_indices(input : Tensor, output_size : List[int], return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.adaptive_max_pool3d_with_indices(input : Tensor, output_size : List[int], return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.affine_grid(theta : Tensor, size : List[int], align_corners : Optional[bool]) -> Tensor torch.nn.functional.alpha_dropout(input : Tensor, p : float=0.5, training : bool=False, inplace : bool=False) -> Tensor torch.nn.functional.assert_int_or_pair(arg : List[int], arg_name : str, message : str) -> NoneType torch.nn.functional.batch_norm(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], weight : Optional[Tensor], bias : Optional[Tensor], training : bool=False, momentum : float=0.1, eps : float=1e-05) -> Tensor torch.nn.functional.binary_cross_entropy(input : Tensor, target : Tensor, weight : Optional[Tensor], size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.binary_cross_entropy_with_logits(input : Tensor, target : Tensor, weight : Optional[Tensor], size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean, pos_weight : Optional[Tensor]) -> Tensor torch.nn.functional.celu(input : Tensor, alpha : float=1.0, inplace : bool=False) -> Tensor torch.nn.functional.cosine_embedding_loss(input1 : Tensor, input2 : Tensor, target : Tensor, margin : float=0.0, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.cross_entropy(input : Tensor, target : Tensor, weight : Optional[Tensor], size_average : Optional[bool], ignore_index : int=-100, reduce : Optional[bool], reduction : str=mean, label_smoothing : float=0.0) -> Tensor torch.nn.functional.ctc_loss(log_probs : Tensor, targets : Tensor, input_lengths : Tensor, target_lengths : Tensor, blank : int=0, reduction : str=mean, zero_infinity : bool=False) -> Tensor torch.nn.functional.dropout(input : Tensor, p : float=0.5, training : bool=True, inplace : bool=False) -> Tensor torch.nn.functional.dropout1d(input : Tensor, p : float=0.5, training : bool=True, inplace : bool=False) -> Tensor torch.nn.functional.dropout2d(input : Tensor, p : float=0.5, training : bool=True, inplace : bool=False) -> Tensor torch.nn.functional.dropout3d(input : Tensor, p : float=0.5, training : bool=True, inplace : bool=False) -> Tensor torch.nn.functional.elu(input", "prev_chunk_id": "chunk_511", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_513", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, alpha : float=1.0, inplace : bool=False) -> Tensor torch.nn.functional.embedding(input : Tensor, weight : Tensor, padding_idx : Optional[int], max_norm : Optional[float], norm_type : float=2.0, scale_grad_by_freq : bool=False, sparse : bool=False) -> Tensor torch.nn.functional.embedding_bag(input : Tensor, weight : Tensor, offsets : Optional[Tensor], max_norm : Optional[float], norm_type : float=2.0, scale_grad_by_freq : bool=False, mode : str=mean, sparse : bool=False, per_sample_weights : Optional[Tensor], include_last_offset : bool=False, padding_idx : Optional[int]) -> Tensor torch.nn.functional.feature_alpha_dropout(input : Tensor, p : float=0.5, training : bool=False, inplace : bool=False) -> Tensor torch.nn.functional.fold(input : Tensor, output_size : List[int], kernel_size : List[int], dilation : List[int]=1, padding : List[int]=0, stride : List[int]=1) -> Tensor torch.nn.functional.fractional_max_pool2d_with_indices(input : Tensor, kernel_size : List[int], output_size : Optional[List[int]], output_ratio : Optional[List[float]], return_indices : bool=False, _random_samples : Optional[Tensor]) -> Tuple[Tensor, Tensor] torch.nn.functional.fractional_max_pool3d_with_indices(input : Tensor, kernel_size : List[int], output_size : Optional[List[int]], output_ratio : Optional[List[float]], return_indices : bool=False, _random_samples : Optional[Tensor]) -> Tuple[Tensor, Tensor] torch.nn.functional.gaussian_nll_loss(input : Tensor, target : Tensor, var : Union[Tensor, float], full : bool=False, eps : float=1e-06, reduction : str=mean) -> Tensor torch.nn.functional.glu(input : Tensor, dim : int=-1) -> Tensor torch.nn.functional.grid_sample(input : Tensor, grid : Tensor, mode : str=bilinear, padding_mode : str=zeros, align_corners : Optional[bool]) -> Tensor torch.nn.functional.group_norm(input : Tensor, num_groups : int, weight : Optional[Tensor], bias : Optional[Tensor], eps : float=1e-05) -> Tensor torch.nn.functional.gumbel_softmax(logits : Tensor, tau : float=1.0, hard : bool=False, eps : float=1e-10, dim : int=-1) -> Tensor torch.nn.functional.hardsigmoid(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.hardswish(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.hardtanh(input : Tensor, min_val : float=-1.0, max_val : float=1.0, inplace : bool=False) -> Tensor torch.nn.functional.hinge_embedding_loss(input : Tensor, target : Tensor, margin : float=1.0, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.huber_loss(input : Tensor, target : Tensor, reduction : str=mean, delta : float=1.0, weight : Optional[Tensor]) -> Tensor torch.nn.functional.instance_norm(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor],", "prev_chunk_id": "chunk_512", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_514", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "weight : Optional[Tensor], bias : Optional[Tensor], use_input_stats : bool=True, momentum : float=0.1, eps : float=1e-05) -> Tensor torch.nn.functional.kl_div(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean, log_target : bool=False) -> Tensor torch.nn.functional.l1_loss(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean, weight : Optional[Tensor]) -> Tensor torch.nn.functional.layer_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], bias : Optional[Tensor], eps : float=1e-05) -> Tensor torch.nn.functional.leaky_relu(input : Tensor, negative_slope : float=0.01, inplace : bool=False) -> Tensor torch.nn.functional.local_response_norm(input : Tensor, size : int, alpha : float=0.0001, beta : float=0.75, k : float=1.0) -> Tensor torch.nn.functional.log_softmax(input : Tensor, dim : Optional[int], _stacklevel : int=3, dtype : Optional[int]) -> Tensor torch.nn.functional.lp_pool1d(input : Tensor, norm_type : Union[float, int], kernel_size : int, stride : Optional[List[int]], ceil_mode : bool=False) -> Tensor torch.nn.functional.lp_pool2d(input : Tensor, norm_type : Union[float, int], kernel_size : List[int], stride : Optional[List[int]], ceil_mode : bool=False) -> Tensor torch.nn.functional.lp_pool3d(input : Tensor, norm_type : Union[float, int], kernel_size : List[int], stride : Optional[List[int]], ceil_mode : bool=False) -> Tensor torch.nn.functional.margin_ranking_loss(input1 : Tensor, input2 : Tensor, target : Tensor, margin : float=0.0, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.max_pool1d_with_indices(input : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0, dilation : List[int]=1, ceil_mode : bool=False, return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.max_pool2d_with_indices(input : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0, dilation : List[int]=1, ceil_mode : bool=False, return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.max_pool3d_with_indices(input : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0, dilation : List[int]=1, ceil_mode : bool=False, return_indices : bool=False) -> Tuple[Tensor, Tensor] torch.nn.functional.max_unpool1d(input : Tensor, indices : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0, output_size : Optional[List[int]]) -> Tensor torch.nn.functional.max_unpool2d(input : Tensor, indices : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0,", "prev_chunk_id": "chunk_513", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_515", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "output_size : Optional[List[int]]) -> Tensor torch.nn.functional.max_unpool3d(input : Tensor, indices : Tensor, kernel_size : List[int], stride : Optional[List[int]], padding : List[int]=0, output_size : Optional[List[int]]) -> Tensor torch.nn.functional.mish(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.mse_loss(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean, weight : Optional[Tensor]) -> Tensor torch.nn.functional.multi_head_attention_forward(query : Tensor, key : Tensor, value : Tensor, embed_dim_to_check : int, num_heads : int, in_proj_weight : Optional[Tensor], in_proj_bias : Optional[Tensor], bias_k : Optional[Tensor], bias_v : Optional[Tensor], add_zero_attn : bool, dropout_p : float, out_proj_weight : Tensor, out_proj_bias : Optional[Tensor], training : bool=True, key_padding_mask : Optional[Tensor], need_weights : bool=True, attn_mask : Optional[Tensor], use_separate_proj_weight : bool=False, q_proj_weight : Optional[Tensor], k_proj_weight : Optional[Tensor], v_proj_weight : Optional[Tensor], static_k : Optional[Tensor], static_v : Optional[Tensor], average_attn_weights : bool=True, is_causal : bool=False) -> Tuple[Tensor, Optional[Tensor]] torch.nn.functional.multi_margin_loss(input : Tensor, target : Tensor, p : int=1, margin : float=1.0, weight : Optional[Tensor], size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.multilabel_margin_loss(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.multilabel_soft_margin_loss(input : Tensor, target : Tensor, weight : Optional[Tensor], size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.nll_loss(input : Tensor, target : Tensor, weight : Optional[Tensor], size_average : Optional[bool], ignore_index : int=-100, reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.normalize(input : Tensor, p : float=2.0, dim : int=1, eps : float=1e-12, out : Optional[Tensor]) -> Tensor torch.nn.functional.pad(input : Tensor, pad : List[int], mode : str=constant, value : Optional[float]) -> Tensor torch.nn.functional.poisson_nll_loss(input : Tensor, target : Tensor, log_input : bool=True, full : bool=False, size_average : Optional[bool], eps : float=1e-08, reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.relu(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.relu6(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.rms_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], eps", "prev_chunk_id": "chunk_514", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_516", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Optional[float]) -> Tensor torch.nn.functional.rrelu(input : Tensor, lower : float=0.125, upper : float=0.3333333333333333, training : bool=False, inplace : bool=False) -> Tensor torch.nn.functional.selu(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.sigmoid(input : Tensor) -> Tensor torch.nn.functional.silu(input : Tensor, inplace : bool=False) -> Tensor torch.nn.functional.smooth_l1_loss(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean, beta : float=1.0) -> Tensor torch.nn.functional.soft_margin_loss(input : Tensor, target : Tensor, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.softmax(input : Tensor, dim : Optional[int], _stacklevel : int=3, dtype : Optional[int]) -> Tensor torch.nn.functional.softmin(input : Tensor, dim : Optional[int], _stacklevel : int=3, dtype : Optional[int]) -> Tensor torch.nn.functional.softsign(input : Tensor) -> Tensor torch.nn.functional.tanh(input : Tensor) -> Tensor torch.nn.functional.tanhshrink(input : Tensor) -> Tensor torch.nn.functional.threshold(input : Tensor, threshold : float, value : float, inplace : bool=False) -> Tensor torch.nn.functional.triplet_margin_loss(anchor : Tensor, positive : Tensor, negative : Tensor, margin : float=1.0, p : float=2.0, eps : float=1e-06, swap : bool=False, size_average : Optional[bool], reduce : Optional[bool], reduction : str=mean) -> Tensor torch.nn.functional.unfold(input : Tensor, kernel_size : List[int], dilation : List[int]=1, padding : List[int]=0, stride : List[int]=1) -> Tensor torch.Generator(device : Optional[Device], seed : Optional[int]) -> Generator torch.Size(sizes : List[int]) -> List[int] torch.abs(self : Tensor) -> Tensor torch.abs(self : Tensor, out : Tensor) -> Tensor torch.abs_(self : Tensor) -> Tensor torch.absolute(self : Tensor) -> Tensor torch.absolute(self : Tensor, out : Tensor) -> Tensor torch.acos(self : Tensor) -> Tensor torch.acos(self : Tensor, out : Tensor) -> Tensor torch.acos(a : int) -> float torch.acos(a : float) -> float torch.acos(a : complex) -> complex torch.acos(a : number) -> number torch.acos_(self : Tensor) -> Tensor torch.acosh(self : Tensor) -> Tensor torch.acosh(self : Tensor, out : Tensor) -> Tensor torch.acosh(a : int) -> float torch.acosh(a : float) -> float torch.acosh(a : complex) -> complex torch.acosh(a : number) -> number", "prev_chunk_id": "chunk_515", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_517", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.acosh_(self : Tensor) -> Tensor torch.adaptive_avg_pool1d(self : Tensor, output_size : List[int]) -> Tensor torch.adaptive_avg_pool1d(self : Tensor, output_size : List[int], out : Tensor) -> Tensor torch.adaptive_max_pool1d(self : Tensor, output_size : List[int]) -> Tuple[Tensor, Tensor] torch.add(self : Tensor, other : Tensor, alpha : number=1) -> Tensor torch.add(self : Tensor, other : number, alpha : number=1) -> Tensor torch.add(self : Tensor, other : Tensor, alpha : number=1, out : Tensor) -> Tensor torch.add(self : Tensor, other : number, alpha : number=1, out : Tensor) -> Tensor torch.add(a : List[t], b : List[t]) -> List[t] torch.add(a : str, b : str) -> str torch.add(a : int, b : int) -> int torch.add(a : complex, b : complex) -> complex torch.add(a : float, b : float) -> float torch.add(a : int, b : complex) -> complex torch.add(a : complex, b : int) -> complex torch.add(a : float, b : complex) -> complex torch.add(a : complex, b : float) -> complex torch.add(a : int, b : float) -> float torch.add(a : float, b : int) -> float torch.add(a : number, b : number) -> number torch.addbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.addbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.addcdiv(self : Tensor, tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor torch.addcdiv(self : Tensor, tensor1 : Tensor, tensor2 : Tensor, value : number=1, out : Tensor) -> Tensor torch.addcmul(self : Tensor, tensor1 : Tensor, tensor2 : Tensor, value : number=1) -> Tensor torch.addcmul(self : Tensor, tensor1 : Tensor, tensor2 : Tensor, value : number=1, out : Tensor) -> Tensor torch.addmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.addmm(self : Tensor, mat1 :", "prev_chunk_id": "chunk_516", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_518", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, mat2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.addmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.addmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1) -> Tensor torch.addmv(self : Tensor, mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.addmv(self : Tensor, mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.addmv_(self : Tensor, mat : Tensor, vec : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.addr(self : Tensor, vec1 : Tensor, vec2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.addr(self : Tensor, vec1 : Tensor, vec2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.adjoint(self : Tensor) -> Tensor torch.affine_grid_generator(theta : Tensor, size : List[int], align_corners : bool) -> Tensor torch.affine_grid_generator(theta : Tensor, size : List[int], align_corners : bool, out : Tensor) -> Tensor torch.alias_copy(self : Tensor) -> Tensor torch.alias_copy(self : Tensor, out : Tensor) -> Tensor torch.align_tensors(tensors : List[Tensor]) -> List[Tensor] torch.all(self : Tensor) -> Tensor torch.all(self : Tensor, dim : int, keepdim : bool=False) -> Tensor torch.all(self : Tensor, dim : Optional[List[int]], keepdim : bool=False) -> Tensor torch.all(self : Tensor, dim : int, keepdim : bool=False, out : Tensor) -> Tensor torch.all(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, out : Tensor) -> Tensor torch.all(self : Tensor, out : Tensor) -> Tensor torch.all(self : Tensor, dim : str, keepdim : bool=False) -> Tensor torch.all(self : Tensor, dim : str, keepdim : bool=False, out : Tensor) -> Tensor torch.all(self : List[int]) -> bool torch.all(self : List[float]) -> bool torch.all(self : List[bool]) -> bool", "prev_chunk_id": "chunk_517", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_519", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.allclose(self : Tensor, other : Tensor, rtol : float=1e-05, atol : float=1e-08, equal_nan : bool=False) -> bool torch.alpha_dropout(input : Tensor, p : float, train : bool) -> Tensor torch.alpha_dropout_(self : Tensor, p : float, train : bool) -> Tensor torch.amax(self : Tensor, dim : List[int]=[], keepdim : bool=False) -> Tensor torch.amax(self : Tensor, dim : List[int]=[], keepdim : bool=False, out : Tensor) -> Tensor torch.amin(self : Tensor, dim : List[int]=[], keepdim : bool=False) -> Tensor torch.amin(self : Tensor, dim : List[int]=[], keepdim : bool=False, out : Tensor) -> Tensor torch.aminmax(self : Tensor, dim : Optional[int], keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.aminmax(self : Tensor, dim : Optional[int], keepdim : bool=False, min : Tensor, max : Tensor) -> Tuple[Tensor, Tensor] torch.angle(self : Tensor) -> Tensor torch.angle(self : Tensor, out : Tensor) -> Tensor torch.angle(a : int) -> float torch.angle(a : float) -> float torch.angle(a : complex) -> float torch.angle(a : number) -> number torch.any(self : Tensor) -> Tensor torch.any(self : Tensor, dim : int, keepdim : bool=False) -> Tensor torch.any(self : Tensor, dim : Optional[List[int]], keepdim : bool=False) -> Tensor torch.any(self : Tensor, dim : int, keepdim : bool=False, out : Tensor) -> Tensor torch.any(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, out : Tensor) -> Tensor torch.any(self : Tensor, out : Tensor) -> Tensor torch.any(self : Tensor, dim : str, keepdim : bool=False) -> Tensor torch.any(self : Tensor, dim : str, keepdim : bool=False, out : Tensor) -> Tensor torch.any(self : List[str]) -> bool torch.any(self : List[int]) -> bool torch.any(self : List[float]) -> bool torch.any(self : List[bool]) -> bool torch.arange(end : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.arange(start : number, end : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.arange(start : number,", "prev_chunk_id": "chunk_518", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_520", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "end : number, step : number=1, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.arange(start : number, end : number, step : number=1, out : Tensor) -> Tensor torch.arange(end : number, out : Tensor) -> Tensor torch.arccos(self : Tensor) -> Tensor torch.arccos(self : Tensor, out : Tensor) -> Tensor torch.arccos_(self : Tensor) -> Tensor torch.arccosh(self : Tensor) -> Tensor torch.arccosh(self : Tensor, out : Tensor) -> Tensor torch.arccosh_(self : Tensor) -> Tensor torch.arcsin(self : Tensor) -> Tensor torch.arcsin(self : Tensor, out : Tensor) -> Tensor torch.arcsin_(self : Tensor) -> Tensor torch.arcsinh(self : Tensor) -> Tensor torch.arcsinh(self : Tensor, out : Tensor) -> Tensor torch.arcsinh_(self : Tensor) -> Tensor torch.arctan(self : Tensor) -> Tensor torch.arctan(self : Tensor, out : Tensor) -> Tensor torch.arctan2(self : Tensor, other : Tensor) -> Tensor torch.arctan2(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.arctan_(self : Tensor) -> Tensor torch.arctanh(self : Tensor) -> Tensor torch.arctanh(self : Tensor, out : Tensor) -> Tensor torch.arctanh_(self : Tensor) -> Tensor torch.argmax(self : Tensor, dim : Optional[int], keepdim : bool=False) -> Tensor torch.argmax(self : Tensor, dim : Optional[int], keepdim : bool=False, out : Tensor) -> Tensor torch.argmin(self : Tensor, dim : Optional[int], keepdim : bool=False) -> Tensor torch.argmin(self : Tensor, dim : Optional[int], keepdim : bool=False, out : Tensor) -> Tensor torch.argsort(self : Tensor, dim : int=-1, descending : bool=False) -> Tensor torch.argsort(self : Tensor, stable : bool, dim : int=-1, descending : bool=False) -> Tensor torch.argsort(self : Tensor, stable : bool, dim : int=-1, descending : bool=False, out : Tensor) -> Tensor torch.argsort(self : Tensor, dim : str, descending : bool=False) -> Tensor torch.argwhere(self : Tensor) -> Tensor torch.as_strided(self : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor torch.as_strided_(self : Tensor, size : List[int], stride : List[int],", "prev_chunk_id": "chunk_519", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_521", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "storage_offset : Optional[int]) -> Tensor torch.as_strided_copy(self : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor torch.as_strided_copy(self : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int], out : Tensor) -> Tensor torch.as_strided_scatter(self : Tensor, src : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int]) -> Tensor torch.as_strided_scatter(self : Tensor, src : Tensor, size : List[int], stride : List[int], storage_offset : Optional[int], out : Tensor) -> Tensor torch.as_tensor(t : bool, dtype : Optional[int], device : Optional[Device]) -> Tensor torch.as_tensor(t : float, dtype : Optional[int], device : Optional[Device]) -> Tensor torch.as_tensor(t : int, dtype : Optional[int], device : Optional[Device]) -> Tensor torch.as_tensor(t : complex, dtype : Optional[int], device : Optional[Device]) -> Tensor torch.as_tensor(data : Tensor, dtype : Optional[int], device : Optional[Device]) -> Tensor torch.as_tensor(data : List[t], dtype : Optional[int], device : Optional[Device]) -> Tensor torch.asin(self : Tensor) -> Tensor torch.asin(self : Tensor, out : Tensor) -> Tensor torch.asin(a : int) -> float torch.asin(a : float) -> float torch.asin(a : complex) -> complex torch.asin(a : number) -> number torch.asin_(self : Tensor) -> Tensor torch.asinh(self : Tensor) -> Tensor torch.asinh(self : Tensor, out : Tensor) -> Tensor torch.asinh(a : int) -> float torch.asinh(a : float) -> float torch.asinh(a : complex) -> complex torch.asinh(a : number) -> number torch.asinh_(self : Tensor) -> Tensor torch.atan(self : Tensor) -> Tensor torch.atan(self : Tensor, out : Tensor) -> Tensor torch.atan(a : int) -> float torch.atan(a : float) -> float torch.atan(a : complex) -> complex torch.atan(a : number) -> number torch.atan2(self : Tensor, other : Tensor) -> Tensor torch.atan2(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.atan2(a : int, b : int) -> float torch.atan2(a : float, b : float) -> float torch.atan2(a : int, b : float) -> float torch.atan2(a : float, b : int) -> float torch.atan2(a", "prev_chunk_id": "chunk_520", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_522", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": number, b : number) -> float torch.atan_(self : Tensor) -> Tensor torch.atanh(self : Tensor) -> Tensor torch.atanh(self : Tensor, out : Tensor) -> Tensor torch.atanh(a : int) -> float torch.atanh(a : float) -> float torch.atanh(a : complex) -> complex torch.atanh(a : number) -> number torch.atanh_(self : Tensor) -> Tensor torch.atleast_1d(self : Tensor) -> Tensor torch.atleast_1d(tensors : List[Tensor]) -> List[Tensor] torch.atleast_2d(self : Tensor) -> Tensor torch.atleast_2d(tensors : List[Tensor]) -> List[Tensor] torch.atleast_3d(self : Tensor) -> Tensor torch.atleast_3d(tensors : List[Tensor]) -> List[Tensor] torch.avg_pool1d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], ceil_mode : bool=False, count_include_pad : bool=True) -> Tensor torch.avg_pool1d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], ceil_mode : bool=False, count_include_pad : bool=True, out : Tensor) -> Tensor torch.baddbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.baddbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.baddbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.baddbmm(self : Tensor, batch1 : Tensor, batch2 : Tensor, out_dtype : int, beta : number=1, alpha : number=1) -> Tensor torch.bartlett_window(window_length : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.bartlett_window(window_length : int, periodic : bool, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.bartlett_window(window_length : int, out : Tensor) -> Tensor torch.bartlett_window(window_length : int, periodic : bool, out : Tensor) -> Tensor torch.batch_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, momentum : float, eps : float, cudnn_enabled : bool) -> Tensor torch.batch_norm_backward_elemt(grad_out : Tensor, input : Tensor, mean : Tensor, invstd : Tensor,", "prev_chunk_id": "chunk_521", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_523", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "weight : Optional[Tensor], sum_dy : Tensor, sum_dy_xmu : Tensor, count : Tensor, out : Tensor) -> Tensor torch.batch_norm_backward_elemt(grad_out : Tensor, input : Tensor, mean : Tensor, invstd : Tensor, weight : Optional[Tensor], sum_dy : Tensor, sum_dy_xmu : Tensor, count : Tensor) -> Tensor torch.batch_norm_backward_reduce(grad_out : Tensor, input : Tensor, mean : Tensor, invstd : Tensor, weight : Optional[Tensor], input_g : bool, weight_g : bool, bias_g : bool, out0 : Tensor, out1 : Tensor, out2 : Tensor, out3 : Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.batch_norm_backward_reduce(grad_out : Tensor, input : Tensor, mean : Tensor, invstd : Tensor, weight : Optional[Tensor], input_g : bool, weight_g : bool, bias_g : bool) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.batch_norm_elemt(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], mean : Tensor, invstd : Tensor, eps : float, out : Tensor) -> Tensor torch.batch_norm_elemt(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], mean : Tensor, invstd : Tensor, eps : float) -> Tensor torch.batch_norm_gather_stats(input : Tensor, mean : Tensor, invstd : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float, eps : float, count : int, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.batch_norm_gather_stats(input : Tensor, mean : Tensor, invstd : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float, eps : float, count : int) -> Tuple[Tensor, Tensor] torch.batch_norm_gather_stats_with_counts(input : Tensor, mean : Tensor, invstd : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float, eps : float, counts : Tensor, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.batch_norm_gather_stats_with_counts(input : Tensor, mean : Tensor, invstd : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float, eps : float, counts : Tensor) -> Tuple[Tensor, Tensor] torch.batch_norm_stats(input : Tensor, eps : float, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.batch_norm_stats(input : Tensor, eps : float) -> Tuple[Tensor, Tensor]", "prev_chunk_id": "chunk_522", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_524", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.batch_norm_update_stats(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float) -> Tuple[Tensor, Tensor] torch.batch_norm_update_stats(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], momentum : float, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.bernoulli(self : Tensor, generator : Optional[Generator]) -> Tensor torch.bernoulli(self : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.bernoulli(self : Tensor, p : float, generator : Optional[Generator]) -> Tensor torch.bernoulli(self : Tensor, p : Tensor, generator : Optional[Generator]) -> Tensor torch.bernoulli(self : Tensor, p : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.bernoulli(self : Tensor, p : float=0.5, generator : Optional[Generator], out : Tensor) -> Tensor torch.bilinear(input1 : Tensor, input2 : Tensor, weight : Tensor, bias : Optional[Tensor]) -> Tensor torch.binary_cross_entropy_with_logits(self : Tensor, target : Tensor, weight : Optional[Tensor], pos_weight : Optional[Tensor], reduction : int=1) -> Tensor torch.binary_cross_entropy_with_logits(self : Tensor, target : Tensor, weight : Optional[Tensor], pos_weight : Optional[Tensor], reduction : int=1, out : Tensor) -> Tensor torch.bincount(self : Tensor, weights : Optional[Tensor], minlength : int=0) -> Tensor torch.bincount(self : Tensor, weights : Optional[Tensor], minlength : int=0, out : Tensor) -> Tensor torch.binomial(count : Tensor, prob : Tensor, generator : Optional[Generator]) -> Tensor torch.binomial(count : Tensor, prob : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.bitwise_and(self : Tensor, other : Tensor) -> Tensor torch.bitwise_and(self : Tensor, other : number) -> Tensor torch.bitwise_and(self : number, other : Tensor) -> Tensor torch.bitwise_and(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.bitwise_and(self : Tensor, other : number, out : Tensor) -> Tensor torch.bitwise_and(self : number, other : Tensor, out : Tensor) -> Tensor torch.bitwise_left_shift(self : Tensor, other : Tensor) -> Tensor torch.bitwise_left_shift(self : Tensor, other : number) -> Tensor torch.bitwise_left_shift(self : number, other : Tensor) -> Tensor torch.bitwise_left_shift(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.bitwise_left_shift(self :", "prev_chunk_id": "chunk_523", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_525", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, other : number, out : Tensor) -> Tensor torch.bitwise_left_shift(self : number, other : Tensor, out : Tensor) -> Tensor torch.bitwise_not(self : Tensor) -> Tensor torch.bitwise_not(self : Tensor, out : Tensor) -> Tensor torch.bitwise_or(self : Tensor, other : Tensor) -> Tensor torch.bitwise_or(self : Tensor, other : number) -> Tensor torch.bitwise_or(self : number, other : Tensor) -> Tensor torch.bitwise_or(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.bitwise_or(self : Tensor, other : number, out : Tensor) -> Tensor torch.bitwise_or(self : number, other : Tensor, out : Tensor) -> Tensor torch.bitwise_right_shift(self : Tensor, other : Tensor) -> Tensor torch.bitwise_right_shift(self : Tensor, other : number) -> Tensor torch.bitwise_right_shift(self : number, other : Tensor) -> Tensor torch.bitwise_right_shift(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.bitwise_right_shift(self : Tensor, other : number, out : Tensor) -> Tensor torch.bitwise_right_shift(self : number, other : Tensor, out : Tensor) -> Tensor torch.bitwise_xor(self : Tensor, other : Tensor) -> Tensor torch.bitwise_xor(self : Tensor, other : number) -> Tensor torch.bitwise_xor(self : number, other : Tensor) -> Tensor torch.bitwise_xor(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.bitwise_xor(self : Tensor, other : number, out : Tensor) -> Tensor torch.bitwise_xor(self : number, other : Tensor, out : Tensor) -> Tensor torch.blackman_window(window_length : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.blackman_window(window_length : int, periodic : bool, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.blackman_window(window_length : int, out : Tensor) -> Tensor torch.blackman_window(window_length : int, periodic : bool, out : Tensor) -> Tensor torch.block_diag(tensors : List[Tensor]) -> Tensor torch.block_diag(tensors : List[Tensor], out : Tensor) -> Tensor torch.bmm(self : Tensor, mat2 : Tensor) -> Tensor torch.bmm(self : Tensor, mat2 : Tensor, out : Tensor) -> Tensor torch.bmm(self : Tensor, mat2 : Tensor, out_dtype :", "prev_chunk_id": "chunk_524", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_526", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "int, out : Tensor) -> Tensor torch.bmm(self : Tensor, mat2 : Tensor, out_dtype : int) -> Tensor torch.broadcast_tensors(tensors : List[Tensor]) -> List[Tensor] torch.broadcast_to(self : Tensor, size : List[int]) -> Tensor torch.bucketize(self : Tensor, boundaries : Tensor, out_int32 : bool=False, right : bool=False) -> Tensor torch.bucketize(self : number, boundaries : Tensor, out_int32 : bool=False, right : bool=False) -> Tensor torch.bucketize(self : Tensor, boundaries : Tensor, out_int32 : bool=False, right : bool=False, out : Tensor) -> Tensor torch.bucketize(self : number, boundaries : Tensor, out_int32 : bool=False, right : bool=False, out : Tensor) -> Tensor torch.can_cast(from_ : int, to : int) -> bool torch.cartesian_prod(tensors : List[Tensor]) -> Tensor torch.cat(tensors : List[Tensor], dim : int=0) -> Tensor torch.cat(tensors : List[Tensor], dim : str) -> Tensor torch.cat(tensors : List[Tensor], dim : str, out : Tensor) -> Tensor torch.cat(tensors : List[Tensor], dim : int=0, out : Tensor) -> Tensor torch.ccol_indices_copy(self : Tensor) -> Tensor torch.ccol_indices_copy(self : Tensor, out : Tensor) -> Tensor torch.ceil(self : Tensor) -> Tensor torch.ceil(self : Tensor, out : Tensor) -> Tensor torch.ceil(a : int) -> int torch.ceil(a : float) -> int torch.ceil(a : number) -> number torch.ceil_(self : Tensor) -> Tensor torch.celu(self : Tensor, alpha : number=1.0) -> Tensor torch.celu(self : Tensor, alpha : number=1.0, out : Tensor) -> Tensor torch.celu_(self : Tensor, alpha : number=1.0) -> Tensor torch.chain_matmul(matrices : List[Tensor]) -> Tensor torch.chain_matmul(matrices : List[Tensor], out : Tensor) -> Tensor torch.channel_shuffle(self : Tensor, groups : int) -> Tensor torch.channel_shuffle(self : Tensor, groups : int, out : Tensor) -> Tensor torch.cholesky(self : Tensor, upper : bool=False) -> Tensor torch.cholesky(self : Tensor, upper : bool=False, out : Tensor) -> Tensor torch.cholesky_inverse(self : Tensor, upper : bool=False) -> Tensor torch.cholesky_inverse(self : Tensor, upper : bool=False, out : Tensor) -> Tensor torch.cholesky_solve(self : Tensor, input2 : Tensor, upper : bool=False) -> Tensor torch.cholesky_solve(self : Tensor,", "prev_chunk_id": "chunk_525", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_527", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "input2 : Tensor, upper : bool=False, out : Tensor) -> Tensor torch.choose_qparams_optimized(input : Tensor, numel : int, n_bins : int, ratio : float, bit_width : int) -> Tuple[Tensor, Tensor] torch.chunk(self : Tensor, chunks : int, dim : int=0) -> List[Tensor] torch.clamp(self : Tensor, min : Optional[number], max : Optional[number]) -> Tensor torch.clamp(self : Tensor, min : Optional[Tensor], max : Optional[Tensor]) -> Tensor torch.clamp(self : Tensor, min : Optional[number], max : Optional[number], out : Tensor) -> Tensor torch.clamp(self : Tensor, min : Optional[Tensor], max : Optional[Tensor], out : Tensor) -> Tensor torch.clamp_(self : Tensor, min : Optional[number], max : Optional[number]) -> Tensor torch.clamp_(self : Tensor, min : Optional[Tensor], max : Optional[Tensor]) -> Tensor torch.clamp_max(self : Tensor, max : number) -> Tensor torch.clamp_max(self : Tensor, max : Tensor) -> Tensor torch.clamp_max(self : Tensor, max : number, out : Tensor) -> Tensor torch.clamp_max(self : Tensor, max : Tensor, out : Tensor) -> Tensor torch.clamp_max_(self : Tensor, max : number) -> Tensor torch.clamp_max_(self : Tensor, max : Tensor) -> Tensor torch.clamp_min(self : Tensor, min : number) -> Tensor torch.clamp_min(self : Tensor, min : Tensor) -> Tensor torch.clamp_min(self : Tensor, min : number, out : Tensor) -> Tensor torch.clamp_min(self : Tensor, min : Tensor, out : Tensor) -> Tensor torch.clamp_min_(self : Tensor, min : number) -> Tensor torch.clamp_min_(self : Tensor, min : Tensor) -> Tensor torch.clip(self : Tensor, min : Optional[number], max : Optional[number]) -> Tensor torch.clip(self : Tensor, min : Optional[Tensor], max : Optional[Tensor]) -> Tensor torch.clip(self : Tensor, min : Optional[number], max : Optional[number], out : Tensor) -> Tensor torch.clip(self : Tensor, min : Optional[Tensor], max : Optional[Tensor], out : Tensor) -> Tensor torch.clip_(self : Tensor, min : Optional[number], max : Optional[number]) -> Tensor torch.clip_(self : Tensor, min : Optional[Tensor], max : Optional[Tensor]) -> Tensor torch.clone(self : Tensor, memory_format : Optional[int]) -> Tensor", "prev_chunk_id": "chunk_526", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_528", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.clone(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.col_indices_copy(self : Tensor) -> Tensor torch.col_indices_copy(self : Tensor, out : Tensor) -> Tensor torch.column_stack(tensors : List[Tensor]) -> Tensor torch.column_stack(tensors : List[Tensor], out : Tensor) -> Tensor torch.combinations(self : Tensor, r : int=2, with_replacement : bool=False) -> Tensor torch.complex(real : Tensor, imag : Tensor) -> Tensor torch.complex(real : Tensor, imag : Tensor, out : Tensor) -> Tensor torch.concat(tensors : List[Tensor], dim : int=0) -> Tensor torch.concat(tensors : List[Tensor], dim : int=0, out : Tensor) -> Tensor torch.concat(tensors : List[Tensor], dim : str) -> Tensor torch.concat(tensors : List[Tensor], dim : str, out : Tensor) -> Tensor torch.concatenate(tensors : List[Tensor], dim : int=0) -> Tensor torch.concatenate(tensors : List[Tensor], dim : int=0, out : Tensor) -> Tensor torch.concatenate(tensors : List[Tensor], dim : str) -> Tensor torch.concatenate(tensors : List[Tensor], dim : str, out : Tensor) -> Tensor torch.conj(self : Tensor) -> Tensor torch.conj_physical(self : Tensor) -> Tensor torch.conj_physical(self : Tensor, out : Tensor) -> Tensor torch.conj_physical_(self : Tensor) -> Tensor torch.constant_pad_nd(self : Tensor, pad : List[int], value : number=0) -> Tensor torch.constant_pad_nd(self : Tensor, pad : List[int], value : number=0, out : Tensor) -> Tensor torch.conv1d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1], padding : List[int]=[0], dilation : List[int]=[1], groups : int=1) -> Tensor torch.conv1d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1], padding : str=valid, dilation : List[int]=[1], groups : int=1) -> Tensor torch.conv2d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], groups : int=1) -> Tensor torch.conv2d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1, 1], padding : str=valid, dilation : List[int]=[1, 1], groups : int=1) -> Tensor torch.conv3d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1,", "prev_chunk_id": "chunk_527", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_529", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "1, 1], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], groups : int=1) -> Tensor torch.conv3d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : str=valid, dilation : List[int]=[1, 1, 1], groups : int=1) -> Tensor torch.conv_tbc(self : Tensor, weight : Tensor, bias : Tensor, pad : int=0) -> Tensor torch.conv_tbc(self : Tensor, weight : Tensor, bias : Tensor, pad : int=0, out : Tensor) -> Tensor torch.conv_transpose1d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1], padding : List[int]=[0], output_padding : List[int]=[0], groups : int=1, dilation : List[int]=[1]) -> Tensor torch.conv_transpose2d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], output_padding : List[int]=[0, 0], groups : int=1, dilation : List[int]=[1, 1]) -> Tensor torch.conv_transpose3d(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], output_padding : List[int]=[0, 0, 0], groups : int=1, dilation : List[int]=[1, 1, 1]) -> Tensor torch.convolution(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], transposed : bool, output_padding : List[int], groups : int) -> Tensor torch.convolution(input : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], transposed : bool, output_padding : List[int], groups : int, out : Tensor) -> Tensor torch.copysign(self : Tensor, other : Tensor) -> Tensor torch.copysign(self : Tensor, other : number) -> Tensor torch.copysign(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.copysign(self : Tensor, other : number, out : Tensor) -> Tensor torch.copysign(a : int, b : int) -> float torch.copysign(a : float, b : float) -> float torch.copysign(a : int, b : float) -> float torch.copysign(a : float, b : int) -> float torch.copysign(a : number, b", "prev_chunk_id": "chunk_528", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_530", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": number) -> float torch.corrcoef(self : Tensor) -> Tensor torch.cos(self : Tensor) -> Tensor torch.cos(self : Tensor, out : Tensor) -> Tensor torch.cos(a : int) -> float torch.cos(a : float) -> float torch.cos(a : complex) -> complex torch.cos(a : number) -> number torch.cos_(self : Tensor) -> Tensor torch.cosh(self : Tensor) -> Tensor torch.cosh(self : Tensor, out : Tensor) -> Tensor torch.cosh(a : int) -> float torch.cosh(a : float) -> float torch.cosh(a : complex) -> complex torch.cosh(a : number) -> number torch.cosh_(self : Tensor) -> Tensor torch.cosine_embedding_loss(input1 : Tensor, input2 : Tensor, target : Tensor, margin : float=0.0, reduction : int=1) -> Tensor torch.cosine_similarity(x1 : Tensor, x2 : Tensor, dim : int=1, eps : float=1e-08) -> Tensor torch.count_nonzero(self : Tensor, dim : List[int]) -> Tensor torch.count_nonzero(self : Tensor, dim : List[int], out : Tensor) -> Tensor torch.count_nonzero(self : Tensor, dim : Optional[int]) -> Tensor torch.count_nonzero(self : Tensor, dim : Optional[int], out : Tensor) -> Tensor torch.cov(self : Tensor, correction : int=1, fweights : Optional[Tensor], aweights : Optional[Tensor]) -> Tensor torch.cross(self : Tensor, other : Tensor, dim : Optional[int]) -> Tensor torch.cross(self : Tensor, other : Tensor, dim : Optional[int], out : Tensor) -> Tensor torch.crow_indices_copy(self : Tensor) -> Tensor torch.crow_indices_copy(self : Tensor, out : Tensor) -> Tensor torch.ctc_loss(log_probs : Tensor, targets : Tensor, input_lengths : List[int], target_lengths : List[int], blank : int=0, reduction : int=1, zero_infinity : bool=False) -> Tensor torch.ctc_loss(log_probs : Tensor, targets : Tensor, input_lengths : Tensor, target_lengths : Tensor, blank : int=0, reduction : int=1, zero_infinity : bool=False) -> Tensor torch.cudnn_affine_grid_generator(theta : Tensor, N : int, C : int, H : int, W : int, out : Tensor) -> Tensor torch.cudnn_affine_grid_generator(theta : Tensor, N : int, C : int, H : int, W : int) -> Tensor torch.cudnn_batch_norm(input : Tensor, weight : Tensor, bias : Optional[Tensor], running_mean :", "prev_chunk_id": "chunk_529", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_531", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Optional[Tensor], running_var : Optional[Tensor], training : bool, exponential_average_factor : float, epsilon : float) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.cudnn_batch_norm(input : Tensor, weight : Tensor, bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, exponential_average_factor : float, epsilon : float, out0 : Tensor, out1 : Tensor, out2 : Tensor, out3 : Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.cudnn_convolution(self : Tensor, weight : Tensor, padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, allow_tf32 : bool) -> Tensor torch.cudnn_convolution(self : Tensor, weight : Tensor, padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, allow_tf32 : bool, out : Tensor) -> Tensor torch.cudnn_convolution_add_relu(self : Tensor, weight : Tensor, z : Tensor, alpha : Optional[number], bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int, out : Tensor) -> Tensor torch.cudnn_convolution_add_relu(self : Tensor, weight : Tensor, z : Tensor, alpha : Optional[number], bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int) -> Tensor torch.cudnn_convolution_relu(self : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int, out : Tensor) -> Tensor torch.cudnn_convolution_relu(self : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int) -> Tensor torch.cudnn_convolution_transpose(self : Tensor, weight : Tensor, padding : List[int], output_padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, allow_tf32 : bool) -> Tensor torch.cudnn_convolution_transpose(self : Tensor, weight : Tensor, padding : List[int], output_padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, allow_tf32 : bool, out : Tensor) -> Tensor torch.cudnn_grid_sampler(self : Tensor,", "prev_chunk_id": "chunk_530", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_532", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "grid : Tensor) -> Tensor torch.cudnn_grid_sampler(self : Tensor, grid : Tensor, out : Tensor) -> Tensor torch.cudnn_is_acceptable(self : Tensor) -> bool torch.cummax(self : Tensor, dim : int) -> Tuple[Tensor, Tensor] torch.cummax(self : Tensor, dim : str) -> Tuple[Tensor, Tensor] torch.cummax(self : Tensor, dim : str, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.cummax(self : Tensor, dim : int, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.cummin(self : Tensor, dim : int) -> Tuple[Tensor, Tensor] torch.cummin(self : Tensor, dim : str) -> Tuple[Tensor, Tensor] torch.cummin(self : Tensor, dim : str, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.cummin(self : Tensor, dim : int, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.cumprod(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch.cumprod(self : Tensor, dim : str, dtype : Optional[int]) -> Tensor torch.cumprod(self : Tensor, dim : str, dtype : Optional[int], out : Tensor) -> Tensor torch.cumprod(self : Tensor, dim : int, dtype : Optional[int], out : Tensor) -> Tensor torch.cumsum(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch.cumsum(self : Tensor, dim : str, dtype : Optional[int]) -> Tensor torch.cumsum(self : Tensor, dim : str, dtype : Optional[int], out : Tensor) -> Tensor torch.cumsum(self : Tensor, dim : int, dtype : Optional[int], out : Tensor) -> Tensor torch.cumulative_trapezoid(y : Tensor, x : Tensor, dim : int=-1) -> Tensor torch.cumulative_trapezoid(y : Tensor, dx : number=1, dim : int=-1) -> Tensor torch.deg2rad(self : Tensor) -> Tensor torch.deg2rad(self : Tensor, out : Tensor) -> Tensor torch.deg2rad_(self : Tensor) -> Tensor torch.dequantize(self : Tensor) -> Tensor torch.dequantize(self : Tensor, out : Tensor) -> Tensor torch.dequantize(tensors : List[Tensor], out : List[Tensor]) -> Tuple[] torch.dequantize(tensors : List[Tensor]) -> List[Tensor] torch.dequantize(qtensor : Tensor) -> Tensor torch.dequantize(qtensors : List[Tensor]) -> List[Tensor] torch.dequantize(tensors : Any) -> Any torch.det(self :", "prev_chunk_id": "chunk_531", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_533", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor) -> Tensor torch.detach(self : Tensor) -> Tensor torch.detach_(self : Tensor) -> Tensor torch.detach_copy(self : Tensor) -> Tensor torch.detach_copy(self : Tensor, out : Tensor) -> Tensor torch.device(a : str) -> Device torch.device(type : str, index : int) -> Device torch.diag(self : Tensor, diagonal : int=0) -> Tensor torch.diag(self : Tensor, diagonal : int=0, out : Tensor) -> Tensor torch.diag_embed(self : Tensor, offset : int=0, dim1 : int=-2, dim2 : int=-1) -> Tensor torch.diag_embed(self : Tensor, offset : int=0, dim1 : int=-2, dim2 : int=-1, out : Tensor) -> Tensor torch.diagflat(self : Tensor, offset : int=0) -> Tensor torch.diagonal(self : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1) -> Tensor torch.diagonal(self : Tensor, outdim : str, dim1 : str, dim2 : str, offset : int=0) -> Tensor torch.diagonal_copy(self : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1) -> Tensor torch.diagonal_copy(self : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1, out : Tensor) -> Tensor torch.diagonal_scatter(self : Tensor, src : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1) -> Tensor torch.diagonal_scatter(self : Tensor, src : Tensor, offset : int=0, dim1 : int=0, dim2 : int=1, out : Tensor) -> Tensor torch.diff(self : Tensor, n : int=1, dim : int=-1, prepend : Optional[Tensor], append : Optional[Tensor]) -> Tensor torch.diff(self : Tensor, n : int=1, dim : int=-1, prepend : Optional[Tensor], append : Optional[Tensor], out : Tensor) -> Tensor torch.digamma(self : Tensor) -> Tensor torch.digamma(self : Tensor, out : Tensor) -> Tensor torch.dist(self : Tensor, other : Tensor, p : number=2) -> Tensor torch.dist(self : Tensor, other : Tensor, p : number=2, out : Tensor) -> Tensor torch.div(self : Tensor, other : Tensor) -> Tensor torch.div(self : Tensor, other : number) -> Tensor torch.div(self : Tensor, other : Tensor, rounding_mode : Optional[str]) -> Tensor torch.div(self :", "prev_chunk_id": "chunk_532", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_534", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, other : number, rounding_mode : Optional[str]) -> Tensor torch.div(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.div(self : Tensor, other : Tensor, rounding_mode : Optional[str], out : Tensor) -> Tensor torch.div(self : Tensor, other : number, out : Tensor) -> Tensor torch.div(self : Tensor, other : number, rounding_mode : Optional[str], out : Tensor) -> Tensor torch.div(a : int, b : int) -> float torch.div(a : complex, b : complex) -> complex torch.div(a : float, b : float) -> float torch.div(a : number, b : number) -> float torch.divide(self : Tensor, other : Tensor) -> Tensor torch.divide(self : Tensor, other : number) -> Tensor torch.divide(self : Tensor, other : Tensor, rounding_mode : Optional[str]) -> Tensor torch.divide(self : Tensor, other : number, rounding_mode : Optional[str]) -> Tensor torch.divide(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.divide(self : Tensor, other : Tensor, rounding_mode : Optional[str], out : Tensor) -> Tensor torch.dot(self : Tensor, tensor : Tensor) -> Tensor torch.dot(self : Tensor, tensor : Tensor, out : Tensor) -> Tensor torch.dropout(input : Tensor, p : float, train : bool) -> Tensor torch.dropout_(self : Tensor, p : float, train : bool) -> Tensor torch.dsplit(self : Tensor, sections : int) -> List[Tensor] torch.dsplit(self : Tensor, indices : List[int]) -> List[Tensor] torch.dstack(tensors : List[Tensor]) -> Tensor torch.dstack(tensors : List[Tensor], out : Tensor) -> Tensor torch.einsum(equation : str, tensors : List[Tensor], path : Optional[List[int]]) -> Tensor torch.einsum(a : Tensor) -> Tensor torch.embedding(weight : Tensor, indices : Tensor, padding_idx : int=-1, scale_grad_by_freq : bool=False, sparse : bool=False) -> Tensor torch.embedding(weight : Tensor, indices : Tensor, padding_idx : int=-1, scale_grad_by_freq : bool=False, sparse : bool=False, out : Tensor) -> Tensor torch.embedding_bag(weight : Tensor, indices : Tensor, offsets : Tensor, scale_grad_by_freq : bool=False, mode : int=0, sparse : bool=False, per_sample_weights : Optional[Tensor], include_last_offset :", "prev_chunk_id": "chunk_533", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_535", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "bool=False) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.embedding_bag(weight : Tensor, indices : Tensor, offsets : Tensor, scale_grad_by_freq : bool, mode : int, sparse : bool, per_sample_weights : Optional[Tensor], include_last_offset : bool, padding_idx : Optional[int]) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.embedding_renorm_(self : Tensor, indices : Tensor, max_norm : float, norm_type : float) -> Tensor torch.empty(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.empty(size : List[int], memory_format : Optional[int], out : Tensor) -> Tensor torch.empty(size : List[int], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.empty(size : List[int], names : Optional[List[str]], memory_format : Optional[int], out : Tensor) -> Tensor torch.empty_like(self : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.empty_like(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.empty_permuted(size : List[int], physical_layout : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.empty_permuted(size : List[int], physical_layout : List[int], out : Tensor) -> Tensor torch.empty_quantized(size : List[int], qtensor : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.empty_quantized(size : List[int], qtensor : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.empty_strided(size : List[int], stride : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.empty_strided(size : List[int], stride : List[int], out : Tensor) -> Tensor torch.eq(self : Tensor, other : Tensor) -> Tensor torch.eq(self : Tensor, other : number) -> Tensor torch.eq(self : Tensor, other : number, out : Tensor) -> Tensor torch.eq(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.eq(a : List[int], b : List[int]) -> bool torch.eq(a : Device, b", "prev_chunk_id": "chunk_534", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_536", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Device) -> bool torch.eq(a : bool, b : bool) -> bool torch.eq(a : AnyEnumType, b : AnyEnumType) -> bool torch.eq(a : int, b : int) -> bool torch.eq(a : complex, b : complex) -> bool torch.eq(a : float, b : float) -> bool torch.eq(a : int, b : float) -> bool torch.eq(a : float, b : int) -> bool torch.eq(a : float, b : complex) -> bool torch.eq(a : complex, b : float) -> bool torch.eq(a : number, b : number) -> bool torch.eq(a : str, b : str) -> bool torch.eq(a : List[float], b : List[float]) -> bool torch.eq(a : List[Tensor], b : List[Tensor]) -> bool torch.eq(a : List[bool], b : List[bool]) -> bool torch.eq(a : List[str], b : List[str]) -> bool torch.equal(self : Tensor, other : Tensor) -> bool torch.erf(self : Tensor) -> Tensor torch.erf(self : Tensor, out : Tensor) -> Tensor torch.erf(a : int) -> float torch.erf(a : float) -> float torch.erf(a : number) -> number torch.erf_(self : Tensor) -> Tensor torch.erfc(self : Tensor) -> Tensor torch.erfc(self : Tensor, out : Tensor) -> Tensor torch.erfc(a : int) -> float torch.erfc(a : float) -> float torch.erfc(a : number) -> number torch.erfc_(self : Tensor) -> Tensor torch.erfinv(self : Tensor) -> Tensor torch.erfinv(self : Tensor, out : Tensor) -> Tensor torch.exp(self : Tensor) -> Tensor torch.exp(self : Tensor, out : Tensor) -> Tensor torch.exp(a : int) -> float torch.exp(a : float) -> float torch.exp(a : complex) -> complex torch.exp(a : number) -> number torch.exp2(self : Tensor) -> Tensor torch.exp2(self : Tensor, out : Tensor) -> Tensor torch.exp2_(self : Tensor) -> Tensor torch.exp_(self : Tensor) -> Tensor torch.expand_copy(self : Tensor, size : List[int], implicit : bool=False) -> Tensor torch.expand_copy(self : Tensor, size : List[int], implicit : bool=False, out : Tensor) -> Tensor torch.expm1(self : Tensor) -> Tensor torch.expm1(self : Tensor,", "prev_chunk_id": "chunk_535", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_537", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "out : Tensor) -> Tensor torch.expm1(a : int) -> float torch.expm1(a : float) -> float torch.expm1(a : number) -> number torch.expm1_(self : Tensor) -> Tensor torch.eye(n : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.eye(n : int, m : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.eye(n : int, out : Tensor) -> Tensor torch.eye(n : int, m : int, out : Tensor) -> Tensor torch.fake_quantize_per_channel_affine(self : Tensor, scale : Tensor, zero_point : Tensor, axis : int, quant_min : int, quant_max : int) -> Tensor torch.fake_quantize_per_tensor_affine(self : Tensor, scale : float, zero_point : int, quant_min : int, quant_max : int) -> Tensor torch.fake_quantize_per_tensor_affine(self : Tensor, scale : Tensor, zero_point : Tensor, quant_min : int, quant_max : int) -> Tensor torch.fbgemm_linear_fp16_weight(input : Tensor, packed_weight : Tensor, bias : Tensor) -> Tensor torch.fbgemm_linear_fp16_weight_fp32_activation(input : Tensor, packed_weight : Tensor, bias : Tensor) -> Tensor torch.fbgemm_linear_int8_weight(input : Tensor, weight : Tensor, packed : Tensor, col_offsets : Tensor, weight_scale : number, weight_zero_point : number, bias : Tensor) -> Tensor torch.fbgemm_linear_int8_weight_fp32_activation(input : Tensor, weight : Tensor, packed : Tensor, col_offsets : Tensor, weight_scale : number, weight_zero_point : number, bias : Tensor) -> Tensor torch.fbgemm_linear_quantize_weight(input : Tensor) -> Tuple[Tensor, Tensor, float, int] torch.fbgemm_pack_gemm_matrix_fp16(input : Tensor) -> Tensor torch.fbgemm_pack_quantized_matrix(input : Tensor) -> Tensor torch.fbgemm_pack_quantized_matrix(input : Tensor, K : int, N : int) -> Tensor torch.feature_alpha_dropout(input : Tensor, p : float, train : bool) -> Tensor torch.feature_alpha_dropout_(self : Tensor, p : float, train : bool) -> Tensor torch.feature_dropout(input : Tensor, p : float, train : bool) -> Tensor torch.feature_dropout_(self : Tensor, p : float, train : bool) -> Tensor torch.fill(self : Tensor, value : number) -> Tensor torch.fill(self : Tensor, value : number, out : Tensor) -> Tensor torch.fill(self : Tensor, value :", "prev_chunk_id": "chunk_536", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_538", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor) -> Tensor torch.fill(self : Tensor, value : Tensor, out : Tensor) -> Tensor torch.fill_(self : Tensor, value : number) -> Tensor torch.fill_(self : Tensor, value : Tensor) -> Tensor torch.fix(self : Tensor) -> Tensor torch.fix(self : Tensor, out : Tensor) -> Tensor torch.fix_(self : Tensor) -> Tensor torch.flatten(self : Tensor, start_dim : int=0, end_dim : int=-1) -> Tensor torch.flatten(self : Tensor, dims : List[str], out_dim : str) -> Tensor torch.flatten(self : Tensor, start_dim : int, end_dim : int, out_dim : str) -> Tensor torch.flatten(self : Tensor, start_dim : str, end_dim : str, out_dim : str) -> Tensor torch.flip(self : Tensor, dims : List[int]) -> Tensor torch.flip(self : Tensor, dims : List[int], out : Tensor) -> Tensor torch.fliplr(self : Tensor) -> Tensor torch.flipud(self : Tensor) -> Tensor torch.float_power(self : Tensor, exponent : Tensor) -> Tensor torch.float_power(self : Tensor, exponent : number) -> Tensor torch.float_power(self : number, exponent : Tensor) -> Tensor torch.float_power(self : Tensor, exponent : Tensor, out : Tensor) -> Tensor torch.float_power(self : number, exponent : Tensor, out : Tensor) -> Tensor torch.float_power(self : Tensor, exponent : number, out : Tensor) -> Tensor torch.floor(self : Tensor) -> Tensor torch.floor(self : Tensor, out : Tensor) -> Tensor torch.floor(a : int) -> int torch.floor(a : float) -> int torch.floor(a : number) -> number torch.floor_(self : Tensor) -> Tensor torch.floor_divide(self : Tensor, other : Tensor) -> Tensor torch.floor_divide(self : Tensor, other : number) -> Tensor torch.floor_divide(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.floor_divide(self : Tensor, other : number, out : Tensor) -> Tensor torch.fmax(self : Tensor, other : Tensor) -> Tensor torch.fmax(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.fmin(self : Tensor, other : Tensor) -> Tensor torch.fmin(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.fmod(self : Tensor, other : Tensor) ->", "prev_chunk_id": "chunk_537", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_539", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor torch.fmod(self : Tensor, other : number) -> Tensor torch.fmod(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.fmod(self : Tensor, other : number, out : Tensor) -> Tensor torch.fmod(a : int, b : int) -> float torch.fmod(a : float, b : float) -> float torch.fmod(a : int, b : float) -> float torch.fmod(a : float, b : int) -> float torch.fmod(a : number, b : number) -> float torch.frac(self : Tensor) -> Tensor torch.frac(self : Tensor, out : Tensor) -> Tensor torch.frac_(self : Tensor) -> Tensor torch.frexp(self : Tensor) -> Tuple[Tensor, Tensor] torch.frexp(self : Tensor, mantissa : Tensor, exponent : Tensor) -> Tuple[Tensor, Tensor] torch.frexp(a : float) -> Tuple[float, int] torch.frobenius_norm(self : Tensor, dim : List[int], keepdim : bool=False) -> Tensor torch.frobenius_norm(self : Tensor, dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor torch.from_file(filename : str, shared : Optional[bool], size : Optional[int]=0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.from_file(filename : str, shared : Optional[bool], size : Optional[int]=0, out : Tensor) -> Tensor torch.full(size : List[int], fill_value : number, names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.full(size : List[int], fill_value : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.full(size : List[int], fill_value : number, names : Optional[List[str]], out : Tensor) -> Tensor torch.full(size : List[int], fill_value : number, out : Tensor) -> Tensor torch.full_like(self : Tensor, fill_value : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.full_like(self : Tensor, fill_value : number, memory_format : Optional[int], out : Tensor) -> Tensor torch.fused_moving_avg_obs_fake_quant(self : Tensor, observer_on : Tensor, fake_quant_on : Tensor, running_min : Tensor, running_max : Tensor, scale : Tensor, zero_point :", "prev_chunk_id": "chunk_538", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_540", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, averaging_const : float, quant_min : int, quant_max : int, ch_axis : int, per_row_fake_quant : bool=False, symmetric_quant : bool=False) -> Tensor torch.gather(self : Tensor, dim : int, index : Tensor, sparse_grad : bool=False) -> Tensor torch.gather(self : Tensor, dim : int, index : Tensor, sparse_grad : bool=False, out : Tensor) -> Tensor torch.gather(self : Tensor, dim : str, index : Tensor, sparse_grad : bool=False) -> Tensor torch.gather(self : Tensor, dim : str, index : Tensor, sparse_grad : bool=False, out : Tensor) -> Tensor torch.gcd(self : Tensor, other : Tensor) -> Tensor torch.gcd(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.gcd(a : int, b : int) -> int torch.gcd_(self : Tensor, other : Tensor) -> Tensor torch.ge(self : Tensor, other : Tensor) -> Tensor torch.ge(self : Tensor, other : number) -> Tensor torch.ge(self : Tensor, other : number, out : Tensor) -> Tensor torch.ge(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.ge(a : int, b : int) -> bool torch.ge(a : float, b : float) -> bool torch.ge(a : int, b : float) -> bool torch.ge(a : float, b : int) -> bool torch.ge(a : number, b : number) -> bool torch.ge(a : str, b : str) -> bool torch.geqrf(self : Tensor) -> Tuple[Tensor, Tensor] torch.geqrf(self : Tensor, a : Tensor, tau : Tensor) -> Tuple[Tensor, Tensor] torch.ger(self : Tensor, vec2 : Tensor) -> Tensor torch.ger(self : Tensor, vec2 : Tensor, out : Tensor) -> Tensor torch.get_autocast_dtype(device_type : str) -> int torch.get_device(self : Tensor) -> int torch.gradient(self : Tensor, spacing : Optional[number], dim : Optional[int], edge_order : int=1) -> List[Tensor] torch.gradient(self : Tensor, spacing : number, dim : List[int], edge_order : int=1) -> List[Tensor] torch.gradient(self : Tensor, dim : List[int], edge_order : int=1) -> List[Tensor] torch.gradient(self : Tensor, spacing : List[number], dim : Optional[int], edge_order", "prev_chunk_id": "chunk_539", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_541", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": int=1) -> List[Tensor] torch.gradient(self : Tensor, spacing : List[number], dim : List[int], edge_order : int=1) -> List[Tensor] torch.gradient(self : Tensor, spacing : List[Tensor], dim : Optional[int], edge_order : int=1) -> List[Tensor] torch.gradient(self : Tensor, spacing : List[Tensor], dim : List[int], edge_order : int=1) -> List[Tensor] torch.greater(self : Tensor, other : Tensor) -> Tensor torch.greater(self : Tensor, other : number) -> Tensor torch.greater(self : Tensor, other : number, out : Tensor) -> Tensor torch.greater(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.greater_equal(self : Tensor, other : Tensor) -> Tensor torch.greater_equal(self : Tensor, other : number) -> Tensor torch.greater_equal(self : Tensor, other : number, out : Tensor) -> Tensor torch.greater_equal(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.grid_sampler(input : Tensor, grid : Tensor, interpolation_mode : int, padding_mode : int, align_corners : bool) -> Tensor torch.grid_sampler_2d(input : Tensor, grid : Tensor, interpolation_mode : int, padding_mode : int, align_corners : bool) -> Tensor torch.grid_sampler_2d(input : Tensor, grid : Tensor, interpolation_mode : int, padding_mode : int, align_corners : bool, out : Tensor) -> Tensor torch.grid_sampler_3d(input : Tensor, grid : Tensor, interpolation_mode : int, padding_mode : int, align_corners : bool) -> Tensor torch.grid_sampler_3d(input : Tensor, grid : Tensor, interpolation_mode : int, padding_mode : int, align_corners : bool, out : Tensor) -> Tensor torch.group_norm(input : Tensor, num_groups : int, weight : Optional[Tensor], bias : Optional[Tensor], eps : float=1e-05, cudnn_enabled : bool=True) -> Tensor torch.gru(input : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool, batch_first : bool) -> Tuple[Tensor, Tensor] torch.gru(data : Tensor, batch_sizes : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool) -> Tuple[Tensor, Tensor] torch.gru_cell(input : Tensor, hx : Tensor, w_ih", "prev_chunk_id": "chunk_540", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_542", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, w_hh : Tensor, b_ih : Optional[Tensor], b_hh : Optional[Tensor]) -> Tensor torch.gt(self : Tensor, other : Tensor) -> Tensor torch.gt(self : Tensor, other : number) -> Tensor torch.gt(self : Tensor, other : number, out : Tensor) -> Tensor torch.gt(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.gt(a : int, b : int) -> bool torch.gt(a : float, b : float) -> bool torch.gt(a : int, b : float) -> bool torch.gt(a : float, b : int) -> bool torch.gt(a : number, b : number) -> bool torch.gt(a : str, b : str) -> bool torch.hamming_window(window_length : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hamming_window(window_length : int, periodic : bool, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hamming_window(window_length : int, periodic : bool, alpha : float, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hamming_window(window_length : int, periodic : bool, alpha : float, beta : float, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hamming_window(window_length : int, out : Tensor) -> Tensor torch.hamming_window(window_length : int, periodic : bool, out : Tensor) -> Tensor torch.hamming_window(window_length : int, periodic : bool, alpha : float, out : Tensor) -> Tensor torch.hamming_window(window_length : int, periodic : bool, alpha : float, beta : float, out : Tensor) -> Tensor torch.hann_window(window_length : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hann_window(window_length : int, periodic : bool, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.hann_window(window_length : int, out : Tensor) -> Tensor torch.hann_window(window_length : int, periodic : bool, out : Tensor) -> Tensor torch.hardshrink(self : Tensor, lambd : number=0.5) -> Tensor torch.hardshrink(self", "prev_chunk_id": "chunk_541", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_543", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, lambd : number=0.5, out : Tensor) -> Tensor torch.heaviside(self : Tensor, values : Tensor) -> Tensor torch.heaviside(self : Tensor, values : Tensor, out : Tensor) -> Tensor torch.hinge_embedding_loss(self : Tensor, target : Tensor, margin : float=1.0, reduction : int=1) -> Tensor torch.histc(self : Tensor, bins : int=100, min : number=0, max : number=0) -> Tensor torch.histc(self : Tensor, bins : int=100, min : number=0, max : number=0, out : Tensor) -> Tensor torch.histogram(self : Tensor, bins : Tensor, weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, Tensor] torch.histogram(self : Tensor, bins : Tensor, weight : Optional[Tensor], density : bool=False, hist : Tensor, bin_edges : Tensor) -> Tuple[Tensor, Tensor] torch.histogram(self : Tensor, bins : int=100, range : Optional[List[float]], weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, Tensor] torch.histogram(self : Tensor, bins : int=100, range : Optional[List[float]], weight : Optional[Tensor], density : bool=False, hist : Tensor, bin_edges : Tensor) -> Tuple[Tensor, Tensor] torch.histogramdd(self : Tensor, bins : List[int], range : Optional[List[float]], weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, List[Tensor]] torch.histogramdd(self : Tensor, bins : int, range : Optional[List[float]], weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, List[Tensor]] torch.histogramdd(self : Tensor, bins : List[Tensor], range : Optional[List[float]], weight : Optional[Tensor], density : bool=False) -> Tuple[Tensor, List[Tensor]] torch.hsplit(self : Tensor, sections : int) -> List[Tensor] torch.hsplit(self : Tensor, indices : List[int]) -> List[Tensor] torch.hspmm(mat1 : Tensor, mat2 : Tensor, out : Tensor) -> Tensor torch.hspmm(mat1 : Tensor, mat2 : Tensor) -> Tensor torch.hstack(tensors : List[Tensor]) -> Tensor torch.hstack(tensors : List[Tensor], out : Tensor) -> Tensor torch.hypot(self : Tensor, other : Tensor) -> Tensor torch.hypot(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.i0(self : Tensor) -> Tensor torch.i0(self : Tensor, out : Tensor) -> Tensor torch.i0_(self : Tensor) -> Tensor torch.igamma(self : Tensor, other : Tensor) -> Tensor torch.igamma(self", "prev_chunk_id": "chunk_542", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_544", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, other : Tensor, out : Tensor) -> Tensor torch.igammac(self : Tensor, other : Tensor) -> Tensor torch.igammac(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.imag(self : Tensor) -> Tensor torch.index_add(self : Tensor, dim : int, index : Tensor, source : Tensor, alpha : number=1) -> Tensor torch.index_add(self : Tensor, dim : int, index : Tensor, source : Tensor, alpha : number=1, out : Tensor) -> Tensor torch.index_add(self : Tensor, dim : str, index : Tensor, source : Tensor, alpha : number=1) -> Tensor torch.index_copy(self : Tensor, dim : int, index : Tensor, source : Tensor) -> Tensor torch.index_copy(self : Tensor, dim : str, index : Tensor, source : Tensor) -> Tensor torch.index_copy(self : Tensor, dim : int, index : Tensor, source : Tensor, out : Tensor) -> Tensor torch.index_fill(self : Tensor, dim : int, index : Tensor, value : Tensor) -> Tensor torch.index_fill(self : Tensor, dim : int, index : Tensor, value : number) -> Tensor torch.index_fill(self : Tensor, dim : str, index : Tensor, value : number) -> Tensor torch.index_fill(self : Tensor, dim : str, index : Tensor, value : Tensor) -> Tensor torch.index_fill(self : Tensor, dim : int, index : Tensor, value : number, out : Tensor) -> Tensor torch.index_fill(self : Tensor, dim : int, index : Tensor, value : Tensor, out : Tensor) -> Tensor torch.index_put(self : Tensor, indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False) -> Tensor torch.index_put(self : Tensor, indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False, out : Tensor) -> Tensor torch.index_put(self : Tensor, indices : List[Tensor], values : Tensor, accumulate : bool=False) -> Tensor torch.index_put_(self : Tensor, indices : List[Optional[Tensor]], values : Tensor, accumulate : bool=False) -> Tensor torch.index_put_(self : Tensor, indices : List[Tensor], values : Tensor, accumulate : bool=False) -> Tensor torch.index_reduce(self : Tensor, dim", "prev_chunk_id": "chunk_543", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_545", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": int, index : Tensor, source : Tensor, reduce : str, include_self : bool=True) -> Tensor torch.index_reduce(self : Tensor, dim : int, index : Tensor, source : Tensor, reduce : str, include_self : bool=True, out : Tensor) -> Tensor torch.index_select(self : Tensor, dim : int, index : Tensor) -> Tensor torch.index_select(self : Tensor, dim : int, index : Tensor, out : Tensor) -> Tensor torch.index_select(self : Tensor, dim : str, index : Tensor) -> Tensor torch.index_select(self : Tensor, dim : str, index : Tensor, out : Tensor) -> Tensor torch.indices_copy(self : Tensor) -> Tensor torch.indices_copy(self : Tensor, out : Tensor) -> Tensor torch.initial_seed(self : Generator) -> int torch.inner(self : Tensor, other : Tensor) -> Tensor torch.inner(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.instance_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], use_input_stats : bool, momentum : float, eps : float, cudnn_enabled : bool) -> Tensor torch.int_repr(self : Tensor, out : Tensor) -> Tensor torch.int_repr(self : Tensor) -> Tensor torch.inverse(self : Tensor) -> Tensor torch.inverse(self : Tensor, out : Tensor) -> Tensor torch.is_autocast_cpu_enabled() -> bool torch.is_autocast_enabled() -> bool torch.is_complex(self : Tensor) -> bool torch.is_conj(self : Tensor) -> bool torch.is_distributed(self : Tensor) -> bool torch.is_floating_point(self : Tensor) -> bool torch.is_grad_enabled() -> bool torch.is_inference(self : Tensor) -> bool torch.is_neg(self : Tensor) -> bool torch.is_nonzero(self : Tensor) -> bool torch.is_same_size(self : Tensor, other : Tensor) -> bool torch.is_signed(self : Tensor) -> bool torch.is_vulkan_available() -> bool torch.isclose(self : Tensor, other : Tensor, rtol : float=1e-05, atol : float=1e-08, equal_nan : bool=False) -> Tensor torch.isfinite(self : Tensor) -> Tensor torch.isfinite(a : float) -> bool torch.isfinite(a : complex) -> bool torch.isin(elements : Tensor, test_elements : Tensor, assume_unique : bool=False, invert : bool=False) -> Tensor torch.isin(elements : Tensor, test_elements : Tensor, assume_unique : bool=False, invert : bool=False, out", "prev_chunk_id": "chunk_544", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_546", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor) -> Tensor torch.isin(elements : Tensor, test_element : number, assume_unique : bool=False, invert : bool=False) -> Tensor torch.isin(elements : Tensor, test_element : number, assume_unique : bool=False, invert : bool=False, out : Tensor) -> Tensor torch.isin(element : number, test_elements : Tensor, assume_unique : bool=False, invert : bool=False) -> Tensor torch.isin(element : number, test_elements : Tensor, assume_unique : bool=False, invert : bool=False, out : Tensor) -> Tensor torch.isinf(self : Tensor) -> Tensor torch.isinf(self : Tensor, out : Tensor) -> Tensor torch.isinf(a : float) -> bool torch.isinf(a : complex) -> bool torch.isnan(self : Tensor) -> Tensor torch.isnan(self : Tensor, out : Tensor) -> Tensor torch.isnan(a : float) -> bool torch.isnan(a : complex) -> bool torch.isneginf(self : Tensor) -> Tensor torch.isneginf(self : Tensor, out : Tensor) -> Tensor torch.isposinf(self : Tensor) -> Tensor torch.isposinf(self : Tensor, out : Tensor) -> Tensor torch.isreal(self : Tensor) -> Tensor torch.istft(self : Tensor, n_fft : int, hop_length : Optional[int], win_length : Optional[int], window : Optional[Tensor], center : bool=True, normalized : bool=False, onesided : Optional[bool], length : Optional[int], return_complex : bool=False) -> Tensor torch.kaiser_window(window_length : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.kaiser_window(window_length : int, periodic : bool, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.kaiser_window(window_length : int, periodic : bool, beta : float, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.kaiser_window(window_length : int, out : Tensor) -> Tensor torch.kaiser_window(window_length : int, periodic : bool, out : Tensor) -> Tensor torch.kaiser_window(window_length : int, periodic : bool, beta : float, out : Tensor) -> Tensor torch.kl_div(self : Tensor, target : Tensor, reduction : int=1, log_target : bool=False) -> Tensor torch.kron(self : Tensor, other : Tensor) -> Tensor torch.kron(self : Tensor, other : Tensor, out : Tensor) ->", "prev_chunk_id": "chunk_545", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_547", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor torch.kthvalue(self : Tensor, k : int, dim : int=-1, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.kthvalue(self : Tensor, k : int, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.kthvalue(self : Tensor, k : int, dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.kthvalue(self : Tensor, k : int, dim : int=-1, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.layer_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], bias : Optional[Tensor], eps : float=1e-05, cudnn_enable : bool=True) -> Tensor torch.lcm(self : Tensor, other : Tensor) -> Tensor torch.lcm(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.lcm_(self : Tensor, other : Tensor) -> Tensor torch.ldexp(self : Tensor, other : Tensor) -> Tensor torch.ldexp(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.ldexp(x : float, i : int) -> float torch.ldexp_(self : Tensor, other : Tensor) -> Tensor torch.le(self : Tensor, other : Tensor) -> Tensor torch.le(self : Tensor, other : number) -> Tensor torch.le(self : Tensor, other : number, out : Tensor) -> Tensor torch.le(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.le(a : int, b : int) -> bool torch.le(a : float, b : float) -> bool torch.le(a : int, b : float) -> bool torch.le(a : float, b : int) -> bool torch.le(a : number, b : number) -> bool torch.le(a : str, b : str) -> bool torch.lerp(self : Tensor, end : Tensor, weight : number) -> Tensor torch.lerp(self : Tensor, end : Tensor, weight : Tensor) -> Tensor torch.lerp(self : Tensor, end : Tensor, weight : number, out : Tensor) -> Tensor torch.lerp(self : Tensor, end : Tensor, weight : Tensor, out : Tensor) -> Tensor torch.less(self : Tensor, other : Tensor) -> Tensor torch.less(self", "prev_chunk_id": "chunk_546", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_548", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, other : number) -> Tensor torch.less(self : Tensor, other : number, out : Tensor) -> Tensor torch.less(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.less_equal(self : Tensor, other : Tensor) -> Tensor torch.less_equal(self : Tensor, other : number) -> Tensor torch.less_equal(self : Tensor, other : number, out : Tensor) -> Tensor torch.less_equal(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.lgamma(self : Tensor) -> Tensor torch.lgamma(self : Tensor, out : Tensor) -> Tensor torch.lgamma(a : int) -> float torch.lgamma(a : float) -> float torch.lgamma(a : number) -> number torch.linspace(start : Tensor, end : Tensor, steps : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.linspace(start : Tensor, end : number, steps : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.linspace(start : number, end : Tensor, steps : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.linspace(start : number, end : number, steps : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.linspace(start : number, end : number, steps : int, out : Tensor) -> Tensor torch.linspace(start : Tensor, end : Tensor, steps : int, out : Tensor) -> Tensor torch.linspace(start : Tensor, end : number, steps : int, out : Tensor) -> Tensor torch.linspace(start : number, end : Tensor, steps : int, out : Tensor) -> Tensor torch.log(self : Tensor) -> Tensor torch.log(self : Tensor, out : Tensor) -> Tensor torch.log(a : int) -> float torch.log(a : float) -> float torch.log(a : complex) -> complex torch.log(a : number) -> number torch.log(a : int, b : int) -> float torch.log(a : float, b : float) -> float torch.log(a : complex, b : complex) -> complex", "prev_chunk_id": "chunk_547", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_549", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.log(a : int, b : float) -> float torch.log(a : float, b : int) -> float torch.log(a : int, b : complex) -> complex torch.log(a : complex, b : int) -> complex torch.log(a : float, b : complex) -> complex torch.log(a : complex, b : float) -> complex torch.log(a : number, b : number) -> float torch.log10(self : Tensor) -> Tensor torch.log10(self : Tensor, out : Tensor) -> Tensor torch.log10(a : int) -> float torch.log10(a : float) -> float torch.log10(a : complex) -> complex torch.log10(a : number) -> number torch.log10_(self : Tensor) -> Tensor torch.log1p(self : Tensor) -> Tensor torch.log1p(self : Tensor, out : Tensor) -> Tensor torch.log1p(a : int) -> float torch.log1p(a : float) -> float torch.log1p(a : number) -> number torch.log1p_(self : Tensor) -> Tensor torch.log2(self : Tensor) -> Tensor torch.log2(self : Tensor, out : Tensor) -> Tensor torch.log2_(self : Tensor) -> Tensor torch.log_(self : Tensor) -> Tensor torch.log_softmax(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch.log_softmax(self : Tensor, dim : str, dtype : Optional[int]) -> Tensor torch.log_softmax(self : Tensor, dim : int, dtype : Optional[int], out : Tensor) -> Tensor torch.logaddexp(self : Tensor, other : Tensor) -> Tensor torch.logaddexp(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.logaddexp2(self : Tensor, other : Tensor) -> Tensor torch.logaddexp2(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.logcumsumexp(self : Tensor, dim : int) -> Tensor torch.logcumsumexp(self : Tensor, dim : str) -> Tensor torch.logcumsumexp(self : Tensor, dim : str, out : Tensor) -> Tensor torch.logcumsumexp(self : Tensor, dim : int, out : Tensor) -> Tensor torch.logdet(self : Tensor) -> Tensor torch.logical_and(self : Tensor, other : Tensor) -> Tensor torch.logical_and(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.logical_not(self : Tensor) -> Tensor torch.logical_not(self : Tensor, out : Tensor) -> Tensor torch.logical_or(self", "prev_chunk_id": "chunk_548", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_550", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, other : Tensor) -> Tensor torch.logical_or(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.logical_xor(self : Tensor, other : Tensor) -> Tensor torch.logical_xor(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.logit(self : Tensor, eps : Optional[float]) -> Tensor torch.logit(self : Tensor, eps : Optional[float], out : Tensor) -> Tensor torch.logit_(self : Tensor, eps : Optional[float]) -> Tensor torch.logspace(start : Tensor, end : Tensor, steps : int, base : float=10.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.logspace(start : Tensor, end : number, steps : int, base : float=10.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.logspace(start : number, end : Tensor, steps : int, base : float=10.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.logspace(start : number, end : number, steps : int, base : float=10.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.logspace(start : number, end : number, steps : int, base : float=10.0, out : Tensor) -> Tensor torch.logspace(start : Tensor, end : Tensor, steps : int, base : float=10.0, out : Tensor) -> Tensor torch.logspace(start : Tensor, end : number, steps : int, base : float=10.0, out : Tensor) -> Tensor torch.logspace(start : number, end : Tensor, steps : int, base : float=10.0, out : Tensor) -> Tensor torch.logsumexp(self : Tensor, dim : List[int], keepdim : bool=False) -> Tensor torch.logsumexp(self : Tensor, dim : List[str], keepdim : bool=False) -> Tensor torch.logsumexp(self : Tensor, dim : List[str], keepdim : bool=False, out : Tensor) -> Tensor torch.logsumexp(self : Tensor, dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor torch.lstm(input : Tensor, hx : List[Tensor], params : List[Tensor], has_biases : bool, num_layers :", "prev_chunk_id": "chunk_549", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_551", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "int, dropout : float, train : bool, bidirectional : bool, batch_first : bool) -> Tuple[Tensor, Tensor, Tensor] torch.lstm(data : Tensor, batch_sizes : Tensor, hx : List[Tensor], params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool) -> Tuple[Tensor, Tensor, Tensor] torch.lstm_cell(input : Tensor, hx : List[Tensor], w_ih : Tensor, w_hh : Tensor, b_ih : Optional[Tensor], b_hh : Optional[Tensor]) -> Tuple[Tensor, Tensor] torch.lt(self : Tensor, other : Tensor) -> Tensor torch.lt(self : Tensor, other : number) -> Tensor torch.lt(self : Tensor, other : number, out : Tensor) -> Tensor torch.lt(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.lt(a : int, b : int) -> bool torch.lt(a : float, b : float) -> bool torch.lt(a : int, b : float) -> bool torch.lt(a : float, b : int) -> bool torch.lt(a : number, b : number) -> bool torch.lt(a : str, b : str) -> bool torch.lu_solve(self : Tensor, LU_data : Tensor, LU_pivots : Tensor) -> Tensor torch.lu_solve(self : Tensor, LU_data : Tensor, LU_pivots : Tensor, out : Tensor) -> Tensor torch.lu_unpack(LU_data : Tensor, LU_pivots : Tensor, unpack_data : bool=True, unpack_pivots : bool=True) -> Tuple[Tensor, Tensor, Tensor] torch.lu_unpack(LU_data : Tensor, LU_pivots : Tensor, unpack_data : bool=True, unpack_pivots : bool=True, P : Tensor, L : Tensor, U : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.manual_seed(seed : int) -> Tuple[] torch.manual_seed(self : Generator, seed : int) -> Generator torch.margin_ranking_loss(input1 : Tensor, input2 : Tensor, target : Tensor, margin : float=0.0, reduction : int=1) -> Tensor torch.masked_fill(self : Tensor, mask : Tensor, value : number) -> Tensor torch.masked_fill(self : Tensor, mask : Tensor, value : Tensor) -> Tensor torch.masked_fill(self : Tensor, mask : Tensor, value : number, out : Tensor) -> Tensor torch.masked_fill(self : Tensor, mask : Tensor, value : Tensor, out : Tensor) ->", "prev_chunk_id": "chunk_550", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_552", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor torch.masked_scatter(self : Tensor, mask : Tensor, source : Tensor) -> Tensor torch.masked_scatter(self : Tensor, mask : Tensor, source : Tensor, out : Tensor) -> Tensor torch.masked_select(self : Tensor, mask : Tensor) -> Tensor torch.masked_select(self : Tensor, mask : Tensor, out : Tensor) -> Tensor torch.matmul(self : Tensor, other : Tensor) -> Tensor torch.matmul(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.matrix_exp(self : Tensor) -> Tensor torch.matrix_power(self : Tensor, n : int) -> Tensor torch.matrix_power(self : Tensor, n : int, out : Tensor) -> Tensor torch.max(self : Tensor, other : Tensor) -> Tensor torch.max(self : Tensor) -> Tensor torch.max(self : Tensor, dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.max(self : Tensor, dim : int, keepdim : bool=False, max : Tensor, max_values : Tensor) -> Tuple[Tensor, Tensor] torch.max(self : Tensor, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.max(self : Tensor, dim : str, keepdim : bool=False, max : Tensor, max_values : Tensor) -> Tuple[Tensor, Tensor] torch.max(self : Tensor, out : Tensor) -> Tensor torch.max(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.max_pool1d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], dilation : List[int]=[1], ceil_mode : bool=False) -> Tensor torch.max_pool1d_with_indices(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], dilation : List[int]=[1], ceil_mode : bool=False) -> Tuple[Tensor, Tensor] torch.max_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False) -> Tensor torch.max_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False) -> Tensor torch.maximum(self : Tensor, other : Tensor) -> Tensor torch.maximum(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.mean(self : Tensor, dtype : Optional[int]) -> Tensor torch.mean(self : Tensor, dim : Optional[List[int]],", "prev_chunk_id": "chunk_551", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_553", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.mean(self : Tensor, dim : List[str], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.mean(self : Tensor, dim : List[str], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.mean(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.mean(self : Tensor, dtype : Optional[int], out : Tensor) -> Tensor torch.median(self : Tensor) -> Tensor torch.median(self : Tensor, dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.median(self : Tensor, dim : int, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.median(self : Tensor, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.median(self : Tensor, dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.median(self : Tensor, out : Tensor) -> Tensor torch.meshgrid(tensors : List[Tensor]) -> List[Tensor] torch.meshgrid(tensors : List[Tensor], indexing : str) -> List[Tensor] torch.min(self : Tensor, other : Tensor) -> Tensor torch.min(self : Tensor) -> Tensor torch.min(self : Tensor, dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.min(self : Tensor, dim : int, keepdim : bool=False, min : Tensor, min_indices : Tensor) -> Tuple[Tensor, Tensor] torch.min(self : Tensor, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.min(self : Tensor, dim : str, keepdim : bool=False, min : Tensor, min_indices : Tensor) -> Tuple[Tensor, Tensor] torch.min(self : Tensor, out : Tensor) -> Tensor torch.min(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.minimum(self : Tensor, other : Tensor) -> Tensor torch.minimum(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.miopen_batch_norm(input : Tensor, weight : Tensor, bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, exponential_average_factor : float, epsilon : float) -> Tuple[Tensor, Tensor, Tensor] torch.miopen_batch_norm(input : Tensor, weight : Tensor, bias :", "prev_chunk_id": "chunk_552", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_554", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, exponential_average_factor : float, epsilon : float, out0 : Tensor, out1 : Tensor, out2 : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.miopen_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, out : Tensor) -> Tensor torch.miopen_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool) -> Tensor torch.miopen_convolution_add_relu(self : Tensor, weight : Tensor, z : Tensor, alpha : Optional[number], bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int) -> Tensor torch.miopen_convolution_relu(self : Tensor, weight : Tensor, bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], groups : int) -> Tensor torch.miopen_convolution_transpose(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], output_padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, out : Tensor) -> Tensor torch.miopen_convolution_transpose(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], output_padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool) -> Tensor torch.miopen_depthwise_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool, out : Tensor) -> Tensor torch.miopen_depthwise_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int, benchmark : bool, deterministic : bool) -> Tensor torch.miopen_rnn(input : Tensor, weight : List[Tensor], weight_stride0 : int, hx : Tensor, cx : Optional[Tensor], mode : int, hidden_size : int, num_layers : int, batch_first : bool, dropout : float,", "prev_chunk_id": "chunk_553", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_555", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "train : bool, bidirectional : bool, batch_sizes : List[int], dropout_state : Optional[Tensor], out0 : Tensor, out1 : Tensor, out2 : Tensor, out3 : Tensor, out4 : Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor] torch.miopen_rnn(input : Tensor, weight : List[Tensor], weight_stride0 : int, hx : Tensor, cx : Optional[Tensor], mode : int, hidden_size : int, num_layers : int, batch_first : bool, dropout : float, train : bool, bidirectional : bool, batch_sizes : List[int], dropout_state : Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor] torch.mkldnn_adaptive_avg_pool2d(self : Tensor, output_size : List[int], out : Tensor) -> Tensor torch.mkldnn_adaptive_avg_pool2d(self : Tensor, output_size : List[int]) -> Tensor torch.mkldnn_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int) -> Tensor torch.mkldnn_convolution(self : Tensor, weight : Tensor, bias : Optional[Tensor], padding : List[int], stride : List[int], dilation : List[int], groups : int, out : Tensor) -> Tensor torch.mkldnn_linear_backward_weights(grad_output : Tensor, input : Tensor, weight : Tensor, bias_defined : bool, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.mkldnn_linear_backward_weights(grad_output : Tensor, input : Tensor, weight : Tensor, bias_defined : bool) -> Tuple[Tensor, Tensor] torch.mkldnn_max_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False, out : Tensor) -> Tensor torch.mkldnn_max_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False) -> Tensor torch.mkldnn_max_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False, out : Tensor) -> Tensor torch.mkldnn_max_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False) -> Tensor torch.mkldnn_rnn_layer(input : Tensor, weight0 : Tensor, weight1 : Tensor, weight2 : Tensor, weight3 : Tensor, hx_ :", "prev_chunk_id": "chunk_554", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_556", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, cx_ : Tensor, reverse : bool, batch_sizes : List[int], mode : int, hidden_size : int, num_layers : int, has_biases : bool, bidirectional : bool, batch_first : bool, train : bool) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.mkldnn_rnn_layer(input : Tensor, weight0 : Tensor, weight1 : Tensor, weight2 : Tensor, weight3 : Tensor, hx_ : Tensor, cx_ : Tensor, reverse : bool, batch_sizes : List[int], mode : int, hidden_size : int, num_layers : int, has_biases : bool, bidirectional : bool, batch_first : bool, train : bool, out0 : Tensor, out1 : Tensor, out2 : Tensor, out3 : Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch.mm(self : Tensor, mat2 : Tensor) -> Tensor torch.mm(self : Tensor, mat2 : Tensor, out : Tensor) -> Tensor torch.mm(self : Tensor, mat2 : Tensor, out_dtype : int, out : Tensor) -> Tensor torch.mm(self : Tensor, mat2 : Tensor, out_dtype : int) -> Tensor torch.mode(self : Tensor, dim : int=-1, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.mode(self : Tensor, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.mode(self : Tensor, dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.mode(self : Tensor, dim : int=-1, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.moveaxis(self : Tensor, source : List[int], destination : List[int]) -> Tensor torch.moveaxis(self : Tensor, source : int, destination : int) -> Tensor torch.movedim(self : Tensor, source : int, destination : int) -> Tensor torch.movedim(self : Tensor, source : List[int], destination : List[int]) -> Tensor torch.msort(self : Tensor) -> Tensor torch.msort(self : Tensor, out : Tensor) -> Tensor torch.mul(self : Tensor, other : Tensor) -> Tensor torch.mul(self : Tensor, other : number) -> Tensor torch.mul(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.mul(self : Tensor, other : number, out : Tensor) -> Tensor", "prev_chunk_id": "chunk_555", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_557", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.mul(l : List[t], n : int) -> List[t] torch.mul(n : int, l : List[t]) -> List[t] torch.mul(a : int, b : int) -> int torch.mul(a : complex, b : complex) -> complex torch.mul(a : float, b : float) -> float torch.mul(a : int, b : complex) -> complex torch.mul(a : complex, b : int) -> complex torch.mul(a : float, b : complex) -> complex torch.mul(a : complex, b : float) -> complex torch.mul(a : int, b : float) -> float torch.mul(a : float, b : int) -> float torch.mul(a : number, b : number) -> number torch.multinomial(self : Tensor, num_samples : int, replacement : bool=False, generator : Optional[Generator]) -> Tensor torch.multinomial(self : Tensor, num_samples : int, replacement : bool=False, generator : Optional[Generator], out : Tensor) -> Tensor torch.multiply(self : Tensor, other : Tensor) -> Tensor torch.multiply(self : Tensor, other : number) -> Tensor torch.multiply(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.mv(self : Tensor, vec : Tensor) -> Tensor torch.mv(self : Tensor, vec : Tensor, out : Tensor) -> Tensor torch.mvlgamma(self : Tensor, p : int) -> Tensor torch.mvlgamma(self : Tensor, p : int, out : Tensor) -> Tensor torch.nan_to_num(self : Tensor, nan : Optional[float], posinf : Optional[float], neginf : Optional[float]) -> Tensor torch.nan_to_num(self : Tensor, nan : Optional[float], posinf : Optional[float], neginf : Optional[float], out : Tensor) -> Tensor torch.nan_to_num_(self : Tensor, nan : Optional[float], posinf : Optional[float], neginf : Optional[float]) -> Tensor torch.nanmean(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.nanmean(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.nanmedian(self : Tensor) -> Tensor torch.nanmedian(self : Tensor, dim : int, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.nanmedian(self : Tensor, dim : int, keepdim : bool=False, values : Tensor, indices : Tensor)", "prev_chunk_id": "chunk_556", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_558", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "-> Tuple[Tensor, Tensor] torch.nanmedian(self : Tensor, dim : str, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.nanmedian(self : Tensor, dim : str, keepdim : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.nanmedian(self : Tensor, out : Tensor) -> Tensor torch.nanquantile(self : Tensor, q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor torch.nanquantile(self : Tensor, q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor torch.nanquantile(self : Tensor, q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor torch.nanquantile(self : Tensor, q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor torch.nansum(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.nansum(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.narrow(self : Tensor, dim : int, start : int, length : int) -> Tensor torch.narrow(self : Tensor, dim : int, start : Tensor, length : int) -> Tensor torch.narrow_copy(self : Tensor, dim : int, start : int, length : int) -> Tensor torch.narrow_copy(self : Tensor, dim : int, start : int, length : int, out : Tensor) -> Tensor torch.native_batch_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, momentum : float, eps : float) -> Tuple[Tensor, Tensor, Tensor] torch.native_batch_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, momentum : float, eps : float, out : Tensor, save_mean : Tensor, save_invstd : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.native_channel_shuffle(self : Tensor, groups : int) -> Tensor torch.native_dropout(input : Tensor, p : float, train : Optional[bool]) -> Tuple[Tensor, Tensor] torch.native_dropout(input : Tensor, p : float, train : Optional[bool], out0", "prev_chunk_id": "chunk_557", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_559", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.native_group_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], N : int, C : int, HxW : int, group : int, eps : float) -> Tuple[Tensor, Tensor, Tensor] torch.native_group_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], N : int, C : int, HxW : int, group : int, eps : float, out0 : Tensor, out1 : Tensor, out2 : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.native_layer_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], bias : Optional[Tensor], eps : float) -> Tuple[Tensor, Tensor, Tensor] torch.native_layer_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], bias : Optional[Tensor], eps : float, out0 : Tensor, out1 : Tensor, out2 : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.native_norm(self : Tensor, p : number=2, out : Tensor) -> Tensor torch.native_norm(self : Tensor, p : Optional[number], dim : List[int], keepdim : bool, dtype : Optional[int], out : Tensor) -> Tensor torch.native_norm(self : Tensor, p : number=2) -> Tensor torch.native_norm(self : Tensor, p : Optional[number], dim : List[int], keepdim : bool, dtype : Optional[int]) -> Tensor torch.ne(self : Tensor, other : Tensor) -> Tensor torch.ne(self : Tensor, other : number) -> Tensor torch.ne(self : Tensor, other : number, out : Tensor) -> Tensor torch.ne(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.ne(a : List[int], b : List[int]) -> bool torch.ne(a : Device, b : Device) -> bool torch.ne(a : bool, b : bool) -> bool torch.ne(a : AnyEnumType, b : AnyEnumType) -> bool torch.ne(a : int, b : int) -> bool torch.ne(a : complex, b : complex) -> bool torch.ne(a : float, b : float) -> bool torch.ne(a : int, b : float) -> bool torch.ne(a : float, b : int) -> bool torch.ne(a : float, b : complex) -> bool torch.ne(a : complex, b : float)", "prev_chunk_id": "chunk_558", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_560", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "-> bool torch.ne(a : number, b : number) -> bool torch.ne(a : str, b : str) -> bool torch.ne(a : List[float], b : List[float]) -> bool torch.ne(a : List[Tensor], b : List[Tensor]) -> bool torch.ne(a : List[bool], b : List[bool]) -> bool torch.ne(a : List[str], b : List[str]) -> bool torch.neg(self : Tensor) -> Tensor torch.neg(self : Tensor, out : Tensor) -> Tensor torch.neg(a : int) -> int torch.neg(a : float) -> float torch.neg(a : complex) -> complex torch.neg(a : number) -> number torch.neg_(self : Tensor) -> Tensor torch.negative(self : Tensor) -> Tensor torch.negative(self : Tensor, out : Tensor) -> Tensor torch.negative_(self : Tensor) -> Tensor torch.nextafter(self : Tensor, other : Tensor) -> Tensor torch.nextafter(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.nonzero(self : Tensor) -> Tensor torch.nonzero(self : Tensor, out : Tensor) -> Tensor torch.nonzero_static(self : Tensor, size : int, fill_value : int=-1) -> Tensor torch.nonzero_static(self : Tensor, size : int, fill_value : int=-1, out : Tensor) -> Tensor torch.norm_except_dim(v : Tensor, pow : int=2, dim : int=0) -> Tensor torch.normal(mean : Tensor, std : float=1.0, generator : Optional[Generator]) -> Tensor torch.normal(mean : Tensor, std : float=1.0, generator : Optional[Generator], out : Tensor) -> Tensor torch.normal(mean : float, std : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.normal(mean : float, std : Tensor, generator : Optional[Generator]) -> Tensor torch.normal(mean : Tensor, std : Tensor, generator : Optional[Generator]) -> Tensor torch.normal(mean : Tensor, std : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.normal(mean : float, std : float, size : List[int], generator : Optional[Generator], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.normal(mean : float, std : float, size : List[int], generator : Optional[Generator], out : Tensor) -> Tensor torch.normal(self : Tensor, mean : float=0.0, std :", "prev_chunk_id": "chunk_559", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_561", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "float=1.0, generator : Optional[Generator], out : Tensor) -> Tensor torch.not_equal(self : Tensor, other : Tensor) -> Tensor torch.not_equal(self : Tensor, other : number) -> Tensor torch.not_equal(self : Tensor, other : number, out : Tensor) -> Tensor torch.not_equal(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.nuclear_norm(self : Tensor, keepdim : bool=False) -> Tensor torch.nuclear_norm(self : Tensor, dim : List[int], keepdim : bool=False) -> Tensor torch.nuclear_norm(self : Tensor, keepdim : bool=False, out : Tensor) -> Tensor torch.nuclear_norm(self : Tensor, dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor torch.numel(self : Tensor) -> int torch.ones(size : List[int], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.ones(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.ones(size : List[int], names : Optional[List[str]], out : Tensor) -> Tensor torch.ones(size : List[int], out : Tensor) -> Tensor torch.ones_like(self : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.ones_like(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.orgqr(self : Tensor, input2 : Tensor) -> Tensor torch.orgqr(self : Tensor, input2 : Tensor, out : Tensor) -> Tensor torch.ormqr(self : Tensor, input2 : Tensor, input3 : Tensor, left : bool=True, transpose : bool=False) -> Tensor torch.ormqr(self : Tensor, input2 : Tensor, input3 : Tensor, left : bool=True, transpose : bool=False, out : Tensor) -> Tensor torch.outer(self : Tensor, vec2 : Tensor) -> Tensor torch.outer(self : Tensor, vec2 : Tensor, out : Tensor) -> Tensor torch.pairwise_distance(x1 : Tensor, x2 : Tensor, p : float=2.0, eps : float=1e-06, keepdim : bool=False) -> Tensor torch.pdist(self : Tensor, p : float=2.0) -> Tensor torch.permute(self : Tensor, dims : List[int]) -> Tensor torch.permute_copy(self : Tensor, dims : List[int]) -> Tensor torch.permute_copy(self", "prev_chunk_id": "chunk_560", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_562", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, dims : List[int], out : Tensor) -> Tensor torch.pinverse(self : Tensor, rcond : float=1e-15) -> Tensor torch.pixel_shuffle(self : Tensor, upscale_factor : int) -> Tensor torch.pixel_shuffle(self : Tensor, upscale_factor : int, out : Tensor) -> Tensor torch.pixel_unshuffle(self : Tensor, downscale_factor : int) -> Tensor torch.pixel_unshuffle(self : Tensor, downscale_factor : int, out : Tensor) -> Tensor torch.poisson(self : Tensor, generator : Optional[Generator]) -> Tensor torch.poisson(self : Tensor, generator : Optional[Generator], out : Tensor) -> Tensor torch.poisson_nll_loss(input : Tensor, target : Tensor, log_input : bool, full : bool, eps : float, reduction : int) -> Tensor torch.polar(abs : Tensor, angle : Tensor) -> Tensor torch.polar(abs : Tensor, angle : Tensor, out : Tensor) -> Tensor torch.polar(a : int, b : int) -> complex torch.polar(a : float, b : float) -> complex torch.polar(a : int, b : float) -> complex torch.polar(a : float, b : int) -> complex torch.polar(a : number, b : number) -> number torch.polygamma(n : int, self : Tensor) -> Tensor torch.polygamma(n : int, self : Tensor, out : Tensor) -> Tensor torch.positive(self : Tensor) -> Tensor torch.pow(self : Tensor, exponent : Tensor) -> Tensor torch.pow(self : Tensor, exponent : number) -> Tensor torch.pow(self : number, exponent : Tensor) -> Tensor torch.pow(self : number, exponent : Tensor, out : Tensor) -> Tensor torch.pow(self : Tensor, exponent : number, out : Tensor) -> Tensor torch.pow(self : Tensor, exponent : Tensor, out : Tensor) -> Tensor torch.pow(a : int, b : int) -> float torch.pow(a : complex, b : complex) -> complex torch.pow(a : float, b : float) -> float torch.pow(a : int, b : float) -> float torch.pow(a : float, b : int) -> float torch.pow(a : float, b : complex) -> complex torch.pow(a : complex, b : float) -> complex torch.pow(a : number, b : number) -> float torch.pow(a", "prev_chunk_id": "chunk_561", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_563", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": int, b : int) -> int torch.prelu(self : Tensor, weight : Tensor) -> Tensor torch.prod(self : Tensor, dtype : Optional[int]) -> Tensor torch.prod(self : Tensor, dim : int, keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.prod(self : Tensor, dim : str, keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.prod(self : Tensor, dim : str, keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.prod(self : Tensor, dim : int, keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.prod(self : Tensor, dtype : Optional[int], out : Tensor) -> Tensor torch.promote_types(type1 : int, type2 : int) -> int torch.put(self : Tensor, index : Tensor, source : Tensor, accumulate : bool=False) -> Tensor torch.put(self : Tensor, index : Tensor, source : Tensor, accumulate : bool=False, out : Tensor) -> Tensor torch.q_per_channel_axis(self : Tensor) -> int torch.q_per_channel_scales(self : Tensor, out : Tensor) -> Tensor torch.q_per_channel_scales(self : Tensor) -> Tensor torch.q_per_channel_zero_points(self : Tensor, out : Tensor) -> Tensor torch.q_per_channel_zero_points(self : Tensor) -> Tensor torch.q_scale(self : Tensor) -> float torch.q_zero_point(self : Tensor) -> int torch.qr(self : Tensor, some : bool=True) -> Tuple[Tensor, Tensor] torch.qr(self : Tensor, some : bool=True, Q : Tensor, R : Tensor) -> Tuple[Tensor, Tensor] torch.qscheme(self : Tensor) -> QScheme torch.quantile(self : Tensor, q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor torch.quantile(self : Tensor, q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear) -> Tensor torch.quantile(self : Tensor, q : Tensor, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor torch.quantile(self : Tensor, q : float, dim : Optional[int], keepdim : bool=False, interpolation : str=linear, out : Tensor) -> Tensor torch.quantize_per_channel(self : Tensor, scales : Tensor, zero_points : Tensor, axis : int, dtype : int) -> Tensor torch.quantize_per_channel(self : Tensor, scales", "prev_chunk_id": "chunk_562", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_564", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, zero_points : Tensor, axis : int, dtype : int, out : Tensor) -> Tensor torch.quantize_per_tensor(self : Tensor, scale : float, zero_point : int, dtype : int) -> Tensor torch.quantize_per_tensor(self : Tensor, scale : Tensor, zero_point : Tensor, dtype : int) -> Tensor torch.quantize_per_tensor(tensors : List[Tensor], scales : Tensor, zero_points : Tensor, dtype : int) -> List[Tensor] torch.quantize_per_tensor(self : Tensor, scale : float, zero_point : int, dtype : int, out : Tensor) -> Tensor torch.quantize_per_tensor(self : Tensor, scale : Tensor, zero_point : Tensor, dtype : int, out : Tensor) -> Tensor torch.quantize_per_tensor(tensors : List[Tensor], scales : Tensor, zero_points : Tensor, dtype : int, out : List[Tensor]) -> Tuple[] torch.quantize_per_tensor_dynamic(self : Tensor, dtype : int, reduce_range : bool) -> Tensor torch.quantize_per_tensor_dynamic(self : Tensor, dtype : int, reduce_range : bool, out : Tensor) -> Tensor torch.quantized_batch_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], mean : Tensor, var : Tensor, eps : float, output_scale : float, output_zero_point : int, out : Tensor) -> Tensor torch.quantized_batch_norm(input : Tensor, weight : Optional[Tensor], bias : Optional[Tensor], mean : Tensor, var : Tensor, eps : float, output_scale : float, output_zero_point : int) -> Tensor torch.quantized_gru_cell(input : Tensor, hx : Tensor, w_ih : Tensor, w_hh : Tensor, b_ih : Tensor, b_hh : Tensor, packed_ih : Tensor, packed_hh : Tensor, col_offsets_ih : Tensor, col_offsets_hh : Tensor, scale_ih : number, scale_hh : number, zero_point_ih : number, zero_point_hh : number) -> Tensor torch.quantized_lstm_cell(input : Tensor, hx : List[Tensor], w_ih : Tensor, w_hh : Tensor, b_ih : Tensor, b_hh : Tensor, packed_ih : Tensor, packed_hh : Tensor, col_offsets_ih : Tensor, col_offsets_hh : Tensor, scale_ih : number, scale_hh : number, zero_point_ih : number, zero_point_hh : number) -> Tuple[Tensor, Tensor] torch.quantized_max_pool1d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], dilation : List[int]=[1], ceil_mode : bool=False, out : Tensor) ->", "prev_chunk_id": "chunk_563", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_565", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor torch.quantized_max_pool1d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0], dilation : List[int]=[1], ceil_mode : bool=False) -> Tensor torch.quantized_max_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False, out : Tensor) -> Tensor torch.quantized_max_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False) -> Tensor torch.quantized_max_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False, out : Tensor) -> Tensor torch.quantized_max_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False) -> Tensor torch.quantized_rnn_relu_cell(input : Tensor, hx : Tensor, w_ih : Tensor, w_hh : Tensor, b_ih : Tensor, b_hh : Tensor, packed_ih : Tensor, packed_hh : Tensor, col_offsets_ih : Tensor, col_offsets_hh : Tensor, scale_ih : number, scale_hh : number, zero_point_ih : number, zero_point_hh : number) -> Tensor torch.quantized_rnn_tanh_cell(input : Tensor, hx : Tensor, w_ih : Tensor, w_hh : Tensor, b_ih : Tensor, b_hh : Tensor, packed_ih : Tensor, packed_hh : Tensor, col_offsets_ih : Tensor, col_offsets_hh : Tensor, scale_ih : number, scale_hh : number, zero_point_ih : number, zero_point_hh : number) -> Tensor torch.rad2deg(self : Tensor) -> Tensor torch.rad2deg(self : Tensor, out : Tensor) -> Tensor torch.rad2deg_(self : Tensor) -> Tensor torch.rand(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.rand(size : List[int], generator : Optional[Generator], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.rand(size : List[int], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.rand(size : List[int], generator : Optional[Generator], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device],", "prev_chunk_id": "chunk_564", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_566", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "pin_memory : Optional[bool]) -> Tensor torch.rand(size : List[int], out : Tensor) -> Tensor torch.rand(size : List[int], generator : Optional[Generator], out : Tensor) -> Tensor torch.rand(size : List[int], names : Optional[List[str]], out : Tensor) -> Tensor torch.rand(size : List[int], generator : Optional[Generator], names : Optional[List[str]], out : Tensor) -> Tensor torch.rand_like(self : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.rand_like(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.randint(high : int, size : List[int], dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randint(high : int, size : List[int], generator : Optional[Generator], dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randint(low : int, high : int, size : List[int], dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randint(low : int, high : int, size : List[int], generator : Optional[Generator], dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randint(high : int, size : List[int], out : Tensor) -> Tensor torch.randint(high : int, size : List[int], generator : Optional[Generator], out : Tensor) -> Tensor torch.randint(low : int, high : int, size : List[int], out : Tensor) -> Tensor torch.randint(low : int, high : int, size : List[int], generator : Optional[Generator], out : Tensor) -> Tensor torch.randint_like(self : Tensor, high : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.randint_like(self : Tensor, low : int, high : int, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.randint_like(self : Tensor, high : int, memory_format : Optional[int], out : Tensor) -> Tensor torch.randint_like(self : Tensor, high : Tensor, dtype : Optional[int],", "prev_chunk_id": "chunk_565", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_567", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.randint_like(self : Tensor, high : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.randint_like(self : Tensor, low : int, high : int, memory_format : Optional[int], out : Tensor) -> Tensor torch.randn(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randn(size : List[int], generator : Optional[Generator], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randn(size : List[int], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randn(size : List[int], generator : Optional[Generator], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randn(size : List[int], out : Tensor) -> Tensor torch.randn(size : List[int], generator : Optional[Generator], out : Tensor) -> Tensor torch.randn(size : List[int], names : Optional[List[str]], out : Tensor) -> Tensor torch.randn(size : List[int], generator : Optional[Generator], names : Optional[List[str]], out : Tensor) -> Tensor torch.randn_like(self : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.randn_like(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch.randperm(n : int, dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randperm(n : int, generator : Optional[Generator], dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.randperm(n : int, out : Tensor) -> Tensor torch.randperm(n : int, generator : Optional[Generator], out : Tensor) -> Tensor torch.range(start : number, end : number, step : number=1, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.range(start : number, end : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.range(start", "prev_chunk_id": "chunk_566", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_568", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": number, end : number, step : number=1, out : Tensor) -> Tensor torch.range(start : number, end : number, out : Tensor) -> Tensor torch.ravel(self : Tensor) -> Tensor torch.real(self : Tensor) -> Tensor torch.reciprocal(self : Tensor) -> Tensor torch.reciprocal(self : Tensor, out : Tensor) -> Tensor torch.reciprocal_(self : Tensor) -> Tensor torch.relu(self : Tensor) -> Tensor torch.relu(self : Tensor, out : Tensor) -> Tensor torch.relu_(self : Tensor) -> Tensor torch.remainder(self : Tensor, other : Tensor) -> Tensor torch.remainder(self : Tensor, other : number) -> Tensor torch.remainder(self : number, other : Tensor) -> Tensor torch.remainder(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.remainder(self : Tensor, other : number, out : Tensor) -> Tensor torch.remainder(self : number, other : Tensor, out : Tensor) -> Tensor torch.remainder(a : int, b : int) -> int torch.remainder(a : float, b : float) -> float torch.remainder(a : int, b : float) -> float torch.remainder(a : float, b : int) -> float torch.remainder(a : number, b : number) -> number torch.renorm(self : Tensor, p : number, dim : int, maxnorm : number) -> Tensor torch.renorm(self : Tensor, p : number, dim : int, maxnorm : number, out : Tensor) -> Tensor torch.repeat_interleave(repeats : Tensor, output_size : Optional[int]) -> Tensor torch.repeat_interleave(self : Tensor, repeats : Tensor, dim : Optional[int], output_size : Optional[int]) -> Tensor torch.repeat_interleave(self : Tensor, repeats : int, dim : Optional[int], output_size : Optional[int]) -> Tensor torch.repeat_interleave(repeats : Tensor, output_size : Optional[int], out : Tensor) -> Tensor torch.reshape(self : Tensor, shape : List[int]) -> Tensor torch.resize_as_(self : Tensor, the_template : Tensor, memory_format : Optional[int]) -> Tensor torch.resize_as_sparse_(self : Tensor, the_template : Tensor) -> Tensor torch.resolve_conj(self : Tensor) -> Tensor torch.resolve_neg(self : Tensor) -> Tensor torch.result_type(tensor : Tensor, other : Tensor) -> int torch.result_type(tensor : Tensor, other : number) -> int torch.result_type(scalar :", "prev_chunk_id": "chunk_567", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_569", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "number, tensor : Tensor) -> int torch.result_type(scalar1 : number, scalar2 : number) -> int torch.rms_norm(input : Tensor, normalized_shape : List[int], weight : Optional[Tensor], eps : Optional[float]) -> Tensor torch.rnn_relu(input : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool, batch_first : bool) -> Tuple[Tensor, Tensor] torch.rnn_relu(data : Tensor, batch_sizes : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool) -> Tuple[Tensor, Tensor] torch.rnn_relu_cell(input : Tensor, hx : Tensor, w_ih : Tensor, w_hh : Tensor, b_ih : Optional[Tensor], b_hh : Optional[Tensor]) -> Tensor torch.rnn_tanh(input : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool, batch_first : bool) -> Tuple[Tensor, Tensor] torch.rnn_tanh(data : Tensor, batch_sizes : Tensor, hx : Tensor, params : List[Tensor], has_biases : bool, num_layers : int, dropout : float, train : bool, bidirectional : bool) -> Tuple[Tensor, Tensor] torch.rnn_tanh_cell(input : Tensor, hx : Tensor, w_ih : Tensor, w_hh : Tensor, b_ih : Optional[Tensor], b_hh : Optional[Tensor]) -> Tensor torch.roll(self : Tensor, shifts : List[int], dims : List[int]=[]) -> Tensor torch.roll(self : Tensor, shifts : List[int], dims : List[int]=[], out : Tensor) -> Tensor torch.rot90(self : Tensor, k : int=1, dims : List[int]=[0, 1]) -> Tensor torch.rot90(self : Tensor, k : int=1, dims : List[int]=[0, 1], out : Tensor) -> Tensor torch.round(self : Tensor) -> Tensor torch.round(self : Tensor, decimals : int) -> Tensor torch.round(self : Tensor, out : Tensor) -> Tensor torch.round(self : Tensor, decimals : int, out : Tensor) -> Tensor torch.round(a : int) -> float torch.round(a : float) -> float torch.round(a : number) -> number torch.round_(self : Tensor) -> Tensor torch.round_(self : Tensor, decimals : int) -> Tensor", "prev_chunk_id": "chunk_568", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_570", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.row_indices_copy(self : Tensor) -> Tensor torch.row_indices_copy(self : Tensor, out : Tensor) -> Tensor torch.row_stack(tensors : List[Tensor]) -> Tensor torch.row_stack(tensors : List[Tensor], out : Tensor) -> Tensor torch.rrelu(self : Tensor, lower : number=0.125, upper : number=0.3333333333333333, training : bool=False, generator : Optional[Generator]) -> Tensor torch.rrelu_(self : Tensor, lower : number=0.125, upper : number=0.3333333333333333, training : bool=False, generator : Optional[Generator]) -> Tensor torch.rsqrt(self : Tensor) -> Tensor torch.rsqrt(self : Tensor, out : Tensor) -> Tensor torch.rsqrt_(self : Tensor) -> Tensor torch.rsub(self : Tensor, other : Tensor, alpha : number=1) -> Tensor torch.rsub(self : Tensor, other : number, alpha : number=1) -> Tensor torch.rsub(self : Tensor, other : Tensor, alpha : number=1, out : Tensor) -> Tensor torch.rsub(self : Tensor, other : number, alpha : number=1, out : Tensor) -> Tensor torch.save(item : t, filename : str) -> Tuple[] torch.scalar_tensor(s : number, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.scalar_tensor(s : number, out : Tensor) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, value : number) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, src : Tensor) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, src : Tensor, reduce : str) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, value : number, reduce : str) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, src : Tensor, out : Tensor) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, value : number, out : Tensor) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, src : Tensor, reduce : str, out : Tensor) -> Tensor torch.scatter(self : Tensor, dim : int, index : Tensor, value : number, reduce : str, out : Tensor) -> Tensor torch.scatter(self : Tensor,", "prev_chunk_id": "chunk_569", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_571", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "dim : str, index : Tensor, src : Tensor) -> Tensor torch.scatter(self : Tensor, dim : str, index : Tensor, value : number) -> Tensor torch.scatter_add(self : Tensor, dim : int, index : Tensor, src : Tensor) -> Tensor torch.scatter_add(self : Tensor, dim : int, index : Tensor, src : Tensor, out : Tensor) -> Tensor torch.scatter_add(self : Tensor, dim : str, index : Tensor, src : Tensor) -> Tensor torch.scatter_reduce(self : Tensor, dim : int, index : Tensor, src : Tensor, reduce : str, include_self : bool=True) -> Tensor torch.scatter_reduce(self : Tensor, dim : int, index : Tensor, src : Tensor, reduce : str, include_self : bool=True, out : Tensor) -> Tensor torch.searchsorted(sorted_sequence : Tensor, self : Tensor, out_int32 : bool=False, right : bool=False, side : Optional[str], sorter : Optional[Tensor]) -> Tensor torch.searchsorted(sorted_sequence : Tensor, self : Tensor, out_int32 : bool=False, right : bool=False, side : Optional[str], sorter : Optional[Tensor], out : Tensor) -> Tensor torch.searchsorted(sorted_sequence : Tensor, self : number, out_int32 : bool=False, right : bool=False, side : Optional[str], sorter : Optional[Tensor]) -> Tensor torch.searchsorted(sorted_sequence : Tensor, self : number, out_int32 : bool=False, right : bool=False, side : Optional[str], sorter : Optional[Tensor], out : Tensor) -> Tensor torch.seed(self : Generator) -> int torch.segment_reduce(data : Tensor, reduce : str, lengths : Optional[Tensor], indices : Optional[Tensor], offsets : Optional[Tensor], axis : int=0, unsafe : bool=False, initial : Optional[number]) -> Tensor torch.segment_reduce(data : Tensor, reduce : str, lengths : Optional[Tensor], indices : Optional[Tensor], offsets : Optional[Tensor], axis : int=0, unsafe : bool=False, initial : Optional[number], out : Tensor) -> Tensor torch.select(self : Tensor, dim : str, index : int) -> Tensor torch.select(self : Tensor, dim : int, index : int) -> Tensor torch.select(list : List[t], idx : int) -> t torch.select_copy(self : Tensor, dim : int, index : int) -> Tensor", "prev_chunk_id": "chunk_570", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_572", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch.select_copy(self : Tensor, dim : int, index : int, out : Tensor) -> Tensor torch.select_scatter(self : Tensor, src : Tensor, dim : int, index : int) -> Tensor torch.select_scatter(self : Tensor, src : Tensor, dim : int, index : int, out : Tensor) -> Tensor torch.selu(self : Tensor) -> Tensor torch.selu_(self : Tensor) -> Tensor torch.set_grad_enabled(val : bool) -> Tuple[] torch.sgn(self : Tensor) -> Tensor torch.sgn(self : Tensor, out : Tensor) -> Tensor torch.sigmoid(self : Tensor) -> Tensor torch.sigmoid(self : Tensor, out : Tensor) -> Tensor torch.sigmoid_(self : Tensor) -> Tensor torch.sign(self : Tensor) -> Tensor torch.sign(self : Tensor, out : Tensor) -> Tensor torch.signbit(self : Tensor) -> Tensor torch.signbit(self : Tensor, out : Tensor) -> Tensor torch.sin(self : Tensor) -> Tensor torch.sin(self : Tensor, out : Tensor) -> Tensor torch.sin(a : int) -> float torch.sin(a : float) -> float torch.sin(a : complex) -> complex torch.sin(a : number) -> number torch.sin_(self : Tensor) -> Tensor torch.sinc(self : Tensor) -> Tensor torch.sinc(self : Tensor, out : Tensor) -> Tensor torch.sinc_(self : Tensor) -> Tensor torch.sinh(self : Tensor) -> Tensor torch.sinh(self : Tensor, out : Tensor) -> Tensor torch.sinh(a : int) -> float torch.sinh(a : float) -> float torch.sinh(a : complex) -> complex torch.sinh(a : number) -> number torch.sinh_(self : Tensor) -> Tensor torch.slice_copy(self : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1) -> Tensor torch.slice_copy(self : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1, out : Tensor) -> Tensor torch.slice_inverse(self : Tensor, src : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1) -> Tensor torch.slice_scatter(self : Tensor, src : Tensor, dim : int=0, start : Optional[int], end : Optional[int], step : int=1) -> Tensor torch.slice_scatter(self : Tensor, src : Tensor, dim : int=0, start : Optional[int],", "prev_chunk_id": "chunk_571", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_573", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "end : Optional[int], step : int=1, out : Tensor) -> Tensor torch.slogdet(self : Tensor) -> Tuple[Tensor, Tensor] torch.slogdet(self : Tensor, sign : Tensor, logabsdet : Tensor) -> Tuple[Tensor, Tensor] torch.smm(self : Tensor, mat2 : Tensor) -> Tensor torch.softmax(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch.softmax(self : Tensor, dim : str, dtype : Optional[int]) -> Tensor torch.softmax(self : Tensor, dim : int, dtype : Optional[int], out : Tensor) -> Tensor torch.sort(self : Tensor, dim : int=-1, descending : bool=False) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, stable : Optional[bool], dim : int=-1, descending : bool=False) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, stable : Optional[bool], dim : int=-1, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, dim : int=-1, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, dim : str, descending : bool=False) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, dim : str, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, stable : Optional[bool], dim : str, descending : bool=False) -> Tuple[Tensor, Tensor] torch.sort(self : Tensor, stable : Optional[bool], dim : str, descending : bool=False, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.sort(self : List[int], reverse : bool=False) -> Tuple[] torch.sort(self : List[float], reverse : bool=False) -> Tuple[] torch.sort(self : List[Tensor], reverse : bool=False) -> Tuple[] torch.sort(self : List[bool], reverse : bool=False) -> Tuple[] torch.sort(self : List[str], reverse : bool=False) -> Tuple[] torch.sort(self : List[t], reverse : bool=False) -> Tuple[] torch.sparse_bsc_tensor(ccol_indices : Tensor, row_indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_bsc_tensor(ccol_indices : Tensor, row_indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False)", "prev_chunk_id": "chunk_572", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_574", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "-> Tensor torch.sparse_bsr_tensor(crow_indices : Tensor, col_indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_bsr_tensor(crow_indices : Tensor, col_indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_compressed_tensor(compressed_indices : Tensor, plain_indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_compressed_tensor(compressed_indices : Tensor, plain_indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_coo_tensor(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_coo_tensor(indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], is_coalesced : Optional[bool]) -> Tensor torch.sparse_coo_tensor(indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], is_coalesced : Optional[bool]) -> Tensor torch.sparse_coo_tensor(size : List[int], out : Tensor) -> Tensor torch.sparse_csc_tensor(ccol_indices : Tensor, row_indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_csc_tensor(ccol_indices : Tensor, row_indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_csr_tensor(crow_indices : Tensor, col_indices : Tensor, values : Tensor, size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.sparse_csr_tensor(crow_indices : Tensor, col_indices : Tensor, values : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]=False) -> Tensor torch.split(self : Tensor, split_size : int, dim : int=0) -> List[Tensor] torch.split(self : Tensor, split_size : List[int], dim : int=0) -> List[Tensor] torch.split(self : str, separator : Optional[str],", "prev_chunk_id": "chunk_573", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_575", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "max : int=-1) -> List[str] torch.split(self : Tensor, split_sizes : List[int], dim : int=0) -> List[Tensor] torch.split_copy(self : Tensor, split_size : int, dim : int=0) -> List[Tensor] torch.split_copy(self : Tensor, split_size : int, dim : int=0, out : List[Tensor]) -> Tuple[] torch.split_with_sizes(self : Tensor, split_sizes : List[int], dim : int=0) -> List[Tensor] torch.split_with_sizes_copy(self : Tensor, split_sizes : List[int], dim : int=0) -> List[Tensor] torch.split_with_sizes_copy(self : Tensor, split_sizes : List[int], dim : int=0, out : List[Tensor]) -> Tuple[] torch.sqrt(self : Tensor) -> Tensor torch.sqrt(self : Tensor, out : Tensor) -> Tensor torch.sqrt(a : int) -> float torch.sqrt(a : float) -> float torch.sqrt(a : complex) -> complex torch.sqrt(a : number) -> number torch.sqrt_(self : Tensor) -> Tensor torch.square(self : Tensor) -> Tensor torch.square(self : Tensor, out : Tensor) -> Tensor torch.square_(self : Tensor) -> Tensor torch.squeeze(self : Tensor) -> Tensor torch.squeeze(self : Tensor, dim : int) -> Tensor torch.squeeze(self : Tensor, dim : List[int]) -> Tensor torch.squeeze(self : Tensor, dim : str) -> Tensor torch.squeeze_copy(self : Tensor) -> Tensor torch.squeeze_copy(self : Tensor, dim : int) -> Tensor torch.squeeze_copy(self : Tensor, dim : List[int]) -> Tensor torch.squeeze_copy(self : Tensor, out : Tensor) -> Tensor torch.squeeze_copy(self : Tensor, dim : int, out : Tensor) -> Tensor torch.squeeze_copy(self : Tensor, dim : List[int], out : Tensor) -> Tensor torch.sspaddmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch.sspaddmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch.stack(tensors : List[Tensor], dim : int=0) -> Tensor torch.stack(tensors : List[Tensor], dim : int=0, out : Tensor) -> Tensor torch.std(self : Tensor, unbiased : bool=True) -> Tensor torch.std(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tensor torch.std(self : Tensor, dim : Optional[List[int]], correction : Optional[number],", "prev_chunk_id": "chunk_574", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_576", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "keepdim : bool=False) -> Tensor torch.std(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tensor torch.std(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor torch.std(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor torch.std(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor torch.std(self : Tensor, dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tensor torch.std(self : Tensor, dim : List[str], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor torch.std_mean(self : Tensor, unbiased : bool=True) -> Tuple[Tensor, Tensor] torch.std_mean(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.std_mean(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.std_mean(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.std_mean(self : Tensor, dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.std_mean(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.sub(self : Tensor, other : Tensor, alpha : number=1) -> Tensor torch.sub(self : Tensor, other : number, alpha : number=1) -> Tensor torch.sub(self : Tensor, other : Tensor, alpha : number=1, out : Tensor) -> Tensor torch.sub(self : Tensor, other : number, alpha : number=1, out : Tensor) -> Tensor torch.sub(a : int, b : int) -> int torch.sub(a : complex, b : complex) -> complex torch.sub(a : float, b : float) -> float torch.sub(a : int, b : complex) -> complex torch.sub(a : complex, b : int) -> complex torch.sub(a : float, b : complex) -> complex torch.sub(a : complex, b : float) -> complex torch.sub(a : int,", "prev_chunk_id": "chunk_575", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_577", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "b : float) -> float torch.sub(a : float, b : int) -> float torch.sub(a : number, b : number) -> number torch.subtract(self : Tensor, other : Tensor, alpha : number=1) -> Tensor torch.subtract(self : Tensor, other : Tensor, alpha : number=1, out : Tensor) -> Tensor torch.subtract(self : Tensor, other : number, alpha : number=1) -> Tensor torch.sum(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.sum(self : Tensor, dtype : Optional[int]) -> Tensor torch.sum(self : Tensor, dim : List[str], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch.sum(self : Tensor, dim : List[str], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.sum(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch.sum(self : Tensor, dtype : Optional[int], out : Tensor) -> Tensor torch.sum(self : List[int]) -> int torch.sum(self : List[float]) -> float torch.sum(self : List[complex]) -> complex torch.sum(self : List[bool]) -> int torch.svd(self : Tensor, some : bool=True, compute_uv : bool=True) -> Tuple[Tensor, Tensor, Tensor] torch.svd(self : Tensor, some : bool=True, compute_uv : bool=True, U : Tensor, S : Tensor, V : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch.swapaxes(self : Tensor, axis0 : int, axis1 : int) -> Tensor torch.swapdims(self : Tensor, dim0 : int, dim1 : int) -> Tensor torch.sym_constrain_range(size : number, min : Optional[int], max : Optional[int]) -> Tuple[] torch.sym_constrain_range_for_size(size : number, min : Optional[int], max : Optional[int]) -> Tuple[] torch.t(self : Tensor) -> Tensor torch.t_copy(self : Tensor) -> Tensor torch.t_copy(self : Tensor, out : Tensor) -> Tensor torch.take(self : Tensor, index : Tensor) -> Tensor torch.take(self : Tensor, index : Tensor, out : Tensor) -> Tensor torch.take_along_dim(self : Tensor, indices : Tensor, dim : Optional[int]) -> Tensor torch.take_along_dim(self : Tensor, indices : Tensor, dim : Optional[int], out : Tensor) -> Tensor torch.tan(self", "prev_chunk_id": "chunk_576", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_578", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor) -> Tensor torch.tan(self : Tensor, out : Tensor) -> Tensor torch.tan(a : int) -> float torch.tan(a : float) -> float torch.tan(a : complex) -> complex torch.tan(a : number) -> number torch.tan_(self : Tensor) -> Tensor torch.tanh(self : Tensor) -> Tensor torch.tanh(self : Tensor, out : Tensor) -> Tensor torch.tanh(a : int) -> float torch.tanh(a : float) -> float torch.tanh(a : complex) -> complex torch.tanh(a : number) -> number torch.tanh_(self : Tensor) -> Tensor torch.tensor(t : bool, dtype : Optional[int], device : Optional[Device], requires_grad : bool=False) -> Tensor torch.tensor(t : float, dtype : Optional[int], device : Optional[Device], requires_grad : bool=False) -> Tensor torch.tensor(t : int, dtype : Optional[int], device : Optional[Device], requires_grad : bool=False) -> Tensor torch.tensor(t : complex, dtype : Optional[int], device : Optional[Device], requires_grad : bool=False) -> Tensor torch.tensor(data : List[t], dtype : Optional[int], device : Optional[Device], requires_grad : bool=False) -> Tensor torch.tensor_split(self : Tensor, sections : int, dim : int=0) -> List[Tensor] torch.tensor_split(self : Tensor, indices : List[int], dim : int=0) -> List[Tensor] torch.tensor_split(self : Tensor, tensor_indices_or_sections : Tensor, dim : int=0) -> List[Tensor] torch.threshold(self : Tensor, threshold : number, value : number) -> Tensor torch.threshold(self : Tensor, threshold : number, value : number, out : Tensor) -> Tensor torch.threshold_(self : Tensor, threshold : number, value : number) -> Tensor torch.tile(self : Tensor, dims : List[int]) -> Tensor torch.topk(self : Tensor, k : int, dim : int=-1, largest : bool=True, sorted : bool=True) -> Tuple[Tensor, Tensor] torch.topk(self : Tensor, k : int, dim : int=-1, largest : bool=True, sorted : bool=True, values : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch.trace(self : Tensor) -> Tensor torch.trace(self : Tensor, out : Tensor) -> Tensor torch.transpose(self : Tensor, dim0 : int, dim1 : int) -> Tensor torch.transpose(self : Tensor, dim0 : str, dim1 : str) -> Tensor torch.transpose_copy(self", "prev_chunk_id": "chunk_577", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_579", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, dim0 : int, dim1 : int) -> Tensor torch.transpose_copy(self : Tensor, dim0 : int, dim1 : int, out : Tensor) -> Tensor torch.trapezoid(y : Tensor, x : Tensor, dim : int=-1) -> Tensor torch.trapezoid(y : Tensor, dx : number=1, dim : int=-1) -> Tensor torch.trapz(y : Tensor, x : Tensor, dim : int=-1) -> Tensor torch.trapz(y : Tensor, dx : float=1.0, dim : int=-1) -> Tensor torch.triangular_solve(self : Tensor, A : Tensor, upper : bool=True, transpose : bool=False, unitriangular : bool=False) -> Tuple[Tensor, Tensor] torch.triangular_solve(self : Tensor, A : Tensor, upper : bool=True, transpose : bool=False, unitriangular : bool=False, X : Tensor, M : Tensor) -> Tuple[Tensor, Tensor] torch.tril(self : Tensor, diagonal : int=0) -> Tensor torch.tril(self : Tensor, diagonal : int=0, out : Tensor) -> Tensor torch.tril_indices(row : int, col : int, offset : int=0, dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.tril_indices(row : int, col : int, offset : int=0, out : Tensor) -> Tensor torch.triplet_margin_loss(anchor : Tensor, positive : Tensor, negative : Tensor, margin : float=1.0, p : float=2.0, eps : float=1e-06, swap : bool=False, reduction : int=1) -> Tensor torch.triu(self : Tensor, diagonal : int=0) -> Tensor torch.triu(self : Tensor, diagonal : int=0, out : Tensor) -> Tensor torch.triu_indices(row : int, col : int, offset : int=0, dtype : Optional[int]=4, layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.triu_indices(row : int, col : int, offset : int=0, out : Tensor) -> Tensor torch.true_divide(self : Tensor, other : Tensor) -> Tensor torch.true_divide(self : Tensor, other : number) -> Tensor torch.true_divide(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.trunc(self : Tensor) -> Tensor torch.trunc(self : Tensor, out : Tensor) -> Tensor torch.trunc_(self : Tensor) -> Tensor torch.unbind(self : Tensor, dim : int=0) ->", "prev_chunk_id": "chunk_578", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_580", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "List[Tensor] torch.unbind(self : Tensor, dim : str) -> List[Tensor] torch.unbind_copy(self : Tensor, dim : int=0) -> List[Tensor] torch.unbind_copy(self : Tensor, dim : int=0, out : List[Tensor]) -> Tuple[] torch.unflatten(self : Tensor, dim : int, sizes : List[int]) -> Tensor torch.unflatten(self : Tensor, dim : str, sizes : List[int], names : List[str]) -> Tensor torch.unfold_copy(self : Tensor, dimension : int, size : int, step : int) -> Tensor torch.unfold_copy(self : Tensor, dimension : int, size : int, step : int, out : Tensor) -> Tensor torch.unsafe_chunk(self : Tensor, chunks : int, dim : int=0) -> List[Tensor] torch.unsafe_split(self : Tensor, split_size : int, dim : int=0) -> List[Tensor] torch.unsafe_split(self : Tensor, split_size : int, dim : int=0, out : List[Tensor]) -> Tuple[] torch.unsafe_split_with_sizes(self : Tensor, split_sizes : List[int], dim : int=0) -> List[Tensor] torch.unsafe_split_with_sizes(self : Tensor, split_sizes : List[int], dim : int=0, out : List[Tensor]) -> Tuple[] torch.unsqueeze(self : Tensor, dim : int) -> Tensor torch.unsqueeze_copy(self : Tensor, dim : int) -> Tensor torch.unsqueeze_copy(self : Tensor, dim : int, out : Tensor) -> Tensor torch.values_copy(self : Tensor) -> Tensor torch.values_copy(self : Tensor, out : Tensor) -> Tensor torch.vander(x : Tensor, N : Optional[int], increasing : bool=False) -> Tensor torch.var(self : Tensor, unbiased : bool=True) -> Tensor torch.var(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tensor torch.var(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False) -> Tensor torch.var(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tensor torch.var(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor torch.var(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False, out : Tensor) -> Tensor torch.var(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor torch.var(self : Tensor,", "prev_chunk_id": "chunk_579", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_581", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tensor torch.var(self : Tensor, dim : List[str], correction : Optional[number], keepdim : bool=False, out : Tensor) -> Tensor torch.var_mean(self : Tensor, unbiased : bool=True) -> Tuple[Tensor, Tensor] torch.var_mean(self : Tensor, dim : Optional[List[int]], unbiased : bool=True, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.var_mean(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.var_mean(self : Tensor, dim : List[str], unbiased : bool=True, keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.var_mean(self : Tensor, dim : List[str], correction : Optional[number], keepdim : bool=False) -> Tuple[Tensor, Tensor] torch.var_mean(self : Tensor, dim : Optional[List[int]], correction : Optional[number], keepdim : bool=False, out0 : Tensor, out1 : Tensor) -> Tuple[Tensor, Tensor] torch.vdot(self : Tensor, other : Tensor) -> Tensor torch.vdot(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.view_as_complex(self : Tensor) -> Tensor torch.view_as_complex_copy(self : Tensor) -> Tensor torch.view_as_complex_copy(self : Tensor, out : Tensor) -> Tensor torch.view_as_real(self : Tensor) -> Tensor torch.view_as_real_copy(self : Tensor) -> Tensor torch.view_as_real_copy(self : Tensor, out : Tensor) -> Tensor torch.view_copy(self : Tensor, size : List[int]) -> Tensor torch.view_copy(self : Tensor, dtype : int) -> Tensor torch.view_copy(self : Tensor, size : List[int], out : Tensor) -> Tensor torch.view_copy(self : Tensor, dtype : int, out : Tensor) -> Tensor torch.vsplit(self : Tensor, sections : int) -> List[Tensor] torch.vsplit(self : Tensor, indices : List[int]) -> List[Tensor] torch.vstack(tensors : List[Tensor]) -> Tensor torch.vstack(tensors : List[Tensor], out : Tensor) -> Tensor torch.wait(self : Future[t]) -> t torch.where(condition : Tensor, self : Tensor, other : Tensor) -> Tensor torch.where(condition : Tensor, self : Tensor, other : number) -> Tensor torch.where(condition : Tensor, self : number, other : Tensor) -> Tensor torch.where(condition : Tensor, self : number, other : number) -> Tensor torch.where(condition : Tensor) -> List[Tensor] torch.where(condition : Tensor, self : Tensor,", "prev_chunk_id": "chunk_580", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_582", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "other : Tensor, out : Tensor) -> Tensor torch.xlogy(self : Tensor, other : Tensor) -> Tensor torch.xlogy(self : Tensor, other : number) -> Tensor torch.xlogy(self : number, other : Tensor) -> Tensor torch.xlogy(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch.xlogy(self : number, other : Tensor, out : Tensor) -> Tensor torch.xlogy(self : Tensor, other : number, out : Tensor) -> Tensor torch.xlogy_(self : Tensor, other : Tensor) -> Tensor torch.xlogy_(self : Tensor, other : number) -> Tensor torch.zero_(self : Tensor) -> Tensor torch.zeros(size : List[int], names : Optional[List[str]], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.zeros(size : List[int], dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch.zeros(size : List[int], names : Optional[List[str]], out : Tensor) -> Tensor torch.zeros(size : List[int], out : Tensor) -> Tensor torch.zeros_like(self : Tensor, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool], memory_format : Optional[int]) -> Tensor torch.zeros_like(self : Tensor, memory_format : Optional[int], out : Tensor) -> Tensor torch._C._nn.adaptive_avg_pool2d(self : Tensor, output_size : List[int]) -> Tensor torch._C._nn.adaptive_avg_pool2d(self : Tensor, output_size : List[int], out : Tensor) -> Tensor torch._C._nn.adaptive_avg_pool3d(self : Tensor, output_size : List[int]) -> Tensor torch._C._nn.adaptive_avg_pool3d(self : Tensor, output_size : List[int], out : Tensor) -> Tensor torch._C._nn.adaptive_max_pool2d(self : Tensor, output_size : List[int]) -> Tuple[Tensor, Tensor] torch._C._nn.adaptive_max_pool2d(self : Tensor, output_size : List[int], out : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.adaptive_max_pool3d(self : Tensor, output_size : List[int]) -> Tuple[Tensor, Tensor] torch._C._nn.adaptive_max_pool3d(self : Tensor, output_size : List[int], out : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.avg_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], ceil_mode : bool=False, count_include_pad : bool=True, divisor_override : Optional[int]) -> Tensor torch._C._nn.avg_pool2d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], ceil_mode : bool=False, count_include_pad", "prev_chunk_id": "chunk_581", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_583", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": bool=True, divisor_override : Optional[int], out : Tensor) -> Tensor torch._C._nn.avg_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], ceil_mode : bool=False, count_include_pad : bool=True, divisor_override : Optional[int]) -> Tensor torch._C._nn.avg_pool3d(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], ceil_mode : bool=False, count_include_pad : bool=True, divisor_override : Optional[int], out : Tensor) -> Tensor torch._C._nn.binary_cross_entropy(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1) -> Tensor torch._C._nn.binary_cross_entropy(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, out : Tensor) -> Tensor torch._C._nn.col2im(self : Tensor, output_size : List[int], kernel_size : List[int], dilation : List[int], padding : List[int], stride : List[int]) -> Tensor torch._C._nn.col2im(self : Tensor, output_size : List[int], kernel_size : List[int], dilation : List[int], padding : List[int], stride : List[int], out : Tensor) -> Tensor torch._C._nn.conv_depthwise3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int], out : Tensor) -> Tensor torch._C._nn.conv_depthwise3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int], padding : List[int], dilation : List[int]) -> Tensor torch._C._nn.cross_entropy_loss(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100, label_smoothing : float=0.0) -> Tensor torch._C._nn.elu(self : Tensor, alpha : number=1, scale : number=1, input_scale : number=1) -> Tensor torch._C._nn.elu(self : Tensor, alpha : number=1, scale : number=1, input_scale : number=1, out : Tensor) -> Tensor torch._C._nn.elu_(self : Tensor, alpha : number=1, scale : number=1, input_scale : number=1) -> Tensor torch._C._nn.flatten_dense_tensors(tensors : List[Tensor]) -> Tensor torch._C._nn.fractional_max_pool2d(self : Tensor, kernel_size : List[int], output_size : List[int], random_samples : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.fractional_max_pool2d(self : Tensor, kernel_size : List[int], output_size : List[int], random_samples : Tensor, output : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.fractional_max_pool3d(self : Tensor, kernel_size : List[int], output_size :", "prev_chunk_id": "chunk_582", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_584", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "List[int], random_samples : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.fractional_max_pool3d(self : Tensor, kernel_size : List[int], output_size : List[int], random_samples : Tensor, output : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.gelu(self : Tensor, approximate : str=none) -> Tensor torch._C._nn.gelu(self : Tensor, approximate : str=none, out : Tensor) -> Tensor torch._C._nn.gelu_(self : Tensor, approximate : str=none) -> Tensor torch._C._nn.glu(self : Tensor, dim : int=-1) -> Tensor torch._C._nn.glu(self : Tensor, dim : int=-1, out : Tensor) -> Tensor torch._C._nn.hardsigmoid(self : Tensor) -> Tensor torch._C._nn.hardsigmoid(self : Tensor, out : Tensor) -> Tensor torch._C._nn.hardsigmoid_(self : Tensor) -> Tensor torch._C._nn.hardswish(self : Tensor) -> Tensor torch._C._nn.hardswish(self : Tensor, out : Tensor) -> Tensor torch._C._nn.hardswish_(self : Tensor) -> Tensor torch._C._nn.hardtanh(self : Tensor, min_val : number=-1, max_val : number=1) -> Tensor torch._C._nn.hardtanh(self : Tensor, min_val : number=-1, max_val : number=1, out : Tensor) -> Tensor torch._C._nn.hardtanh_(self : Tensor, min_val : number=-1, max_val : number=1) -> Tensor torch._C._nn.huber_loss(self : Tensor, target : Tensor, reduction : int=1, delta : float=1.0) -> Tensor torch._C._nn.huber_loss(self : Tensor, target : Tensor, reduction : int=1, delta : float=1.0, out : Tensor) -> Tensor torch._C._nn.im2col(self : Tensor, kernel_size : List[int], dilation : List[int], padding : List[int], stride : List[int]) -> Tensor torch._C._nn.im2col(self : Tensor, kernel_size : List[int], dilation : List[int], padding : List[int], stride : List[int], out : Tensor) -> Tensor torch._C._nn.l1_loss(self : Tensor, target : Tensor, reduction : int=1) -> Tensor torch._C._nn.leaky_relu(self : Tensor, negative_slope : number=0.01) -> Tensor torch._C._nn.leaky_relu(self : Tensor, negative_slope : number=0.01, out : Tensor) -> Tensor torch._C._nn.leaky_relu_(self : Tensor, negative_slope : number=0.01) -> Tensor torch._C._nn.linear(input : Tensor, weight : Tensor, bias : Optional[Tensor]) -> Tensor torch._C._nn.linear(input : Tensor, weight : Tensor, bias : Optional[Tensor], out : Tensor) -> Tensor torch._C._nn.log_sigmoid(self : Tensor) -> Tensor torch._C._nn.log_sigmoid(self : Tensor, out : Tensor) -> Tensor torch._C._nn.max_pool2d_with_indices(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding", "prev_chunk_id": "chunk_583", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_585", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False) -> Tuple[Tensor, Tensor] torch._C._nn.max_pool2d_with_indices(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], ceil_mode : bool=False, out : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.max_pool3d_with_indices(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False) -> Tuple[Tensor, Tensor] torch._C._nn.max_pool3d_with_indices(self : Tensor, kernel_size : List[int], stride : List[int]=[], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], ceil_mode : bool=False, out : Tensor, indices : Tensor) -> Tuple[Tensor, Tensor] torch._C._nn.max_unpool2d(self : Tensor, indices : Tensor, output_size : List[int]) -> Tensor torch._C._nn.max_unpool2d(self : Tensor, indices : Tensor, output_size : List[int], out : Tensor) -> Tensor torch._C._nn.max_unpool3d(self : Tensor, indices : Tensor, output_size : List[int], stride : List[int], padding : List[int]) -> Tensor torch._C._nn.max_unpool3d(self : Tensor, indices : Tensor, output_size : List[int], stride : List[int], padding : List[int], out : Tensor) -> Tensor torch._C._nn.mish(self : Tensor) -> Tensor torch._C._nn.mish(self : Tensor, out : Tensor) -> Tensor torch._C._nn.mish_(self : Tensor) -> Tensor torch._C._nn.mkldnn_linear(self : Tensor, weight : Tensor, bias : Optional[Tensor], out : Tensor) -> Tensor torch._C._nn.mkldnn_linear(self : Tensor, weight : Tensor, bias : Optional[Tensor]) -> Tensor torch._C._nn.mkldnn_reorder_conv2d_weight(self : Tensor, padding : List[int]=[0, 0], stride : List[int]=[1, 1], dilation : List[int]=[1, 1], groups : int=1, input_size : Optional[List[int]], out : Tensor) -> Tensor torch._C._nn.mkldnn_reorder_conv2d_weight(self : Tensor, padding : List[int]=[0, 0], stride : List[int]=[1, 1], dilation : List[int]=[1, 1], groups : int=1, input_size : Optional[List[int]]) -> Tensor torch._C._nn.mkldnn_reorder_conv3d_weight(self : Tensor, padding : List[int]=[0, 0, 0], stride : List[int]=[1, 1, 1], dilation : List[int]=[1, 1, 1], groups : int=1, input_size : Optional[List[int]], out : Tensor) -> Tensor torch._C._nn.mkldnn_reorder_conv3d_weight(self : Tensor, padding : List[int]=[0, 0, 0], stride : List[int]=[1, 1, 1], dilation : List[int]=[1, 1, 1], groups", "prev_chunk_id": "chunk_584", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_586", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": int=1, input_size : Optional[List[int]]) -> Tensor torch._C._nn.mse_loss(self : Tensor, target : Tensor, reduction : int=1) -> Tensor torch._C._nn.mse_loss(self : Tensor, target : Tensor, reduction : int=1, out : Tensor) -> Tensor torch._C._nn.multi_margin_loss(self : Tensor, target : Tensor, p : number=1, margin : number=1, weight : Optional[Tensor], reduction : int=1) -> Tensor torch._C._nn.multi_margin_loss(self : Tensor, target : Tensor, p : number=1, margin : number=1, weight : Optional[Tensor], reduction : int=1, out : Tensor) -> Tensor torch._C._nn.multilabel_margin_loss(self : Tensor, target : Tensor, reduction : int=1) -> Tensor torch._C._nn.multilabel_margin_loss(self : Tensor, target : Tensor, reduction : int=1, out : Tensor) -> Tensor torch._C._nn.nll_loss(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100) -> Tensor torch._C._nn.nll_loss(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100, out : Tensor) -> Tensor torch._C._nn.nll_loss2d(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100) -> Tensor torch._C._nn.nll_loss2d(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100, out : Tensor) -> Tensor torch._C._nn.nll_loss_nd(self : Tensor, target : Tensor, weight : Optional[Tensor], reduction : int=1, ignore_index : int=-100) -> Tensor torch._C._nn.one_hot(self : Tensor, num_classes : int=-1) -> Tensor torch._C._nn.pad(self : Tensor, pad : List[int], mode : str=constant, value : Optional[float]) -> Tensor torch._C._nn.pad_sequence(sequences : List[Tensor], batch_first : bool=False, padding_value : float=0.0, padding_side : str=right) -> Tensor torch._C._nn.reflection_pad1d(self : Tensor, padding : List[int]) -> Tensor torch._C._nn.reflection_pad1d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.reflection_pad2d(self : Tensor, padding : List[int]) -> Tensor torch._C._nn.reflection_pad2d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.reflection_pad3d(self : Tensor, padding : List[int]) -> Tensor torch._C._nn.reflection_pad3d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.relu6(self : Tensor) -> Tensor torch._C._nn.relu6_(self : Tensor) -> Tensor torch._C._nn.replication_pad1d(self : Tensor, padding : List[int])", "prev_chunk_id": "chunk_585", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_587", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "-> Tensor torch._C._nn.replication_pad1d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.replication_pad2d(self : Tensor, padding : List[int]) -> Tensor torch._C._nn.replication_pad2d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.replication_pad3d(self : Tensor, padding : List[int]) -> Tensor torch._C._nn.replication_pad3d(self : Tensor, padding : List[int], out : Tensor) -> Tensor torch._C._nn.rrelu_with_noise(self : Tensor, noise : Tensor, lower : number=0.125, upper : number=0.3333333333333333, training : bool=False, generator : Optional[Generator]) -> Tensor torch._C._nn.rrelu_with_noise(self : Tensor, noise : Tensor, lower : number=0.125, upper : number=0.3333333333333333, training : bool=False, generator : Optional[Generator], out : Tensor) -> Tensor torch._C._nn.rrelu_with_noise_(self : Tensor, noise : Tensor, lower : number=0.125, upper : number=0.3333333333333333, training : bool=False, generator : Optional[Generator]) -> Tensor torch._C._nn.scaled_dot_product_attention(query : Tensor, key : Tensor, value : Tensor, attn_mask : Optional[Tensor], dropout_p : float=0.0, is_causal : bool=False, scale : Optional[float], enable_gqa : bool=False) -> Tensor torch._C._nn.silu(self : Tensor) -> Tensor torch._C._nn.silu(self : Tensor, out : Tensor) -> Tensor torch._C._nn.silu_(self : Tensor) -> Tensor torch._C._nn.slow_conv3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0]) -> Tensor torch._C._nn.slow_conv3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], out : Tensor) -> Tensor torch._C._nn.slow_conv_dilated2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], dilation : List[int]=[1, 1]) -> Tensor torch._C._nn.slow_conv_dilated2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], dilation : List[int]=[1, 1], out : Tensor) -> Tensor torch._C._nn.slow_conv_dilated3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1]) -> Tensor torch._C._nn.slow_conv_dilated3d(self : Tensor, weight : Tensor, kernel_size", "prev_chunk_id": "chunk_586", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_588", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], out : Tensor) -> Tensor torch._C._nn.slow_conv_transpose2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], output_padding : List[int]=[0, 0], dilation : List[int]=[1, 1]) -> Tensor torch._C._nn.slow_conv_transpose2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0], output_padding : List[int]=[0, 0], dilation : List[int]=[1, 1], out : Tensor) -> Tensor torch._C._nn.slow_conv_transpose3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], output_padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1]) -> Tensor torch._C._nn.slow_conv_transpose3d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1, 1], padding : List[int]=[0, 0, 0], output_padding : List[int]=[0, 0, 0], dilation : List[int]=[1, 1, 1], out : Tensor) -> Tensor torch._C._nn.smooth_l1_loss(self : Tensor, target : Tensor, reduction : int=1, beta : float=1.0) -> Tensor torch._C._nn.smooth_l1_loss(self : Tensor, target : Tensor, reduction : int=1, beta : float=1.0, out : Tensor) -> Tensor torch._C._nn.soft_margin_loss(self : Tensor, target : Tensor, reduction : int=1) -> Tensor torch._C._nn.soft_margin_loss(self : Tensor, target : Tensor, reduction : int=1, out : Tensor) -> Tensor torch._C._nn.softplus(self : Tensor, beta : number=1, threshold : number=20) -> Tensor torch._C._nn.softplus(self : Tensor, beta : number=1, threshold : number=20, out : Tensor) -> Tensor torch._C._nn.softshrink(self : Tensor, lambd : number=0.5) -> Tensor torch._C._nn.softshrink(self : Tensor, lambd : number=0.5, out : Tensor) -> Tensor torch._C._nn.thnn_conv2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0, 0]) -> Tensor torch._C._nn.thnn_conv2d(self : Tensor, weight : Tensor, kernel_size : List[int], bias : Optional[Tensor], stride : List[int]=[1, 1], padding : List[int]=[0,", "prev_chunk_id": "chunk_587", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_589", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "0], out : Tensor) -> Tensor torch._C._nn.unflatten_dense_tensors(flat : Tensor, tensors : List[Tensor]) -> List[Tensor] torch._C._nn.upsample_bicubic2d(self : Tensor, output_size : List[int], align_corners : bool, scales_h : Optional[float], scales_w : Optional[float]) -> Tensor torch._C._nn.upsample_bicubic2d(input : Tensor, output_size : Optional[List[int]], align_corners : bool, scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_bicubic2d(self : Tensor, output_size : List[int], align_corners : bool, scales_h : Optional[float], scales_w : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_bilinear2d(self : Tensor, output_size : List[int], align_corners : bool, scales_h : Optional[float], scales_w : Optional[float]) -> Tensor torch._C._nn.upsample_bilinear2d(input : Tensor, output_size : Optional[List[int]], align_corners : bool, scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_bilinear2d(self : Tensor, output_size : List[int], align_corners : bool, scales_h : Optional[float], scales_w : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_bilinear2d(input : Tensor, output_size : Optional[List[int]], align_corners : bool, scale_factors : Optional[List[float]], out : Tensor) -> Tensor torch._C._nn.upsample_linear1d(self : Tensor, output_size : List[int], align_corners : bool, scales : Optional[float]) -> Tensor torch._C._nn.upsample_linear1d(input : Tensor, output_size : Optional[List[int]], align_corners : bool, scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_linear1d(self : Tensor, output_size : List[int], align_corners : bool, scales : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_nearest1d(self : Tensor, output_size : List[int], scales : Optional[float]) -> Tensor torch._C._nn.upsample_nearest1d(input : Tensor, output_size : Optional[List[int]], scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_nearest1d(self : Tensor, output_size : List[int], scales : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_nearest2d(self : Tensor, output_size : List[int], scales_h : Optional[float], scales_w : Optional[float]) -> Tensor torch._C._nn.upsample_nearest2d(input : Tensor, output_size : Optional[List[int]], scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_nearest2d(self : Tensor, output_size : List[int], scales_h : Optional[float], scales_w : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_nearest2d(input : Tensor, output_size : Optional[List[int]], scale_factors : Optional[List[float]], out : Tensor) -> Tensor torch._C._nn.upsample_nearest3d(self : Tensor, output_size : List[int], scales_d : Optional[float], scales_h : Optional[float], scales_w : Optional[float]) -> Tensor torch._C._nn.upsample_nearest3d(input : Tensor, output_size : Optional[List[int]], scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_nearest3d(self :", "prev_chunk_id": "chunk_588", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_590", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor, output_size : List[int], scales_d : Optional[float], scales_h : Optional[float], scales_w : Optional[float], out : Tensor) -> Tensor torch._C._nn.upsample_trilinear3d(self : Tensor, output_size : List[int], align_corners : bool, scales_d : Optional[float], scales_h : Optional[float], scales_w : Optional[float]) -> Tensor torch._C._nn.upsample_trilinear3d(input : Tensor, output_size : Optional[List[int]], align_corners : bool, scale_factors : Optional[List[float]]) -> Tensor torch._C._nn.upsample_trilinear3d(self : Tensor, output_size : List[int], align_corners : bool, scales_d : Optional[float], scales_h : Optional[float], scales_w : Optional[float], out : Tensor) -> Tensor torch._C._fft.fft_fft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_fft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_fft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_fft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_fftfreq(n : int, d : float=1.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch._C._fft.fft_fftfreq(n : int, d : float=1.0, out : Tensor) -> Tensor torch._C._fft.fft_fftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_fftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_fftshift(self : Tensor, dim : Optional[List[int]]) -> Tensor torch._C._fft.fft_hfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_hfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_hfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_hfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_hfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_hfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out", "prev_chunk_id": "chunk_589", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_591", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor) -> Tensor torch._C._fft.fft_ifft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_ifft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_ifft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_ifft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_ifftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_ifftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_ifftshift(self : Tensor, dim : Optional[List[int]]) -> Tensor torch._C._fft.fft_ihfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_ihfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_ihfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_ihfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_ihfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_ihfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_irfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_irfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_irfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_irfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_irfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_irfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_rfft(self : Tensor,", "prev_chunk_id": "chunk_590", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_592", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "n : Optional[int], dim : int=-1, norm : Optional[str]) -> Tensor torch._C._fft.fft_rfft(self : Tensor, n : Optional[int], dim : int=-1, norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_rfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str]) -> Tensor torch._C._fft.fft_rfft2(self : Tensor, s : Optional[List[int]], dim : List[int]=[-2, -1], norm : Optional[str], out : Tensor) -> Tensor torch._C._fft.fft_rfftfreq(n : int, d : float=1.0, dtype : Optional[int], layout : Optional[int], device : Optional[Device], pin_memory : Optional[bool]) -> Tensor torch._C._fft.fft_rfftfreq(n : int, d : float=1.0, out : Tensor) -> Tensor torch._C._fft.fft_rfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str]) -> Tensor torch._C._fft.fft_rfftn(self : Tensor, s : Optional[List[int]], dim : Optional[List[int]], norm : Optional[str], out : Tensor) -> Tensor torch._C._linalg.linalg_cholesky(self : Tensor, upper : bool=False) -> Tensor torch._C._linalg.linalg_cholesky(self : Tensor, upper : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_cholesky_ex(self : Tensor, upper : bool=False, check_errors : bool=False) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_cholesky_ex(self : Tensor, upper : bool=False, check_errors : bool=False, L : Tensor, info : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_cond(self : Tensor, p : Optional[number]) -> Tensor torch._C._linalg.linalg_cond(self : Tensor, p : str) -> Tensor torch._C._linalg.linalg_cond(self : Tensor, p : Optional[number], out : Tensor) -> Tensor torch._C._linalg.linalg_cond(self : Tensor, p : str, out : Tensor) -> Tensor torch._C._linalg.linalg_cross(self : Tensor, other : Tensor, dim : int=-1) -> Tensor torch._C._linalg.linalg_cross(self : Tensor, other : Tensor, dim : int=-1, out : Tensor) -> Tensor torch._C._linalg.linalg_det(A : Tensor) -> Tensor torch._C._linalg.linalg_det(A : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_diagonal(A : Tensor, offset : int=0, dim1 : int=-2, dim2 : int=-1) -> Tensor torch._C._linalg.linalg_eig(self : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_eig(self : Tensor, eigenvalues : Tensor, eigenvectors : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_eigh(self : Tensor, UPLO : str=L) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_eigh(self : Tensor, UPLO : str=L, eigvals : Tensor, eigvecs :", "prev_chunk_id": "chunk_591", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_593", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_eigvals(self : Tensor) -> Tensor torch._C._linalg.linalg_eigvals(self : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_eigvalsh(self : Tensor, UPLO : str=L) -> Tensor torch._C._linalg.linalg_eigvalsh(self : Tensor, UPLO : str=L, out : Tensor) -> Tensor torch._C._linalg.linalg_householder_product(input : Tensor, tau : Tensor) -> Tensor torch._C._linalg.linalg_householder_product(input : Tensor, tau : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_inv(A : Tensor) -> Tensor torch._C._linalg.linalg_inv(A : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_inv_ex(A : Tensor, check_errors : bool=False) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_inv_ex(A : Tensor, check_errors : bool=False, inverse : Tensor, info : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_ldl_factor(self : Tensor, hermitian : bool=False) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_ldl_factor(self : Tensor, hermitian : bool=False, LD : Tensor, pivots : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_ldl_factor_ex(self : Tensor, hermitian : bool=False, check_errors : bool=False) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_ldl_factor_ex(self : Tensor, hermitian : bool=False, check_errors : bool=False, LD : Tensor, pivots : Tensor, info : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_ldl_solve(LD : Tensor, pivots : Tensor, B : Tensor, hermitian : bool=False) -> Tensor torch._C._linalg.linalg_ldl_solve(LD : Tensor, pivots : Tensor, B : Tensor, hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_lstsq(self : Tensor, b : Tensor, rcond : Optional[float], driver : Optional[str]) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch._C._linalg.linalg_lstsq(self : Tensor, b : Tensor, rcond : Optional[float], driver : Optional[str], solution : Tensor, residuals : Tensor, rank : Tensor, singular_values : Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor] torch._C._linalg.linalg_lu(A : Tensor, pivot : bool=True) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_lu(A : Tensor, pivot : bool=True, P : Tensor, L : Tensor, U : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_lu_factor(A : Tensor, pivot : bool=True) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_lu_factor(A : Tensor, pivot : bool=True, LU : Tensor, pivots : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_lu_factor_ex(A : Tensor, pivot : bool=True, check_errors : bool=False) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_lu_factor_ex(A : Tensor, pivot : bool=True,", "prev_chunk_id": "chunk_592", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_594", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "check_errors : bool=False, LU : Tensor, pivots : Tensor, info : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_lu_solve(LU : Tensor, pivots : Tensor, B : Tensor, left : bool=True, adjoint : bool=False) -> Tensor torch._C._linalg.linalg_lu_solve(LU : Tensor, pivots : Tensor, B : Tensor, left : bool=True, adjoint : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_matmul(self : Tensor, other : Tensor) -> Tensor torch._C._linalg.linalg_matmul(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_exp(self : Tensor) -> Tensor torch._C._linalg.linalg_matrix_exp(self : Tensor, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_norm(self : Tensor, ord : number, dim : List[int]=[-2, -1], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch._C._linalg.linalg_matrix_norm(self : Tensor, ord : str=fro, dim : List[int]=[-2, -1], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch._C._linalg.linalg_matrix_norm(self : Tensor, ord : number, dim : List[int]=[-2, -1], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_norm(self : Tensor, ord : str=fro, dim : List[int]=[-2, -1], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_power(self : Tensor, n : int) -> Tensor torch._C._linalg.linalg_matrix_power(self : Tensor, n : int, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_rank(self : Tensor, tol : float, hermitian : bool=False) -> Tensor torch._C._linalg.linalg_matrix_rank(input : Tensor, tol : Tensor, hermitian : bool=False) -> Tensor torch._C._linalg.linalg_matrix_rank(input : Tensor, atol : Optional[Tensor], rtol : Optional[Tensor], hermitian : bool=False) -> Tensor torch._C._linalg.linalg_matrix_rank(self : Tensor, atol : Optional[float], rtol : Optional[float], hermitian : bool=False) -> Tensor torch._C._linalg.linalg_matrix_rank(input : Tensor, atol : Optional[Tensor], rtol : Optional[Tensor], hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_rank(self : Tensor, atol : Optional[float], rtol : Optional[float], hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_rank(self : Tensor, tol : float, hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_matrix_rank(input : Tensor, tol : Tensor, hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_multi_dot(tensors : List[Tensor]) -> Tensor torch._C._linalg.linalg_multi_dot(tensors", "prev_chunk_id": "chunk_593", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_595", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": List[Tensor], out : Tensor) -> Tensor torch._C._linalg.linalg_norm(self : Tensor, ord : Optional[number], dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch._C._linalg.linalg_norm(self : Tensor, ord : str, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch._C._linalg.linalg_norm(self : Tensor, ord : Optional[number], dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch._C._linalg.linalg_norm(self : Tensor, ord : str, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, rcond : float, hermitian : bool=False) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, atol : Optional[float], rtol : Optional[float], hermitian : bool=False) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, atol : Optional[Tensor], rtol : Optional[Tensor], hermitian : bool=False) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, atol : Optional[Tensor], rtol : Optional[Tensor], hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, atol : Optional[float], rtol : Optional[float], hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, rcond : float, hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, rcond : Tensor, hermitian : bool=False) -> Tensor torch._C._linalg.linalg_pinv(self : Tensor, rcond : Tensor, hermitian : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_qr(A : Tensor, mode : str=reduced) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_qr(A : Tensor, mode : str=reduced, Q : Tensor, R : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_slogdet(A : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_slogdet(A : Tensor, sign : Tensor, logabsdet : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_solve(A : Tensor, B : Tensor, left : bool=True) -> Tensor torch._C._linalg.linalg_solve(A : Tensor, B : Tensor, left : bool=True, out : Tensor) -> Tensor torch._C._linalg.linalg_solve_ex(A : Tensor, B : Tensor, left : bool=True, check_errors : bool=False) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_solve_ex(A : Tensor, B : Tensor, left : bool=True, check_errors : bool=False, result : Tensor, info : Tensor) -> Tuple[Tensor, Tensor] torch._C._linalg.linalg_solve_triangular(self : Tensor, B", "prev_chunk_id": "chunk_594", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_596", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, upper : bool, left : bool=True, unitriangular : bool=False) -> Tensor torch._C._linalg.linalg_solve_triangular(self : Tensor, B : Tensor, upper : bool, left : bool=True, unitriangular : bool=False, out : Tensor) -> Tensor torch._C._linalg.linalg_svd(A : Tensor, full_matrices : bool=True, driver : Optional[str]) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_svd(A : Tensor, full_matrices : bool=True, driver : Optional[str], U : Tensor, S : Tensor, Vh : Tensor) -> Tuple[Tensor, Tensor, Tensor] torch._C._linalg.linalg_svdvals(A : Tensor, driver : Optional[str]) -> Tensor torch._C._linalg.linalg_svdvals(A : Tensor, driver : Optional[str], out : Tensor) -> Tensor torch._C._linalg.linalg_tensorinv(self : Tensor, ind : int=2) -> Tensor torch._C._linalg.linalg_tensorinv(self : Tensor, ind : int=2, out : Tensor) -> Tensor torch._C._linalg.linalg_tensorsolve(self : Tensor, other : Tensor, dims : Optional[List[int]]) -> Tensor torch._C._linalg.linalg_tensorsolve(self : Tensor, other : Tensor, dims : Optional[List[int]], out : Tensor) -> Tensor torch._C._linalg.linalg_vander(x : Tensor, N : Optional[int]) -> Tensor torch._C._linalg.linalg_vecdot(x : Tensor, y : Tensor, dim : int=-1) -> Tensor torch._C._linalg.linalg_vecdot(x : Tensor, y : Tensor, dim : int=-1, out : Tensor) -> Tensor torch._C._linalg.linalg_vector_norm(self : Tensor, ord : number=2, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int]) -> Tensor torch._C._linalg.linalg_vector_norm(self : Tensor, ord : number=2, dim : Optional[List[int]], keepdim : bool=False, dtype : Optional[int], out : Tensor) -> Tensor torch._C._nested.nested_to_padded_tensor(self : Tensor, padding : float, output_size : Optional[List[int]]) -> Tensor torch._C._sparse.sparse_sampled_addmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1, out : Tensor) -> Tensor torch._C._sparse.sparse_sampled_addmm(self : Tensor, mat1 : Tensor, mat2 : Tensor, beta : number=1, alpha : number=1) -> Tensor torch._C._special.special_airy_ai(x : Tensor) -> Tensor torch._C._special.special_airy_ai(x : Tensor, out : Tensor) -> Tensor torch._C._special.special_bessel_j0(self : Tensor) -> Tensor torch._C._special.special_bessel_j0(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_bessel_j1(self : Tensor) -> Tensor torch._C._special.special_bessel_j1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_bessel_y0(self : Tensor) -> Tensor torch._C._special.special_bessel_y0(self : Tensor, out : Tensor) ->", "prev_chunk_id": "chunk_595", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_597", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor torch._C._special.special_bessel_y1(self : Tensor) -> Tensor torch._C._special.special_bessel_y1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : number, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : Tensor, n : number) -> Tensor torch._C._special.special_chebyshev_polynomial_t(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : number, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : Tensor, n : number) -> Tensor torch._C._special.special_chebyshev_polynomial_u(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : number, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : Tensor, n : number) -> Tensor torch._C._special.special_chebyshev_polynomial_v(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : number, n : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : Tensor, n : number) -> Tensor torch._C._special.special_chebyshev_polynomial_w(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_digamma(self : Tensor) -> Tensor torch._C._special.special_digamma(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_entr(self : Tensor) -> Tensor torch._C._special.special_entr(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_erf(self : Tensor) -> Tensor torch._C._special.special_erf(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_erfc(self : Tensor) -> Tensor torch._C._special.special_erfc(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_erfcx(self : Tensor) -> Tensor torch._C._special.special_erfcx(self", "prev_chunk_id": "chunk_596", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_598", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": ": Tensor, out : Tensor) -> Tensor torch._C._special.special_erfinv(self : Tensor) -> Tensor torch._C._special.special_erfinv(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_exp2(self : Tensor) -> Tensor torch._C._special.special_exp2(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_expit(self : Tensor) -> Tensor torch._C._special.special_expit(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_expm1(self : Tensor) -> Tensor torch._C._special.special_expm1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_gammainc(self : Tensor, other : Tensor) -> Tensor torch._C._special.special_gammainc(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_gammaincc(self : Tensor, other : Tensor) -> Tensor torch._C._special.special_gammaincc(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_gammaln(self : Tensor) -> Tensor torch._C._special.special_gammaln(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_h(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_h(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_h(x : number, n : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_h(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_h(x : Tensor, n : number) -> Tensor torch._C._special.special_hermite_polynomial_h(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_he(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_he(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_he(x : number, n : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_he(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_hermite_polynomial_he(x : Tensor, n : number) -> Tensor torch._C._special.special_hermite_polynomial_he(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_i0(self : Tensor) -> Tensor torch._C._special.special_i0(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_i0e(self : Tensor) -> Tensor torch._C._special.special_i0e(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_i1(self : Tensor) -> Tensor torch._C._special.special_i1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_i1e(self : Tensor) -> Tensor torch._C._special.special_i1e(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : number, n :", "prev_chunk_id": "chunk_597", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_599", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "Tensor) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : Tensor, n : number) -> Tensor torch._C._special.special_laguerre_polynomial_l(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_legendre_polynomial_p(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_legendre_polynomial_p(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_legendre_polynomial_p(x : number, n : Tensor) -> Tensor torch._C._special.special_legendre_polynomial_p(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_legendre_polynomial_p(x : Tensor, n : number) -> Tensor torch._C._special.special_legendre_polynomial_p(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_log1p(self : Tensor) -> Tensor torch._C._special.special_log1p(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_log_ndtr(self : Tensor) -> Tensor torch._C._special.special_log_ndtr(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_log_softmax(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch._C._special.special_logit(self : Tensor, eps : Optional[float]) -> Tensor torch._C._special.special_logit(self : Tensor, eps : Optional[float], out : Tensor) -> Tensor torch._C._special.special_logsumexp(self : Tensor, dim : List[int], keepdim : bool=False) -> Tensor torch._C._special.special_logsumexp(self : Tensor, dim : List[int], keepdim : bool=False, out : Tensor) -> Tensor torch._C._special.special_modified_bessel_i0(self : Tensor) -> Tensor torch._C._special.special_modified_bessel_i0(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_modified_bessel_i1(self : Tensor) -> Tensor torch._C._special.special_modified_bessel_i1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_modified_bessel_k0(self : Tensor) -> Tensor torch._C._special.special_modified_bessel_k0(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_modified_bessel_k1(self : Tensor) -> Tensor torch._C._special.special_modified_bessel_k1(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_multigammaln(self : Tensor, p : int) -> Tensor torch._C._special.special_multigammaln(self : Tensor, p : int, out : Tensor) -> Tensor torch._C._special.special_ndtr(self : Tensor) -> Tensor torch._C._special.special_ndtr(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_ndtri(self : Tensor) -> Tensor torch._C._special.special_ndtri(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_polygamma(n : int, self : Tensor) -> Tensor torch._C._special.special_polygamma(n : int, self : Tensor, out : Tensor) -> Tensor torch._C._special.special_psi(self : Tensor) -> Tensor torch._C._special.special_psi(self : Tensor, out : Tensor) -> Tensor", "prev_chunk_id": "chunk_598", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_600", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "torch._C._special.special_round(self : Tensor, decimals : int=0) -> Tensor torch._C._special.special_round(self : Tensor, decimals : int=0, out : Tensor) -> Tensor torch._C._special.special_scaled_modified_bessel_k0(x : Tensor) -> Tensor torch._C._special.special_scaled_modified_bessel_k0(x : Tensor, out : Tensor) -> Tensor torch._C._special.special_scaled_modified_bessel_k1(x : Tensor) -> Tensor torch._C._special.special_scaled_modified_bessel_k1(x : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : number, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : Tensor, n : number) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_t(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : number, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : Tensor, n : number) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_u(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : number, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : Tensor, n : number) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_v(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : Tensor, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : Tensor, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : number, n : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : number, n : Tensor, out : Tensor) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : Tensor, n : number) -> Tensor torch._C._special.special_shifted_chebyshev_polynomial_w(x : Tensor, n : number, out : Tensor) -> Tensor torch._C._special.special_sinc(self : Tensor) -> Tensor torch._C._special.special_sinc(self : Tensor, out : Tensor) -> Tensor torch._C._special.special_softmax(self : Tensor, dim : int, dtype : Optional[int]) -> Tensor torch._C._special.special_spherical_bessel_j0(x : Tensor)", "prev_chunk_id": "chunk_599", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_601", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Supported PyTorch Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Supported PyTorch Functions#", "content": "-> Tensor torch._C._special.special_spherical_bessel_j0(x : Tensor, out : Tensor) -> Tensor torch._C._special.special_xlog1py(self : Tensor, other : Tensor) -> Tensor torch._C._special.special_xlog1py(self : Tensor, other : number) -> Tensor torch._C._special.special_xlog1py(self : number, other : Tensor) -> Tensor torch._C._special.special_xlog1py(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_xlog1py(self : number, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_xlog1py(self : Tensor, other : number, out : Tensor) -> Tensor torch._C._special.special_xlogy(self : Tensor, other : Tensor) -> Tensor torch._C._special.special_xlogy(self : Tensor, other : number) -> Tensor torch._C._special.special_xlogy(self : number, other : Tensor) -> Tensor torch._C._special.special_xlogy(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_xlogy(self : number, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_xlogy(self : Tensor, other : number, out : Tensor) -> Tensor torch._C._special.special_zeta(self : Tensor, other : Tensor) -> Tensor torch._C._special.special_zeta(self : Tensor, other : number) -> Tensor torch._C._special.special_zeta(self : number, other : Tensor) -> Tensor torch._C._special.special_zeta(self : Tensor, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_zeta(self : number, other : Tensor, out : Tensor) -> Tensor torch._C._special.special_zeta(self : Tensor, other : number, out : Tensor) -> Tensor", "prev_chunk_id": "chunk_600", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_602", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtin Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtin Functions#", "content": "TorchScript Builtin Functions# collections.OrderedDict() -> Dict[str, Tensor] collections.OrderedDict(inputs : List[Tuple[str, tVal]]) -> Dict[str, tVal] collections.OrderedDict(self : Dict[str, t]) -> Dict[str, t] collections.OrderedDict(inputs : List[Tuple[int, tVal]]) -> Dict[int, tVal] collections.OrderedDict(self : Dict[int, t]) -> Dict[int, t] collections.OrderedDict(inputs : List[Tuple[bool, tVal]]) -> Dict[bool, tVal] collections.OrderedDict(self : Dict[bool, t]) -> Dict[bool, t] collections.OrderedDict(inputs : List[Tuple[float, tVal]]) -> Dict[float, tVal] collections.OrderedDict(self : Dict[float, t]) -> Dict[float, t] collections.OrderedDict(inputs : List[Tuple[complex, tVal]]) -> Dict[complex, tVal] collections.OrderedDict(self : Dict[complex, t]) -> Dict[complex, t] collections.OrderedDict(inputs : List[Tuple[Tensor, tVal]]) -> Dict[Tensor, tVal] collections.OrderedDict(self : Dict[Tensor, t]) -> Dict[Tensor, t] builtins.dict() -> Dict[str, Tensor] builtins.dict(inputs : List[Tuple[str, tVal]]) -> Dict[str, tVal] builtins.dict(self : Dict[str, t]) -> Dict[str, t] builtins.dict(inputs : List[Tuple[int, tVal]]) -> Dict[int, tVal] builtins.dict(self : Dict[int, t]) -> Dict[int, t] builtins.dict(inputs : List[Tuple[bool, tVal]]) -> Dict[bool, tVal] builtins.dict(self : Dict[bool, t]) -> Dict[bool, t] builtins.dict(inputs : List[Tuple[float, tVal]]) -> Dict[float, tVal] builtins.dict(self : Dict[float, t]) -> Dict[float, t] builtins.dict(inputs : List[Tuple[complex, tVal]]) -> Dict[complex, tVal] builtins.dict(self : Dict[complex, t]) -> Dict[complex, t] builtins.dict(inputs : List[Tuple[Tensor, tVal]]) -> Dict[Tensor, tVal] builtins.dict(self : Dict[Tensor, t]) -> Dict[Tensor, t] torch.backends.cudnn.is_acceptable(self : Tensor) -> bool cmath.isnan(self : Tensor) -> Tensor cmath.isnan(self : Tensor, out : Tensor) -> Tensor cmath.isnan(a : float) -> bool cmath.isnan(a : complex) -> bool cmath.isfinite(self : Tensor) -> Tensor cmath.isfinite(a : float) -> bool cmath.isfinite(a : complex) -> bool cmath.isinf(self : Tensor) -> Tensor cmath.isinf(self : Tensor, out : Tensor) -> Tensor cmath.isinf(a : float) -> bool cmath.isinf(a : complex) -> bool cmath.phase(self : Tensor) -> Tensor cmath.phase(self : Tensor, out : Tensor) -> Tensor cmath.phase(a : int) -> float cmath.phase(a : float) -> float cmath.phase(a : complex) -> float cmath.phase(a : number) -> number cmath.rect(abs : Tensor, angle : Tensor) -> Tensor cmath.rect(abs : Tensor, angle : Tensor, out : Tensor) -> Tensor cmath.rect(a : int,", "prev_chunk_id": "chunk_601", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_603", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtin Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtin Functions#", "content": "b : int) -> complex cmath.rect(a : float, b : float) -> complex cmath.rect(a : int, b : float) -> complex cmath.rect(a : float, b : int) -> complex cmath.rect(a : number, b : number) -> number cmath.log(self : Tensor) -> Tensor cmath.log(self : Tensor, out : Tensor) -> Tensor cmath.log(a : int) -> float cmath.log(a : float) -> float cmath.log(a : complex) -> complex cmath.log(a : number) -> number cmath.log(a : int, b : int) -> float cmath.log(a : float, b : float) -> float cmath.log(a : complex, b : complex) -> complex cmath.log(a : int, b : float) -> float cmath.log(a : float, b : int) -> float cmath.log(a : int, b : complex) -> complex cmath.log(a : complex, b : int) -> complex cmath.log(a : float, b : complex) -> complex cmath.log(a : complex, b : float) -> complex cmath.log(a : number, b : number) -> float cmath.log10(self : Tensor) -> Tensor cmath.log10(self : Tensor, out : Tensor) -> Tensor cmath.log10(a : int) -> float cmath.log10(a : float) -> float cmath.log10(a : complex) -> complex cmath.log10(a : number) -> number cmath.sqrt(self : Tensor) -> Tensor cmath.sqrt(self : Tensor, out : Tensor) -> Tensor cmath.sqrt(a : int) -> float cmath.sqrt(a : float) -> float cmath.sqrt(a : complex) -> complex cmath.sqrt(a : number) -> number cmath.exp(self : Tensor) -> Tensor cmath.exp(self : Tensor, out : Tensor) -> Tensor cmath.exp(a : int) -> float cmath.exp(a : float) -> float cmath.exp(a : complex) -> complex cmath.exp(a : number) -> number cmath.sin(self : Tensor) -> Tensor cmath.sin(self : Tensor, out : Tensor) -> Tensor cmath.sin(a : int) -> float cmath.sin(a : float) -> float cmath.sin(a : complex) -> complex cmath.sin(a : number) -> number cmath.tan(self : Tensor) -> Tensor cmath.tan(self : Tensor, out : Tensor) -> Tensor cmath.tan(a : int) -> float", "prev_chunk_id": "chunk_602", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_604", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtin Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtin Functions#", "content": "cmath.tan(a : float) -> float cmath.tan(a : complex) -> complex cmath.tan(a : number) -> number cmath.cos(self : Tensor) -> Tensor cmath.cos(self : Tensor, out : Tensor) -> Tensor cmath.cos(a : int) -> float cmath.cos(a : float) -> float cmath.cos(a : complex) -> complex cmath.cos(a : number) -> number cmath.asin(self : Tensor) -> Tensor cmath.asin(self : Tensor, out : Tensor) -> Tensor cmath.asin(a : int) -> float cmath.asin(a : float) -> float cmath.asin(a : complex) -> complex cmath.asin(a : number) -> number cmath.acos(self : Tensor) -> Tensor cmath.acos(self : Tensor, out : Tensor) -> Tensor cmath.acos(a : int) -> float cmath.acos(a : float) -> float cmath.acos(a : complex) -> complex cmath.acos(a : number) -> number cmath.atan(self : Tensor) -> Tensor cmath.atan(self : Tensor, out : Tensor) -> Tensor cmath.atan(a : int) -> float cmath.atan(a : float) -> float cmath.atan(a : complex) -> complex cmath.atan(a : number) -> number cmath.sinh(self : Tensor) -> Tensor cmath.sinh(self : Tensor, out : Tensor) -> Tensor cmath.sinh(a : int) -> float cmath.sinh(a : float) -> float cmath.sinh(a : complex) -> complex cmath.sinh(a : number) -> number cmath.cosh(self : Tensor) -> Tensor cmath.cosh(self : Tensor, out : Tensor) -> Tensor cmath.cosh(a : int) -> float cmath.cosh(a : float) -> float cmath.cosh(a : complex) -> complex cmath.cosh(a : number) -> number cmath.tanh(self : Tensor) -> Tensor cmath.tanh(self : Tensor, out : Tensor) -> Tensor cmath.tanh(a : int) -> float cmath.tanh(a : float) -> float cmath.tanh(a : complex) -> complex cmath.tanh(a : number) -> number cmath.asinh(self : Tensor) -> Tensor cmath.asinh(self : Tensor, out : Tensor) -> Tensor cmath.asinh(a : int) -> float cmath.asinh(a : float) -> float cmath.asinh(a : complex) -> complex cmath.asinh(a : number) -> number cmath.acosh(self : Tensor) -> Tensor cmath.acosh(self : Tensor, out : Tensor) -> Tensor cmath.acosh(a : int) -> float cmath.acosh(a : float)", "prev_chunk_id": "chunk_603", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_605", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtin Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtin Functions#", "content": "-> float cmath.acosh(a : complex) -> complex cmath.acosh(a : number) -> number cmath.atanh(self : Tensor) -> Tensor cmath.atanh(self : Tensor, out : Tensor) -> Tensor cmath.atanh(a : int) -> float cmath.atanh(a : float) -> float cmath.atanh(a : complex) -> complex cmath.atanh(a : number) -> number torch.autograd.grad(outputs : List[Tensor], inputs : List[Tensor], grad_outputs : Optional[List[Optional[Tensor]]], retain_graph : Optional[bool], create_graph : bool=False, allow_unused : bool=False) -> List[Optional[Tensor]] torch.autograd.backward(self : Tensor, gradient : Optional[Tensor], retain_graph : Optional[bool], create_graph : bool=False) -> Tuple[] torch.autograd.backward(tensors : List[Tensor], grad_tensors : Optional[List[Optional[Tensor]]], retain_graph : Optional[bool], create_graph : bool=False) -> Tuple[] torch.Size(sizes : List[int]) -> List[int] torch.functional.align_tensors(tensors : List[Tensor]) -> List[Tensor] torch.functional.atleast_1d(self : Tensor) -> Tensor torch.functional.atleast_1d(tensors : List[Tensor]) -> List[Tensor] torch.functional.atleast_2d(self : Tensor) -> Tensor torch.functional.atleast_2d(tensors : List[Tensor]) -> List[Tensor] torch.functional.atleast_3d(self : Tensor) -> Tensor torch.functional.atleast_3d(tensors : List[Tensor]) -> List[Tensor] torch.functional.block_diag(tensors : List[Tensor]) -> Tensor torch.functional.block_diag(tensors : List[Tensor], out : Tensor) -> Tensor torch.functional.broadcast_tensors(tensors : List[Tensor]) -> List[Tensor] torch.functional.cartesian_prod(tensors : List[Tensor]) -> Tensor torch.functional.chain_matmul(matrices : List[Tensor]) -> Tensor torch.functional.chain_matmul(matrices : List[Tensor], out : Tensor) -> Tensor torch.device(a : str) -> Device torch.device(type : str, index : int) -> Device torch.functional.einsum(equation : str, tensors : List[Tensor], path : Optional[List[int]]) -> Tensor torch.functional.einsum(a : Tensor) -> Tensor torch.get_autocast_dtype(device_type : str) -> int torch.random.initial_seed(self : Generator) -> int torch.is_autocast_cpu_enabled() -> bool torch.is_autocast_enabled() -> bool torch.is_grad_enabled() -> bool torch.random.manual_seed(seed : int) -> Tuple[] torch.random.manual_seed(self : Generator, seed : int) -> Generator torch.functional.meshgrid(tensors : List[Tensor]) -> List[Tensor] torch.functional.meshgrid(tensors : List[Tensor], indexing : str) -> List[Tensor] torch.qscheme(self : Tensor) -> QScheme torch.serialization.save(item : t, filename : str) -> Tuple[] torch.random.seed(self : Generator) -> int torch.autograd.grad_mode.set_grad_enabled(val : bool) -> Tuple[] torch.functional.split(self : Tensor, split_size : int, dim : int=0) -> List[Tensor] torch.functional.split(self : Tensor, split_size : List[int], dim : int=0) -> List[Tensor] torch.functional.split(self : str, separator : Optional[str], max : int=-1) -> List[str] torch.functional.split(self", "prev_chunk_id": "chunk_604", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_606", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "TorchScript Builtin Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Builtin Functions#", "content": ": Tensor, split_sizes : List[int], dim : int=0) -> List[Tensor] torch.wait(self : Future[t]) -> t", "prev_chunk_id": "chunk_605", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_607", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Python Built-in Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Python Built-in Functions#", "content": "Python Built-in Functions# The functions in the following table are supported but do not have a static schema The following functions will use the corresponding magic method on TorchScript Classes These built-in functions use the schema float(a : Tensor) -> float float(a : number) -> float float(a : int) -> float float(a : bool) -> float float(a : str) -> float complex(a : number) -> complex complex(a : Tensor, b : Tensor) -> complex complex(x : int, y : bool) -> complex complex(x : bool, y : int) -> complex complex(x : float, y : bool) -> complex complex(x : bool, y : float) -> complex complex(x : float, y : int) -> complex complex(x : int, y : float) -> complex complex(x : int, y : int) -> complex complex(x : bool, y : bool) -> complex complex(x : float, y : float) -> complex complex(x : Tensor, y : float) -> complex complex(x : float, y : Tensor) -> complex complex(x : Tensor, y : int) -> complex complex(x : int, y : Tensor) -> complex complex(x : Tensor, y : bool) -> complex complex(x : bool, y : Tensor) -> complex int(a : Tensor) -> int int(a : bool) -> int int(a : float) -> int int(a : number) -> int int(a : str) -> int bool(a : Tensor) -> bool bool(a : int) -> bool bool(a : float) -> bool str(elem : t) -> str len(a : List[t]) -> int len(t : Tensor) -> int len(s : str) -> int len(self : Dict[str, t]) -> int len(self : Dict[int, t]) -> int len(self : Dict[bool, t]) -> int len(self : Dict[float, t]) -> int len(self : Dict[complex, t]) -> int len(self : Dict[Tensor, t]) -> int len(a : List[Any]) -> int hex(i : int) -> str", "prev_chunk_id": "chunk_606", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_608", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Python Built-in Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Python Built-in Functions#", "content": "oct(i : int) -> str round(self : Tensor) -> Tensor round(self : Tensor, decimals : int) -> Tensor round(self : Tensor, out : Tensor) -> Tensor round(self : Tensor, decimals : int, out : Tensor) -> Tensor round(a : int) -> float round(a : float) -> float round(a : number) -> number hash(value : t) -> int min(a : int, b : int) -> int min(a : float, b : float) -> float min(a : int, b : float) -> float min(a : float, b : int) -> float min(a : number, b : number) -> number min(l : List[int], r : List[int]) -> List[int] min(self : List[int]) -> int min(l : List[float], r : List[float]) -> List[float] min(self : List[float]) -> float min(l : List[bool], r : List[bool]) -> List[bool] min(self : List[bool]) -> bool max(a : int, b : int) -> int max(a : float, b : float) -> float max(a : int, b : float) -> float max(a : float, b : int) -> float max(a : number, b : number) -> number max(l : List[int], r : List[int]) -> List[int] max(self : List[int]) -> int max(l : List[float], r : List[float]) -> List[float] max(self : List[float]) -> float max(l : List[bool], r : List[bool]) -> List[bool] max(self : List[bool]) -> bool abs(a : int) -> int abs(a : float) -> float abs(a : complex) -> float abs(a : number) -> number abs(x : Tensor) -> Tensor all(self : Tensor) -> Tensor all(self : Tensor, dim : int, keepdim : bool=False) -> Tensor all(self : Tensor, dim : Optional[List[int]], keepdim : bool=False) -> Tensor all(self : Tensor, dim : int, keepdim : bool=False, out : Tensor) -> Tensor all(self : Tensor, dim : Optional[List[int]], keepdim : bool=False, out : Tensor) -> Tensor all(self : Tensor, out :", "prev_chunk_id": "chunk_607", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_609", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "Python Built-in Functions#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "Python Built-in Functions#", "content": "Tensor) -> Tensor all(self : Tensor, dim : str, keepdim : bool=False) -> Tensor all(self : Tensor, dim : str, keepdim : bool=False, out : Tensor) -> Tensor all(self : List[int]) -> bool all(self : List[float]) -> bool all(self : List[bool]) -> bool divmod(x : int, y : int) -> Tuple[int, int] divmod(x : float, y : float) -> Tuple[float, float] divmod(x : int, y : float) -> Tuple[float, float] divmod(x : float, y : int) -> Tuple[float, float] list(t : str) -> List[str] list(l : List[t]) -> List[t] ord(string : str) -> int chr(i : int) -> str bin(i : int) -> str sorted(input : List[int]) -> List[int] sorted(input : List[float]) -> List[float] sorted(input : List[Tensor]) -> List[Tensor] sorted(input : List[bool]) -> List[bool] sorted(input : List[str]) -> List[str] sorted(self : List[t]) -> List[t]", "prev_chunk_id": "chunk_608", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_610", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "math Module#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "math Module#", "content": "math Module# aten::ceil.int(int a) -> int aten::ceil.float(float a) -> int aten::ceil.Scalar(Scalar a) -> Scalar aten::copysign.int(int a, int b) -> float aten::copysign.float(float a, float b) -> float aten::copysign.int_float(int a, float b) -> float aten::copysign.float_int(float a, int b) -> float aten::copysign(Scalar a, Scalar b) -> float aten::erf.int(int a) -> float aten::erf.float(float a) -> float aten::erf.Scalar(Scalar a) -> Scalar aten::erfc.int(int a) -> float aten::erfc.float(float a) -> float aten::erfc.Scalar(Scalar a) -> Scalar aten::exp.int(int a) -> float aten::exp.float(float a) -> float aten::exp.complex(complex a) -> complex aten::exp.Scalar(Scalar a) -> Scalar aten::expm1.int(int a) -> float aten::expm1.float(float a) -> float aten::expm1.Scalar(Scalar a) -> Scalar aten::fabs.int(int a) -> float aten::fabs.float(float a) -> float aten::fabs.Scalar(Scalar a) -> Scalar aten::floor.int(int a) -> int aten::floor.float(float a) -> int aten::floor.Scalar(Scalar a) -> Scalar aten::gamma.int(int a) -> float aten::gamma.float(float a) -> float aten::gamma.Scalar(Scalar a) -> Scalar aten::lgamma.int(int a) -> float aten::lgamma.float(float a) -> float aten::lgamma.Scalar(Scalar a) -> Scalar aten::log.int(int a) -> float aten::log.float(float a) -> float aten::log.complex(complex a) -> complex aten::log.Scalar(Scalar a) -> Scalar aten::log.int_int(int a, int b) -> float aten::log.float_float(float a, float b) -> float aten::log.complex_complex(complex a, complex b) -> complex aten::log.int_float(int a, float b) -> float aten::log.float_int(float a, int b) -> float aten::log.int_complex(int a, complex b) -> complex aten::log.complex_int(complex a, int b) -> complex aten::log.float_complex(float a, complex b) -> complex aten::log.complex_float(complex a, float b) -> complex aten::log.Scalar_Scalar(Scalar a, Scalar b) -> float aten::log10.int(int a) -> float aten::log10.float(float a) -> float aten::log10.complex(complex a) -> complex aten::log10.Scalar(Scalar a) -> Scalar aten::log1p.int(int a) -> float aten::log1p.float(float a) -> float aten::log1p.Scalar(Scalar a) -> Scalar aten::pow.int(int a, int b) -> float aten::pow.complex(complex a, complex b) -> complex aten::pow.float(float a, float b) -> float aten::pow.int_float(int a, float b) -> float aten::pow.float_int(float a, int b) -> float aten::pow.float_complex(float a, complex b) -> complex aten::pow.complex_float(complex a, float b) -> complex aten::pow.Scalar_Scalar(Scalar a, Scalar b) -> float aten::pow.int_to_int(int a, int b)", "prev_chunk_id": "chunk_609", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_611", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "math Module#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "math Module#", "content": "-> int aten::sqrt.int(int a) -> float aten::sqrt.float(float a) -> float aten::sqrt.complex(complex a) -> complex aten::sqrt.Scalar(Scalar a) -> Scalar aten::isnan.float(float a) -> bool aten::isnan.complex(complex a) -> bool aten::asinh.int(int a) -> float aten::asinh.float(float a) -> float aten::asinh.complex(complex a) -> complex aten::asinh.Scalar(Scalar a) -> Scalar aten::atanh.int(int a) -> float aten::atanh.float(float a) -> float aten::atanh.complex(complex a) -> complex aten::atanh.Scalar(Scalar a) -> Scalar aten::cosh.int(int a) -> float aten::cosh.float(float a) -> float aten::cosh.complex(complex a) -> complex aten::cosh.Scalar(Scalar a) -> Scalar aten::sinh.int(int a) -> float aten::sinh.float(float a) -> float aten::sinh.complex(complex a) -> complex aten::sinh.Scalar(Scalar a) -> Scalar aten::tanh.int(int a) -> float aten::tanh.float(float a) -> float aten::tanh.complex(complex a) -> complex aten::tanh.Scalar(Scalar a) -> Scalar aten::acos.int(int a) -> float aten::acos.float(float a) -> float aten::acos.complex(complex a) -> complex aten::acos.Scalar(Scalar a) -> Scalar aten::asin.int(int a) -> float aten::asin.float(float a) -> float aten::asin.complex(complex a) -> complex aten::asin.Scalar(Scalar a) -> Scalar aten::atan.int(int a) -> float aten::atan.float(float a) -> float aten::atan.complex(complex a) -> complex aten::atan.Scalar(Scalar a) -> Scalar aten::atan2.int(int a, int b) -> float aten::atan2.float(float a, float b) -> float aten::atan2.int_float(int a, float b) -> float aten::atan2.float_int(float a, int b) -> float aten::atan2.Scalar_Scalar(Scalar a, Scalar b) -> float aten::cos.int(int a) -> float aten::cos.float(float a) -> float aten::cos.complex(complex a) -> complex aten::cos.Scalar(Scalar a) -> Scalar aten::sin.int(int a) -> float aten::sin.float(float a) -> float aten::sin.complex(complex a) -> complex aten::sin.Scalar(Scalar a) -> Scalar aten::tan.int(int a) -> float aten::tan.float(float a) -> float aten::tan.complex(complex a) -> complex aten::tan.Scalar(Scalar a) -> Scalar aten::asinh.int(int a) -> float aten::asinh.float(float a) -> float aten::asinh.complex(complex a) -> complex aten::asinh.Scalar(Scalar a) -> Scalar aten::atanh.int(int a) -> float aten::atanh.float(float a) -> float aten::atanh.complex(complex a) -> complex aten::atanh.Scalar(Scalar a) -> Scalar aten::acosh.int(int a) -> float aten::acosh.float(float a) -> float aten::acosh.complex(complex a) -> complex aten::acosh.Scalar(Scalar a) -> Scalar aten::fmod.int(int a, int b) -> float aten::fmod.float(float a, float b) -> float aten::fmod.int_float(int a, float b) -> float aten::fmod.float_int(float a,", "prev_chunk_id": "chunk_610", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_612", "url": "https://docs.pytorch.org/docs/stable/jit_builtin_functions.html", "title": "math Module#", "page_title": "TorchScript Builtins — PyTorch 2.8 documentation", "breadcrumbs": "math Module#", "content": "int b) -> float aten::fmod(Scalar a, Scalar b) -> float aten::modf(float a) -> (float, float) aten::factorial.int(int a) -> int aten::frexp(float a) -> (float, int) aten::isinf.float(float a) -> bool aten::isinf.complex(complex a) -> bool aten::degrees.int(int a) -> float aten::degrees.float(float a) -> float aten::degrees.Scalar(Scalar a) -> Scalar aten::radians.int(int a) -> float aten::radians.float(float a) -> float aten::radians.Scalar(Scalar a) -> Scalar aten::ldexp(float x, int i) -> float aten::gcd.int(int a, int b) -> int aten::isfinite.float(float a) -> bool aten::isfinite.complex(complex a) -> bool aten::mathremainder.int(int a, int b) -> float aten::mathremainder.float(float a, float b) -> float aten::mathremainder.int_float(int a, float b) -> float aten::mathremainder.float_int(float a, int b) -> float aten::mathremainder(Scalar a, Scalar b) -> float", "prev_chunk_id": "chunk_611", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_613", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "Migrating from functorch to torch.func#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "Migrating from functorch to torch.func#", "content": "Migrating from functorch to torch.func# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025 torch.func, previously known as “functorch”, is JAX-like composable function transforms for PyTorch. functorch started as an out-of-tree library over at the pytorch/functorch repository. Our goal has always been to upstream functorch directly into PyTorch and provide it as a core PyTorch library. As the final step of the upstream, we’ve decided to migrate from being a top level package (functorch) to being a part of PyTorch to reflect how the function transforms are integrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating import functorch and ask that users migrate to the newest APIs, which we will maintain going forward. import functorch will be kept around to maintain backwards compatibility for a couple of releases.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_614", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "function transforms#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "function transforms#", "content": "function transforms# The following APIs are a drop-in replacement for the following functorch APIs. They are fully backwards compatible. Furthermore, if you are using torch.autograd.functional APIs, please try out the torch.func equivalents instead. torch.func function transforms are more composable and more performant in many cases.", "prev_chunk_id": "chunk_613", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_615", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "NN module utilities#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "NN module utilities#", "content": "NN module utilities# We’ve changed the APIs to apply function transforms over NN modules to make them fit better into the PyTorch design philosophy. The new API is different, so please read this section carefully.", "prev_chunk_id": "chunk_614", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_616", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "functorch.make_functional#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "functorch.make_functional#", "content": "functorch.make_functional# torch.func.functional_call() is the replacement for functorch.make_functional and functorch.make_functional_with_buffers. However, it is not a drop-in replacement. If you’re in a hurry, you can use helper functions in this gist that emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers. We recommend using torch.func.functional_call() directly because it is a more explicit and flexible API. Concretely, functorch.make_functional returns a functional module and parameters. The functional module accepts parameters and inputs to the model as arguments. torch.func.functional_call() allows one to call the forward pass of an existing module using new parameters and buffers and inputs. Here’s an example of how to compute gradients of parameters of a model using functorch vs torch.func: # --------------- # using functorch # --------------- import torch import functorch inputs = torch.randn(64, 3) targets = torch.randn(64, 3) model = torch.nn.Linear(3, 3) fmodel, params = functorch.make_functional(model) def compute_loss(params, inputs, targets): prediction = fmodel(params, inputs) return torch.nn.functional.mse_loss(prediction, targets) grads = functorch.grad(compute_loss)(params, inputs, targets) # ------------------------------------ # using torch.func (as of PyTorch 2.0) # ------------------------------------ import torch inputs = torch.randn(64, 3) targets = torch.randn(64, 3) model = torch.nn.Linear(3, 3) params = dict(model.named_parameters()) def compute_loss(params, inputs, targets): prediction = torch.func.functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) grads = torch.func.grad(compute_loss)(params, inputs, targets) And here’s an example of how to compute jacobians of model parameters: # --------------- # using functorch # --------------- import torch import functorch inputs = torch.randn(64, 3) model = torch.nn.Linear(3, 3) fmodel, params = functorch.make_functional(model) jacobians = functorch.jacrev(fmodel)(params, inputs) # ------------------------------------ # using torch.func (as of PyTorch 2.0) # ------------------------------------ import torch from torch.func import jacrev, functional_call inputs = torch.randn(64, 3) model = torch.nn.Linear(3, 3) params = dict(model.named_parameters()) # jacrev computes jacobians of argnums=0 by default. # We set it to 1 to compute jacobians of params jacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,)) Note that it is important for memory consumption that you should only", "prev_chunk_id": "chunk_615", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_617", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "functorch.make_functional#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "functorch.make_functional#", "content": "carry around a single copy of your parameters. model.named_parameters() does not copy the parameters. If in your model training you update the parameters of the model in-place, then the nn.Module that is your model has the single copy of the parameters and everything is OK. However, if you want to carry your parameters around in a dictionary and update them out-of-place, then there are two copies of parameters: the one in the dictionary and the one in the model. In this case, you should change model to not hold memory by converting it to the meta device via model.to('meta').", "prev_chunk_id": "chunk_616", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_618", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "functorch.combine_state_for_ensemble#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "functorch.combine_state_for_ensemble#", "content": "functorch.combine_state_for_ensemble# Please use torch.func.stack_module_state() instead of functorch.combine_state_for_ensemble torch.func.stack_module_state() returns two dictionaries, one of stacked parameters, and one of stacked buffers, that can then be used with torch.vmap() and torch.func.functional_call() for ensembling. For example, here is an example of how to ensemble over a very simple model: import torch num_models = 5 batch_size = 64 in_features, out_features = 3, 3 models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)] data = torch.randn(batch_size, 3) # --------------- # using functorch # --------------- import functorch fmodel, params, buffers = functorch.combine_state_for_ensemble(models) output = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data) assert output.shape == (num_models, batch_size, out_features) # ------------------------------------ # using torch.func (as of PyTorch 2.0) # ------------------------------------ import copy # Construct a version of the model with no memory by putting the Tensors on # the meta device. base_model = copy.deepcopy(models[0]) base_model.to('meta') params, buffers = torch.func.stack_module_state(models) # It is possible to vmap directly over torch.func.functional_call, # but wrapping it in a function makes it clearer what is going on. def call_single_model(params, buffers, data): return torch.func.functional_call(base_model, (params, buffers), (data,)) output = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data) assert output.shape == (num_models, batch_size, out_features)", "prev_chunk_id": "chunk_617", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_619", "url": "https://docs.pytorch.org/docs/stable/func.migrating.html", "title": "functorch.compile#", "page_title": "Migrating from functorch to torch.func — PyTorch 2.8 documentation", "breadcrumbs": "functorch.compile#", "content": "functorch.compile# We are no longer supporting functorch.compile (also known as AOTAutograd) as a frontend for compilation in PyTorch; we have integrated AOTAutograd into PyTorch’s compilation story. If you are a user, please use torch.compile() instead.", "prev_chunk_id": "chunk_618", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_620", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "UX Limitations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "UX Limitations#", "content": "UX Limitations# Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025 torch.func, like JAX, has restrictions around what can be transformed. In general, JAX’s limitations are that transforms only work with pure functions: that is, functions where the output is completely determined by the input and that do not involve side effects (like mutation). We have a similar guarantee: our transforms work well with pure functions. However, we do support certain in-place operations. On one hand, writing code compatible with function transforms may involve changing how you write PyTorch code, on the other hand, you may find that our transforms let you express things that were previously difficult to express in PyTorch.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_621", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "General limitations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "General limitations#", "content": "General limitations# All torch.func transforms share a limitation in that a function should not assign to global variables. Instead, all outputs to a function must be returned from the function. This restriction comes from how torch.func is implemented: each transform wraps Tensor inputs in special torch.func Tensor subclasses that facilitate the transform. So, instead of the following: import torch from torch.func import grad # Don't do this intermediate = None def f(x): global intermediate intermediate = x.sin() z = intermediate.sin() return z x = torch.randn([]) grad_x = grad(f)(x) Please rewrite f to return intermediate: def f(x): intermediate = x.sin() z = intermediate.sin() return z, intermediate grad_x, intermediate = grad(f, has_aux=True)(x)", "prev_chunk_id": "chunk_620", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_622", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "torch.autograd APIs#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "torch.autograd APIs#", "content": "torch.autograd APIs# If you are trying to use a torch.autograd API like torch.autograd.grad or torch.autograd.backward inside of a function being transformed by vmap() or one of torch.func’s AD transforms (vjp(), jvp(), jacrev(), jacfwd()), the transform may not be able to transform over it. If it is unable to do so, you’ll receive an error message. This is a fundamental design limitation in how PyTorch’s AD support is implemented and the reason why we designed the torch.func library. Please instead use the torch.func equivalents of the torch.autograd APIs: - torch.autograd.grad,Tensor.backward->torch.func.vjportorch.func.grad - torch.autograd.functional.jvp->torch.func.jvp - torch.autograd.functional.jacobian->torch.func.jacrevortorch.func.jacfwd - torch.autograd.functional.hessian->torch.func.hessian", "prev_chunk_id": "chunk_621", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_623", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "vmap limitations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "vmap limitations#", "content": "vmap limitations# vmap(func) is a transform that returns a function that maps func over some new dimension of each input Tensor. The mental model for vmap is that it is like running a for-loop: for pure functions (i.e. in the absence of side effects), vmap(f)(x) is equivalent to: torch.stack([f(x_i) for x_i in x.unbind(0)])", "prev_chunk_id": "chunk_622", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_624", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Mutation: Arbitrary mutation of Python data structures#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Mutation: Arbitrary mutation of Python data structures#", "content": "Mutation: Arbitrary mutation of Python data structures# In the presence of side effects, vmap() no longer acts like it is running a for-loop. For example, the following function: def f(x, list): list.pop() print(\"hello!\") return x.sum(0) x = torch.randn(3, 1) lst = [0, 1, 2, 3] result = vmap(f, in_dims=(0, None))(x, lst) will print “hello!” once and pop only one element from lst. vmap() executes f a single time, so all side effects only happen once. This is a consequence of how vmap is implemented. torch.func has a special, internal BatchedTensor class. vmap(f)(*inputs) takes all Tensor inputs, turns them into BatchedTensors, and calls f(*batched_tensor_inputs). BatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized) behavior for each PyTorch operator.", "prev_chunk_id": "chunk_623", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_625", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Mutation: in-place PyTorch Operations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Mutation: in-place PyTorch Operations#", "content": "Mutation: in-place PyTorch Operations# You might be here due to receiving an error about vmap-incompatible in-place operations. vmap() will raise an error if it encounters an unsupported PyTorch in-place operation and it will succeed otherwise. Unsupported operations are those that would cause a Tensor with more elements to be written to a Tensor with fewer elements. Here’s an example of how this can occur: def f(x, y): x.add_(y) return x x = torch.randn(1) y = torch.randn(3, 1) # When vmapped over, looks like it has shape [1] # Raises an error because `x` has fewer elements than `y`. vmap(f, in_dims=(None, 0))(x, y) x is a Tensor with one element, y is a Tensor with three elements. x + y has three elements (due to broadcasting), but attempting to write three elements back into x, which only has one element, raises an error due to attempting to write three elements into a Tensor with a single element. There is no problem if the Tensor being written to is batched under vmap() (i.e. it is being vmapped over). def f(x, y): x.add_(y) return x x = torch.randn(3, 1) y = torch.randn(3, 1) expected = x + y # Does not raise an error because x is being vmapped over. vmap(f, in_dims=(0, 0))(x, y) assert torch.allclose(x, expected) One common fix for this is to replace calls to factory functions with their “new_*” equivalent. For example: - Replacetorch.zeros()withTensor.new_zeros() - Replacetorch.empty()withTensor.new_empty() To see why this helps, consider the following. def diag_embed(vec): assert vec.dim() == 1 result = torch.zeros(vec.shape[0], vec.shape[0]) result.diagonal().copy_(vec) return result vecs = torch.tensor([[0., 1, 2], [3., 4, 5]]) # RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ... vmap(diag_embed)(vecs) Inside of vmap(), result is a Tensor of shape [3, 3]. However, although vec looks like it has shape [3], vec actually has underlying", "prev_chunk_id": "chunk_624", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_626", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Mutation: in-place PyTorch Operations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Mutation: in-place PyTorch Operations#", "content": "shape [2, 3]. It is not possible to copy vec into result.diagonal(), which has shape [3], because it has too many elements. def diag_embed(vec): assert vec.dim() == 1 result = vec.new_zeros(vec.shape[0], vec.shape[0]) result.diagonal().copy_(vec) return result vecs = torch.tensor([[0., 1, 2], [3., 4, 5]]) vmap(diag_embed)(vecs) Replacing torch.zeros() with Tensor.new_zeros() makes it so that result has an underlying Tensor of shape [2, 3, 3], so it is now possible to copy vec, which has underlying shape [2, 3], into result.diagonal().", "prev_chunk_id": "chunk_625", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_627", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Mutation: out= PyTorch Operations#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Mutation: out= PyTorch Operations#", "content": "Mutation: out= PyTorch Operations# vmap() doesn’t support the out= keyword argument in PyTorch operations. It will error out gracefully if it encounters that in your code. This is not a fundamental limitation; we could theoretically support this in the future but we have chosen not to for now.", "prev_chunk_id": "chunk_626", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_628", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Data-dependent Python control flow#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Data-dependent Python control flow#", "content": "Data-dependent Python control flow# We don’t yet support vmap over data-dependent control flow. Data-dependent control flow is when the condition of an if-statement, while-loop, or for-loop is a Tensor that is being vmap’ed over. For example, the following will raise an error message: def relu(x): if x > 0: return x return 0 x = torch.randn(3) vmap(relu)(x) However, any control flow that is not dependent on the values in vmap’ed tensors will work: def custom_dot(x): if x.dim() == 1: return torch.dot(x, x) return (x * x).sum() x = torch.randn(3) vmap(custom_dot)(x) JAX supports transforming over data-dependent control flow using special control flow operators (e.g. jax.lax.cond, jax.lax.while_loop). We’re investigating adding equivalents of those to PyTorch.", "prev_chunk_id": "chunk_627", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_629", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Data-dependent operations (.item())#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Data-dependent operations (.item())#", "content": "Data-dependent operations (.item())# We do not (and will not) support vmap over a user-defined function that calls .item() on a Tensor. For example, the following will raise an error message: def f(x): return x.item() x = torch.randn(3) vmap(f)(x) Please try to rewrite your code to not use .item() calls. You may also encounter an error message about using .item() but you might not have used it. In those cases, it is possible that PyTorch internally is calling .item() – please file an issue on GitHub and we’ll fix PyTorch internals.", "prev_chunk_id": "chunk_628", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_630", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Dynamic shape operations (nonzero and friends)#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic shape operations (nonzero and friends)#", "content": "Dynamic shape operations (nonzero and friends)# vmap(f) requires that f applied to every “example” in your input returns a Tensor with the same shape. Operations such as torch.nonzero, torch.is_nonzero are not supported and will error as a result. To see why, consider the following example: xs = torch.tensor([[0, 1, 2], [0, 0, 3]]) vmap(torch.nonzero)(xs) torch.nonzero(xs[0]) returns a Tensor of shape 2; but torch.nonzero(xs[1]) returns a Tensor of shape 1. We are unable to construct a single Tensor as an output; the output would need to be a ragged Tensor (and PyTorch does not yet have the concept of a ragged Tensor).", "prev_chunk_id": "chunk_629", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_631", "url": "https://docs.pytorch.org/docs/stable/func.ux_limitations.html", "title": "Randomness#", "page_title": "UX Limitations — PyTorch 2.8 documentation", "breadcrumbs": "Randomness#", "content": "Randomness# The user’s intention when calling a random operation can be unclear. Specifically, some users may want the random behavior to be the same across batches while others may want it to differ across batches. To address this, vmap takes a randomness flag. The flag can only be passed to vmap and can take on 3 values, “error,” “different,” or “same,” defaulting to error. Under “error” mode, any call to a random function will produce an error asking the user to use one of the other two flags based on their use case. Under “different” randomness, elements in a batch produce different random values. For instance, def add_noise(x): y = torch.randn(()) # y will be different across the batch return x + y x = torch.ones(3) result = vmap(add_noise, randomness=\"different\")(x) # we get 3 different values Under “same” randomness, elements in a batch produce same random values. For instance, def add_noise(x): y = torch.randn(()) # y will be the same across the batch return x + y x = torch.ones(3) result = vmap(add_noise, randomness=\"same\")(x) # we get the same value, repeated 3 times", "prev_chunk_id": "chunk_630", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_632", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "Patching Batch Norm#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "Patching Batch Norm#", "content": "Patching Batch Norm# Created On: Jan 03, 2023 | Last Updated On: Jun 11, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_633", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "What’s happening?#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "What’s happening?#", "content": "What’s happening?# Batch Norm requires in-place updates to running_mean and running_var of the same size as the input. Functorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e. regular.add_(batched) is not allowed). So when vmapping over a batch of inputs to a single module, we end up with this error", "prev_chunk_id": "chunk_632", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_634", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "How to fix#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "How to fix#", "content": "How to fix# One of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this All of these options assume that you don’t need running stats. If you’re using a module this means that it’s assumed you won’t use batch norm in evaluation mode. If you have a use case that involves running batch norm with vmap in evaluation mode, please file an issue", "prev_chunk_id": "chunk_633", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_635", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "Option 1: Change the BatchNorm#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "Option 1: Change the BatchNorm#", "content": "Option 1: Change the BatchNorm# If you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with: BatchNorm2d(C, G, track_running_stats=False) Here C is the same C as in the original BatchNorm. G is the number of groups to break C into. As such, C % G == 0 and as a fallback, you can set C == G, meaning each channel will be treated separately. If you must use BatchNorm and you’ve built the module yourself, you can change the module to not use running stats. In other words, anywhere that there’s a BatchNorm module, set the track_running_stats flag to be False BatchNorm2d(64, track_running_stats=False)", "prev_chunk_id": "chunk_634", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_636", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "Option 2: torchvision parameter#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "Option 2: torchvision parameter#", "content": "Option 2: torchvision parameter# Some torchvision models, like resnet and regnet, can take in a norm_layer parameter. These are often defaulted to be BatchNorm2d if they’ve been defaulted. Instead you can set it to be GroupNorm. import torchvision from functools import partial torchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c)) Here, once again, c % g == 0 so as a fallback, set g = c. If you are attached to BatchNorm, be sure to use a version that doesn’t use running stats import torchvision from functools import partial torchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))", "prev_chunk_id": "chunk_635", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_637", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "Option 3: functorch’s patching#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "Option 3: functorch’s patching#", "content": "Option 3: functorch’s patching# functorch has added some functionality to allow for quick, in-place patching of the module to not use running stats. Changing the norm layer is more fragile, so we have not offered that. If you have a net where you want the BatchNorm to not use running stats, you can run replace_all_batch_norm_modules_ to update the module in-place to not use running stats from torch.func import replace_all_batch_norm_modules_ replace_all_batch_norm_modules_(net)", "prev_chunk_id": "chunk_636", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_638", "url": "https://docs.pytorch.org/docs/stable/func.batch_norm.html", "title": "Option 4: eval mode#", "page_title": "Patching Batch Norm — PyTorch 2.8 documentation", "breadcrumbs": "Option 4: eval mode#", "content": "Option 4: eval mode# When run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode model.eval() vmap(model)(x) model.train()", "prev_chunk_id": "chunk_637", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_639", "url": "https://docs.pytorch.org/docs/stable/func.api.html", "title": "torch.func API Reference#", "page_title": "torch.func API Reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.func API Reference#", "content": "torch.func API Reference# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_640", "url": "https://docs.pytorch.org/docs/stable/func.api.html", "title": "Utilities for working with torch.nn.Modules#", "page_title": "torch.func API Reference — PyTorch 2.8 documentation", "breadcrumbs": "Utilities for working with torch.nn.Modules#", "content": "Utilities for working with torch.nn.Modules# In general, you can transform over a function that calls a torch.nn.Module. For example, the following is an example of computing a jacobian of a function that takes three values and returns three values: model = torch.nn.Linear(3, 3) def f(x): return model(x) x = torch.randn(3) jacobian = jacrev(f)(x) assert jacobian.shape == (3, 3) However, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That’s what functional_call() is for: it accepts an nn.Module, the transformed parameters, and the inputs to the Module’s forward pass. It returns the value of running the Module’s forward pass with the replaced parameters. Here’s how we would compute the Jacobian over the parameters model = torch.nn.Linear(3, 3) def f(params, x): return torch.func.functional_call(model, params, x) x = torch.randn(3) jacobian = jacrev(f)(dict(model.named_parameters()), x) If you’re looking for information on fixing Batch Norm modules, please follow the guidance here", "prev_chunk_id": "chunk_639", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_641", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "torch.func Whirlwind Tour#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "torch.func Whirlwind Tour#", "content": "torch.func Whirlwind Tour# Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_642", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "What is torch.func?#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "What is torch.func?#", "content": "What is torch.func?# torch.func, previously known as functorch, is a library for JAX-like composable function transforms in PyTorch. - A “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity. - torch.func has auto-differentiation transforms (grad(f)returns a function that computes the gradient off), a vectorization/batching transform (vmap(f)returns a function that computesfover batches of inputs), and others. - These function transforms can compose with each other arbitrarily. For example, composingvmap(grad(f))computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.", "prev_chunk_id": "chunk_641", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_643", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "Why composable function transforms?#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "Why composable function transforms?#", "content": "Why composable function transforms?# There are a number of use cases that are tricky to do in PyTorch today: - computing per-sample-gradients (or other per-sample quantities) - running ensembles of models on a single machine - efficiently batching together tasks in the inner-loop of MAML - efficiently computing Jacobians and Hessians - efficiently computing batched Jacobians and Hessians Composing vmap(), grad(), vjp(), and jvp() transforms allows us to express the above without designing a separate subsystem for each.", "prev_chunk_id": "chunk_642", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_644", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "grad() (gradient computation)#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "grad() (gradient computation)#", "content": "grad() (gradient computation)# grad(func) is our gradient computation transform. It returns a new function that computes the gradients of func. It assumes func returns a single-element Tensor and by default it computes the gradients of the output of func w.r.t. to the first input. import torch from torch.func import grad x = torch.randn([]) cos_x = grad(lambda x: torch.sin(x))(x) assert torch.allclose(cos_x, x.cos()) # Second-order gradients neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x) assert torch.allclose(neg_sin_x, -x.sin())", "prev_chunk_id": "chunk_643", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_645", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "vmap() (auto-vectorization)#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "vmap() (auto-vectorization)#", "content": "vmap() (auto-vectorization)# Note: vmap() imposes restrictions on the code that it can be used on. For more details, please see UX Limitations. vmap(func)(*inputs) is a transform that adds a dimension to all Tensor operations in func. vmap(func) returns a new function that maps func over some dimension (default: 0) of each Tensor in inputs. vmap is useful for hiding batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func), leading to a simpler modeling experience: import torch from torch.func import vmap batch_size, feature_size = 3, 5 weights = torch.randn(feature_size, requires_grad=True) def model(feature_vec): # Very simple linear model with activation assert feature_vec.dim() == 1 return feature_vec.dot(weights).relu() examples = torch.randn(batch_size, feature_size) result = vmap(model)(examples) When composed with grad(), vmap() can be used to compute per-sample-gradients: from torch.func import vmap batch_size, feature_size = 3, 5 def model(weights,feature_vec): # Very simple linear model with activation assert feature_vec.dim() == 1 return feature_vec.dot(weights).relu() def compute_loss(weights, example, target): y = model(weights, example) return ((y - target) ** 2).mean() # MSELoss weights = torch.randn(feature_size, requires_grad=True) examples = torch.randn(batch_size, feature_size) targets = torch.randn(batch_size) inputs = (weights,examples, targets) grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)", "prev_chunk_id": "chunk_644", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_646", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "vjp() (vector-Jacobian product)#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "vjp() (vector-Jacobian product)#", "content": "vjp() (vector-Jacobian product)# The vjp() transform applies func to inputs and returns a new function that computes the vector-Jacobian product (vjp) given some cotangents Tensors. from torch.func import vjp inputs = torch.randn(3) func = torch.sin cotangents = (torch.randn(3),) outputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)", "prev_chunk_id": "chunk_645", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_647", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "jvp() (Jacobian-vector product)#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "jvp() (Jacobian-vector product)#", "content": "jvp() (Jacobian-vector product)# The jvp() transforms computes Jacobian-vector-products and is also known as “forward-mode AD”. It is not a higher-order function unlike most other transforms, but it returns the outputs of func(inputs) as well as the jvps. from torch.func import jvp x = torch.randn(5) y = torch.randn(5) f = lambda x, y: (x * y) _, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5))) assert torch.allclose(out_tangent, x + y)", "prev_chunk_id": "chunk_646", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_648", "url": "https://docs.pytorch.org/docs/stable/func.whirlwind_tour.html", "title": "jacrev(), jacfwd(), and hessian()#", "page_title": "torch.func Whirlwind Tour — PyTorch 2.8 documentation", "breadcrumbs": "jacrev(), jacfwd(), and hessian()#", "content": "jacrev(), jacfwd(), and hessian()# The jacrev() transform returns a new function that takes in x and returns the Jacobian of the function with respect to x using reverse-mode AD. from torch.func import jacrev x = torch.randn(5) jacobian = jacrev(torch.sin)(x) expected = torch.diag(torch.cos(x)) assert torch.allclose(jacobian, expected) jacrev() can be composed with vmap() to produce batched jacobians: x = torch.randn(64, 5) jacobian = vmap(jacrev(torch.sin))(x) assert jacobian.shape == (64, 5, 5) jacfwd() is a drop-in replacement for jacrev that computes Jacobians using forward-mode AD: from torch.func import jacfwd x = torch.randn(5) jacobian = jacfwd(torch.sin)(x) expected = torch.diag(torch.cos(x)) assert torch.allclose(jacobian, expected) Composing jacrev() with itself or jacfwd() can produce hessians: def f(x): return x.sin().sum() x = torch.randn(5) hessian0 = jacrev(jacrev(f))(x) hessian1 = jacfwd(jacrev(f))(x) hessian() is a convenience function that combines jacfwd and jacrev: from torch.func import hessian def f(x): return x.sin().sum() x = torch.randn(5) hess = hessian(f)(x)", "prev_chunk_id": "chunk_647", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_649", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Custom Backends#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Custom Backends#", "content": "Custom Backends# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_650", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Overview#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# torch.compile provides a straightforward method to enable users to define custom backends. A backend function has the contract (gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable. Backend functions can be called by TorchDynamo, the graph tracing component of torch.compile, after tracing an FX graph and are expected to return a compiled function that is equivalent to the traced FX graph. The returned callable should have the same contract as the forward function of the original torch.fx.GraphModule passed into the backend: (*args: torch.Tensor) -> List[torch.Tensor]. In order for TorchDynamo to call your backend, pass your backend function as the backend kwarg in torch.compile. For example, import torch def my_custom_backend(gm, example_inputs): return gm.forward def f(...): ... f_opt = torch.compile(f, backend=my_custom_backend) @torch.compile(backend=my_custom_backend) def g(...): ... See below for more examples.", "prev_chunk_id": "chunk_649", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_651", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Registering Custom Backends#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Registering Custom Backends#", "content": "Registering Custom Backends# You can register your backend using the register_backend decorator, for example, from torch._dynamo import register_backend @register_backend def my_compiler(gm, example_inputs): ... Besides the register_backend decorator, if your backend is in another python package, you could also register your backend through entry points of python package, which provides a way for a package to register a plugin for another one. To register your backend through entry_points, you could add your backend function to the torch_dynamo_backends entry point group in the setup.py file of your package like: ... setup( ... 'torch_dynamo_backends': [ 'my_compiler = your_module.submodule:my_compiler', ] ... ) Please replace the my_compiler before = to the name of your backend’s name and replace the part after = to the module and function name of your backend function. The entry point will be added to your python environment after the installation of the package. When you call torch.compile(model, backend=\"my_compiler\"), PyTorch would first search the backend named my_compiler that has been registered with register_backend. If not found, it will continue to search in all backends registered via entry_points. Registration serves two purposes: - You can pass a string containing your backend function’s name totorch.compileinstead of the function itself, for example,torch.compile(model,backend=\"my_compiler\"). - It is required for use with theminifier. Any generated code from the minifier must call your code that registers your backend function, typically through animportstatement.", "prev_chunk_id": "chunk_650", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_652", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Custom Backends after AOTAutograd#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Custom Backends after AOTAutograd#", "content": "Custom Backends after AOTAutograd# It is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo. This is useful for 2 main reasons: - Users can define backends that support model training, as AOTAutograd can generate the backward graph for compilation. - AOTAutograd produces FX graphs consisting ofcore Aten ops. As a result, custom backends only need to support the core Aten opset, which is a significantly smaller opset than the entire torch/Aten opset. Wrap your backend with torch._dynamo.backends.common.aot_autograd and use torch.compile with the backend kwarg as before. Backend functions wrapped by aot_autograd should have the same contract as before. Backend functions are passed to aot_autograd through the fw_compiler (forward compiler) or bw_compiler (backward compiler) kwargs. If bw_compiler is not specified, the backward compile function defaults to the forward compile function. One caveat is that AOTAutograd requires compiled functions returned by backends to be “boxed”. This can be done by wrapping the compiled function with functorch.compile.make_boxed_func. For example, from torch._dynamo.backends.common import aot_autograd from functorch.compile import make_boxed_func def my_compiler(gm, example_inputs): return make_boxed_func(gm.forward) my_backend = aot_autograd(fw_compiler=my_compiler) # bw_compiler=my_compiler model_opt = torch.compile(model, backend=my_backend)", "prev_chunk_id": "chunk_651", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_653", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Debugging Backend#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Backend#", "content": "Debugging Backend# If you want to better understand what is going on during a compilation, you can create a custom compiler, which is referred to as backend in this section, that will print pretty print the fx GraphModule extracted from Dynamo’s bytecode analysis and return a forward() callable. For example: from typing import List import torch def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): print(\"my_compiler() called with FX graph:\") gm.graph.print_tabular() return gm.forward # return a python callable @torch.compile(backend=my_compiler) def fn(x, y): a = torch.cos(x) b = torch.sin(y) return a + b fn(torch.randn(10), torch.randn(10)) Running the above example produces the following output: my_compiler() called with FX graph: opcode name target args kwargs ------------- ------ ------------------------------------------------------ ---------- -------- placeholder x x () {} placeholder y y () {} call_function cos <built-in method cos of type object at 0x7f1a894649a8> (x,) {} call_function sin <built-in method sin of type object at 0x7f1a894649a8> (y,) {} call_function add <built-in function add> (cos, sin) {} output output output ((add,),) {} This works for torch.nn.Module as well as shown below: from typing import List import torch def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): print(\"my_compiler() called with FX graph:\") gm.graph.print_tabular() return gm.forward # return a python callable class MockModule(torch.nn.Module): def __init__(self): super().__init__() self.relu = torch.nn.ReLU() def forward(self, x): return self.relu(torch.cos(x)) mod = MockModule() optimized_mod = torch.compile(mod, backend=my_compiler) optimized_mod(torch.randn(10)) Let’s take a look at one more example with control flow: from typing import List import torch def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): print(\"my_compiler() called with FX graph:\") gm.graph.print_tabular() return gm.forward # return a python callable @torch.compile(backend=my_compiler) def toy_example(a, b): x = a / (torch.abs(a) + 1) if b.sum() < 0: b = b * -1 return x * b for _ in range(100): toy_example(torch.randn(10), torch.randn(10)) Running this example produces the following output: my_compiler() called with FX graph: opcode name target args kwargs ------------- ------- ------------------------------------------------------ ----------------", "prev_chunk_id": "chunk_652", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_654", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Debugging Backend#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Backend#", "content": "-------- placeholder a a () {} placeholder b b () {} call_function abs_1 <built-in method abs of type object at 0x7f8d259298a0> (a,) {} call_function add <built-in function add> (abs_1, 1) {} call_function truediv <built-in function truediv> (a, add) {} call_method sum_1 sum (b,) {} call_function lt <built-in function lt> (sum_1, 0) {} output output output ((truediv, lt),) {} my_compiler() called with FX graph: opcode name target args kwargs ------------- ------ ----------------------- ----------- -------- placeholder b b () {} placeholder x x () {} call_function mul <built-in function mul> (b, -1) {} call_function mul_1 <built-in function mul> (x, mul) {} output output output ((mul_1,),) {} my_compiler() called with FX graph: opcode name target args kwargs ------------- ------ ----------------------- --------- -------- placeholder b b () {} placeholder x x () {} call_function mul <built-in function mul> (x, b) {} output output output ((mul,),) {} The order of the last two graphs is nondeterministic depending on which one is encountered first by the just-in-time compiler.", "prev_chunk_id": "chunk_653", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_655", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Speedy Backend#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Speedy Backend#", "content": "Speedy Backend# Integrating a custom backend that offers superior performance is also easy and we’ll integrate a real one with optimize_for_inference: def optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): scripted = torch.jit.script(gm) return torch.jit.optimize_for_inference(scripted) And then you should be able to optimize any existing code with: @torch.compile(backend=optimize_for_inference_compiler) def code_to_accelerate(): ...", "prev_chunk_id": "chunk_654", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_656", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html", "title": "Composable Backends#", "page_title": "Custom Backends — PyTorch 2.8 documentation", "breadcrumbs": "Composable Backends#", "content": "Composable Backends# TorchDynamo includes many backends, which can be listed with torch._dynamo.list_backends(). You can combine these backends together with the following code: from torch._dynamo import lookup_backend def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): try: trt_compiled = lookup_backend(\"tensorrt\")(gm, example_inputs) if trt_compiled is not None: return trt_compiled except Exception: pass # first backend failed, try something else... try: inductor_compiled = lookup_backend(\"inductor\")(gm, example_inputs) if inductor_compiled is not None: return inductor_compiled except Exception: pass return gm.forward", "prev_chunk_id": "chunk_655", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_657", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "CUDAGraph Trees#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "CUDAGraph Trees#", "content": "CUDAGraph Trees# Created On: May 19, 2023 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_658", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "CUDAGraph#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "CUDAGraph#", "content": "CUDAGraph# For a longer background on CUDAGraphs, read accelerating pytorch with CUDAGraphs. CUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads. CUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses. - Control Flow is not possible - Kernels which trigger host to device syncs (such as .item()) errors - All input arguments to kernels are fixed to what they were recorded - CUDA Memory addresses are fixed, however the values of the memory at those addresses can change - No Essential CPU ops or CPU side effects", "prev_chunk_id": "chunk_657", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_659", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "PyTorch CUDAGraph Integration#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch CUDAGraph Integration#", "content": "PyTorch CUDAGraph Integration# PyTorch provides a convenience wrapper around CUDAGraphs that handles a couple of tricky interactions with PyTorch’s caching allocator. The CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs. Using a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.", "prev_chunk_id": "chunk_658", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_660", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Make Graphed Callables#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Make Graphed Callables#", "content": "Make Graphed Callables# Make Graphed Callables is a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.", "prev_chunk_id": "chunk_659", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_661", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "TorchDynamo Previous CUDA Graphs Integration#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo Previous CUDA Graphs Integration#", "content": "TorchDynamo Previous CUDA Graphs Integration# Running with cudagraph_trees=False does not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.", "prev_chunk_id": "chunk_660", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_662", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "CUDAGraph Trees Integration#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "CUDAGraph Trees Integration#", "content": "CUDAGraph Trees Integration# Like Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let’s take a look at an illustrative example: @torch.compile(mode=\"reduce-overhead\") def foo(x): # GRAPH 1 y = x * x * x # graph break triggered here if y.sum() > 0: # GRAPH 2 z = y ** y else: # GRAPH 3 z = (y.abs() ** y.abs()) torch._dynamo.graph_break() # GRAPH 4 return z * torch.rand_like(z) # the first run warms up each graph, which does things like CuBlas or Triton benchmarking foo(torch.arange(0, 10, device=\"cuda\")) # The second run does a CUDA Graph recording, and replays it foo(torch.arange(0, 10, device=\"cuda\")) # Finally we hit the optimized, CUDA Graph replay path foo(torch.arange(0, 10, device=\"cuda\")) In this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4. We share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten. - Same constraints from CUDA Graphs apply: same kernels must be invoked with the same arguments (static sizes, addresses, etc) - The same pattern of memory must be observed between recording and replay: if a tensor output of one graph dies subsequent to another graph during recording, it must also do so during replay. - Live memory in the CUDA pool forces a dependence between two recordings - These recordings can only be invoked in a", "prev_chunk_id": "chunk_661", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_663", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "CUDAGraph Trees Integration#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "CUDAGraph Trees Integration#", "content": "single order 1 - > 2 -> 4 All of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3? Graph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph. First, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation. The second time we hit graph 3 we are warmed up and ready to record. We record graph 3 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree! 1 / \\\\ 2 3 \\\\ \\\\ 4 4", "prev_chunk_id": "chunk_662", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_664", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Input Mutation Support#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Input Mutation Support#", "content": "Input Mutation Support# Input mutation function refers to a function conducting in-place writes to an input tensor, as illustrated below: def foo(x, y): # mutates input x x.add_(1) return x + y Input mutation functions generally lead to challenges for CUDAGraph Trees. Due to the static CUDA memory address requirement from CUDAGraph, for each input tensor x, CUDAGraph Trees may allocate a static memory address x’. During execution, CUDAGraph Trees first copy the input tensor x to the static memory address x’, and then replay the recorded CUDAGraph. For input mutation function, x’ is in-place updated, which is not reflected on the input tensor x since x and x’ reside on different CUDA memory addresses. A closer look at input mutation functions reveals that there are three types of inputs: - inputs from eager: These tensors we assume will vary input tensor addresses from execution to execution. Because cudagraphs freeze memory addresses, we need to copy these inputs to a static address tensor prior to graph recording and execution. - Parameters and buffers: These tensors we assume (and runtime-check) have the same tensor addresses on every execution. We do not need to copy over their contents because the recorded memory address will be the same as the executed memory address. - Tensors which are prior outputs from CUDAGraph Trees: Because the output tensor addresses of a cudagraph are fixed, if we run CUDAGraph1, then run CUDAGraph2, the inputs which came from CUDAGraph1 into CUDAGraph2 will have a fixed memory address. These inputs, like parameters and buffers, do not require copying over to a static address tensor. We check to make sure that these inputs are stable at runtime, and if they’re not we will re-record. CUDAGraph Trees support input mutation on parameters and buffers, and tensors which are prior outputs", "prev_chunk_id": "chunk_663", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_665", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Input Mutation Support#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Input Mutation Support#", "content": "from CUDAGraph Trees. For mutation on inputs from eager, CUDAGraph Trees will run the function without CUDAGraph and emit skipping due to mutated inputs log. The following example shows CUDAGraph Trees’ support for tensors which are prior outputs from CUDAGraph Trees. import torch @torch.compile(mode=\"reduce-overhead\") def foo(x): return x + 1 @torch.compile(mode=\"reduce-overhead\") def mut(x): return x.add_(2) # Enable input mutation support torch._inductor.config.triton.cudagraph_support_input_mutation = True for i in range(3): torch.compiler.cudagraph_mark_step_begin() inp = torch.rand([4], device=\"cuda\") # CUDAGraph is applied since `foo` does not mutate `inp` tmp = foo(inp) # Although `mut` mutates `tmp`, which is an output of a CUDAGraph # managed function. So CUDAGraph is still applied. mut(tmp) torch.compiler.cudagraph_mark_step_begin() inp = torch.rand([4], device=\"cuda\") tmp = foo(inp) # While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()` # is not. So CUDAGraph is not applied to `mut` and there is a log # `skipping cudagraphs due to mutated inputs` mut(tmp.clone()) To enable CUDAGraph Trees for a function mutating inputs from eager, please re-write the function to avoid input mutation.", "prev_chunk_id": "chunk_664", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_666", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Dynamic Shape Support#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Shape Support#", "content": "Dynamic Shape Support# Dynamic shape means that an input tensor has different shapes across function calls. Since CUDAGraph requires fixed tensor addresses, CUDAGraph Trees re-record CUDAGraph for every unique shape of an input tensor. This leads to multiple CUDAGraphs for a single inductor graph. When there are limited shapes (e.g., batch sizes in inference), it is profitable to re-record CUDAGraphs. However, if input tensor shapes change frequently or even on every invocation, re-recording CUDAGraph may not be profitable. Nvidia uses 64 KB of device memory per kernel launch in CUDAGraph, up until CUDA 12.4 and Driver Version 550+. This memory cost can be significant with many CUDAGraph re-recordings. For functions with frequently changing input tensor shapes, we suggest padding input tensors to a few fixed tensor shapes to still enjoy benefits from CUDAGraph. In addition, setting torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True allows to skip cudagraphing functions with dynamic shape inputs and only cudagraphing functions with static input tensor shapes.", "prev_chunk_id": "chunk_665", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_667", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "NCCL Support#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "NCCL Support#", "content": "NCCL Support# CUDAGraph Trees support functions with nccl operators. While CUDAGraph Trees perform per-device record for CUDAGraph, NCCL support allows cross-device communication. @torch.compile(mode=\"reduce-overhead\") def func(x): y = x * x y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM) x = torch.nn.functional.silu(x) return x * y", "prev_chunk_id": "chunk_666", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_668", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Reasons for Skipping CUDAGraph#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Reasons for Skipping CUDAGraph#", "content": "Reasons for Skipping CUDAGraph# Since CUDAGraph has requirements such as static input tensor addresses and not supporting CPU operators, CUDAGraph Trees check whether a function satisfies these requirements and may skip CUDAGraph when necessary. Here, we list common reasons for skipping CUDAGraph. - Input mutation: CUDAGraph Trees skip functions that in-place mutates eager input. In-place mutating parameters and buffers, or output tensors from CUDAGraph Tree managed functions are still supported. Please seeInput Mutation Supportsection for more details. - CPU operators: Functions containing CPU operator are skipped. Please split the function into multiple functions and apply CUDAGraph Trees on functions with only GPU operators. - Multi-device operators: A function is skipped if it contains operators on multiple devices. Currently, CUDAGraph is applied on a per-device basis. Please use supported libraries such as NCCL for cross-device communication. Please seeNCCL Supportsection for more details. - Free unbacked symbols: Free unbacked symbols usually happen duringdynamic shapes. CUDAGraph Trees currently record a CUDAGraph for every unique input tensor shapes. Please seeDynamic Shape Supportfor more details. - Incompatible operators: CUDAGraph Trees skip a function if it contain incompatible operators. Please replace these operators in a function with supported operators. We show an exhaustive list of incompatible operators: aten._fused_moving_avg_obs_fq_helper.default aten._fused_moving_avg_obs_fq_helper_functional.default aten.multinomial.default fbgemm.dense_to_jagged.default fbgemm.jagged_to_padded_dense.default run_and_save_rng_state run_with_rng_state aten._local_scalar_dense aten._assert_scalar The following operators are incompatible when torch.are_deterministic_algorithms_enabled(). aten._fused_moving_avg_obs_fq_helper.default aten._fused_moving_avg_obs_fq_helper_functional.default aten.multinomial.default fbgemm.dense_to_jagged.default fbgemm.jagged_to_padded_dense.default run_and_save_rng_state run_with_rng_state aten._local_scalar_dense aten._assert_scalar", "prev_chunk_id": "chunk_667", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_669", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html", "title": "Limitations#", "page_title": "CUDAGraph Trees — PyTorch 2.8 documentation", "breadcrumbs": "Limitations#", "content": "Limitations# Because CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation. Let’s say we are benchmarking running inference with the following code: import torch @torch.compile(mode=\"reduce-overhead\") def my_model(x): y = torch.matmul(x, x) return y x = torch.randn(10, 10, device=\"cuda\") y1 = my_model(x) y2 = my_model(x) print(y1) # RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. In the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. In CUDAGraph Trees, we don’t want to add unintended dependencies between iterations that would cause us to not hit the hot path, nor do we want we want to prematurely free memory from a prior invocation. Our heuristics are in inference we start a new iteration on each invocation for torch.compile, and in training we do the same so long as there is not a pending backward that has not been invoked. If those heuristics are wrong, you can mark the start of a new iteration with torch.compiler.mark_step_begin(), or clone tensors of a prior iteration (outside of torch.compile) before you begin the next run.", "prev_chunk_id": "chunk_668", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_670", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_nn_module.html", "title": "PyTorch 2.0 NNModule Support#", "page_title": "PyTorch 2.0 NNModule Support — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch 2.0 NNModule Support#", "content": "PyTorch 2.0 NNModule Support# Created On: Apr 06, 2023 | Last Updated On: Jun 10, 2025 Author: Will Constable torch.compile has special handling for torch.nn.Module objects, tracing them differently than it traces arbitrary python classes, with the intent of producing faster code by making assumptions about the structure. This doc describes some of the tradeoffs or edge cases that come up due to this specialization.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_671", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_nn_module.html", "title": "NNModule Hooks Support#", "page_title": "PyTorch 2.0 NNModule Support — PyTorch 2.8 documentation", "breadcrumbs": "NNModule Hooks Support#", "content": "NNModule Hooks Support# Previously, torch.compile had no support for hooks on nn.Modules, and if hooks were registered they would simply be ignored in the compiled program. Indeed many users do not use nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases for composing nn.Module hooks with torch.compile. Hooks that are orchestrated via nn.Module.call implementation include _forward_pre_hooks, forward_hooks, _backward_pre_hooks, and _backward_hooks, and will be referred to as ‘call hooks’. These hooks are partially supported by torch.compile with limitations described below. Another category of hooks includes _state_dict_hooks and its pre and load_ variants, and are still unsupported by torch.compile.", "prev_chunk_id": "chunk_670", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_672", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_nn_module.html", "title": "nn.Module.__call__ Hooks Usage and limitations#", "page_title": "PyTorch 2.0 NNModule Support — PyTorch 2.8 documentation", "breadcrumbs": "nn.Module.__call__ Hooks Usage and limitations#", "content": "nn.Module.__call__ Hooks Usage and limitations# By default, torch.compile will trace the contents of nn.Module.__call__ which means it will encounter and run forward/pre-forward hooks. If you install hooks before calling torch.compile and then do not remove or alter the hooks later, your use case should be supported by default. Backward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo occur when accessing backward_hooks dicts, which is probably avoiable with some work. Graph-breaks also impact the timing of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at the same time. Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would still expect the backward hooks for a series of modules to all fire together after the whole compiled graph’s backward ran. hooks on ‘allowed modules’ torch.compile treats common modules such as torch.conv, as well as modules that are difficult to trace, specially by allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo. For such modules, hooks currently trigger a graph-break so that the affected modules run outside of dynamo. Depending on the model, this could introduce a significant performance regression, and additional work is required to improve this support. skip_nnmodule_hook_guards By default, torch._dynamo.config.skip_nnmodule_hook_guards is set to True, meaning no guards will be installed on each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing if any hook dict is changed after compilation. If you want to be able to remove or modify hooks after compilation and have torch.compile react appropriately (by recompiling), then you need to set skip_nnmodule_hook_guards=False and expect a runtime penalty for the added guards. TODO: confirm if backward/pre_backward hooks are working or not and document accordingly", "prev_chunk_id": "chunk_671", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_673", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_nn_module.html", "title": "state_dict Hooks#", "page_title": "PyTorch 2.0 NNModule Support — PyTorch 2.8 documentation", "breadcrumbs": "state_dict Hooks#", "content": "state_dict Hooks# State dict hooks have not yet been supported in torch.compile. TODO: warn_once if graph-breaking on hooks. warn_once to point to this doc if hooks are present.", "prev_chunk_id": "chunk_672", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_674", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_provenance.html", "title": "TorchInductor and AOTInductor Provenance Tracking#", "page_title": "TorchInductor and AOTInductor Provenance Tracking — PyTorch 2.8 documentation", "breadcrumbs": "TorchInductor and AOTInductor Provenance Tracking#", "content": "TorchInductor and AOTInductor Provenance Tracking# Created On: May 09, 2025 | Last Updated On: May 23, 2025 This section describes how to use the provenance tracking feature for TorchInductor and AOTInductor in tlparse. Provenance tracking helps you visualize the relationships between the input GraphModule to (AOT)Inductor and the optimized code generated. This feature allows you to trace how your original operations are transformed during compilation. Some example screenshots of the provenance tracking tool are shown below. The tool visualizes the mapping between nodes in the input graph (panel 1), the post grad graph (panel 2), and the Inductor generated code (panel 3). The bolded lines represent nodes/kernels covered by the current provenance tracing functionality. We currently cover triton kernels, cpp kernels, and combo kernels. The yellow highlighting shows the provenance of the nodes/kernels.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_675", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_provenance.html", "title": "Using the Provenance Tracking Highlighter#", "page_title": "TorchInductor and AOTInductor Provenance Tracking — PyTorch 2.8 documentation", "breadcrumbs": "Using the Provenance Tracking Highlighter#", "content": "Using the Provenance Tracking Highlighter# Follow these steps to enable and use provenance tracking in your PyTorch project: - Installtlparsebycargoinstalltlparse. If you don’t havecargo, seeThe Cargo Bookfor instructions to install. - Run your program with required flags:TORCH_TRACE=~/my_trace_log_dirTORCH_LOGS=\"+inductor\"TORCH_COMPILE_DEBUG=1pythonyour_program.pyThis will generate a log file in~/my_trace_log_dir. The log file will be used by tlparse to generate the provenance tracking highlighter. - Runtlparseon the log with--inductor-provenanceflag. For example:tlparselog_file_name.log--inductor-provenanceEven if you don’t add the--inductor-provenanceflag, you should be able to see the mapping in json format in theinductor_provenance_tracking_node_mappings_<number>.jsonfile in theindex.htmltlparse output.Runtlparedirectly on the log file. It might not work if you run “tlparse parse <folder_name> –inductor-provenance”.Thetlparseartifacts used by the provenance tracking highlighter are:before_pre_grad_graph.txtafter_post_grad_graph.txtinductor_aot_wrapper_code.txtinductor_output_code.txtinductor_provenance_tracking_node_mappings.json After running tlparse <file_name> --inductor-provenance, you should see an additional “Provenance Tracking” section in the tlparse output. Clicking into the link(s) to access the provenance tracking tool. For a demo, see: pytorch/tlparse#93", "prev_chunk_id": "chunk_674", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_676", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_provenance.html", "title": "See Also#", "page_title": "TorchInductor and AOTInductor Provenance Tracking — PyTorch 2.8 documentation", "breadcrumbs": "See Also#", "content": "See Also# tlparse is a tool written in Rust. - Link to the tlparse GitHub repo:pytorch/tlparse - Learn more abouttlparseattorch.compile Troubleshooting", "prev_chunk_id": "chunk_675", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_677", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_performance_dashboard.html", "title": "PyTorch 2.0 Performance Dashboard#", "page_title": "PyTorch 2.0 Performance Dashboard — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch 2.0 Performance Dashboard#", "content": "PyTorch 2.0 Performance Dashboard# Created On: May 04, 2023 | Last Updated On: Jun 10, 2025 Author: Bin Bao and Huy Do PyTorch 2.0’s performance is tracked nightly on this dashboard. The performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and a 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be found here.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_678", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_performance_dashboard.html", "title": "How to read the dashboard?#", "page_title": "PyTorch 2.0 Performance Dashboard — PyTorch 2.8 documentation", "breadcrumbs": "How to read the dashboard?#", "content": "How to read the dashboard?# The landing page shows tables for all three benchmark suites we measure, TorchBench, Huggingface, and TIMM, and graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP training performance trend in the past 7 days for TorchBench. Droplists on the top of that page can be selected to view tables and graphs with different options. In addition to the pass rate, there are 3 key performance metrics reported there: Geometric mean speedup, Mean compilation time, and Peak memory footprint compression ratio. Both Geometric mean speedup and Peak memory footprint compression ratio are compared against the PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked, which will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.", "prev_chunk_id": "chunk_677", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_679", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_performance_dashboard.html", "title": "What is measured on the dashboard?#", "page_title": "PyTorch 2.0 Performance Dashboard — PyTorch 2.8 documentation", "breadcrumbs": "What is measured on the dashboard?#", "content": "What is measured on the dashboard?# All the dashboard tests are defined in this function. The exact test configurations are subject to change, but at the moment, we measure both inference and training performance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor, including default, with_cudagraphs (default + cudagraphs), and dynamic (default + dynamic_shapes).", "prev_chunk_id": "chunk_678", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_680", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_performance_dashboard.html", "title": "Can I check if my PR affects TorchInductor’s performance on the dashboard before merging?#", "page_title": "PyTorch 2.0 Performance Dashboard — PyTorch 2.8 documentation", "breadcrumbs": "Can I check if my PR affects TorchInductor’s performance on the dashboard before merging?#", "content": "Can I check if my PR affects TorchInductor’s performance on the dashboard before merging?# Individual dashboard runs can be triggered manually by clicking the Run workflow button here and submitting with your PR’s branch selected. This will kick off a whole dashboard run with your PR’s changes. Once it is done, you can check the results by selecting the corresponding branch name and commit ID on the performance dashboard UI. Be aware that this is an expensive CI run. With the limited resources, please use this functionality wisely.", "prev_chunk_id": "chunk_679", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_681", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_performance_dashboard.html", "title": "How can I run any performance test locally?#", "page_title": "PyTorch 2.0 Performance Dashboard — PyTorch 2.8 documentation", "breadcrumbs": "How can I run any performance test locally?#", "content": "How can I run any performance test locally?# The exact command lines used during a complete dashboard run can be found in any recent CI run logs. The workflow page is a good place to look for logs from some of the recent runs. In those logs, you can search for lines like python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda and run them locally if you have a GPU working with PyTorch 2.0. python benchmarks/dynamo/huggingface.py -h will give you a detailed explanation on options of the benchmarking script.", "prev_chunk_id": "chunk_680", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_682", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "torch.compile Troubleshooting#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "torch.compile Troubleshooting#", "content": "torch.compile Troubleshooting# Created On: Nov 28, 2022 | Last Updated On: Jun 10, 2025 You’re trying to use torch.compile on your PyTorch model to enhance its performance but it’s not working as expected. Perhaps performance isn’t improving, crashes are happening, or compilation time is too long. This article provides tips, workarounds, and debugging tools to help you overcome these challenges. Contents", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_683", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Setting Expectations#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Setting Expectations#", "content": "Setting Expectations# torch.compile is designed as a general-purpose PyTorch compiler. Unlike the previous compiler solution, TorchScript, torch.compile requires fewer code changes, meaning models typically don’t need to be rewritten from scratch. It also manages unsupported code more gracefully - unsupported code results in a lost optimization opportunity rather than a crash. In the ideal world, one can simply apply torch.compile to any PyTorch model and enjoy automatic speedups. However, in reality, code complexities can lead to one of three scenarios: - torch.compileworks seamlessly, providing speedups. - Some code modifications are necessary.torch.compiledoesn’t crash or take too long, but you might not be seeing significant performance gains. - Extensive changes to your code are required. We anticipate most code will fall under scenarios (1) and (2). This document provides tips, arranged by level of involvement, to help address code issues in scenario (2).", "prev_chunk_id": "chunk_682", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_684", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Compile times#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Compile times#", "content": "Compile times# torch.compile functions as a just-in-time compiler, so the initial one or two runs of the compiled function are expected to be significantly slower. Recompilations, which can occur under certain conditions (detailed below), will also make runs slower. Various torch.compile components cache results to reduce compilation time for future invocations, even in different processes. Cold-start (uncached) compilation time typically ranges from seconds to minutes for common or benchmarked models. Larger models may take upwards of 30 minutes to a few hours.", "prev_chunk_id": "chunk_683", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_685", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Terminology#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Terminology#", "content": "Terminology# The following terms are relevant to troubleshooting torch.compile problems.", "prev_chunk_id": "chunk_684", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_686", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Graph break#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Graph break#", "content": "Graph break# torch.compile traces your code and attempts to capture your PyTorch code into a single computation graph of PyTorch operators (FX graph). However, this is not always possible. When encountering code that can’t be traced, a “graph break” occurs. A graph break involves compiling the FX graph has been determined so far, running the unsupported code, then resuming tracing after the unsupported code with a new FX graph. Because the computation graph is broken up, we lose optimization opportunities, so model code should avoid graph breaks whenever possible. Graph breaks occur on things like: - Data-dependent if-statements - Many Python built-in functions - C functions Below is an example of a graph break due to the function copy.deepcopy from a Python builtin library (exact output may differ). import torch @torch.compile def fn(x): x = x + 1 with open(\"test.txt\", \"r\") as f: return x + len(f.read()) fn(torch.ones(3, 3)) $TORCH_LOGS=\"graph_breaks\" python playground.py Graph break in user code at /data/users/williamwen/pytorch/playground.py:7 Reason: Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False User code traceback: File \"/data/users/williamwen/pytorch/playground.py\", line 7, in fn with open(\"test.txt\", \"r\") as f: Traceback (most recent call last): File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 635, in wrapper return inner_fn(self, inst) ^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2414, in CALL self._call(inst) File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2408, in _call self.call_function(fn, args, kwargs) File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 962, in call_function self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 997, in call_function return handler(tx, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 831, in <lambda> return lambda *args: unimplemented(error_msg) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 313, in unimplemented raise Unsupported(msg, case_name=case_name) torch._dynamo.exc.Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False", "prev_chunk_id": "chunk_685", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_687", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Guards#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Guards#", "content": "Guards# torch.compile makes some assumptions about runtime values as we trace through code. During tracing, we generate “guards”, which are runtime checks for these assumptions. Guards are run in future calls to the compiled function to determine if we can reuse previously compiled code. Examples of runtime checks are constant values, types, and object IDs. Below is an example of generated guards. The TENSOR_MATCH guard checks for the input’s type, device, dtype, shape, etc. import torch @torch.compile def fn(x): return x + 1 fn(torch.ones(3, 3)) $ TORCH_LOGS=\"guards\" python playground.py GUARDS: TREE_GUARD_MANAGER: +- RootGuardManager | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None # _dynamo/output_graph.py:471 in init_ambient_guards | +- GLOBAL_STATE: ___check_global_state() | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack() | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x) | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3, 3], stride=[3, 1]) # return x + 1 # playground.py:6 in fn | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False # return x + 1 # playground.py:6 in fn", "prev_chunk_id": "chunk_686", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_688", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Recompilation#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Recompilation#", "content": "Recompilation# If the guards fail for every instance of previously compiled code, then torch.compile must “recompile” the function, requiring the original code to be traced again. In the example below, recompilation is necessary because the guard checking the tensor argument’s shape failed. import torch @torch.compile def fn(x): return x + 1 fn(torch.ones(3, 3)) fn(torch.ones(4, 4)) $ TORCH_LOGS=\"recompiles\" python playground.py Recompiling function fn in /data/users/williamwen/pytorch/playground.py:3 triggered by the following guard failure(s): - 0/0: tensor 'L['x']' size mismatch at index 0. expected 3, actual 4", "prev_chunk_id": "chunk_687", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_689", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Dynamic Shapes#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Shapes#", "content": "Dynamic Shapes# torch.compile initially assumes tensor shapes are static/constant and guards based on these assumptions. By using “dynamic shapes,” we can get torch.compile to produce compiled code that can accept tensor inputs with different shapes - we avoid recompiling every time shapes differ. By default, automatic dynamic shapes are enabled torch.compile(dynamic=None) - if compilation fails due to shape mismatch, recompilation is attempted with dynamic shapes. Dynamic shapes can also be fully enabled dynamic=True or disabled dynamic=False. Below, we enable dynamic shapes and note that we no longer need to recompile. import torch @torch.compile(dynamic=True) def fn(x): return x + 1 fn(torch.ones(3, 3)) fn(torch.ones(4, 4)) $ TORCH_LOGS=\"dynamic,recompiles\" python playground.py create_symbol s0 = 3 for L['x'].size()[0] [2, int_oo] at playground.py:5 in fn (_dynamo/variables/builder.py:2718 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s0\" produce_guards produce_guards For more information on dynamic shapes, see The dynamic shapes manual.", "prev_chunk_id": "chunk_688", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_690", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "tlparse / TORCH_TRACE#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "tlparse / TORCH_TRACE#", "content": "tlparse / TORCH_TRACE# tlparse / TORCH_TRACE are a pair of tools that produce compilation reports that look like this: https://web.mit.edu/~ezyang/Public/bhack-20240609-tlparse/index.html. Traces are very easy to collect. To collect a trace, run your reproduction command with TORCH_TRACE=\"/tmp/tracedir\" python foo.py pip install tlparse tlparse /tmp/tracedir This approach works even if you are running a distributed job, providing a trace for each rank. It will open your browser with HTML similar to what’s generated above. If you are making a bug report for a complicated problem that you don’t have a standalone reproduction for, you can still greatly assist PyTorch developers by attaching the trace log generated in /tmp/tracedir. The output of tlparse is primarily aimed for PyTorch developers, and the log format is easy to upload and share on GitHub. However, as a non-PyTorch developer, you can still extract useful information from it. We recommend starting with the inline help text in the report, which explains its contents. Here are some insights you can gain from a tlparse: - What model code was compiled by looking at the stack trie? This is especially useful if you’re not familiar with the codebase being compiled! - How many graph breaks / distinct compilation regions are there? (Each distinct compile is its own color coded block like[0/0]). Frames that are potentially graph-broken are light green[2/4]. If there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks, or maybe your code isn’t a good match fortorch.compile. - How many times did I recompile a particular frame? Something that recompiled a lot will look like:[10/0][10/1][10/2]- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn’t the root cause of your problem. - Was there a compilation error? Frames that errored", "prev_chunk_id": "chunk_689", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_691", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "tlparse / TORCH_TRACE#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "tlparse / TORCH_TRACE#", "content": "will look like[0/1]. - What intermediate compiler products did I generate for a given frame? For example, you can look at the high-level generated FX graph or the generated Triton code. - Is there relevant information for a particular frame? You can find these incompilation_metrics.", "prev_chunk_id": "chunk_690", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_692", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "TORCH_LOGS#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "TORCH_LOGS#", "content": "TORCH_LOGS# You can use the TORCH_LOGS environment variable to selectively enable parts of the torch.compile stack to log. TORCH_LOGS is in fact the source of logs for tlparse. The format of the TORCH_LOGS environment variable looks like this: TORCH_LOGS=\"<option1>,<option2>,...\" python foo.py Useful high-level options include: - graph_breaks: logs locations of graph breaks in user code and the reason for the graph break - guards: logs guards that are generated - recompiles: logs which function recompiled and the guards that failed, leading to the recompilation - dynamic: logs related to dynamic shapes Also, you can programmatically set logging options using torch._logging.set_logs: import logging torch._logging.set_logs(graph_breaks=True) ... More TORCH_LOGS options are Summary of TORCH_LOGS options. For the full list of options, see torch._logging and torch._logging.set_logs.", "prev_chunk_id": "chunk_691", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_693", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "tlparse vs. TORCH_LOGS#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "tlparse vs. TORCH_LOGS#", "content": "tlparse vs. TORCH_LOGS# Generally, we suggest first using tlparse when encountering issues. tlparse is ideal for debugging large models and gaining a high-level overview of how your model was compiled. On the other hand, TORCH_LOGS is preferred for small examples and fine-grained debugging detail, when we already have an idea of which torch.compile component is causing the problem.", "prev_chunk_id": "chunk_692", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_694", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Simple Workarounds#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Simple Workarounds#", "content": "Simple Workarounds# Here, we describe some workarounds to torch.compile issues involving small code modifications or changing some torch.compile settings.", "prev_chunk_id": "chunk_693", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_695", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Where to apply torch.compile?#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Where to apply torch.compile?#", "content": "Where to apply torch.compile?# We recommend applying torch.compile to the highest-level function that doesn’t cause excessive problems. Typically, it is your train or eval step with the optimizer but without the loop, your top-level nn.Module, or some sub-nn.Module``s. ``torch.compile specifically doesn’t handle distributed wrapper modules like DDP or FSDP very well, so consider applying torch.compile to the inner module passed to the wrapper. # inference model = ... opt_model = torch.compile(model) for _ in range(N_ITERS): inp = ... out = opt_model(inp) # training model = ... opt = torch.optim.Adam(model.parameters()) @torch.compile def train(mod, data): opt.zero_grad(True) pred = mod(data[0]) loss = torch.nn.CrossEntropyLoss()(pred, data[1]) loss.backward() opt.step() for _ in range(N_ITERS): inp = ... train(model, inp) # DistributedDataParallel model = ... opt_model = torch.compile(model) model_ddp = DistributedDataParallel(opt_model, ...) for _ in range(N_ITERS): inp = ... out = model_ddp(inp)", "prev_chunk_id": "chunk_694", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_696", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Disabling and Suppressing Errors#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Disabling and Suppressing Errors#", "content": "Disabling and Suppressing Errors# For some model architectures, there are portions of the model which are particularly difficult to compile - either there are many graph breaks, or there are crashes. You may want to explicitly disable these portions of the model which are problematic so that you can apply torch.compile to the parts that work. You can do this by using the @torch.compiler.disable decorator. When torch.compile attempts to call a disabled function, it breaks the graph and skips tracing the disabled function, resuming tracing after the call. By default, all recursive calls made from a disabled function are also disabled. Use the recursive=False option to allow compilation for recursive calls. def bad1_inner(...): # skipped @torch.compiler.disable def bad1_outer(...): # skipped bad1_inner(...) def bad2_inner(...) # traced @torch.compiler.disable(recursive=False) def bad2_outer(...): # skipped bad2_inner(...) @torch.compile def fn(...): # graph break bad1_outer(...) ... # graph break bad2_outer(...) For example, we use torch.compiler.disable to disable torch.compile on sparse architecture in recommendation models, as the sparse arch is difficult to compile. Preprocessing and logging functions are other examples of functions that typically cause a lot of graph breaks and do not get value from being compiled. If you are experiencing compiler crashes and you want to continue regardless, you can set torch._dynamo.config.suppress_errors = True. When the compiler crashes, we will just skip tracing the function and try again later. This is not best practice - it is better to eventually manually add disable annotations as necessary.", "prev_chunk_id": "chunk_695", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_697", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Resolving graph breaks#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Resolving graph breaks#", "content": "Resolving graph breaks# To maximize optimization opportunities, it’s important to reduce the number of graph breaks. Recall that you can see what graph breaks are happening using tlparse or TORCH_LOGS=\"graph_breaks\". In general, graph breaks are caused by one of the following: - You’re trying to do something that fundamentally cannot be traced, such as data-dependent control flow. - You’re trying to do something not yet supported. . For example, we currently have limited support for tracing code that uses the built-in Pythoninspectmodule. - Your code has an error in it. For example, you may have tried calling a function with an incorrect number of arguments. Graph break logs will tell you the user code location and reason for the graph break. Unfortunately, many graph breaks are not actionable without a deeper understanding of Dynamo. It can even be challenging to determine which of the three causes was the true cause of your graph break. We are working on making graph break messages more actionable. Additionally, the impact of lost optimization opportunities differs between graph breaks. For example, graph breaks that happen in the middle of your model’s forward are likely to have a more negatie impact than graph breaks in a preprocessing part at the beginning of the forward. So it is not crucial to prevent every single break, but rather to prevent the ones that cause significant performance hits. If a graph break message doesn’t suggest any action, you suspect that the cause of your graph break is (2), and you believe that the graph break is causing performance hits, then please report the graph break as an issue. If a function has many graph breaks, consider disabling compilation on that function, as the overhead cost for the graph breaks may become prohibitive. Below are some common graph breaks", "prev_chunk_id": "chunk_696", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_698", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Resolving graph breaks#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Resolving graph breaks#", "content": "and some workarounds.", "prev_chunk_id": "chunk_697", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_699", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Data-dependent operations#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Data-dependent operations#", "content": "Data-dependent operations# torch.compile graph breaks on data-dependent operations such as data-dependent control flow (if-statements, loops with tensors) and direct tensor data accesses (.item, .data_ptr). import torch @torch.compile def fn(x): y = x.sum() if y > 0: return x + y.item() return x - y.item() fn(torch.ones(3, 3)) $ TORCH_LOGS=\"graph_breaks\" python playground.py Graph break in user code at /data/users/williamwen/pytorch/playground.py:6 Reason: Data-dependent jump User code traceback: File \"/data/users/williamwen/pytorch/playground.py\", line 6, in fn if y > 0: Graph break in user code at /data/users/williamwen/pytorch/playground.py:7 Reason: Unsupported: Tensor.item User code traceback: File \"/data/users/williamwen/pytorch/playground.py\", line 7, in torch_dynamo_resume_in_fn_at_6 return x + y.item() Traceback (most recent call last): File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 616, in wrapper return inner_fn(self, inst) ^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2288, in CALL self._call(inst) File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2282, in _call self.call_function(fn, args, kwargs) File \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 838, in call_function self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py\", line 1038, in call_function return self.obj.call_method(tx, self.name, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 527, in call_method result = handler_method(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 773, in method_item unimplemented(\"Tensor.item\") File \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 304, in unimplemented raise Unsupported(msg, case_name=case_name) torch._dynamo.exc.Unsupported: Tensor.item The general workaround for these graph breaks is to avoid doing data-dependent operations. Some specific workarounds are: - If your control flow doesn’t actually depend on data values, consider modifying your code to perform control flow on constants. # old x = torch.randn(3, 3) @torch.compile def fn(y): if x.sum() > 0: return y + x else: return y - x # new x = torch.randn(3, 3) cond = (x.sum() > 0).item() @torch.compile def fn(y): if cond: return y + x else: return y - x - Use higher-order ops liketorch.cond(https://pytorch.org/docs/main/cond.html) in place of data-dependent control flow # old @torch.compile def fn(x): if x.sum() > 0: return x + 1 return x - 1 # new @torch.compile def fn(x): return", "prev_chunk_id": "chunk_698", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_700", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Data-dependent operations#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Data-dependent operations#", "content": "torch.cond( x.sum() > 0, lambda x: x + 1, lambda x: x - 1, (x,), ) - If you have a.item()call, trytorch._dynamo.config.capture_scalar_outputs=TrueorTORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1 - Wrap problematic parts of the function in a custom op", "prev_chunk_id": "chunk_699", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_701", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Custom ops#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Custom ops#", "content": "Custom ops# If you have code that torch.compile has trouble tracing through, either due to missing support or fundamental incompatibility, you can consider wrapping the problematic code in a custom op. Custom ops require a little bit of additional work to get them to be compatible with torch.compile. See https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details.", "prev_chunk_id": "chunk_700", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_702", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Printing#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Printing#", "content": "Printing# Printing/logging/issuing warnings will result in a graph break. If you have a function that makes many logging calls, for example, a function that logs data about a training iteration, consider applying torch.compiler.disable on it. Alternatively, you can try using torch._dynamo.config.reorderable_logging_functions. This config is used to reorder logging functions so that they are called at the end of the traced function, thus avoiding a graph break. However, the logged contents may differ if, for example, a mutation occurs. import torch torch._dynamo.config.reorderable_logging_functions.add(print) @torch.compile def fn(x): x += 1 print(\"log!\") return torch.sin(x) fn(torch.ones(3, 3)) $ TORCH_LOGS=\"graph_breaks\" python playground.py log!", "prev_chunk_id": "chunk_701", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_703", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Incorrect code#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Incorrect code#", "content": "Incorrect code# Your code may be wrong, or is otherwise encountering an error from outside torch.compile. In the code below, we made a typo in the torch.sin call by providing an extra argument. import torch @torch.compile def fn(x): y = torch.sin(x, x) return y fn(torch.ones(3, 3)) $ TORCH_LOGS=\"graph_breaks\" python playground.py Graph break in user code at /data/users/williamwen/pytorch/playground.py:5 Reason: Unsupported: TypeError <built-in method sin of type object at 0x7fd6fd764600>: sin() takes 1 positional argument but 2 were given User code traceback: File \"/data/users/williamwen/pytorch/playground.py\", line 5, in fn y = torch.sin(x, x) ... It can be difficult to tell from the logs if the error is caused by your code or because of a torch.compile bug. In order to differentiate, we recommend trying to run your code without torch.compile to see if you still get the error.", "prev_chunk_id": "chunk_702", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_704", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Dealing with recompilations#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Dealing with recompilations#", "content": "Dealing with recompilations# You can view recompilations and their reasons using tlparse or TORCH_LOGS=recompiles.", "prev_chunk_id": "chunk_703", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_705", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Is dynamic shapes enabled?#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Is dynamic shapes enabled?#", "content": "Is dynamic shapes enabled?# Recompilations due to mismatched shapes are in the form: tensor 'L['x']' size mismatch at index 0. expected 3, actual 4 Make sure that the dynamic option of torch.compile is not set to False. The default option, dynamic=None, will only attempt dynamic shapes after the first compilation. You can set dynamic=True to upfront compile as dynamic as possible. For more information on dynamic shapes, see The dynamic shapes manual.", "prev_chunk_id": "chunk_704", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_706", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Changing the cache size limit#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Changing the cache size limit#", "content": "Changing the cache size limit# There is a limit to how many times a function can be recompiled, determined by torch._dynamo.config.recompile_limit and torch._dynamo.config.accumulated_recompile_limit. If either limit is exceeded, then we will not attempt to compile the function again and instead will run the function eagerly. torch.compile will also issue a warning containing the affected function and which limit was hit. In the example below, each function call results in a recompile attempt. When we hit the cache size limit (8), we stop attempting to recompile. import torch @torch.compile(dynamic=False) def fn(x): return x + 1 for i in range(1, 10): fn(torch.ones(i)) $ python playground.py torch._dynamo hit config.recompile_limit (8) function: 'fn' (/data/users/williamwen/pytorch/playground.py:5) last reason: 0/0: tensor 'L['x']' size mismatch at index 0. expected 1, actual 9 If you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit. If the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.", "prev_chunk_id": "chunk_705", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_707", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Wrapping constants with tensors#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Wrapping constants with tensors#", "content": "Wrapping constants with tensors# By default, int / float variables are treated as constants and are guarded as such. In the below example, we have a recompilation for each function call. import torch @torch.compile def fn(x, c): return x + c for i in range(1, 10): fn(torch.ones(i), 0.5 + i) $ TORCH_LOGS=\"recompiles\" python playground.py Recompiling function fn in /data/users/williamwen/pytorch/playground.py:3 triggered by the following guard failure(s): - 0/7: L['c'] == 8.5 - 0/6: L['c'] == 7.5 - 0/5: L['c'] == 6.5 - 0/4: L['c'] == 5.5 - 0/3: L['c'] == 4.5 - 0/2: L['c'] == 3.5 - 0/1: L['c'] == 2.5 - 0/0: L['c'] == 1.5 torch._dynamo hit config.recompile_limit (8) function: 'fn' (/data/users/williamwen/pytorch/playground.py:3) last reason: 0/0: L['c'] == 1.5 In particular, for LR schedulers, initializing with a constant can lead to recompilations: import torch mod = torch.nn.Linear(3, 3) opt = torch.optim.Adam(mod.parameters(), lr=0.01) sched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9) @torch.compile def fn(inp): opt.zero_grad(True) out = mod(inp).sum() out.backward() opt.step() sched.step() for i in range(1, 10): fn(torch.ones(3, 3)) $ TORCH_LOGS=\"recompiles\" python playground.py Recompiling function step in /data/users/williamwen/pytorch/torch/optim/adam.py:189 triggered by the following guard failure(s): - 3/7: L['self'].param_groups[0]['lr'] == 0.004782969000000002 - 3/6: L['self'].param_groups[0]['lr'] == 0.005314410000000002 - 3/5: L['self'].param_groups[0]['lr'] == 0.005904900000000002 - 3/4: L['self'].param_groups[0]['lr'] == 0.006561000000000002 - 3/3: L['self'].param_groups[0]['lr'] == 0.007290000000000001 - 3/2: L['self'].param_groups[0]['lr'] == 0.008100000000000001 - 3/1: L['self'].param_groups[0]['lr'] == 0.009000000000000001 - 3/0: L['self'].param_groups[0]['lr'] == 0.01 torch._dynamo hit config.recompile_limit (8) function: 'step' (/data/users/williamwen/pytorch/torch/optim/adam.py:189) last reason: 3/0: L['self'].param_groups[0]['lr'] == 0.01 In both examples, we can wrap float variables in tensors in order to prevent recompilations. # first example for i in range(1, 10): fn(torch.ones(i), torch.tensor(0.5 + i)) # second example opt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01)) sched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))", "prev_chunk_id": "chunk_706", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_708", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Reporting Issues#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Reporting Issues#", "content": "Reporting Issues# If the workarounds provided above were not enough to get torch.compile working, then you should consider reporting the issue to PyTorch. But there are a few things that you can do to make our lives significantly easier.", "prev_chunk_id": "chunk_707", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_709", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Ablation#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Ablation#", "content": "Ablation# Check which component of the torch.compile stack is the one causing the issue using the backend= option for torch.compile. In particular, try: - torch.compile(fn,backend=\"eager\"), which only runs TorchDynamo, the graph capture component oftorch.compile. - torch.compile(fn,backend=\"aot_eager\"), which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation. - torch.compile(fn,backend=\"aot_eager_decomp_partition\"), which runs TorchDynamo and AOTAutograd with operator decompositions/partitions. - torch.compile(fn,backend=\"inductor\"), which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels. If you only fail with the Inductor backend, you can additionally test various Inductor modes: - torch.compile(fn,backend=\"inductor\",mode=\"default\") - torch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\") - torch.compile(fn,backend=\"inductor\",mode=\"max-autotune\") You can also check if dynamic shapes is causing issues with any backend: - torch.compile(fn,dynamic=True)(always use dynamic shapes) - torch.compile(fn,dynamic=False)(never use dynamic shapes) - torch.compile(fn,dynamic=None)(automatic dynamic shapes)", "prev_chunk_id": "chunk_708", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_710", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Bisecting#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Bisecting#", "content": "Bisecting# Did you try on the latest nightly? Did something work in the past but now no longer works? Can you bisect to determine the first nightly where your issue occurs? Bisecting is especially helpful for performance, accuracy, or compile time regressions, where it is not immediately obvious where the problem originates from.", "prev_chunk_id": "chunk_709", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_711", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Creating a reproducer#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Creating a reproducer#", "content": "Creating a reproducer# Creating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it. However, if you are a motivated user unfamiliar with the internals of torch.compile, creating a standalone reproducer can have a huge impact on our ability to fix the bug. Without a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch. Here’s a list of useful reproducers, ranked from most to least preferred: - Self-contained, small reproducer:A script with no external dependencies, under 100 lines of code, that reproduces the problem when run. - Self-contained, large reproducer:Even if it’s large, being self-contained is a huge advantage! - Non-self-contained reproducer with manageable dependencies:For example, if you can reproduce the problem by running a script afterpipinstalltransformers, that’s manageable. We can likely run it and investigate. - Non-self-contained reproducer requiring substantial setup:This might involve downloading datasets, multiple environment setup steps, or specific system library versions requiring a Docker image. The more complex the setup, the harder it is for us to recreate the environment.NoteDocker simplifies setup but complicates changes to the environment, so it's not a perfect solution, though we'll use it if necessary. Somewhat orthogonally, a reproducer that can be run in a single process is better than a reproducer that requires multiprocess training (but once again, if you only have a multiprocess reproducer, we’ll take it!). Additionally, below is a non-exhaustive list of aspects to check in your issue that you can attempt to replicate in your reproducer: - Autograd. Did you have tensor inputs withrequires_grad=True? Did you callbackward()on the output? - Dynamic shapes. Did you setdynamic=True? Or did you run the test code multiple times with varying shapes? - Custom operators.", "prev_chunk_id": "chunk_710", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_712", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Creating a reproducer#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Creating a reproducer#", "content": "Is there a custom operator involved in the real workflow? Can you replicate some of its important characteristics using the Python custom operator API? - Configuration. Did you set all the same configuration? This includestorch._dynamo.configandtorch._inductor.configsettings, as well as arguments totorch.compilelikebackend/mode. - Context managers. Did you replicate any active context managers? This could betorch.no_grad, automatic mixed precision,TorchFunctionMode/TorchDispatchMode, activation checkpointing, compiled autograd etc. - Tensor subclasses. Is there a tensor subclass involved?", "prev_chunk_id": "chunk_711", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_713", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Minifier#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Minifier#", "content": "Minifier# The minifier is an early torch.compile tool that, given an FX graph that crashes when we attempt to run or compile it, finds a subgraph that also crashes and outputs the code that performs that subgraph’s operations. Essentially, the minifier finds a minimal repro for a certain class of torch.compile-related crashes. This assumes that we were able to successfully trace through code. Unfortunately, most of the time nowadays, the minifier doesn’t work as expected, and alternative methods may be necessary. This is likely because bugs that can be automatically reproduced in this manner are generally easier to fix and have already been addressed, leaving more complex issues that do not reproduce easily. However, it is straightforward to attempt using the minifier, so it is worth trying even if it may not succeed. Instructions for operating the minifier can be found here. If the compiler is crashing, you can set TORCHDYNAMO_REPRO_AFTER=\"dynamo\" or TORCHDYNAMO_REPRO_AFTER=\"aot\" The aot option is more likely to succeed, although it may not identify the AOTAutograd issues. This will generate the repro.py file which may help to diagnose the problem. For accuracy-related issues, consider setting TORCHDYNAMO_REPRO_LEVEL=4. Please note that this may not always successfully identify the problematic subgraph.", "prev_chunk_id": "chunk_712", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_714", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Debugging Deeper#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Deeper#", "content": "Debugging Deeper# This section provides tools and techniques for independently debugging torch.compile issues or for gaining a deeper understanding of the torch.compile stack. These methods are more involved than those presented above and are used by PyTorch developers regularly to debug real torch.compile issues. Below is a high-level overview of the stack: The stack comprises three main components: TorchDynamo, AOTAutograd, and Inductor. Our debugging strategy involves first identifying the component in which the error occurs and then individually debugging the component. To determine the component responsible for the issue, see the Ablation section under Reporting Issues above. For guidance on debugging a specific component, consult the sections below.", "prev_chunk_id": "chunk_713", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_715", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Logging what Dynamo is tracing#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Logging what Dynamo is tracing#", "content": "Logging what Dynamo is tracing# The TORCH_LOGS=trace_bytecode option enables you to view the precise bytecode instructions that Dynamo is tracing, as well as a symbolic representation of the Python interpreter stack. When encountering a graph break or crash, it is advisable to inspect the last few bytecode instructions traced. You can also use TORCH_LOGS=trace_source to see which lines of source code Dynamo is tracing through. This is useful in combination with trace_bytecode to see the line of source code each traced bytecode instruction corresponds to. Finally, you can use TORCH_LOGS=graph_code to see the Python code representing the FX graph that Dynamo traced. You can view this code to double check that the correct ops are being traced. import torch def g(x, y): return x + y @torch.compile(backend=\"eager\") def f(x): x = torch.sin(x) x = g(x, x) return x f(torch.ones(3, 3)) $ TORCH_LOGS=\"trace_bytecode,trace_source,graph_code\" python playground.py TRACE starts_line /data/users/williamwen/pytorch/playground.py:6 in f () @torch.compile(backend=\"eager\") TRACE RESUME 0 [] TRACE starts_line /data/users/williamwen/pytorch/playground.py:8 in f (f) x = torch.sin(x) TRACE LOAD_GLOBAL torch [] TRACE LOAD_ATTR sin [NullVariable(), PythonModuleVariable(<module 'torch' from '/data/users/williamwen/pytorch/torch/__init__.py'>)] TRACE LOAD_FAST x [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>)] TRACE CALL 1 [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>), LazyVariableTracker()] TRACE STORE_FAST x [TensorVariable()] TRACE starts_line /data/users/williamwen/pytorch/playground.py:9 in f (f) x = g(x, x) TRACE LOAD_GLOBAL g [] TRACE LOAD_FAST x [NullVariable(), UserFunctionVariable()] TRACE LOAD_FAST x [NullVariable(), UserFunctionVariable(), TensorVariable()] TRACE CALL 2 [NullVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()] TRACE starts_line /data/users/williamwen/pytorch/playground.py:3 in g (g) (inline depth: 1) def g(x, y): TRACE RESUME 0 [] TRACE starts_line /data/users/williamwen/pytorch/playground.py:4 in g (g) (inline depth: 1) return x + y TRACE LOAD_FAST x [] TRACE LOAD_FAST y [TensorVariable()] TRACE BINARY_OP 0 [TensorVariable(), TensorVariable()] TRACE RETURN_VALUE None [TensorVariable()] TRACE STORE_FAST x [TensorVariable()] TRACE starts_line /data/users/williamwen/pytorch/playground.py:10 in f (f) return x TRACE LOAD_FAST x [] TRACE", "prev_chunk_id": "chunk_714", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_716", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Logging what Dynamo is tracing#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Logging what Dynamo is tracing#", "content": "RETURN_VALUE None [TensorVariable()] TRACED GRAPH ===== __compiled_fn_1 ===== /data/users/williamwen/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module): def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\"): l_x_ = L_x_ # File: /data/users/williamwen/pytorch/playground.py:8 in f, code: x = torch.sin(x) x: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_); l_x_ = None # File: /data/users/williamwen/pytorch/playground.py:4 in g, code: return x + y x_1: \"f32[3, 3][3, 1]cpu\" = x + x; x = None return (x_1,)", "prev_chunk_id": "chunk_715", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_717", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Breakpointing Dynamo tracing#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Breakpointing Dynamo tracing#", "content": "Breakpointing Dynamo tracing# Inserting a breakpoint in Dynamo/user code is helpful at times to see what the state of Dynamo is when tracing through user code. Unfortunately, inserting a breakpoint in the normal Python fashion will result in a graph break in TorchDynamo, so we will not be able to view the state of Dynamo at the point where we intended to breakpoint. The first method for setting a breakpoint is to insert it within the Dynamo source code. Three recommended locations to place a breakpoint are: - Intorch/_dynamo/symbolic_convert.py, breakpoint at functions that are named after the problematic bytecode instruction, such asdefCALL_FUNCTIONanddefSTORE_ATTR. You can conditionally breakpoint depending on inputs, for example, theargvalof the instruction, or the name of the object at the top of the stack since some bytecode opcodes are frequently used. - Breakpoint where the graph break or error originates from. Typically, graph breaks are emitted from a call tounimplemented(...). - Breakpoint intorch/_dynamo/variables/builder.py,function:_wrap. You will likely have to conditionally breakpoint on the input. This function determines how to symbolically represent a given value. Consider breakpointing here if you suspect that a value is represented incorrectly. The second way to insert a breakpoint is to use torch._dynamo.comptime.comptime.breakpoint: from torch._dynamo.comptime import comptime @torch.compile def f(...): ... comptime.breakpoint() ... A comptime breakpoint is convenient as it enables you to inspect the Dynamo state at a specific location within the user code being traced. It does not require you to insert a breakpoint in the Dynamo source or to conditionally breakpoint based on variables. When a comptime breakpoint is triggered, you can do the following: - ctx.print_bt()to print the user stack trace - ctx.print_locals()to print all current locals - ctx.print_graph()to print the currently traced graph - ctx.disas()to print the currently traced function’s bytecode - Use standardpdbcommands, such asbt/u/d/n/s/r, - you can go", "prev_chunk_id": "chunk_716", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_718", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Breakpointing Dynamo tracing#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Breakpointing Dynamo tracing#", "content": "up thepdbstack to inspect more Dynamo internals import torch from torch._dynamo.comptime import comptime @torch.compile(backend=\"eager\") def f(x): y = x + 1 comptime.breakpoint() y = y + 1 return y f(torch.ones(3, 3)) $ python playground.py --Return-- > /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None -> builtins.breakpoint() (Pdb) ctx.print_bt() File \"/data/users/williamwen/pytorch/playground.py\", line 7, in f comptime.breakpoint() (Pdb) ctx.print_locals() x = FakeTensor(..., size=(3, 3)) y = FakeTensor(..., size=(3, 3)) (Pdb) bt ... /data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py(826)call_function() -> self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type] /data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py(331)call_function() -> func(ComptimeContext(tx)) > /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None -> builtins.breakpoint() (Pdb) ctx.print_graph() def forward(self, L_x_: \"f32[3, 3]\"): l_x_ = L_x_ # File: /data/users/williamwen/pytorch/playground.py:6 in f, code: y = x + 1 y: \"f32[3, 3]\" = l_x_ + 1; l_x_ = y = None", "prev_chunk_id": "chunk_717", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_719", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Bytecode generation errors#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Bytecode generation errors#", "content": "Bytecode generation errors# Although uncommon, Dynamo may generate incorrect bytecode. This may occur if you determine the following: - Ablation reveals the error is happening at the TorchDynamo level - The error is not being emitted from TorchDynamo stack frames - The error looks more like a user error rather than a Dynamo error, or is a segmentation fault - The error does not occur withouttorch.compile Bytecode generation bugs are generally tricky to fix and we recommend submitting an issue instead of trying to fix those yourself. If you are interested in seeing the bytecode that Dynamo generates, you can use TORCH_LOGS=bytecode. You can see a high-level overview on what bytecode Dynamo generates here.", "prev_chunk_id": "chunk_718", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_720", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "AOTAutograd#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "AOTAutograd#", "content": "AOTAutograd# AOTAutograd errors are typically difficult to debug - we recommend just submitting an issue. AOTAutograd logging output is primarily helpful to see what the input to Inductor is.", "prev_chunk_id": "chunk_719", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_721", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Summary of TORCH_LOGS options#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Summary of TORCH_LOGS options#", "content": "Summary of TORCH_LOGS options# A summary of helpful TORCH_LOGS options is: For the full list of options, see torch._logging and torch._logging.set_logs.", "prev_chunk_id": "chunk_720", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_722", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html", "title": "Related Articles#", "page_title": "torch.compile Troubleshooting — PyTorch 2.8 documentation", "breadcrumbs": "Related Articles#", "content": "Related Articles# - torch.compile tutorial - torch.compile fine-grained APIs - torch.compile FAQ - torch.compiler namespace overview - torch.compiler API reference - Profiling torch.compile - torch.compile missing manual - The dynamic shapes manual - TorchInductor caching tutorial", "prev_chunk_id": "chunk_721", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_723", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Frequently Asked Questions#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025 Author: Mark Saroufim", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_724", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Does torch.compile support training?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Does torch.compile support training?#", "content": "Does torch.compile support training?# torch.compile supports training, using AOTAutograd to capture backwards: - The.forward()graph andoptimizer.step()is captured by TorchDynamo’s pythonevalframefrontend. - For each segment of.forward()that torchdynamo captures, it uses AOTAutograd to generate a backward graph segment. - Each pair of forward and backward graph are (optionally) min-cut partitioned to save the minimal state between forward and backward. - The forward and backward pairs are wrapped inautograd.functionmodules. - User code calling.backward()still triggers eager’s autograd engine, which runs eachcompiled backwardgraph as if it were one op, also running any non-compiled eager ops’.backward()functions.", "prev_chunk_id": "chunk_723", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_725", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Do you support Distributed code?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Do you support Distributed code?#", "content": "Do you support Distributed code?# torch.compile supports DistributedDataParallel (DDP). Support for other distributed training libraries is being considered. The main reason why Distributed code is challenging with dynamo is because AOTAutograd unrolls both the forward and backward pass and provides 2 graphs for backends to optimize. This is a problem for distributed code because we’d like to ideally overlap communication operations with computations. Eager pytorch accomplishes this in different ways for DDP/FSDP- using autograd hooks, module hooks, and modifications/mutations of module states. In a naive application of dynamo, hooks that should run directly after an operation during backwards may be delayed until after the entire compiled region of backwards ops, due to how AOTAutograd compiled functions interact with dispatcher hooks. The basic strategy for optimizing DDP with Dynamo is outlined in distributed.py where the main idea will be to graph break on DDP bucket boundaries. When each node in DDP needs to synchronize its weights with the other nodes it organizes its gradients and parameters into buckets which reduces communication times and allows a node to broadcast a fraction of its gradients to other waiting nodes. Graph breaks in distributed code mean you can expect dynamo and its backends to optimize the compute overhead of a distributed program but not its communication overhead. Graph-breaks may interfere with compilation speedups, if the reduced graph-size robs the compiler of fusion opportunities. However, there are diminishing returns with increasing graph size since most of the current compute optimizations are local fusions. So in practice this approach may be sufficient.", "prev_chunk_id": "chunk_724", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_726", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Do I still need to export whole graphs?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Do I still need to export whole graphs?#", "content": "Do I still need to export whole graphs?# For the vast majority of models you probably don’t and you can use torch.compile() as is but there are a few situations where full graphs are necessary and you can can ensure a full graph by simply running torch.compile(..., fullgraph=True). These situations include: - Large scale training runs, such as $250K+ that require pipeline parallelism and other advanced sharding strategies. - Inference optimizers likeTensorRTorAITemplatethat rely on fusing much more aggressively than training optimizers. - Mobile training or inference. Future work will include tracing communication operations into graphs, coordinating these operations with compute optimizations, and optimizing the communication operations.", "prev_chunk_id": "chunk_725", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_727", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why is my code crashing?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why is my code crashing?#", "content": "Why is my code crashing?# If your code ran just fine without torch.compile and started to crash with it is enabled, then the most important first step is figuring out which part of the stack your failure occurred. To troubleshoot that, follow the steps below and only try the next step if the previous one succeeded. - torch.compile(...,backend=\"eager\")which only runs TorchDynamo forward graph capture and then runs the captured graph with PyTorch. If this fails then there’s an issue with TorchDynamo. - torch.compile(...,backend=\"aot_eager\")which runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph without any additional backend compiler steps. PyTorch eager will then be used to run the forward and backward graphs. If this fails then there’s an issue with AOTAutograd. - torch.compile(...,backend=\"inductor\")which runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph with the TorchInductor compiler. If this fails then there’s an issue with TorchInductor", "prev_chunk_id": "chunk_726", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_728", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why is compilation slow?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why is compilation slow?#", "content": "Why is compilation slow?# - Dynamo Compilation– TorchDynamo has a builtin stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by callingtorch._dynamo.utils.compile_times()after executingtorch._dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name. - Inductor Compilation– TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump.envTORCH_COMPILE_DEBUG=1pythonrepro.py. This is a debugging tool designed to make it easier to debug/understand the internals of TorchInductor with an output that will look something likethisEach file in that debug trace can be enabled/disabled viatorch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate. See theexample debug directory outputfor more examples. - Excessive RecompilationWhen TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up totorch._dynamo.config.recompile_limittimes. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it. The UseTORCH_TRACE/tlparseorTORCH_LOGS=recompilesto trace the root of the issue, checktorch.compile Troubleshootingfor more details.", "prev_chunk_id": "chunk_727", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_729", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why are you recompiling in production?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why are you recompiling in production?#", "content": "Why are you recompiling in production?# In some cases, you may not want unexpected compiles after a program has warmed up. For example, if you are serving production traffic in a latency critical application. For this, TorchDynamo provides an alternate mode where prior compiled graphs are used, but no new ones are generated: frozen_toy_example = dynamo.run(toy_example) frozen_toy_example(torch.randn(10), torch.randn(10))", "prev_chunk_id": "chunk_728", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_730", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "How are you speeding up my code?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "How are you speeding up my code?#", "content": "How are you speeding up my code?# There are 3 major ways to accelerate PyTorch code: - Kernel fusion via vertical fusions which fuse sequential operations to avoid excessive read/writes. For example, fuse 2 subsequent cosines means you can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion: the simplest example being batching where a single matrix is multiplied with a batch of examples but the more general scenario is a grouped GEMM where a group of matrix multiplications are scheduled together - Out of order execution: A general optimization for compilers, by looking ahead at the exact data dependencies within a graph we can decide on the most opportune time to execute a node and which buffers can be reused - Automatic work placement: Similar of the out of order execution point, but by matching nodes of a graph to resources like physical hardware or memory we can design an appropriate schedule The above are general principles for accelerating PyTorch code but different backends will each make different tradeoffs on what to optimize. For example Inductor first takes care of fusing whatever it can and only then generates Triton kernels. Triton in addition offers speedups because of automatic memory coalescing, memory management and scheduling within each Streaming Multiprocessor and has been designed to handle tiled computations. However, regardless of the backend you use it’s best to use a benchmark and see approach so try out the PyTorch profiler, visually inspect the generated kernels and try to see what’s going on for yourself.", "prev_chunk_id": "chunk_729", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_731", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Graph Breaks#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Graph Breaks#", "content": "Graph Breaks# The main reason you won’t see the speedups you’d like to by using dynamo is excessive graph breaks. So what’s a graph break? Given a program like: def some_fun(x): ... torch.compile(some_fun)(x) ... Torchdynamo will attempt to compile all of the torch/tensor operations within some_fun() into a single FX graph, but it may fail to capture everything into one graph. Some graph break reasons are insurmountable to TorchDynamo like calling into a C extension other than PyTorch is invisible to TorchDynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse.", "prev_chunk_id": "chunk_730", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_732", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Identifying the cause of a graph break#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Identifying the cause of a graph break#", "content": "Identifying the cause of a graph break# To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage: import torch import torch._dynamo as dynamo def toy_example(a, b): x = a / (torch.abs(a) + 1) print(\"woo\") if b.sum() < 0: b = b * -1 return x * b explanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10)) print(explanation) \"\"\" Graph Count: 3 Graph Break Count: 2 Op Count: 5 Break Reasons: Break Reason 1: Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False User Stack: <FrameSummary file foo.py, line 5 in toy_example> Break Reason 2: Reason: generic_jump TensorVariable() User Stack: <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5> Ops per Graph: ... Out Guards: ... \"\"\" To throw an error on the first graph break encountered you can disable python fallbacks by using fullgraph=True, this should be familiar if you’ve worked with export based compilers. def toy_example(a, b): ... torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)", "prev_chunk_id": "chunk_731", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_733", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why didn’t my code recompile when I changed it?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why didn’t my code recompile when I changed it?#", "content": "Why didn’t my code recompile when I changed it?# If you enabled dynamic shapes by setting env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py then your code won’t recompile on shape changes. We’ve added support for dynamic shapes which avoids recompilations in the case when shapes vary by less than a factor of 2. This is especially useful in scenarios like varying image sizes in CV or variable sequence length in NLP. In inference scenarios it’s often not possible to know what a batch size will be beforehand because you take what you can get from different client apps. In general, TorchDynamo tries very hard not to recompile things unnecessarily so if for example TorchDynamo finds 3 graphs and your change only modified one graph then only that graph will recompile. So another tip to avoid potentially slow compilation times is to warmup a model by compiling it once after which subsequent compilations will be much faster. Cold start compile times is still a metric we track visibly.", "prev_chunk_id": "chunk_732", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_734", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why am I getting incorrect results?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why am I getting incorrect results?#", "content": "Why am I getting incorrect results?# Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it’s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler. If you’d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True", "prev_chunk_id": "chunk_733", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_735", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Why am I getting OOMs?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Why am I getting OOMs?#", "content": "Why am I getting OOMs?# Dynamo is still an alpha product so there’s a few sources of OOMs and if you’re seeing an OOM try disabling the following configurations in this order and then open an issue on GitHub so we can solve the root problem 1. If you’re using dynamic shapes try disabling them, we’ve disabled them by default: env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py 2. CUDA graphs with Triton are enabled by default in inductor but removing them may alleviate some OOM issues: torch._inductor.config.triton.cudagraphs = False.", "prev_chunk_id": "chunk_734", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_736", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Does torch.func work with torch.compile (for grad and vmap transforms)?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Does torch.func work with torch.compile (for grad and vmap transforms)?#", "content": "Does torch.func work with torch.compile (for grad and vmap transforms)?# Applying a torch.func transform to a function that uses torch.compile does work: import torch @torch.compile def f(x): return torch.sin(x) def g(x): return torch.grad(f)(x) x = torch.randn(2, 3) g(x)", "prev_chunk_id": "chunk_735", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_737", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Compiling torch.func.grad with torch.compile#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Compiling torch.func.grad with torch.compile#", "content": "Compiling torch.func.grad with torch.compile# import torch def wrapper_fn(x): return torch.func.grad(lambda x: x.sin().sum())(x) x = torch.randn(3, 3, 3) grad_x = torch.compile(wrapper_fn)(x)", "prev_chunk_id": "chunk_736", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_738", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Compiling torch.vmap with torch.compile#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Compiling torch.vmap with torch.compile#", "content": "Compiling torch.vmap with torch.compile# import torch def my_fn(x): return torch.vmap(lambda x: x.sum(1))(x) x = torch.randn(3, 3, 3) output = torch.compile(my_fn)(x)", "prev_chunk_id": "chunk_737", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_739", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Compiling functions besides the ones which are supported (escape hatch)#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Compiling functions besides the ones which are supported (escape hatch)#", "content": "Compiling functions besides the ones which are supported (escape hatch)# For other transforms, as a workaround, use torch._dynamo.allow_in_graph allow_in_graph is an escape hatch. If your code does not work with torch.compile, which introspects Python bytecode, but you believe it will work via a symbolic tracing approach (like jax.jit), then use allow_in_graph. By using allow_in_graph to annotate a function, you must make sure your code meets the following requirements: - All outputs in your function only depend on the inputs and do not depend on any captured Tensors. - Your function is functional. That is, it does not mutate any state. This may be relaxed; we actually support functions that appear to be functional from the outside: they may have in-place PyTorch operations, but may not mutate global state or inputs to the function. - Your function does not raise data-dependent errors. import torch @torch.compile def f(x): return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x) x = torch.randn(2, 3) f(x) A common pitfall is using allow_in_graph to annotate a function that invokes an nn.Module. This is because the outputs now depend on the parameters of the nn.Module. To get this to work, use torch.func.functional_call to extract the module state.", "prev_chunk_id": "chunk_738", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_740", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Does NumPy work with torch.compile?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Does NumPy work with torch.compile?#", "content": "Does NumPy work with torch.compile?# Starting in 2.1, torch.compile understands native NumPy programs that work on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch to NumPy and back via x.numpy(), torch.from_numpy, and related functions.", "prev_chunk_id": "chunk_739", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_741", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Which NumPy features does torch.compile support?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Which NumPy features does torch.compile support?#", "content": "Which NumPy features does torch.compile support?# NumPy within torch.compile follows NumPy 2.0 pre-release. Generally, torch.compile is able to trace through most NumPy constructions, and when it cannot, it falls back to eager and lets NumPy execute that piece of code. Even then, there are a few features where torch.compile semantics slightly deviate from those of NumPy: - NumPy scalars: We model them as 0-D arrays. That is,np.float32(3)returns a 0-D array undertorch.compile. To avoid a graph break, it is best to use this 0-D array. If this breaks your code, you can workaround this by casting the NumPy scalar to the relevant Python scalar typebool/int/float. - Negative strides:np.flipand slicing with a negative step return a copy. - Type promotion: NumPy’s type promotion will change in NumPy 2.0. The new rules are described inNEP 50.torch.compileimplements NEP 50 rather than the current soon-to-be deprecated rules. - {tril,triu}_indices_from/{tril,triu}_indicesreturn arrays rather than a tuple of arrays. There are other features for which we do not support tracing and we gracefully fallback to NumPy for their execution: - Non-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays. - Long dtypesnp.float128/np.complex256and some unsigned dtypesnp.uint16/np.uint32/np.uint64. - ndarraysubclasses. - Masked arrays. - Esoteric ufunc machinery likeaxes=[(n,k),(k,m)->(n,m)]and ufunc methods (e.g.,np.add.reduce). - Sorting / orderingcomplex64/complex128arrays. - NumPynp.poly1dandnp.polynomial. - Positionalout1,out2args in functions with 2 or more returns (out=tupledoes work). - __array_function__,__array_interface__and__array_wrap__. - ndarray.ctypesattribute.", "prev_chunk_id": "chunk_740", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_742", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Can I compile NumPy code using torch.compile?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Can I compile NumPy code using torch.compile?#", "content": "Can I compile NumPy code using torch.compile?# Of course you do! torch.compile understands NumPy code natively, and treats it as if it were PyTorch code. To do so, simply wrap NumPy code with the torch.compile decorator. import torch import numpy as np @torch.compile def numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray: return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)) X = np.random.randn(1024, 64) Y = np.random.randn(1024, 64) Z = numpy_fn(X, Y) assert isinstance(Z, np.ndarray) Executing this example with the environment variable TORCH_LOGS=output_code, we can see that torch.compile was able to fuse the multiplication and the sum into one C++ kernel. It was also able to execute them in parallel using OpenMP (native NumPy is single-threaded). This can easily make your NumPy code n times faster, where n is the number of cores in your processor! Tracing NumPy code this way also supports graph breaks within the compiled code.", "prev_chunk_id": "chunk_741", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_743", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Can I execute NumPy code on CUDA and compute gradients via torch.compile?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Can I execute NumPy code on CUDA and compute gradients via torch.compile?#", "content": "Can I execute NumPy code on CUDA and compute gradients via torch.compile?# Yes you can! To do so, you may simply execute your code within a torch.device(\"cuda\") context. Consider the example import torch import numpy as np @torch.compile def numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray: return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)) X = np.random.randn(1024, 64) Y = np.random.randn(1024, 64) with torch.device(\"cuda\"): Z = numpy_fn(X, Y) assert isinstance(Z, np.ndarray) In this example, numpy_fn will be executed in CUDA. For this to be possible, torch.compile automatically moves X and Y from CPU to CUDA, and then it moves the result Z from CUDA to CPU. If we are executing this function several times in the same program run, we may want to avoid all these rather expensive memory copies. To do so, we just need to tweak our numpy_fn so that it accepts cuda Tensors and returns tensors. We can do so by using torch.compiler.wrap_numpy: @torch.compile(fullgraph=True) @torch.compiler.wrap_numpy def numpy_fn(X, Y): return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)) X = torch.randn(1024, 64, device=\"cuda\") Y = torch.randn(1024, 64, device=\"cuda\") Z = numpy_fn(X, Y) assert isinstance(Z, torch.Tensor) assert Z.device.type == \"cuda\" Here, we explicitly create the tensors in CUDA memory, and pass them to the function, which performs all the computations on the CUDA device. wrap_numpy is in charge of marking any torch.Tensor input as an input with np.ndarray semantics at a torch.compile level. Marking tensors inside the compiler is a very cheap operation, so no data copy or data movement happens during runtime. Using this decorator, we can also differentiate through NumPy code! @torch.compile(fullgraph=True) @torch.compiler.wrap_numpy def numpy_fn(X, Y): return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))) X = torch.randn(1024, 64, device=\"cuda\", requires_grad=True) Y = torch.randn(1024, 64, device=\"cuda\") Z = numpy_fn(X, Y) assert isinstance(Z, torch.Tensor) Z.backward()", "prev_chunk_id": "chunk_742", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_744", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Can I execute NumPy code on CUDA and compute gradients via torch.compile?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Can I execute NumPy code on CUDA and compute gradients via torch.compile?#", "content": "# X.grad now holds the gradient of the computation print(X.grad) We have been using fullgraph=True as graph break are problematic in this context. When a graph break occurs, we need to materialize the NumPy arrays. Since NumPy arrays do not have a notion of device or requires_grad, this information is lost during a graph break. We cannot propagate gradients through a graph break, as the graph break code may execute arbitrary code that don’t know how to differentiate. On the other hand, in the case of the CUDA execution, we can work around this problem as we did in the first example, by using the torch.device(\"cuda\") context manager: @torch.compile @torch.compiler.wrap_numpy def numpy_fn(X, Y): prod = X[:, :, None] * Y[:, None, :] print(\"oops, a graph break!\") return np.sum(prod, axis=(-2, -1)) X = torch.randn(1024, 64, device=\"cuda\") Y = torch.randn(1024, 64, device=\"cuda\") with torch.device(\"cuda\"): Z = numpy_fn(X, Y) assert isinstance(Z, torch.Tensor) assert Z.device.type == \"cuda\" During the graph break, the intermediary tensors still need to be moved to CPU, but when the tracing is resumed after the graph break, the rest of the graph is still traced on CUDA. Given this CUDA <> CPU and CPU <> CUDA movement, graph breaks are fairly costly in the NumPy context and should be avoided, but at least they allow tracing through complex pieces of code.", "prev_chunk_id": "chunk_743", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_745", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "How do I debug NumPy code under torch.compile?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "How do I debug NumPy code under torch.compile?#", "content": "How do I debug NumPy code under torch.compile?# Debugging JIT compiled code is challenging, given the complexity of modern compilers and the daunting errors that they raise. The torch.compile troubleshooting doc contains a few tips and tricks on how to tackle this task. If the above is not enough to pinpoint the origin of the issue, there are still a few other NumPy-specific tools we can use. We can discern whether the bug is entirely in the PyTorch code by disabling tracing through NumPy functions: from torch._dynamo import config config.trace_numpy = False If the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without torch.compile) using PyTorch as a backend by importing import torch._numpy as np. This should just be used for debugging purposes and is in no way a replacement for the PyTorch API, as it is much less performant and, as a private API, may change without notice. At any rate, torch._numpy is a Python implementation of NumPy in terms of PyTorch and it is used internally by torch.compile to transform NumPy code into Pytorch code. It is rather easy to read and modify, so if you find any bug in it feel free to submit a PR fixing it or simply open an issue. If the program does work when importing torch._numpy as np, chances are that the bug is in TorchDynamo. If this is the case, please feel free to open an issue with a minimal reproducer.", "prev_chunk_id": "chunk_744", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_746", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "I torch.compile some NumPy code and I did not see any speed-up.#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "I torch.compile some NumPy code and I did not see any speed-up.#", "content": "I torch.compile some NumPy code and I did not see any speed-up.# The best place to start is the tutorial with general advice for how to debug these sort of torch.compile issues. Some graph breaks may happen because of the use of unsupported features. See Which NumPy features does torch.compile support?. More generally, it is useful to keep in mind that some widely used NumPy features do not play well with compilers. For example, in-place modifications make reasoning difficult within the compiler and often yield worse performance than their out-of-place counterparts.As such, it is best to avoid them. Same goes for the use of the out= parameter. Instead, prefer out-of-place ops and let torch.compile optimize the memory use. Same goes for data-dependent ops like masked indexing through boolean masks, or data-dependent control flow like if or while constructions.", "prev_chunk_id": "chunk_745", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_747", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "Which API to use for fine grain tracing?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Which API to use for fine grain tracing?#", "content": "Which API to use for fine grain tracing?# In some cases, you might need to exclude small parts of your code from the torch.compile compilations. This section provides some of the answers and you can find more information in TorchDynamo APIs for fine-grained tracing.", "prev_chunk_id": "chunk_746", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_748", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "How do I graph break on a function?#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "How do I graph break on a function?#", "content": "How do I graph break on a function?# Graph break on a function is not enough to sufficiently express what you want PyTorch to do. You need to be more specific about your use case. Some of the most common use cases you might want to consider: - If you want to disable compilation on this function frame and the recursively invoked frames, usetorch._dynamo.disable. - If you want a particular operator, such asfbgemmto use the eager mode, usetorch._dynamo.disallow_in_graph. Some of the uncommon use cases include: - If you want to disable TorchDynamo on the function frame but enable it back on the recursively invoked frames – usetorch._dynamo.disable(recursive=False). - If you want to prevent inlining of a function frame – usetorch._dynamo.graph_breakat the beginning of the function you want to prevent inlining.", "prev_chunk_id": "chunk_747", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_749", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "What’s the difference between torch._dynamo.disable and torch._dynamo.disallow_in_graph#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "What’s the difference between torch._dynamo.disable and torch._dynamo.disallow_in_graph#", "content": "What’s the difference between torch._dynamo.disable and torch._dynamo.disallow_in_graph# Disallow-in-graph works at the level of operators, or more specifically, the operators that you see in the TorchDynamo extracted graphs. Disable works at the function frame level and decides if TorchDynamo should look into the function frame or not.", "prev_chunk_id": "chunk_748", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_750", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_faq.html", "title": "What’s the difference between torch._dynamo.disable and torch._dynamo_skip#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "What’s the difference between torch._dynamo.disable and torch._dynamo_skip#", "content": "What’s the difference between torch._dynamo.disable and torch._dynamo_skip# You most likely need torch._dynamo.disable. But in an unlikely scenario, you might need even finer control. Suppose you want to disable the tracing on just the a_fn function, but want to continue the tracing back in aa_fn and ab_fn. The image below demonstrates this use case: In this case, you can use torch._dynamo.disable(recursive=False). In previous versions, this functionality was provided by torch._dynamo.skip. This is now supported by the recursive flag inside torch._dynamo.disable.", "prev_chunk_id": "chunk_749", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_751", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Profiling to understand torch.compile performance#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Profiling to understand torch.compile performance#", "content": "Profiling to understand torch.compile performance# Created On: Jun 06, 2023 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_752", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "What to use torch.profiler for:#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "What to use torch.profiler for:#", "content": "What to use torch.profiler for:# torch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and resources utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance. To understand kernel-level performance, other tools exist, such as Nvidia Nsight compute tool, AMD Omnitrace, Intel® VTune™ Profiler or inductor’s profiling tools can be used. See also the general pytorch profiler guide.", "prev_chunk_id": "chunk_751", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_753", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Basics of using torch.profiler and viewing traces#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Basics of using torch.profiler and viewing traces#", "content": "Basics of using torch.profiler and viewing traces# Example program: We’ll use this example of profiling resnet18. Notice the following parts of this example program: - Include a warm-up run to wait for compilation to complete (this will warm up systems like the CUDA caching allocator) - Usetorch.profiler.profile()context for profiling the section we are interested in - Useprof.export_chrome_trace(\"trace.json\")to export the profiling artifact. import torch from torchvision.models import resnet18 device = 'cuda' # or 'cpu', 'xpu', etc. model = resnet18().to(device) inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)] model_c = torch.compile(model) def fwd_bwd(inp): out = model_c(inp) out.sum().backward() # warm up fwd_bwd(inputs[0]) with torch.profiler.profile() as prof: for i in range(1, 4): fwd_bwd(inputs[i]) prof.step() prof.export_chrome_trace(\"trace.json\") Viewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the “w” and “s” keys to zoom in and out, and use “a” and “d” to scroll left and right. “?” will show a “help” screen with a list of shortcuts. Here, we observe: - CompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions. - CPU events at the top, and GPU events at the bottom. Flows between CPU and accelerator events Every kernel on the accelerator occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. “flows”) between the accelerator and CPU events to show which CPU event launched a accelerator kernel. This is particularly helpful because, with a few exceptions, accelerator kernels are launched asynchronously. To view a flow connection, click on a GPU kernel and click “ac2g”: Alternatively, turn on all flows with the “Flow events” dropdown at the top.", "prev_chunk_id": "chunk_752", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_754", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Working around CUDA Graph profiling issues#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Working around CUDA Graph profiling issues#", "content": "Working around CUDA Graph profiling issues# When CUDA graphs are enabled, some CUDA configurations (driver version under 525.85.12 or CUDA < 12) can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program: import torch torch.profiler._utils._init_for_cuda_graphs() # ... rest of program", "prev_chunk_id": "chunk_753", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_755", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Understanding compilation time#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Understanding compilation time#", "content": "Understanding compilation time# To understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool. Note: roughly the same information can also be obtained in non-graphical format with :code:torch._dynamo.utils.compile_times(). This utility won’t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead. See an example below: import torch from torchvision.models import resnet18 # user can switch between cuda and xpu device = 'cuda' model = resnet18().to(device) inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)] model_c = torch.compile(model) def fwd_bwd(inp): out = model_c(inp) out.sum().backward() def warmup_compile(): def fn(x): return x.sin().relu() x = torch.rand((2, 2), device=device, requires_grad=True) fn_c = torch.compile(fn) out = fn_c(x) out.sum().backward() with torch.profiler.profile() as prof: with torch.profiler.record_function(\"warmup compile\"): warmup_compile() with torch.profiler.record_function(\"resnet18 compile\"): fwd_bwd(inputs[0]) prof.export_chrome_trace(\"trace_compile.json\") Note a few things: - The first invocation should occurduringprofiling in order to capture compilation - Add a warm-up compilation in order to initialize any systems that need to be lazily initialized.", "prev_chunk_id": "chunk_754", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_756", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Finding graph breaks: “Torch-Compiled Region” and “CompiledFunction”#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Finding graph breaks: “Torch-Compiled Region” and “CompiledFunction”#", "content": "Finding graph breaks: “Torch-Compiled Region” and “CompiledFunction”# Although there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying :ref:graph breaks <torch.compiler_graph_breaks>. There are two profiler events to look for: Torch-Compiled Region and CompiledFunction. Torch-Compiled Region - which was introduced in PyTorch 2.2 - is a profiler event that covers the entire compiled region. Graph breaks almost always look the same: nested “Torch-Compiled Region” events. If you run two separate functions with torch.compile() applied independently on each of them, you should generally expect to see two adjacent (i.e NOT stacked/nested) Torch-Compiled regions. Meanwhile, if you encounter graph breaks (or disable()’ed/skipped regions), expect nested “Torch-Compiled Region” events. CompiledFunction - introduced in PyTorch 2.0 - is a profiler event that appears when gradients are required for any inputs. Each graph break will interrupt a CompiledFunction block, splitting it in two. CompiledFunction events only appear when Autograd is involved, i.e. some of the input tensors to the graph have requires_grad=True. When a CompiledFunction appears in a trace, it is typically paired with a CompiledFunctionBackward event in the backward pass. A “fwd-bwd link” should appear in the trace connecting the two, if the backward function is called. If your use case includes a graph that doesn’t require grad and doesn’t include “Torch-Compiled Region” events, it can be more difficult to identify whether torch.compile is being applied correctly. One clue can be the existence of Inductor-generated Triton kernels. See the synthetic example below for a demonstration: import torch import torch._dynamo # user can switch between cuda and xpu device = 'cuda' class ModelWithBreaks(torch.nn.Module): def __init__(self): super().__init__() def create_sequential(): return torch.nn.Sequential( torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), ) self.mod1 = create_sequential() self.mod2 = create_sequential() self.mod3 = create_sequential() self.mod4 = create_sequential() def forward(self, inp): mod1 = self.mod1(inp) torch._dynamo.graph_break() mod2 = self.mod2(mod1)", "prev_chunk_id": "chunk_755", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_757", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Finding graph breaks: “Torch-Compiled Region” and “CompiledFunction”#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Finding graph breaks: “Torch-Compiled Region” and “CompiledFunction”#", "content": "torch._dynamo.graph_break() mod3 = self.mod3(mod2) torch._dynamo.graph_break() mod4 = self.mod4(mod3) return mod4 model = ModelWithBreaks().to(device) inputs = [torch.randn((128, 128), device=device) for _ in range(10)] model_c = torch.compile(model) def fwd_bwd(inp): out = model_c(inp) out.sum().backward() # warm up fwd_bwd(inputs[0]) with torch.profiler.profile() as prof: for i in range(1, 4): fwd_bwd(inputs[i]) prof.step() prof.export_chrome_trace(\"trace_break.json\")", "prev_chunk_id": "chunk_756", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_758", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Operator Kernels#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Operator Kernels#", "content": "Operator Kernels# When an operator is launched, we expect to see a few events: - CPU-side event - Kernel launch (if dealing with a GPU kernel) - GPU-side event Inductor-generated Triton kernels: - TheCPU-side eventshould appear as an event prefixed with “triton_”. The events currently have minimal information - the kernel name and a launch, but less information than typical aten kernel launches (which contain input shapes, types, etc.). - Thekernel launchshould appear as cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops) - TheGPU-side eventshould appear, and how descriptive the name will be depends on the inductor config for unique_kernel_names Non-Inductor generated Triton kernels: - TheCPU-sideevent may not appear in traces; the machinery for automatically inserting a profiler event is currently implemented at the Inductor level, so Triton kernels that bypass Inductor may not appear in traces, unless users have annotated them manually - Thekernel launchshould appear s cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops) - TheGPU-sideevent should appear, named similarly to the triton kernel that was authored. Inductor-generated CPU kernels: - TheCPU-side eventwill not appear in traces; we haven’t added profiling for this yet. - Thekernel launchandGPU-side eventsdon’t exist Non-Triton kernels (i.e. aten kernels or custom ops) should also be expected to sometimes appear in traces. Sometimes, Inductor will fall back to the original op implementation, in which case you will see a call to the aten op.", "prev_chunk_id": "chunk_757", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_759", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html", "title": "Launch overhead#", "page_title": "Profiling to understand torch.compile performance — PyTorch 2.8 documentation", "breadcrumbs": "Launch overhead#", "content": "Launch overhead# One common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU: This is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes. When using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern.", "prev_chunk_id": "chunk_758", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_760", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html", "title": "TorchInductor GPU Profiling#", "page_title": "TorchInductor GPU Profiling — PyTorch 2.8 documentation", "breadcrumbs": "TorchInductor GPU Profiling#", "content": "TorchInductor GPU Profiling# Created On: Jul 28, 2023 | Last Updated On: Jun 10, 2025 This section lists useful commands and workflows that can help you dive into a model’s performance in TorchInductor. When a model is not running as fast as expected, you may want to check individual kernels of the model. Usually, those kernels taking the majority of the GPU time are the most interesting ones. After that, you may also want to run individual kernels directly and inspect its perf. PyTorch provides tools to cover everything mentioned above.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_761", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html", "title": "Relevant Environment Variables#", "page_title": "TorchInductor GPU Profiling — PyTorch 2.8 documentation", "breadcrumbs": "Relevant Environment Variables#", "content": "Relevant Environment Variables# You can use the following environment variables in your analysis: - TORCHINDUCTOR_UNIQUE_KERNEL_NAMESBy default, TorchInductor names a Triton kernel as‘triton\\_’. When this environmental variable is enabled, inductor generates a more meaningful kernel name in the trace, for example,triton_poi_fused_cat_155which contains the kernel category (poifor pointwise) and original ATen operator. This config is disabled by default to improve the chance of compilation cache hit. - TORCHINDUCTOR_BENCHMARK_KERNELEnabling this will make inductor codegen harness to benchmark individual triton kernels. - TORCHINDUCTOR_MAX_AUTOTUNEInductor autotuner will benchmark moretriton.Configsand pick the one with the best performance results. This will increase compilation time with the hope to improve performance.", "prev_chunk_id": "chunk_760", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_762", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html", "title": "Breakdown Model GPU Time#", "page_title": "TorchInductor GPU Profiling — PyTorch 2.8 documentation", "breadcrumbs": "Breakdown Model GPU Time#", "content": "Breakdown Model GPU Time# Below are the steps to breakdown execution time of a model into individual kernels. We take mixnet_l as an example. - Run the benchmark script for the model:TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1TORCHINDUCTOR_BENCHMARK_KERNEL=1python-ubenchmarks/dynamo/timm_models.py–backendinductor–amp–performance–dashboard–onlymixnet_l–disable-cudagraphs–trainingNoteThe tool relies on kernel name to decide its category. EnablingTORCHINDUCTOR_UNIQUE_KERNEL_NAMESis crucial for that. - In the output log, look for lines:**Compiledmodulepath:/tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py** We have one line for each compiled module. If there are no extra graph breaks, we would see 2 such lines in the log, one for the forward graph and one for the backward graph. For our example command, we get the following compiled module for the forward and backward graphs respectively: - Forward graph compiled module - Backward graph compiled module - Now we can dive into the perf for each individual compiled module. Let’s pick the one for the forward graph for illustration purposes. I’ll name itfwd.pyfor convenience. Run it directly with the-pargument:**>pythonfwd.py-p** See the full output log in this example gist In the output, you can notice the following: - We write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.Chrome trace for the profile is written to /tmp/compiled_module_profile.jsonLoading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:You can zoom in and out to check the profile. - We report the percent of GPU time regarding to the wall time by log line like:Percent of time when GPU is busy: 102.88%Sometimes you may see a value larger than 100%. The reason is because PyTorch uses the kernel execution time with profiling enabled while using wall time with profiling disabled. Profiling may distort the kernel execution time", "prev_chunk_id": "chunk_761", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_763", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html", "title": "Breakdown Model GPU Time#", "page_title": "TorchInductor GPU Profiling — PyTorch 2.8 documentation", "breadcrumbs": "Breakdown Model GPU Time#", "content": "a bit. But overall it should not be a big deal.If we run the model likedensenet121with a small batch size, we would see low percent of time when GPU is busy:(Forwardgraph)PercentoftimewhenGPUisbusy:32.69%This means the model has a lot of CPU overhead. This is consistent with the fact that enabling cudagraphs improve densenet121’s perf a lot. - We can break down the GPU time to different categories of kernels. In themixnet_lexample, we seepointwise kernel takes 28.58%reduction kernel takes 13.85%persistent reduction kernel takes 3.89%the rest are cutlass/cudnn kernels for mm/conv which takes 56.57%This information can be found in the summary line (last line) of the report for each kernel category. - We also call zoom into a certain category of kernels. For example, let’s check reduction kernels:We can see an ordered table of execution time for each individual reduction kernel. We also see how many times a kernel is executed. This is helpful for a few reasons:If a kernel only takes a tiny amount of time, for example, 0.1%, improving it will at most bring 0.1% overall gain. It is not worth spending a lot of effort on it.Ff a kernel takes 2% of time, improving it by 2x will bring in 1% overall gain which justifies the effort.", "prev_chunk_id": "chunk_762", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_764", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html", "title": "Benchmark Individual Triton Kernel#", "page_title": "TorchInductor GPU Profiling — PyTorch 2.8 documentation", "breadcrumbs": "Benchmark Individual Triton Kernel#", "content": "Benchmark Individual Triton Kernel# Let’s say we want to take a closer look at triton_red_fused\\__native_batch_norm_legit_functional_16 which is the most expensive reduction kernel and takes 2.19% of overall wall time for the forward graph. We can lookup the kernel name in the fwd.py, and find comment like: # kernel path: /tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py I’ll rename it k.py for convenience. Here is a paste for this file. k.py is a standalone Python module containing the kernel code and its benchmark. Run k.py directly will report its execution time and bandwidth: We can check if max-autotune helps this kernel, by running: **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py** We may also temporarily add more reduction heuristics and run the script again to check how that helps with the kernel.", "prev_chunk_id": "chunk_763", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_765", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html", "title": "TorchDynamo APIs for fine-grained tracing#", "page_title": "TorchDynamo APIs for fine-grained tracing — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo APIs for fine-grained tracing#", "content": "TorchDynamo APIs for fine-grained tracing# Created On: Jul 28, 2023 | Last Updated On: Jun 16, 2025 torch.compile performs TorchDynamo tracing on the whole user model. However, it is possible that a small part of the model code cannot be handled by torch.compiler. In this case, you might want to disable the compiler on that particular portion, while running compilation on the rest of the model. This section describe the existing APIs that use to define parts of your code in which you want to skip compilation and the relevant use cases. The API that you can use to define portions of the code on which you can disable compilation are listed in the following table:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_766", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html", "title": "torch.compiler.disable#", "page_title": "TorchDynamo APIs for fine-grained tracing — PyTorch 2.8 documentation", "breadcrumbs": "torch.compiler.disable#", "content": "torch.compiler.disable# torch.compiler.disable disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame. TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function fn calls functions a_fn and b_fn. And a_fn calls aa_fn and ab_fn. When you use the PyTorch eager mode rather than torch.compile, these function frames run as is. With torch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color): Let’s imagine, that function a_fn is causing troubles with torch.compile. And this is a non-critical portion of the model. You can use compiler.disable on function a_fn. As shown above, TorchDynamo will stop looking at frames originating from the a_fn call (white color indicates original Python behavior). To skip compilation, you can decorate the offending function with @torch.compiler.disable. You can also use the non-decorator syntax if you don’t want to change the source code However, we recommend that you avoid this style if possible. Here, you have to take care that all users of the original function are now using the patched version.", "prev_chunk_id": "chunk_765", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_767", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html", "title": "torch._dynamo.disallow_in_graph#", "page_title": "TorchDynamo APIs for fine-grained tracing — PyTorch 2.8 documentation", "breadcrumbs": "torch._dynamo.disallow_in_graph#", "content": "torch._dynamo.disallow_in_graph# torch._dynamo.disallow_in_graph disallows an operator but not the function to be present in the TorchDynamo extracted graph. Note that this is suitable for operators and not general functions as in the case of _dynamo.disable. Let’s imagine you compile your model with PyTorch. TorchDynamo is able to extract a graph, but then you see the downstream compiler failing. For example, the meta kernel is missing, or some Autograd dispatch key is set incorrectly for a particular operator. Then you can mark that operator as disallow_in_graph, and TorchDynamo will cause a graph break and run that operator by using the PyTorch eager mode. The catch is that you will have to find the corresponding Dynamo level operator, and not the ATen level operator. See more in the Limitations section of the doc.", "prev_chunk_id": "chunk_766", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_768", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html", "title": "torch.compiler.allow_in_graph#", "page_title": "TorchDynamo APIs for fine-grained tracing — PyTorch 2.8 documentation", "breadcrumbs": "torch.compiler.allow_in_graph#", "content": "torch.compiler.allow_in_graph# torch.compiler.allow_in_graph is useful when the relevant function frame has some known hard-to-support TorchDynamo feature, such as hooks and autograd.Function, and you are confident that downstream PyTorch components such as AOTAutograd can safely trace through the decorated function. When a function is decorated with allow_in_graph, TorchDynamo treats it as a black-box and puts it as is in the generated graph.", "prev_chunk_id": "chunk_767", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_769", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html", "title": "Limitations#", "page_title": "TorchDynamo APIs for fine-grained tracing — PyTorch 2.8 documentation", "breadcrumbs": "Limitations#", "content": "Limitations# All the existing APIs are applied at the TorchDynamo level. Therefore, these APIs have visibility to only what TorchDynamo sees. This can lead to confusing scenarios. For example, torch._dynamo.disallow_in_graph will not work for ATen operators because they are visible to AOT Autograd. For example, torch._dynamo.disallow_in_graph(torch.ops.aten.add) will not work in the above example.", "prev_chunk_id": "chunk_768", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_770", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "PyTorch 2.0 Troubleshooting (old)#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch 2.0 Troubleshooting (old)#", "content": "PyTorch 2.0 Troubleshooting (old)# Created On: Jun 06, 2025 | Last Updated On: Jun 06, 2025 Author: Michael Lazos We are actively developing debug tools, profilers, and improving our error and warning messages. Below is a table of the available tools and their typical usage. For additional help see Diagnosing Runtime Errors. In addition to info and debug logging, you can use torch._logging for more fine-grained logging.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_771", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Diagnosing Runtime Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Diagnosing Runtime Errors#", "content": "Diagnosing Runtime Errors# At a high level, the TorchDynamo stack consists of a graph capture from Python code (TorchDynamo) and a backend compiler. For example, a backend compiler may consist of backward graph tracing (AOTAutograd) and graph lowering (TorchInductor)*. Errors can occur in any component of the stack and will provide full stack traces. To determine in which component an error occurred, you may use info-level logging torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\" and look for Step #: ... outputs. Logs are made at the beginning and end of each step, so the step that an error should correspond to is the most recently logged step whose end has not yet been logged. The steps correspond to the following parts of the stack: If info logging is insufficient, you can use available backend options. These options include: - \"eager\": only runs TorchDynamo forward graph capture and then runs the captured graph with PyTorch. This provides an indication as to whether TorchDynamo is raising the error. - \"aot_eager\": runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph without any additional backend compiler steps. PyTorch eager will then be used to run the forward and backward graphs. This is useful to narrow down the issue to AOTAutograd. The general procedure to narrow down an issue is the following: - Run your program with the\"eager\"backend. If the error no longer occurs, the issue is in the backend compiler that is being used (if using TorchInductor, proceed to step 2. If not, seeMinifying Backend Compiler Errors). If the error still occurs with the\"eager\"backend, it is due toTorchdynamo Errors. - This step is only necessary ifTorchInductoris used as the backend compiler. Run the model with the\"aot_eager\"backend. If this backend raises an error then the error is occurring during AOTAutograd tracing.", "prev_chunk_id": "chunk_770", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_772", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Diagnosing Runtime Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Diagnosing Runtime Errors#", "content": "If the error no longer occurs with this backend, thenMinifying TorchInductor Errors. Each of these cases are analyzed in the following sections.", "prev_chunk_id": "chunk_771", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_773", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Torchdynamo Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Torchdynamo Errors#", "content": "Torchdynamo Errors# If the error that is generated occurs with the \"eager\" backend, then TorchDynamo is most likely the source of the error. Here is a sample code which will generate an error. import torch import torch._dynamo as dynamo def test_assertion_error(): y = torch.ones(200, 200) z = {y: 5} return z compiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\") compiled_test_assertion_error() The code above generates the following error: torch._dynamo.convert_frame: [ERROR] WON'T CONVERT test_assertion_error /scratch/mlazos/torchdynamo/../test/errors.py line 26 due to: Traceback (most recent call last): File \"/scratch/mlazos/torchdynamo/torchdynamo/symbolic_convert.py\", line 837, in BUILD_MAP assert isinstance(k, ConstantVariable) or ( AssertionError from user code: File \"/scratch/mlazos/torchdynamo/../test/errors.py\", line 34, in test_assertion_error z = {y: 5} Set torch._dynamo.config.verbose=True for more information ========== As the message suggests you can set torch._dynamo.config.verbose=True to get a full stack trace to both the error in TorchDynamo and the user code. In addition to this flag, you can also set the log_level of TorchDynamo through torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\". These levels include: - logging.DEBUGorTORCH_LOGS=\"+dynamo\": Print every instruction that is encountered in addition to all the log levels listed below. - logging.INFO: Print each function that is compiled (original and modified bytecode) and the graph that is captured in addition to all the log levels listed below. - logging.WARNING(default): Print graph breaks in addition to all the log levels listed below. - logging.ERROR: Print errors only. If a model is very large, the logs can become overwhelming. If an error occurs deep within a model’s Python code, it can be useful to execute only the frame in which the error occurs to enable easier debugging. There are two tools available to enable this: - Setting the environment variableTORCHDYNAMO_DEBUG_FUNCTIONto the desired function name will only run torchdynamo on functions with that name. - Enabling the record/replay tool (settorch._dynamo.config.replay_record_enabled=True) which dumps an execution record when an error is encountered. This", "prev_chunk_id": "chunk_772", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_774", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Torchdynamo Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Torchdynamo Errors#", "content": "record can then be replayed to run only the frame where an error occurred.", "prev_chunk_id": "chunk_773", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_775", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Diagnosing TorchInductor Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Diagnosing TorchInductor Errors#", "content": "Diagnosing TorchInductor Errors# If the error does not occur with the \"eager\" backend, then the backend compiler is the source of the error (example error). There are different choices for backend compilers for TorchDynamo, with TorchInductor fitting the needs of most users. This section focuses on TorchInductor as the motivating example, but some tools can also be used with other backend compilers. Below is the portion of the stack which we are focusing on: With TorchInductor as the chosen backend, AOTAutograd is used to generate the backward graph from the forward graph captured by torchdynamo. It is important to note that errors can occur during this tracing and also while TorchInductor lowers the forward and backward graphs to GPU code or C++. A model can often consist of hundreds or thousands of FX nodes, so narrowing the exact nodes where this problem occurred can be very difficult. Fortunately, there are tools available to automatically minify these input graphs to the nodes which are causing the issue. The first step is to determine whether the error occurs during tracing of the backward graph with AOTAutograd or during TorchInductor lowering. As mentioned above in step 2, the \"aot_eager\" backend can be used to run only AOTAutograd in isolation without lowering. If the error still occurs with this backend, this indicates that the error is occurring during AOTAutograd tracing. Here is an example: import torch import torch._dynamo as dynamo model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)]) def test_backend_error(): y = torch.ones(200, 200) x = torch.ones(200, 200) z = x + y a = torch.ops.aten._foobar(z) # dummy function which errors return model(a) compiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\") compiled_test_backend_error() Running this should give you this error with a longer stack trace below it: Traceback (most recent call last): File \"/scratch/mlazos/torchdynamo/torchinductor/graph.py\", line 246, in call_function return", "prev_chunk_id": "chunk_774", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_776", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Diagnosing TorchInductor Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Diagnosing TorchInductor Errors#", "content": "lowerings[target](*args, **kwargs) File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 185, in wrapped return decomp_fn(*args, **kwargs) File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 810, in _foobar assert False AssertionError ... error with full stack trace If you then change torch.compile(backend=\"inductor\") to torch.compile(backend=\"aot_eager\"), it will run without error, because the issue is in the TorchInductor lowering process, not in AOTAutograd.", "prev_chunk_id": "chunk_775", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_777", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Minifying TorchInductor Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Minifying TorchInductor Errors#", "content": "Minifying TorchInductor Errors# From here, let’s run the minifier to get a minimal repro. Setting the environment variable TORCHDYNAMO_REPRO_AFTER=\"aot\" (or setting torch._dynamo.config.repro_after=\"aot\" directly) will generate a Python program which reduces the graph produced by AOTAutograd to the smallest subgraph which reproduces the error. (See below for an example where we minify the graph produced by TorchDynamo) Running the program with this environment variable should show nearly identical output, with an additional line indicating where minifier_launcher.py has been written to. The output directory is configurable by setting torch._dynamo.config.base_dir to a valid directory name. The final step is to run the minifier and check that it runs successfully. A successful run looks like this. If the minifier runs successfully, it generates runnable python code which reproduces the exact error. For our example this is the following code: import torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf from torch.fx.experimental.proxy_tensor import make_fx # torch version: 1.13.0a0+gitfddfc44 # torch cuda version: 11.6 # torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5 # CUDA Info: # nvcc: NVIDIA (R) Cuda compiler driver # Copyright (c) 2005-2022 NVIDIA Corporation # Built on Thu_Feb_10_18:23:41_PST_2022 # Cuda compilation tools, release 11.6, V11.6.112 # Build cuda_11.6.r11.6/compiler.30978841_0 # GPU Hardware Info: # NVIDIA A100-SXM4-40GB : 8 from torch.nn import * class Repro(torch.nn.Module): def __init__(self): super().__init__() def forward(self, add): _foobar = torch.ops.aten._foobar.default(add); add = None return (_foobar,) args = [((200, 200), (200, 1), torch.float32, 'cpu')] args = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args] mod = make_fx(Repro())(*args) from torch._inductor.compile_fx import compile_fx_inner compiled = compile_fx_inner(mod, args) compiled(*args) The forward method of the Repro module contains the exact op which causes the issue. When filing an issue, please include any minified repros to aid in debugging.", "prev_chunk_id": "chunk_776", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_778", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Minifying Backend Compiler Errors#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Minifying Backend Compiler Errors#", "content": "Minifying Backend Compiler Errors# With backend compilers other than TorchInductor the process for finding the subgraph causing the error is nearly identical to the procedure in Minifying TorchInductor Errors with one important caveat. Namely, that the minifier will now be run on the graph that is traced by TorchDynamo, not the output graph of AOTAutograd. Let’s walk through an example. import torch import torch._dynamo as dynamo model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)]) # toy compiler which fails if graph contains relu def toy_compiler(gm: torch.fx.GraphModule, _): for node in gm.graph.nodes: if node.target == torch.relu: assert False return gm def test_backend_error(): y = torch.ones(200, 200) x = torch.ones(200, 200) z = x + y a = torch.relu(z) return model(a) compiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler) compiled_test_backend_error() In order to run the code after TorchDynamo has traced the forward graph, you can use the TORCHDYNAMO_REPRO_AFTER environment variable. Running this program with TORCHDYNAMO_REPRO_AFTER=\"dynamo\" (or torch._dynamo.config.repro_after=\"dynamo\") should produce this outputand the following code in {torch._dynamo.config.base_dir}/repro.py. import torch import torch._dynamo as dynamo from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf from torch._dynamo.debug_utils import run_fwd_maybe_bwd from torch.nn import * class Repro(torch.nn.Module): def __init__(self): super().__init__() def forward(self, add): relu = torch.relu(add); add = None return (relu,) mod = Repro().cuda() opt_mod = torch.compile(mod, backend=\"None\") args = [((200, 200), (200, 1), torch.float32, 'cpu', False)] args = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args] with torch.cuda.amp.autocast(enabled=False): ref = run_fwd_maybe_bwd(mod, args) res = run_fwd_maybe_bwd(opt_mod, args) The minifier successfully reduced the graph to the op that raises the error in toy_compiler. The other difference from the procedure in Minifying TorchInductor Errors is that the minifier is automatically run after encountering a backend compiler error. After a successful run, the minifier writes repro.py to torch._dynamo.config.base_dir.", "prev_chunk_id": "chunk_777", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_779", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Accessing TorchDynamo Profiler#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Accessing TorchDynamo Profiler#", "content": "Accessing TorchDynamo Profiler# TorchDynamo has a built-in stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by calling torch._dynamo.utils.compile_times() after executing Torch._Dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name.", "prev_chunk_id": "chunk_778", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_780", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "TorchInductor Debugging using TORCH_COMPILE_DEBUG#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "TorchInductor Debugging using TORCH_COMPILE_DEBUG#", "content": "TorchInductor Debugging using TORCH_COMPILE_DEBUG# TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump. This is a debugging tool designed to make it easier to understand and troubleshoot the internals of TorchInductor. Let’s run an example with the following test program (repro.py): import torch @torch.compile() def test_model(x): model = torch.nn.Sequential( torch.nn.Linear(10, 10), torch.nn.LayerNorm(10), torch.nn.ReLU(), ) return model(x) y = test_model(torch.ones(10, 10)) Setting the environment variable TORCH_COMPILE_DEBUG=1 will cause a debug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug (this can be overridden in the torchdynamo configuration field debug_dir_root and also the env var TORCH_COMPILE_DEBUG_DIR). Inside this directory, each run will have a separate folder named with the timestamp and process id of the run: $ env TORCH_COMPILE_DEBUG=1 python repro.py $ cd torch_compile_debug $ ls run_2023_03_01_08_20_52_143510-pid_180167 In the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor folder which contains a subfolder for each compiled kernel with inductor debug artifacts. $ cd run_2023_03_01_08_20_52_143510-pid_180167 $ ls torchinductor torchdynamo Moving further into the torchinductor directory, the \\*.log files are logs from the AOT Autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts. $ cd torchinductor $ ls aot_model___0_debug.log model__0_forward_1.0 $ cd model__0_forward_1.0 $ ls debug.log fx_graph_readable.py fx_graph_runnable.py fx_graph_transformed.py ir_post_fusion.txt ir_pre_fusion.txt output_code.py Here is a summary of the contents: - fx_graph_readable.pyandfx_graph_runnable.pyare the readable and runnable versions of thefx_graphreceived by inductor. - fx_graph_transformed.pyis the fx graph after inductor has run all fx passes. - ir\\*.txtis the inductor ir pre and post fusion. - output_code.pyis the compiled triton kernel for the subgraph. Here are example debug directory contents for the test program: import torch @torch.compile() def test_model(x): model = torch.nn.Sequential( torch.nn.Linear(10, 10), torch.nn.LayerNorm(10), torch.nn.ReLU(), )", "prev_chunk_id": "chunk_779", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_781", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "TorchInductor Debugging using TORCH_COMPILE_DEBUG#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "TorchInductor Debugging using TORCH_COMPILE_DEBUG#", "content": "return model(x) y = test_model(torch.ones(10, 10)) Each file in that debug trace can be enabled and disabled through torch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate. A single node in this new debug format looks like: buf1: SchedulerNode(ComputedBuffer) buf1.writes = { MemoryDep(name='buf1', index=0, size=()), MemoryDep(name='buf1', index=0, size=(s0,))} buf1.unmet_dependencies = {MemoryDep(name='buf0', index=c0, size=(s0,))} buf1.met_dependencies = {MemoryDep(name='primals_2', index=c0, size=(s0,))} buf1.group.device = cuda:0 buf1.group.iteration = (1, s0) buf1.sizes = ([], [s0]) class buf1_loop_body: var_ranges = {z0: s0} index0 = z0 index1 = 0 def body(self, ops): get_index = self.get_index('index0') load = ops.load('buf0', get_index, False) get_index_1 = self.get_index('index0') load_1 = ops.load('primals_2', get_index_1, False) add = ops.add(load, load_1) get_index_2 = self.get_index('index1') reduction = ops.reduction('buf1', torch.float32, torch.float32, 'sum', get_index_2, add) return reduction See the example debug directory output for more examples.", "prev_chunk_id": "chunk_780", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_782", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Graph Breaks#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Graph Breaks#", "content": "Graph Breaks# Given a program like this: def some_fun(x): ... compiled_fun = torch.compile(some_fun, ...) ... TorchDynamo will attempt to compile all of the torch/tensor operations within some_fun into a single FX graph, but it may fail to capture everything into one graph. Some graph break reasons are insurmountable to TorchDynamo, and can’t be easily fixed. - calling into a C extension other than torch is invisible to torchdynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards (see Making Dynamo Sound: Guards) to ensure that the compiled program would be safe to reuse. Graph breaks can hinder performance if the resulting fragments are small. To maximize performance, it’s important to have as few graph breaks as possible.", "prev_chunk_id": "chunk_781", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_783", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Identifying the Cause of a Graph Break#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Identifying the Cause of a Graph Break#", "content": "Identifying the Cause of a Graph Break# To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage: import torch import torch._dynamo as dynamo def toy_example(a, b): x = a / (torch.abs(a) + 1) print(\"woo\") if b.sum() < 0: b = b * -1 return x * b explanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10)) print(explanation_verbose) \"\"\" Graph Count: 3 Graph Break Count: 2 Op Count: 5 Break Reasons: Break Reason 1: Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False User Stack: <FrameSummary file foo.py, line 5 in toy_example> Break Reason 2: Reason: generic_jump TensorVariable() User Stack: <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5> Ops per Graph: ... Out Guards: ... \"\"\" Outputs include: - out_guards- a list of lists where each sublist contains the guards that must pass to ensure the traced graphs are valid. - graphs- a list of graph modules which were successfully traced. - ops_per_graph- a list of lists where each sublist contains the ops that are run in the graph. To throw an error on the first graph break encountered, use the fullgraph mode. This mode disables TorchDynamo’s Python fallback, and only succeeds if the entire program is convertible into a single graph. Example usage: def toy_example(a, b): ... compiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)", "prev_chunk_id": "chunk_782", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_784", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Excessive Recompilation#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Excessive Recompilation#", "content": "Excessive Recompilation# When TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up to torch._dynamo.config.recompile_limit times. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it. If your program exhibits a bounded amount of dynamism, you may be able to tune the TorchDynamo cache limit to allow for each variation to be compiled and cached, but if the cache limit is too high you may find the cost of recompilation outweighs any optimization benefits. torch._dynamo.config.recompile_limit = <your desired cache limit> TorchDynamo plans to support many common cases of dynamic tensor shapes, such as varying batch size or sequence length. It does not plan to support rank-dynamism. In the meantime, setting a specific cache limit can be used in coordination with bucketing techniques to achieve an acceptable number of recompilations for some dynamic models.", "prev_chunk_id": "chunk_783", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_785", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Accuracy Debugging#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Accuracy Debugging#", "content": "Accuracy Debugging# Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it’s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler. If you’d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True", "prev_chunk_id": "chunk_784", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_786", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Extended Debugging#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Extended Debugging#", "content": "Extended Debugging# Extended debugging can be enabled by using the following experimental flags. TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED - provides extended debug information if the string representation of a guard matches this flag value. For example, set it to “Ne(s0, 10)” to generate full Python and C++ backtrace whenever guard was issued. TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL - provides extended debug information when a particular symbol is allocated. For example, set this to “u2” to generate full Python and C++ backtrace whenever this symbol was created. TORCHDYNAMO_EXTENDED_DEBUG_CPP - provides extended debug information (C++ backtrace) for all extended debug settings as well as errors. For example, set this to “1”. The C++ backtrace is slow and very spammy so it is not included by default with extended debugging.", "prev_chunk_id": "chunk_785", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_787", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html", "title": "Cold Start Timing and Cache Corruption Debugging#", "page_title": "PyTorch 2.0 Troubleshooting (old) — PyTorch 2.8 documentation", "breadcrumbs": "Cold Start Timing and Cache Corruption Debugging#", "content": "Cold Start Timing and Cache Corruption Debugging# In order to measure the cold start compilation time or debug a cache corruption, it is possible pass TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 or set torch._inductor.config.force_disable_caches = True which will override any other caching config option and disable all compile time caching.", "prev_chunk_id": "chunk_786", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_788", "url": "https://docs.pytorch.org/docs/stable/torch.compiler.config.html", "title": "torch.compiler.config#", "page_title": "torch.compiler.config — PyTorch 2.8 documentation", "breadcrumbs": "torch.compiler.config#", "content": "torch.compiler.config# Created On: Nov 01, 2024 | Last Updated On: Jun 11, 2025 This is the top-level configuration module for the compiler, containing cross-cutting configuration options that affect all parts of the compiler stack. You may also be interested in the per-component configuration modules, which contain configuration options that affect only a specific part of the compiler: - torch._dynamo.config - torch._inductor.config - torch._functorch.config - torch.fx.experimental.config", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_789", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_api.html", "title": "torch.compiler API reference#", "page_title": "torch.compiler API reference — PyTorch 2.8 documentation", "breadcrumbs": "torch.compiler API reference#", "content": "torch.compiler API reference# Created On: Jun 02, 2023 | Last Updated On: Jun 22, 2025 For a quick overview of torch.compiler, see torch.compiler.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_790", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_get_started.html", "title": "Getting Started#", "page_title": "Getting Started — PyTorch 2.8 documentation", "breadcrumbs": "Getting Started#", "content": "Getting Started# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025 Before you read this section, make sure to read the torch.compiler let’s start by looking at a simple torch.compile example that demonstrates how to use torch.compile for inference. This example demonstrates the torch.cos() and torch.sin() features which are examples of pointwise operators as they operate element by element on a vector. This example might not show significant performance gains but should help you form an intuitive understanding of how you can use torch.compile in your own programs. import torch def fn(x): a = torch.cos(x) b = torch.sin(a) return b new_fn = torch.compile(fn, backend=\"inductor\") input_tensor = torch.randn(10000).to(device=\"cuda:0\") a = new_fn(input_tensor) A more famous pointwise operator you might want to use would be something like torch.relu(). Pointwise ops in eager mode are suboptimal because each one would need to read a tensor from the memory, make some changes, and then write back those changes. The single most important optimization that inductor performs is fusion. In the example above we can turn 2 reads (x, a) and 2 writes (a, b) into 1 read (x) and 1 write (b), which is crucial especially for newer GPUs where the bottleneck is memory bandwidth (how quickly you can send data to a GPU) rather than compute (how quickly your GPU can crunch floating point operations). Another major optimization that inductor provides is automatic support for CUDA graphs. CUDA graphs help eliminate the overhead from launching individual kernels from a Python program which is especially relevant for newer GPUs. TorchDynamo supports many different backends, but TorchInductor specifically works by generating Triton kernels. Let’s save our example above into a file called example.py. We can inspect the code generated Triton kernels by running TORCH_COMPILE_DEBUG=1 python example.py. As the script executes, you should see", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_791", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_get_started.html", "title": "Getting Started#", "page_title": "Getting Started — PyTorch 2.8 documentation", "breadcrumbs": "Getting Started#", "content": "DEBUG messages printed to the terminal. Closer to the end of the log, you should see a path to a folder that contains torchinductor_<your_username>. In that folder, you can find the output_code.py file that contains the generated kernel code similar to the following: @pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]}) @triton.jit def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr): xnumel = 10000 xoffset = tl.program_id(0) * XBLOCK xindex = xoffset + tl.arange(0, XBLOCK)[:] xmask = xindex < xnumel x0 = xindex tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0) tmp1 = tl.cos(tmp0) tmp2 = tl.sin(tmp1) tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask) And you can verify that fusing the cos and sin did actually occur because the cos and sin operations occur within a single Triton kernel and the temporary variables are held in registers with very fast access. Read more on Triton’s performance here. Because the code is written in Python, it’s fairly easy to understand even if you have not written all that many CUDA kernels. Next, let’s try a real model like resnet50 from the PyTorch hub. import torch model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True) opt_model = torch.compile(model, backend=\"inductor\") opt_model(torch.randn(1,3,64,64)) And that is not the only available backend, you can run in a REPL torch.compiler.list_backends() to see all the available backends. Try out the cudagraphs next as inspiration.", "prev_chunk_id": "chunk_790", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_792", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_get_started.html", "title": "Using a pretrained model#", "page_title": "Getting Started — PyTorch 2.8 documentation", "breadcrumbs": "Using a pretrained model#", "content": "Using a pretrained model# PyTorch users frequently leverage pretrained models from transformers or TIMM and one of the design goals is TorchDynamo and TorchInductor is to work out of the box with any model that people would like to author. Let’s download a pretrained model directly from the HuggingFace hub and optimize it: import torch from transformers import BertTokenizer, BertModel # Copy pasted from here https://huggingface.co/bert-base-uncased tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\") model = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed text = \"Replace me by any text you'd like.\" encoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\") output = model(**encoded_input) If you remove the to(device=\"cuda:0\") from the model and encoded_input, then Triton will generate C++ kernels that will be optimized for running on your CPU. You can inspect both Triton or C++ kernels for BERT. They are more complex than the trigonometry example we tried above but you can similarly skim through it and see if you understand how PyTorch works. Similarly, let’s try out a TIMM example: import timm import torch model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2) opt_model = torch.compile(model, backend=\"inductor\") opt_model(torch.randn(64,3,7,7))", "prev_chunk_id": "chunk_791", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_793", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_get_started.html", "title": "Next Steps#", "page_title": "Getting Started — PyTorch 2.8 documentation", "breadcrumbs": "Next Steps#", "content": "Next Steps# In this section, we have reviewed a few inference examples and developed a basic understanding of how torch.compile works. Here is what you check out next: - torch.compile tutorial on training - torch.compiler API reference - TorchDynamo APIs for fine-grained tracing", "prev_chunk_id": "chunk_792", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_794", "url": "https://docs.pytorch.org/docs/stable/elastic/kubernetes.html", "title": "TorchElastic Kubernetes#", "page_title": "TorchElastic Kubernetes — PyTorch 2.8 documentation", "breadcrumbs": "TorchElastic Kubernetes#", "content": "TorchElastic Kubernetes# Created On: May 04, 2021 | Last Updated On: Jan 23, 2023 Please refer to our GitHub’s Kubernetes README for more information on Elastic Job Controller and custom resource definition.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_795", "url": "https://docs.pytorch.org/docs/stable/elastic/customization.html", "title": "Customization#", "page_title": "Customization — PyTorch 2.8 documentation", "breadcrumbs": "Customization#", "content": "Customization# Created On: May 04, 2021 | Last Updated On: May 04, 2021 This section describes how to customize TorchElastic to fit your needs.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_796", "url": "https://docs.pytorch.org/docs/stable/elastic/customization.html", "title": "Launcher#", "page_title": "Customization — PyTorch 2.8 documentation", "breadcrumbs": "Launcher#", "content": "Launcher# The launcher program that ships with TorchElastic should be sufficient for most use-cases (see torchrun (Elastic Launch)). You can implement a custom launcher by programmatically creating an agent and passing it specs for your workers as shown below. # my_launcher.py if __name__ == \"__main__\": args = parse_args(sys.argv[1:]) rdzv_handler = RendezvousHandler(...) spec = WorkerSpec( local_world_size=args.nproc_per_node, fn=trainer_entrypoint_fn, args=(trainer_entrypoint_fn args.fn_args,...), rdzv_handler=rdzv_handler, max_restarts=args.max_restarts, monitor_interval=args.monitor_interval, ) agent = LocalElasticAgent(spec, start_method=\"spawn\") try: run_result = agent.run() if run_result.is_failed(): print(f\"worker 0 failed with: run_result.failures[0]\") else: print(f\"worker 0 return value is: run_result.return_values[0]\") except Exception ex: # handle exception", "prev_chunk_id": "chunk_795", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_797", "url": "https://docs.pytorch.org/docs/stable/elastic/customization.html", "title": "Rendezvous Handler#", "page_title": "Customization — PyTorch 2.8 documentation", "breadcrumbs": "Rendezvous Handler#", "content": "Rendezvous Handler# To implement your own rendezvous, extend torch.distributed.elastic.rendezvous.RendezvousHandler and implement its methods. Once implemented you can pass your custom rendezvous handler to the worker spec when creating the agent. spec = WorkerSpec( rdzv_handler=MyRendezvousHandler(params), ... ) elastic_agent = LocalElasticAgent(spec, start_method=start_method) elastic_agent.run(spec.role)", "prev_chunk_id": "chunk_796", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_798", "url": "https://docs.pytorch.org/docs/stable/elastic/customization.html", "title": "Metric Handler#", "page_title": "Customization — PyTorch 2.8 documentation", "breadcrumbs": "Metric Handler#", "content": "Metric Handler# TorchElastic emits platform level metrics (see Metrics). By default metrics are emitted to /dev/null so you will not see them. To have the metrics pushed to a metric handling service in your infrastructure, implement a torch.distributed.elastic.metrics.MetricHandler and configure it in your custom launcher. # my_launcher.py import torch.distributed.elastic.metrics as metrics class MyMetricHandler(metrics.MetricHandler): def emit(self, metric_data: metrics.MetricData): # push metric_data to your metric sink def main(): metrics.configure(MyMetricHandler()) spec = WorkerSpec(...) agent = LocalElasticAgent(spec) agent.run()", "prev_chunk_id": "chunk_797", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_799", "url": "https://docs.pytorch.org/docs/stable/elastic/customization.html", "title": "Events Handler#", "page_title": "Customization — PyTorch 2.8 documentation", "breadcrumbs": "Events Handler#", "content": "Events Handler# TorchElastic supports events recording (see Events). The events module defines API that allows you to record events and implement custom EventHandler. EventHandler is used for publishing events produced during torchelastic execution to different sources, e.g. AWS CloudWatch. By default it uses torch.distributed.elastic.events.NullEventHandler that ignores events. To configure custom events handler you need to implement torch.distributed.elastic.events.EventHandler interface and configure it in your custom launcher. # my_launcher.py import torch.distributed.elastic.events as events class MyEventHandler(events.EventHandler): def record(self, event: events.Event): # process event def main(): events.configure(MyEventHandler()) spec = WorkerSpec(...) agent = LocalElasticAgent(spec) agent.run()", "prev_chunk_id": "chunk_798", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_800", "url": "https://docs.pytorch.org/docs/stable/elastic/control_plane.html", "title": "Control Plane#", "page_title": "Control Plane — PyTorch 2.8 documentation", "breadcrumbs": "Control Plane#", "content": "Control Plane# Created On: May 30, 2024 | Last Updated On: Jun 04, 2024 This module contains optional helpers that add extra debug and control handlers into your application.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_801", "url": "https://docs.pytorch.org/docs/stable/elastic/subprocess_handler.html", "title": "Subprocess Handling#", "page_title": "Subprocess Handling — PyTorch 2.8 documentation", "breadcrumbs": "Subprocess Handling#", "content": "Subprocess Handling# Created On: Mar 08, 2024 | Last Updated On: Mar 08, 2024", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_802", "url": "https://docs.pytorch.org/docs/stable/elastic/events.html", "title": "Events#", "page_title": "Events — PyTorch 2.8 documentation", "breadcrumbs": "Events#", "content": "Events# Created On: May 04, 2021 | Last Updated On: Jun 10, 2024 Module contains events processing mechanisms that are integrated with the standard python logging. Example of usage: from torch.distributed.elastic import events event = events.Event( name=\"test_event\", source=events.EventSource.WORKER, metadata={...} ) events.get_logging_handler(destination=\"console\").info(event)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_803", "url": "https://docs.pytorch.org/docs/stable/elastic/metrics.html", "title": "Metrics#", "page_title": "Metrics — PyTorch 2.8 documentation", "breadcrumbs": "Metrics#", "content": "Metrics# Created On: May 04, 2021 | Last Updated On: May 04, 2021 Metrics API. Overview: The metrics API in torchelastic is used to publish telemetry metrics. It is designed to be used by torchelastic’s internal modules to publish metrics for the end user with the goal of increasing visibility and helping with debugging. However you may use the same API in your jobs to publish metrics to the same metrics sink. A metric can be thought of as timeseries data and is uniquely identified by the string-valued tuple (metric_group, metric_name). torchelastic makes no assumptions about what a metric_group is and what relationship it has with metric_name. It is totally up to the user to use these two fields to uniquely identify a metric. A sensible way to use metric groups is to map them to a stage or module in your job. You may also encode certain high level properties the job such as the region or stage (dev vs prod). Publish Metrics: Using torchelastic’s metrics API is similar to using python’s logging framework. You first have to configure a metrics handler before trying to add metric data. The example below measures the latency for the calculate() function. import time import torch.distributed.elastic.metrics as metrics # makes all metrics other than the one from \"my_module\" to go /dev/null metrics.configure(metrics.NullMetricsHandler()) metrics.configure(metrics.ConsoleMetricsHandler(), \"my_module\") def my_method(): start = time.time() calculate() end = time.time() metrics.put_metric(\"calculate_latency\", int(end - start), \"my_module\") You may also use the torch.distributed.elastic.metrics.prof` decorator to conveniently and succinctly profile functions # -- in module examples.foobar -- import torch.distributed.elastic.metrics as metrics metrics.configure(metrics.ConsoleMetricsHandler(), \"foobar\") metrics.configure(metrics.ConsoleMetricsHandler(), \"Bar\") @metrics.prof def foo(): pass class Bar: @metrics.prof def baz(): pass @metrics.prof will publish the following metrics <leaf_module or classname>.success - 1 if the function finished successfully <leaf_module or classname>.failure - 1 if the function threw an exception <leaf_module", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_804", "url": "https://docs.pytorch.org/docs/stable/elastic/metrics.html", "title": "Metrics#", "page_title": "Metrics — PyTorch 2.8 documentation", "breadcrumbs": "Metrics#", "content": "or classname>.duration.ms - function duration in milliseconds Configuring Metrics Handler: torch.distributed.elastic.metrics.MetricHandler is responsible for emitting the added metric values to a particular destination. Metric groups can be configured with different metric handlers. By default torchelastic emits all metrics to /dev/null. By adding the following configuration metrics, torchelastic and my_app metric groups will be printed out to console. import torch.distributed.elastic.metrics as metrics metrics.configure(metrics.ConsoleMetricHandler(), group=\"torchelastic\") metrics.configure(metrics.ConsoleMetricHandler(), group=\"my_app\") Writing a Custom Metric Handler: If you want your metrics to be emitted to a custom location, implement the torch.distributed.elastic.metrics.MetricHandler interface and configure your job to use your custom metric handler. Below is a toy example that prints the metrics to stdout import torch.distributed.elastic.metrics as metrics class StdoutMetricHandler(metrics.MetricHandler): def emit(self, metric_data): ts = metric_data.timestamp group = metric_data.group_name name = metric_data.name value = metric_data.value print(f\"[{ts}][{group}]: {name}={value}\") metrics.configure(StdoutMetricHandler(), group=\"my_app\") Now all metrics in the group my_app will be printed to stdout as: [1574213883.4182858][my_app]: my_metric=<value> [1574213940.5237644][my_app]: my_metric=<value>", "prev_chunk_id": "chunk_803", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_805", "url": "https://docs.pytorch.org/docs/stable/elastic/metrics.html", "title": "Metric Handlers#", "page_title": "Metrics — PyTorch 2.8 documentation", "breadcrumbs": "Metric Handlers#", "content": "Metric Handlers# Below are the metric handlers that come included with torchelastic.", "prev_chunk_id": "chunk_804", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_806", "url": "https://docs.pytorch.org/docs/stable/elastic/timer.html", "title": "Expiration Timers#", "page_title": "Expiration Timers — PyTorch 2.8 documentation", "breadcrumbs": "Expiration Timers#", "content": "Expiration Timers# Created On: May 04, 2021 | Last Updated On: Apr 25, 2024 Expiration timers are set up on the same process as the agent and used from your script to deal with stuck workers. When you go into a code-block that has the potential to get stuck you can acquire an expiration timer, which instructs the timer server to kill the process if it does not release the timer by the self-imposed expiration deadline. Usage: import torchelastic.timer as timer import torchelastic.agent.server as agent def main(): start_method = \"spawn\" message_queue = mp.get_context(start_method).Queue() server = timer.LocalTimerServer(message, max_interval=0.01) server.start() # non-blocking spec = WorkerSpec( fn=trainer_func, args=(message_queue,), ...<OTHER_PARAMS...>) agent = agent.LocalElasticAgent(spec, start_method) agent.run() def trainer_func(message_queue): timer.configure(timer.LocalTimerClient(message_queue)) with timer.expires(after=60): # 60 second expiry # do some work In the example above if trainer_func takes more than 60 seconds to complete, then the worker process is killed and the agent retries the worker group.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_807", "url": "https://docs.pytorch.org/docs/stable/elastic/timer.html", "title": "Server/Client Implementations#", "page_title": "Expiration Timers — PyTorch 2.8 documentation", "breadcrumbs": "Server/Client Implementations#", "content": "Server/Client Implementations# Below are the timer server and client pairs that are provided by torchelastic. Below is a pair of timer server and client that is implemented based on a multiprocess.Queue. Below is another pair of timer server and client that is implemented based on a named pipe.", "prev_chunk_id": "chunk_806", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_808", "url": "https://docs.pytorch.org/docs/stable/elastic/timer.html", "title": "Writing a custom timer server/client#", "page_title": "Expiration Timers — PyTorch 2.8 documentation", "breadcrumbs": "Writing a custom timer server/client#", "content": "Writing a custom timer server/client# To write your own timer server and client extend the torch.distributed.elastic.timer.TimerServer for the server and torch.distributed.elastic.timer.TimerClient for the client. The TimerRequest object is used to pass messages between the server and client.", "prev_chunk_id": "chunk_807", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_809", "url": "https://docs.pytorch.org/docs/stable/elastic/rendezvous.html", "title": "Rendezvous#", "page_title": "Rendezvous — PyTorch 2.8 documentation", "breadcrumbs": "Rendezvous#", "content": "Rendezvous# Created On: May 04, 2021 | Last Updated On: May 22, 2024 In the context of Torch Distributed Elastic we use the term rendezvous to refer to a particular functionality that combines a distributed synchronization primitive with peer discovery. It is used by Torch Distributed Elastic to gather participants of a training job (i.e. nodes) such that they all agree on the same list of participants and everyone’s roles, as well as make a consistent collective decision on when training can begin/resume. Torch Distributed Elastic rendezvous provides the following critical functionalities: Barrier: Nodes performing rendezvous will all block until the rendezvous is considered complete - this happens when at least min total number of nodes have joined the rendezvous barrier (for the same job). This also implies the barrier is not necessarily of fixed size. There’s an additional small waiting time after reaching min number of nodes - this is used to ensure the rendezvous is not completed “too quickly” (which could potentially exclude additional nodes attempting to join at approximately the same time). If max number of nodes is gathered at the barrier, the rendezvous is completed immediately. There’s also an overall timeout which causes the rendezvous to fail if min number of nodes is never reached - this is meant to be a simple fail-safe to help release partially allocated job resources, in case there’s a problem with the resource manager, and is meant to be interpreted as non-retryable. Exclusivity: A simple distributed barrier would not be sufficient, as we also need to ensure that only one group of nodes exists at any given time (for a given job). In other words, new nodes (i.e. joining late) should not be able to form a parallel independent group of workers for the same job. Torch Distributed Elastic rendezvous", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_810", "url": "https://docs.pytorch.org/docs/stable/elastic/rendezvous.html", "title": "Rendezvous#", "page_title": "Rendezvous — PyTorch 2.8 documentation", "breadcrumbs": "Rendezvous#", "content": "ensures that if a group of nodes has already completed a rendezvous (and hence might already be training), then additional “late” nodes attempting to rendezvous will only announce themselves as waiting, and will have to wait until the (previously completed) existing rendezvous is destroyed first. Consistency: When a rendezvous is completed, all its members will agree on the job membership and everyone’s role in it. This role is represented using an integer, called rank, that is between between 0 and world size. Note that ranks are not stable, in the sense that the same node can be assigned a different rank in the next (re-)rendezvous. Fault-tolerance: Torch Distributed Elastic rendezvous is designed to tolerate node failures during the rendezvous process. Should a process crash (or lose network connectivity, etc), between joining the rendezvous and it being completed, then a re-rendezvous with remaining healthy nodes will happen automatically. A node can also fail after it has completed (or has been observed by other nodes to have completed) the rendezvous - this scenario will be handled by the Torch Distributed Elastic train_loop instead (where it will also trigger a re-rendezvous). Shared key-value store: When the rendezvous is completed, a shared key-value store is created and returned. This store implements a torch.distributed.Store API (see distributed communication docs). This store is only shared by the members of the completed rendezvous. It is intended to be used by Torch Distributed Elastic to exchange information necessary to initialize job control and data-planes. Waiting workers and rendezvous closing: Torch Distributed Elastic rendezvous handler object provides additional functionalities, which are technically not part of the rendezvous process: - Querying how many workers arrived late at the barrier, who can participate innextrendezvous. - Setting the rendezvousclosedto signal all nodes not to participate in next rendezvous. DynamicRendezvousHandler: Torch Distributed Elastic", "prev_chunk_id": "chunk_809", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_811", "url": "https://docs.pytorch.org/docs/stable/elastic/rendezvous.html", "title": "Rendezvous#", "page_title": "Rendezvous — PyTorch 2.8 documentation", "breadcrumbs": "Rendezvous#", "content": "comes with the DynamicRendezvousHandler class that implements the rendezvous mechanism described above. It is a backend- agnostic type that expects a particular RendezvousBackend instance to be specified during construction. Torch distributed users can either implement their own backend type or use one of the following implementations that come with PyTorch: - C10dRendezvousBackend: Uses a C10d store (by defaultTCPStore) as the rendezvous backend. The main advantage of using a C10d store is that it requires no 3rd-party dependency (such as etcd) to establish a rendezvous. - EtcdRendezvousBackend: Supersedes the legacyEtcdRendezvousHandlerclass. Passing anEtcdRendezvousBackendinstance toDynamicRendezvousHandleris functionally equivalent to instantiating anEtcdRendezvousHandler.store=TCPStore(\"localhost\")backend=C10dRendezvousBackend(store,\"my_run_id\")rdzv_handler=DynamicRendezvousHandler.from_backend(run_id=\"my_run_id\",store=store,backend=backend,min_nodes=2,max_nodes=4) Below is a state diagram describing how rendezvous works.", "prev_chunk_id": "chunk_810", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_812", "url": "https://docs.pytorch.org/docs/stable/elastic/rendezvous.html", "title": "Etcd Store#", "page_title": "Rendezvous — PyTorch 2.8 documentation", "breadcrumbs": "Etcd Store#", "content": "Etcd Store# The EtcdStore is the C10d Store instance type returned by next_rendezvous() when etcd is used as the rendezvous backend.", "prev_chunk_id": "chunk_811", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_813", "url": "https://docs.pytorch.org/docs/stable/elastic/rendezvous.html", "title": "Etcd Server#", "page_title": "Rendezvous — PyTorch 2.8 documentation", "breadcrumbs": "Etcd Server#", "content": "Etcd Server# The EtcdServer is a convenience class that makes it easy for you to start and stop an etcd server on a subprocess. This is useful for testing or single-node (multi-worker) deployments where manually setting up an etcd server on the side is cumbersome.", "prev_chunk_id": "chunk_812", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_814", "url": "https://docs.pytorch.org/docs/stable/elastic/errors.html", "title": "Error Propagation#", "page_title": "Error Propagation — PyTorch 2.8 documentation", "breadcrumbs": "Error Propagation#", "content": "Error Propagation# Created On: May 04, 2021 | Last Updated On: Jul 08, 2021 Each host in a distributed PyTorch job runs with a single TorchElastic agent, and multiple workers (as children processes of the TorchElastic agent). Since the workers are user-provided (your PyTorch script/job), TorchElastic has a way to propagate errors on the trainers through the agent and up to the scheduler, which ultimately informs the end-user about the state of the job and applies any retry policies. TorchElastic categorizes errors into 3 categories: All errors other than “Worker Failure” are either raised canonically from the agent process or implicitly or explicitly crash the agent process. So the standard language (python) provided exception handling strategies apply. Worker Failures are special because the exception/failure originates on a different process from the agent so the error needs to be propagated inter-process (e.g. the agent cannot simply try-catch an exception raised on the worker process). TorchElastic agents use torch.distributed.elastic.multiprocessing.start_processes() to launch the workers which has a simple file based inter-process error propagation built-in. Any function or binary entrypoint decorated with record() will write uncaught exceptions (with the trace information) to a file specified by the environment variable TORCHELASTIC_ERROR_FILE. The parent process (e.g. agent) sets this env var on each child it launches, then aggregates the error files for all children, and propagates the one with the smallest timestamp (e.g. the first error).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_815", "url": "https://docs.pytorch.org/docs/stable/elastic/multiprocessing.html", "title": "Multiprocessing#", "page_title": "Multiprocessing — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing#", "content": "Multiprocessing# Created On: May 04, 2021 | Last Updated On: Feb 29, 2024 Library that launches and manages n copies of worker subprocesses either specified by a function or a binary. For functions, it uses torch.multiprocessing (and therefore python multiprocessing) to spawn/fork worker processes. For binaries it uses python subprocessing.Popen to create worker processes. Usage 1: Launching two trainers as a function from torch.distributed.elastic.multiprocessing import Std, start_processes def trainer(a, b, c): pass # train # runs two trainers # LOCAL_RANK=0 trainer(1,2,3) # LOCAL_RANK=1 trainer(4,5,6) ctx = start_processes( name=\"trainer\", entrypoint=trainer, args={0: (1, 2, 3), 1: (4, 5, 6)}, envs={0: {\"LOCAL_RANK\": 0}, 1: {\"LOCAL_RANK\": 1}}, log_dir=\"/tmp/foobar\", redirects=Std.ALL, # write all worker stdout/stderr to a log file tee={0: Std.ERR}, # tee only local rank 0's stderr to console ) # waits for all copies of trainer to finish ctx.wait() Usage 2: Launching 2 echo workers as a binary # same as invoking # echo hello # echo world > stdout.log ctx = start_processes( name=\"echo\" entrypoint=\"echo\", log_dir=\"/tmp/foobar\", args={0: \"hello\", 1: \"world\"}, redirects={1: Std.OUT}, ) Just like torch.multiprocessing, the return value of the function start_processes() is a process context (api.PContext). If a function was launched, a api.MultiprocessContext is returned and if a binary was launched a api.SubprocessContext is returned. Both are specific implementations of the parent api.PContext class.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_816", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Elastic Agent#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Elastic Agent#", "content": "Elastic Agent# Created On: May 04, 2021 | Last Updated On: Jun 07, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_817", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Server#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Server#", "content": "Server# The elastic agent is the control plane of torchelastic. It is a process that launches and manages underlying worker processes. The agent is responsible for: - Working with distributed torch: the workers are started with all the necessary information to successfully and trivially calltorch.distributed.init_process_group(). - Fault tolerance: monitors workers and upon detecting worker failures or unhealthiness, tears down all workers and restarts everyone. - Elasticity: Reacts to membership changes and restarts workers with the new members. The simplest agents are deployed per node and works with local processes. A more advanced agent can launch and manage workers remotely. Agents can be completely decentralized, making decisions based on the workers it manages. Or can be coordinated, communicating to other agents (that manage workers in the same job) to make a collective decision. Below is a diagram of an agent that manages a local group of workers.", "prev_chunk_id": "chunk_816", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_818", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Concepts#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Concepts#", "content": "Concepts# This section describes the high-level classes and concepts that are relevant to understanding the role of the agent in torchelastic.", "prev_chunk_id": "chunk_817", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_819", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Implementations#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Implementations#", "content": "Implementations# Below are the agent implementations provided by torchelastic.", "prev_chunk_id": "chunk_818", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_820", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Extending the Agent#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Extending the Agent#", "content": "Extending the Agent# To extend the agent you can implement ElasticAgent directly, however we recommend you extend SimpleElasticAgent instead, which provides most of the scaffolding and leaves you with a few specific abstract methods to implement.", "prev_chunk_id": "chunk_819", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_821", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Watchdog in the Agent#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Watchdog in the Agent#", "content": "Watchdog in the Agent# A named pipe based watchdog can be enabled in LocalElasticAgent if an environment variable TORCHELASTIC_ENABLE_FILE_TIMER with value 1 has been defined in the LocalElasticAgent process. Optionally, another environment variable TORCHELASTIC_TIMER_FILE can be set with a unique file name for the named pipe. If the environment variable TORCHELASTIC_TIMER_FILE is not set, LocalElasticAgent will internally create a unique file name and set it to the environment variable TORCHELASTIC_TIMER_FILE, and this environment variable will be propagated to the worker processes to allow them to connect to the same named pipe that LocalElasticAgent uses.", "prev_chunk_id": "chunk_820", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_822", "url": "https://docs.pytorch.org/docs/stable/elastic/agent.html", "title": "Health Check Server#", "page_title": "Elastic Agent — PyTorch 2.8 documentation", "breadcrumbs": "Health Check Server#", "content": "Health Check Server# A health check monitoring server can be enabled in LocalElasticAgent if an environment variable TORCHELASTIC_HEALTH_CHECK_PORT has been defined in the LocalElasticAgent process. Adding interface for health check server which can be extended by starting tcp/http server on the specified port number. Additionally, health check server will have callback to check watchdog is alive.", "prev_chunk_id": "chunk_821", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_823", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "torchrun (Elastic Launch)#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "torchrun (Elastic Launch)#", "content": "torchrun (Elastic Launch)# Created On: May 04, 2021 | Last Updated On: Aug 26, 2021 Module torch.distributed.run. torch.distributed.run is a module that spawns up multiple distributed training processes on each of the training nodes. torchrun is a python console script to the main module torch.distributed.run declared in the entry_points configuration in setup.py. It is equivalent to invoking python -m torch.distributed.run. torchrun can be used for single-node distributed training, in which one or more processes per node will be spawned. It can be used for either CPU training or GPU training. If it is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. torchrun can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, torchrun will launch the given number of processes per node (--nproc-per-node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_824", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Single-node multi-worker#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Single-node multi-worker#", "content": "Single-node multi-worker# torchrun --standalone --nnodes=1 --nproc-per-node=$NUM_TRAINERS YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)", "prev_chunk_id": "chunk_823", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_825", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Stacked single-node multi-worker#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Stacked single-node multi-worker#", "content": "Stacked single-node multi-worker# To run multiple instances (separate jobs) of single-node, multi-worker on the same host, we need to make sure that each instance (job) is setup on different ports to avoid port conflicts (or worse, two jobs being merged as a single job). To do this you have to run with --rdzv-backend=c10d and specify a different port by setting --rdzv-endpoint=localhost:$PORT_k. For --nodes=1, its often convenient to let torchrun pick a free random port automatically instead of manually assigning different ports for each run. torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:0 --nnodes=1 --nproc-per-node=$NUM_TRAINERS YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)", "prev_chunk_id": "chunk_824", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_826", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures)#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures)#", "content": "Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures)# torchrun --nnodes=$NUM_NODES --nproc-per-node=$NUM_TRAINERS --max-restarts=3 --rdzv-id=$JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$HOST_NODE_ADDR YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...) HOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and the port on which the C10d rendezvous backend should be instantiated and hosted. It can be any node in your training cluster, but ideally you should pick a node that has a high bandwidth.", "prev_chunk_id": "chunk_825", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_827", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Elastic (min=1, max=4, tolerates up to 3 membership changes or failures)#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Elastic (min=1, max=4, tolerates up to 3 membership changes or failures)#", "content": "Elastic (min=1, max=4, tolerates up to 3 membership changes or failures)# torchrun --nnodes=1:4 --nproc-per-node=$NUM_TRAINERS --max-restarts=3 --rdzv-id=$JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$HOST_NODE_ADDR YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...) HOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and the port on which the C10d rendezvous backend should be instantiated and hosted. It can be any node in your training cluster, but ideally you should pick a node that has a high bandwidth.", "prev_chunk_id": "chunk_826", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_828", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Note on rendezvous backend#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Note on rendezvous backend#", "content": "Note on rendezvous backend# For multi-node training you need to specify: - --rdzv-id: A unique job id (shared by all nodes participating in the job) - --rdzv-backend: An implementation oftorch.distributed.elastic.rendezvous.RendezvousHandler - --rdzv-endpoint: The endpoint where the rendezvous backend is running; usually in formhost:port. Currently c10d (recommended), etcd-v2, and etcd (legacy) rendezvous backends are supported out of the box. To use etcd-v2 or etcd, setup an etcd server with the v2 api enabled (e.g. --enable-v2).", "prev_chunk_id": "chunk_827", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_829", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Definitions#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Definitions#", "content": "Definitions# - Node- A physical instance or a container; maps to the unit that the job manager works with. - Worker- A worker in the context of distributed training. - WorkerGroup- The set of workers that execute the same function (e.g. trainers). - LocalWorkerGroup- A subset of the workers in the worker group running on the same node. - RANK- The rank of the worker within a worker group. - WORLD_SIZE- The total number of workers in a worker group. - LOCAL_RANK- The rank of the worker within a local worker group. - LOCAL_WORLD_SIZE- The size of the local worker group. - rdzv_id- A user-defined id that uniquely identifies the worker group for a job. This id is used by each node to join as a member of a particular worker group. - rdzv_backend- The backend of the rendezvous (e.g.c10d). This is typically a strongly consistent key-value store. - rdzv_endpoint- The rendezvous backend endpoint; usually in form<host>:<port>. A Node runs LOCAL_WORLD_SIZE workers which comprise a LocalWorkerGroup. The union of all LocalWorkerGroups in the nodes in the job comprise the WorkerGroup.", "prev_chunk_id": "chunk_828", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_830", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Environment Variables#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Environment Variables#", "content": "Environment Variables# The following environment variables are made available to you in your script: - LOCAL_RANK- The local rank. - RANK- The global rank. - GROUP_RANK- The rank of the worker group. A number between 0 andmax_nnodes. When running a single worker group per node, this is the rank of the node. - ROLE_RANK- The rank of the worker across all the workers that have the same role. The role of the worker is specified in theWorkerSpec. - LOCAL_WORLD_SIZE- The local world size (e.g. number of workers running locally); equals to--nproc-per-nodespecified ontorchrun. - WORLD_SIZE- The world size (total number of workers in the job). - ROLE_WORLD_SIZE- The total number of workers that was launched with the same role specified inWorkerSpec. - MASTER_ADDR- The FQDN of the host that is running worker with rank 0; used to initialize the Torch Distributed backend. - MASTER_PORT- The port on theMASTER_ADDRthat can be used to host the C10d TCP store. - TORCHELASTIC_RESTART_COUNT- The number of worker group restarts so far. - TORCHELASTIC_MAX_RESTARTS- The configured maximum number of restarts. - TORCHELASTIC_RUN_ID- Equal to the rendezvousrun_id(e.g. unique job id). - PYTHON_EXEC- System executable override. If provided, the python user script will use the value ofPYTHON_EXECas executable. Thesys.executableis used by default.", "prev_chunk_id": "chunk_829", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_831", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Deployment#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Deployment#", "content": "Deployment# - (Not needed for the C10d backend) Start the rendezvous backend server and get the endpoint (to be passed as--rdzv-endpointtotorchrun) - Single-node multi-worker: Starttorchrunon the host to start the agent process which creates and monitors a local worker group. - Multi-node multi-worker: Starttorchrunwith the same arguments on all the nodes participating in training. When using a job/cluster manager, the entry point command to the multi-node job should be torchrun.", "prev_chunk_id": "chunk_830", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_832", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Failure Modes#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Failure Modes#", "content": "Failure Modes# - Worker failure: For a training job withnworkers, ifk<=nworkers fail all workers are stopped and restarted up tomax_restarts. - Agent failure: An agent failure results in a local worker group failure. It is up to the job manager to fail the entire job (gang semantics) or attempt to replace the node. Both behaviors are supported by the agent. - Node failure: Same as agent failure.", "prev_chunk_id": "chunk_831", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_833", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Membership Changes#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Membership Changes#", "content": "Membership Changes# - Node departure (scale-down): The agent is notified of the departure, all existing workers are stopped, a newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE. - Node arrival (scale-up): The new node is admitted to the job, all existing workers are stopped, a newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE.", "prev_chunk_id": "chunk_832", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_834", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Important Notices#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Important Notices#", "content": "Important Notices# - This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. - The environment variables necessary to initialize a Torch process group are provided to you by this module, no need for you to passRANKmanually. To initialize a process group in your training script, simply run: >>> import torch.distributed as dist >>> dist.init_process_group(backend=\"gloo|nccl\") - In your training program, you can either use regular distributed functions or usetorch.nn.parallel.DistributedDataParallel()module. If your training program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module, here is how to configure it. local_rank = int(os.environ[\"LOCAL_RANK\"]) model = torch.nn.parallel.DistributedDataParallel( model, device_ids=[local_rank], output_device=local_rank ) Please ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [int(os.environ(\"LOCAL_RANK\"))], and output_device needs to be int(os.environ(\"LOCAL_RANK\")) in order to use this utility - On failures or membership changes ALL surviving workers are killed immediately. Make sure to checkpoint your progress. The frequency of checkpoints should depend on your job’s tolerance for lost work. - This module only supports homogeneousLOCAL_WORLD_SIZE. That is, it is assumed that all nodes run the same number of local workers (per role). - RANKis NOT stable. Between restarts, the local workers on a node can be assigned a different range of ranks than before. NEVER hard code any assumptions about the stable-ness of ranks or some correlation betweenRANKandLOCAL_RANK. - When using elasticity (min_size!=max_size) DO NOT hard code assumptions aboutWORLD_SIZEas the world size can change as nodes are allowed to leave and join. - It is recommended for your script to have the following structure: def", "prev_chunk_id": "chunk_833", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_835", "url": "https://docs.pytorch.org/docs/stable/elastic/run.html", "title": "Important Notices#", "page_title": "torchrun (Elastic Launch) — PyTorch 2.8 documentation", "breadcrumbs": "Important Notices#", "content": "main(): load_checkpoint(checkpoint_path) initialize() train() def train(): for batch in iter(dataset): train_step(batch) if should_checkpoint: save_checkpoint(checkpoint_path) - (Recommended) On worker errors, this tool will summarize the details of the error (e.g. time, rank, host, pid, traceback, etc). On each node, the first error (by timestamp) is heuristically reported as the “Root Cause” error. To get tracebacks as part of this error summary print out, you must decorate your main entrypoint function in your training script as shown in the example below. If not decorated, then the summary will not include the traceback of the exception and will only contain the exitcode. For details on torchelastic error handling see:https://pytorch.org/docs/stable/elastic/errors.html from torch.distributed.elastic.multiprocessing.errors import record @record def main(): # do train pass if __name__ == \"__main__\": main()", "prev_chunk_id": "chunk_834", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_836", "url": "https://docs.pytorch.org/docs/stable/elastic/examples.html", "title": "Examples#", "page_title": "Examples — PyTorch 2.8 documentation", "breadcrumbs": "Examples#", "content": "Examples# Created On: May 04, 2021 | Last Updated On: May 04, 2021 Please refer to the elastic/examples README.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_837", "url": "https://docs.pytorch.org/docs/stable/elastic/train_script.html", "title": "Train script#", "page_title": "Train script — PyTorch 2.8 documentation", "breadcrumbs": "Train script#", "content": "Train script# Created On: May 04, 2021 | Last Updated On: Feb 09, 2023 If your train script works with torch.distributed.launch it will continue working with torchrun with these differences: - No need to manually passRANK,WORLD_SIZE,MASTER_ADDR, andMASTER_PORT. - rdzv_backendandrdzv_endpointcan be provided. For most users this will be set toc10d(seerendezvous). The defaultrdzv_backendcreates a non-elastic rendezvous whererdzv_endpointholds the master address. - Make sure you have aload_checkpoint(path)andsave_checkpoint(path)logic in your script. When any number of workers fail we restart all the workers with the same program arguments so you will lose progress up to the most recent checkpoint (seeelastic launch). - use_envflag has been removed. If you were parsing local rank by parsing the--local-rankoption, you need to get the local rank from the environment variableLOCAL_RANK(e.g.int(os.environ[\"LOCAL_RANK\"])). Below is an expository example of a training script that checkpoints on each epoch, hence the worst-case progress lost on failure is one full epoch worth of training. def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) initialize(state) # torch.distributed.run ensures that this will work # by exporting all the env vars needed to initialize the process group torch.distributed.init_process_group(backend=args.backend) for i in range(state.epoch, state.total_num_epochs) for batch in iter(state.dataset) train(batch, state.model) state.epoch += 1 save_checkpoint(state) For concrete examples of torchelastic-compliant train scripts, visit our examples page.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_838", "url": "https://docs.pytorch.org/docs/stable/elastic/quickstart.html", "title": "Quickstart#", "page_title": "Quickstart — PyTorch 2.8 documentation", "breadcrumbs": "Quickstart#", "content": "Quickstart# Created On: May 04, 2021 | Last Updated On: Feb 09, 2023 To launch a fault-tolerant job, run the following on all nodes. torchrun --nnodes=NUM_NODES --nproc-per-node=TRAINERS_PER_NODE --max-restarts=NUM_ALLOWED_FAILURES --rdzv-id=JOB_ID --rdzv-backend=c10d --rdzv-endpoint=HOST_NODE_ADDR YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...) To launch an elastic job, run the following on at least MIN_SIZE nodes and at most MAX_SIZE nodes. torchrun --nnodes=MIN_SIZE:MAX_SIZE --nproc-per-node=TRAINERS_PER_NODE --max-restarts=NUM_ALLOWED_FAILURES_OR_MEMBERSHIP_CHANGES --rdzv-id=JOB_ID --rdzv-backend=c10d --rdzv-endpoint=HOST_NODE_ADDR YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...) HOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and the port on which the C10d rendezvous backend should be instantiated and hosted. It can be any node in your training cluster, but ideally you should pick a node that has a high bandwidth. If torchrun does not meet your requirements you may use our APIs directly for more powerful customization. Start by taking a look at the elastic agent API.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_839", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Dynamo Deep-Dive#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Dynamo Deep-Dive#", "content": "Dynamo Deep-Dive# Created On: Apr 02, 2024 | Last Updated On: Jun 10, 2025 TorchDynamo (or simply Dynamo) is the tracer within torch.compile, and it is, more often than not, the one to blame for those insane backtraces. However, we cannot blindly blame Dynamo for these errors. In order to provide the user with the flexibility it does, Dynamo is given the arduous task of understanding any Python program. In particular, Dynamo has to implement a good part of the Python programming language internally! In this post, we will go over the internal design of Dynamo from the ground up. We will discuss the functionality it provides, and how it is implemented. By the end of this post, you will have a better understanding of what went wrong when you torch.compiled a PyTorch program and the compilation errored out, or succeeded but the speed-up was not what you expected.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_840", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "A Gentle Introduction to Dynamo#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "A Gentle Introduction to Dynamo#", "content": "A Gentle Introduction to Dynamo# Before getting our hands dirty with all the implementation details, let’s start by discussing what it is that Dynamo does. Dynamo is a tracer. This means, given and function and inputs to it, it executes the function and records a linear sequence of instructions (without control flow) into a graph. For example, consider the following program: import torch @torch.compile def mse(x, y): z = (x - y) ** 2 return z.sum() x = torch.randn(200) y = torch.randn(200) mse(x, y) If we save this program into the file example.py and we run TORCH_LOGS=graph_code python example.py we see the output that Dynamo traced def forward(l_x_: torch.Tensor, l_y_: torch.Tensor): # File: example.py:5, code: z = (x - y) ** 2 sub = l_x_ - l_y_ z = sub ** 2 # File: example.py:6, code: return z.sum() sum_1 = z.sum() return (sum_1,) We call this a graph (or trace) of the function for the given inputs. This is represented via an FX graph. We will simply think of an FX graph as a container that stores a list of function calls. The first thing we should notice is that the graph is a linear sequence of PyTorch operations. 1 Dynamo records all the PyTorch operations and stores them sequentially. For example, it split z = (x - y) ** 2 into its two constituting operations, sub = l_x_ - l_y_ and z = sub ** 2. When we say that the trace is linear, we mean that there is no branching or any control flow. To see this, consider import torch @torch.compile def fn(x, n): y = x ** 2 if n >= 0: return (n + 1) * y else: return y / n x = torch.randn(200) fn(x, 2) which, when executed with TORCH_LOGS=graph_code, returns def forward(l_x_: torch.Tensor):", "prev_chunk_id": "chunk_839", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_841", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "A Gentle Introduction to Dynamo#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "A Gentle Introduction to Dynamo#", "content": "# File: example.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: example.py:7, code: return (n + 1) * y mul = 3 * y return (mul,) We see that Dynamo completely removed the if statement from the trace and just recorded the operations that were executed with the inputs. As such, it should be clear that the trace of a function depends on the inputs. In particular, this means that the trace is not generated when we write @torch.compile, but when we execute the function fn(x, 2) with the actual arguments. The other interesting thing to note here is that Dynamo removed the second argument to the function. Instead, it treated it as a constant and recorded the result of the operation n + 1 in the graph. This is another feature of Dynamo: Dynamo will treat as constant any non-tensor value… other than ints. Let’s see now how are ints special. The last defining property of Dynamo is that it knows how to handle dynamic shapes. Symbolic shapes refer to Dynamo’s ability of tracing shapes, and more generally, integers, rather than leaving them as constants. This allows for avoiding recompilations and deploying generic models that work for any size in production. The main examples of places where dynamic shapes appear are the batch size, where we might train a model with a fixed batch size but then perform inference for an arbitrary batch size, or the variable sequence length that one encounters when processing text or audio. We can see this by executing a few more times the example above import torch @torch.compile def fn(x, n): y = x ** 2 if n >= 0: return (n + 1) * y else: return y / n x = torch.randn(200) fn(x, 2) fn(x, 3) fn(x,", "prev_chunk_id": "chunk_840", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_842", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "A Gentle Introduction to Dynamo#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "A Gentle Introduction to Dynamo#", "content": "-2) In this case, TORCH_LOGS=graph_code generates two more graphs # Graph for n==2 omitted def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt): # File: a.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: a.py:7, code: return (n + 1) * y add = l_n_ + 1 mul = add * y return (mul,) def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt): # File: a.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: a.py:9, code: return y / n truediv = y / l_n_ return (truediv,) Dynamo detected that one integer changed its value after the first call and started tracing it. We see that these graphs are generic, and trace the variable n symbolically via an object of type SymInt. If after these calls we call fn(x, 4), Dynamo would not recompile, but rather reuse the graph that was already traced. To summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it returns an FX graph with the PyTorch functions that were executed 3. It can also trace integers if it detects that they changed between calls 4. It specializes any other value that is not a tensor or a scalar Of course, Dynamo does many more things, like figuring out when it needs to retrace, rewriting the bytecode of the function, implementing graph breaks… To keep the introduction short, we will incrementally discuss all these in the sequel.", "prev_chunk_id": "chunk_841", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_843", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "PEP 523: Adding a frame evaluation API to CPython#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "PEP 523: Adding a frame evaluation API to CPython#", "content": "PEP 523: Adding a frame evaluation API to CPython# Imagine now that we are given the task to implement Dynamo. Where would we even start? Rather conveniently for us, PEP 523 was released with Python 3.6. This PEP was designed to allow third parties to create JIT compilers for Python. Let’s see how. A note on CPython: CPython is internally implemented as a stack machine. A Python program is compiled into bytecodes that then are executed by this interpreter. To learn more about these bytecodes, see the dis module from the standard library. See also the developer docs for an introduction to CPython’s interpreter. We will assume that the reader is familiar with the notion of a stack machine. PEP 523 exposes an API where a user can add a custom per-function interpreter. Then, CPython will use this interpreter rather than its own to execute the function. In order to be able to execute the function, on entry, CPython provides the custom interpreter with things like - The bytecode of the function - The value of the arguments of the function (i.e., the local variables) and their names - The value of the global variables and their names - The builtin functions like abs or print You can see all the fields here. 2 In summary, CPython provides the user’s interpreter with all the information necessary to execute the function. 3 With this API, we can implement a tracer by implementing an interpreter that runs the code and records in a graph all the PyTorch operations that occur during this execution. This is exactly what Dynamo does. Dynamo uses this CPython API to parse all these objects and packs them into a Python structure. After it has done so… it goes back from C to python. Other than for this", "prev_chunk_id": "chunk_842", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_844", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "PEP 523: Adding a frame evaluation API to CPython#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "PEP 523: Adding a frame evaluation API to CPython#", "content": "piece of code that communicates with CPython, Dynamo is fully implemented in Python. It should be clear that it is the decorator @torch.compile’s job to install the necessary scaffolding that will pass the bytecode, the args, global variables and so on to Dynamo when the function is called. Again, @torch.compile does not actually compile anything.", "prev_chunk_id": "chunk_843", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_845", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Implementing CPython in Python#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Implementing CPython in Python#", "content": "Implementing CPython in Python# So, we are back in the Python world. We have the bytecode of a function, and all the context necessary to execute it. In particular, we have landed at _convert_frame_assert. This is the function that the decorator torch.compile returns! We get to this function from _dynamo.optimize. The decorator torch.compile is just a nice API around _dynamo.optimize. Before getting into implementing a Python interpreter, we want to define an IR. In particular, we want to wrap all the local and global variables in our own internal classes. This allows us to better track these objects and group together objects that can be treated in the same way to the eyes of Dynamo. The parent class of the internal class structure is VariableTracker and represents the different objects that Dynamo understands. For example, ListVariable, represents a list object, and keeps internally a list of VariableTrackers. Another example of VariableTracker is ConstantVariable. ConstantVariable wraps all the objects considered constant by Dynamo. We also have special subclasses for objects that require special attention, like TensorVariable. All these internal classes are defined in the torch/_dynamo/variables folder. Python objects are wrapped into their corresponding VariableTracker class in VariableBuilder._wrap. This function is just a very long chain of elifs that tries to recursively pattern-match the Python inputs into the appropriate type of VariableTracker. Debugging tip. When we get unexpected results from dynamo, it is sometimes caused by the builder. If the logic of the builder is wrong, sometimes Dynamo may wrap a variable in the incorrect VariableTracker type, and this may cause issues later on. It is rather useful to have a look at the VariableTracker types that appear in the errors, and the VariableTracker method that throws the exception when you encounter a Dynamo error. In particular, sometimes we find that an", "prev_chunk_id": "chunk_844", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_846", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Implementing CPython in Python#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Implementing CPython in Python#", "content": "object is tracked as a UserDefinedObjectVariable (this is Dynamo’s catch-all class), when it should have been tracked as something more specific. In these cases, the SourceBuilder.__call__ logic is often to blame. Debugging tip. When running a program with TORCH_LOGS=dynamo, one of the artifacts that are printed out is lines of the form TRACE LOAD_GLOBAL y [TorchInGraphFunctionVariable(<built-in method any>), TensorVariable()] This is the bytecode for the original program and the state of the stack at that point. This is very useful to find where an object was not traced into the right VariableTracker. Ok, so we have an IR for our tracer, now we just need to reimplement CPython’s stack machine. This is implemented by InstructorTranslatorBase in symbolic_convert.py. InstructionTranslatorBase has about 200 methods, implementing almost all of Python bytecodes. As an example, we can see the implementation of BUILD_LIST def BUILD_LIST(self, inst): items = self.popn(inst.argval) self.push(ListVariable(items, mutation_type=ValueMutationNew())) This is the bytecode generated by constructions like l = [2, 3, 4]. In this case, since there are three elements, the generated bytecode is BUILD_LIST 3. This means that we pop the top 3 elements of the stack and push a new list object to the top of the stack formed by these three elements.", "prev_chunk_id": "chunk_845", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_847", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Generating the Output Graph#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Generating the Output Graph#", "content": "Generating the Output Graph# With a way to symbolically execute Python code, we are set to extract the PyTorch operations that happen during the symbolic execution of a program given some inputs. This is implemented in Dynamo via the OutputGraph object. The OutputGraph object is bound to an `InstructionTranslator object and it tracks all the data necessary to create the FX graph which will be returned by Dynamo. All the inputs and intermediary elements of the FX graph are fx.Nodes. In Dynamo, fx.Nodes are wrapped in fx.Proxys. fx.Proxys are used to build the FX graph. In particular, they record every PyTorch operation performed on them into the graph. You can create a new operation to be added to the graph by calling create_proxy. Then, we can add it to the graph through the function wrap_fx_proxy. A graph stores operations on tensors… and operations on symbolic integers. We will discuss symbolic integers later on, but first we will discuss how Dynamo addresses a rather important correctness issue.", "prev_chunk_id": "chunk_846", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_848", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Sound: Guards#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Sound: Guards#", "content": "Making Dynamo Sound: Guards# At this point, we have a way to trace programs completely disregarding control flow. And for that, we have reimplemented all of CPython… If this sounds like a bit of an overkill, that is because it is. torch.jit.trace already implements this without all this machinery, so what gives? The issue with torch.jit.trace, as it is warned in its docs, is that it just works if the traced program is not data dependent. In other words, it will just work if the program itself is linear. This means writing our program without using if-elses, for-while loops, exceptions. Even more, none of the libraries that we use can use any control flow! All in all, not using control flow in a language as dynamic as Python is, in fact, a huge constraint. JAX solves this problem by always retracing and caching the graph after retracing. Dynamo, on the other hand, uses guards to avoid retracing the whole program every time. A guard is an assumption (a boolean expression on an input) made in order to specialize a frame for one set of example inputs. Reusing the graph is only valid if these assumptions hold on the new inputs. For example, any constant input to a function, like a string, installs a guard stating that that input should be of type str and equal to the string we passed. Running import torch @torch.compile def fn(a, b): return a * len(b) fn(torch.arange(10), \"Hello\") with TORCH_LOGS=guards prints (among other guards) ___check_type_id(L['b'], 94334122025024) L['b'] == 'Hello' This reads as “the local variable b should have a specific type (str in this case, represented by the constant 9433...) and its value should be 'Hello'”. If we then execute the function again passing a different argument import torch @torch.compile def fn(a, b): return a", "prev_chunk_id": "chunk_847", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_849", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Sound: Guards#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Sound: Guards#", "content": "* len(b) fn(torch.arange(10), \"Hello\") fn(torch.arange(10), \"Hi\") we can see the guard that failed by running TORCH_LOGS=recompiles Recompiling function fn in script.py:3 triggered by the following guard failure(s): - L['b'] == 'Hello' Guards are accumulated while the inputs to the function are wrapped in the builder and during the execution of the program. We will show many more examples of guards in the next section, but first let us discuss sources. A source tracks how to reconstruct a variable from the original local or global variables present when entering the current frame. In particular, it tracks the original local and global objects and any of the objects they contain. In def foo(x: Tensor, y: List[Tensor]): a = x * y[0] return a * x x and y have LocalSource as their source, and y[0] has GetItemSource, which stores a LocalSource inside. On the other hand, a will not have a source as it is an intermediate variable that only exists within the fx graph. All these are defined in torch/_dynamo/source.py. We can see the guard generated by GetItemSource in the following example: import torch @torch.compile def fn(x, l): return x * len(l[0]) fn(torch.randn(8), [\"Hi\", \"Hello\"]) generates the following guards ___check_type_id(L['l'], 94439025877664) len(L['l']) == 2 ___check_type_id(L['l'][0], 94439025840192) L['l'][0] == 'Hi' ___check_type_id(L['l'][1], 94439025840192) L['l'][1] == 'Hello' Here, we see the code generated by GetItemSource ([0] and [1]) wrapping a LocalSource (L['l']). At this point, with sources and guards, we are able to implement a caching system to avoid recompilation without having to retrace every time. We will discuss a bit more in detail this caching system in the sequel. The attentive reader will have noticed that this does not explain yet why we need to have such fine control over the Python interpreter as to having to reimplement it. The examples of guards", "prev_chunk_id": "chunk_848", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_850", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Sound: Guards#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Sound: Guards#", "content": "that we have shown depend on the input objects, so we could still compute these before executing the function. In other words, we could implement this guard system on top of torch.jit.trace and get the same functionality with much less effort… Enter symbolic shapes.", "prev_chunk_id": "chunk_849", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_851", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Symbolic Shapes#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Symbolic Shapes#", "content": "Symbolic Shapes# Another point we discussed in the introduction is that Dynamo knows how to trace integers. In order to implement this, we use a symbolic class torch.SymInt that acts like an int but it records all the operations performed on it in the output FX graph. 4 We already saw this class in the introduction when introducing symbolic integer tracing. Let us now discuss the three properties that define symbolic shape tracing in Dynamo, and how to implement them.", "prev_chunk_id": "chunk_850", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_852", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Static by default#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Static by default#", "content": "Static by default# Dynamo assumes that every integer, let that be an input or the shape of a tensor, is static by default. In other words, no integers will be traced on the first execution of a function. Then, only if it detects that an integer or a shape changed value during the execution, it will trace it and generate a graph generic on that variable. We already saw this behavior in the introduction using integers. Let us now look at an example using shapes of tensors. import torch @torch.compile def fn(a, b): return a.shape[0] * a * b fn(torch.randn(4, 3), torch.randn(4, 3)) fn(torch.randn(8, 3), torch.randn(8, 3)) Running this program with TORCH_LOGS=graph_code we see that these two calls are traced as def forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor): mul = 4 * l_a_ mul_1 = mul * l_b_ return (mul_1,) def forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor): size = l_a_.size() getitem = size[0] mul = getitem * l_a_ mul_1 = mul * l_b_ return (mul_1,) In the first graph the shape is traced as a constant, but once it changes, it traces it symbolically using a SymInts. In general, a simpler way to see the shapes of the intermediary values is by running the program with TORCH_LOGS=graph_sizes TRACED GRAPH TENSOR SIZES ===== __compiled_fn_1 ===== l_a_: (s0, 3) l_a_ (concrete): (8, 3) l_b_: (s0, 3) l_b_ (concrete): (8, 3) mul: (s0, 3) mul (concrete): (8, 3) mul_1: (s0, 3) mul_1 (concrete): (8, 3) where we can see that the first dimension of the two tensor args is dynamic, given that it is represented by the s0 variable. We can find how Dynamo implements this by running TORCH_LOGS=guards # Guards first call check_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1]) check_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1]) # Guards second call", "prev_chunk_id": "chunk_851", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_853", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Static by default#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Static by default#", "content": "check_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1]) check_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1]) L['b'].size()[0] == L['a'].size()[0] 2 <= L['a'].size()[0] We see that on the first call, the guards check that the tensors have some fixed sizes and strides. These guards fail in the second execution, so it retraces. Since it was an int guard that failed, in this second iteration it traces this int symbolically and it installs more general guards on this more generic kernel. Compilation performance tip. If you know that a dimension will vary in size, you can mark it as dynamic by calling torch._dynamo.mark_dynamic before calling torch.compile. This will avoid the first compilation with a static shape. There are other useful utility functions like maybe_mark_dynamic or mark_static. You can also have all integers and shapes traced by calling torch.compile(dynamic=True). This is mostly useful for debugging purposes.", "prev_chunk_id": "chunk_852", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_854", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "0, 1 are always specialized#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "0, 1 are always specialized#", "content": "0, 1 are always specialized# Regardless of whether we mark a dimension as dynamic, if we pass an input where that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it will generate a specific graph for it. This is the reason why in the example above we find guards of the form 2 <= L['a'].size()[0]. There are several reasons for this choice. There are two particularly important - A tensor is empty if and only if any of its dimensions is zero - A tensor can only be contiguous if one of the strides is one This policy decision does NOT apply to plain Python ints; if we think a Python int should be compiled dynamically, we won’t specialize them by default; instead, whether or not it gets specialized depends on its usage.", "prev_chunk_id": "chunk_853", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_855", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Duck shaping#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Duck shaping#", "content": "Duck shaping# Dynamo performs what we call “duck shaping”. If two dynamic integers have the same value at trace time, we will assume that they are equal and guard on it. Effectively, this means that rather than having two symbols s0, s1 in the example above, we just unified them to s0 and had the guard L['b'].size()[0] == L['a'].size()[0]. This enables performing fusions within the compiler while being able to generate kernels that are generic enough.", "prev_chunk_id": "chunk_854", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_856", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Guards on symbolic ints#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Guards on symbolic ints#", "content": "Guards on symbolic ints# We now understand how symbolic shapes are implemented at a high level and the properties they have. Now, why is that symbolic shapes forced us through the tricky route of getting control of the CPython interpreter? Consider the following example: import torch @torch.compile(dynamic=True) def fn(a): if a.shape[0] * 2 < 16: return a else: return a + 1 fn(torch.randn(8)) This code has a guard of the form 2*L['a'].size()[0] >= 16. This is a non-trivial guard in terms of the inputs of the function, but it is registered in the middle of the execution of the program. Even more so, we cannot know this guard is needed until we see the if statement conditional on a SymNodeVariable argument. Such conditions are invisible to torch.jit.trace and require deep analysis of the python code. Debugging tip Running this code with TORCH_LOGS=dynamo tells us where this guard was added eval 2*s0 >= 16 [guard added] at script.py:5 in fn (_dynamo/variables/tensor.py:812 in evaluate_expr) Placing a breakpoint there and looking at the backtrace is rather useful to understand where a guard came from.", "prev_chunk_id": "chunk_855", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_857", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Complete: Graph Breaks#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Complete: Graph Breaks#", "content": "Making Dynamo Complete: Graph Breaks# With all the tools we have discussed, we have a tracer that can trace PyTorch operations on tensors and integers and has a caching system that knows when it can reuse a previously traced graph and when it needs to retrace. All this executing arbitrary Python code! There is just one small issue with this. The statement “executing arbitrary Python code” is perhaps a bit too general. Dynamo implements a good part of Python, but does it implement the more complex parts, like coroutines or async? Does it implement the whole Python standard library? NumPy also has a Python API. Does torch.compile also understand NumPy? and Django? 5 Python’s ecosystem is massive, and a good part of it is written in other more performant languages like C++ or Rust, and it just exposes Python bindings. There is no hope in Dynamo tracing through Python objects that are implemented in C++. What can a tracer do when it finds an operation that it does not understand? The usual way machine learning tracers handle this issue is by informing the user that the operation they choked on and giving up tracing altogether. This would pose a real usability issue in the case of PyTorch, where its users are used to the flexibility it gives them. As a real-world example the doctr_det_predictor model uses NumPy and the cv2 library to postprocess the model’s result. Here is another place where having access to CPython is interesting. Rather than erroring out, Dynamo can let CPython run that problematic code! To do this, Dynamo generates at trace time one graph with all the operations before the problematic code, and one with all the operations after. 6 Then, at runtime, it will delegate to CPython to execute the first graph, then the", "prev_chunk_id": "chunk_856", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_858", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Complete: Graph Breaks#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Complete: Graph Breaks#", "content": "problematic code, and then the second graph. This process of stopping the tracing and generating multiple graphs is called a graph break. A small confession: I lied all throughout the introduction and the first sections. Dynamo does not generate one graph, but multiple graphs! For all practical purposes, starting retracing after a second graph can be thought of as starting tracing a new function. The new graph after the graph break will have its own guards, its new set of local variables, and so on. To discuss how to implement graph breaks, we need to first revisit how Dynamo interacts with CPython. Using PEP 523, CPython allows a user to use their own frame evaluation mechanism. What we had not discussed is that CPython also exposes its own frame evaluation for others to use. Dynamo leverages this to let the fast CPython interpreter run the compiled code. For a function without graph breaks, the whole tracing / execution process of a program that calls the function 2 times with the same arguments looks like this: - In the first call to the functionDynamo traces the function into an FX graphThe FX graph is compiled by the compiler (Inductor) into efficient low-level code… but that’s a story for another dayIt rewrites the bytecode of the function so that it simply calls the compiled functionIt gives CPython this new bytecode and asks it to run ithere - In the second call to the functionIt checks the guards from the first call against the new argumentshere. Since they are the same arguments as before, they passIt asks CPython to run the bytecode associated to those guardshere This process on its own looks overly complicated. Why generate new bytecode and ask CPython to run it rather than simply creating a C++ binding to the", "prev_chunk_id": "chunk_857", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_859", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Complete: Graph Breaks#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Complete: Graph Breaks#", "content": "compiled function and executing it? Well, this pattern allows us to implement graph breaks! The bytecode generated by a graph break has the following structure: - Bytecode that executes the first graph - Bytecode that leaves the stack as it would be if CPython would have executed the first graph. It also replays any modifications to local or global variables that would be visible at this point - The bytecode that made Dynamo graph break - Bytecode that executes the second graph Let us see this in a simple example import torch @torch.compile def fn(a): b = a + 2 print(\"Hi\") return b + a fn(torch.randn(4)) Running this with TORCH_LOGS=bytecode shows us the initial bytecode and the modified bytecode MODIFIED BYTECODE fn script.py line 3 0 LOAD_GLOBAL 1 (__compiled_fn_0) 2 LOAD_FAST 0 (a) 4 CALL_FUNCTION 1 6 STORE_FAST 3 (graph_out_0) 8 LOAD_GLOBAL 0 (print) 10 LOAD_CONST 2 ('Hi') 12 LOAD_FAST 3 (graph_out_0) 14 LOAD_CONST 3 (0) 16 BINARY_SUBSCR 18 STORE_FAST 1 (b) 20 CALL_FUNCTION 1 22 LOAD_GLOBAL 2 (__resume_at_14_1) 24 ROT_TWO 26 LOAD_FAST 0 (a) 28 LOAD_FAST 1 (b) 30 CALL_FUNCTION 3 32 RETURN_VALUE MODIFIED BYTECODE resume_in_fn script.py line 6 0 LOAD_GLOBAL 1 (__compiled_fn_2) 2 LOAD_FAST 2 (b) 4 LOAD_FAST 1 (a) 6 CALL_FUNCTION 2 8 UNPACK_SEQUENCE 1 10 RETURN_VALUE We can see that the modified bytecode is split into two functions, fn, the original function, and a function called resume_in_fn. This second function is a function created by Dynamo to implement the execution of the program starting at the graph break. This is often called a continuation function. This continuation function simply calls the second compiled function with the right arguments. The code for the initial function is rewritten implementing the strategy that we described before - L0-4. Call the compiled function (a+2). - L6. Store its result in", "prev_chunk_id": "chunk_858", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_860", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Making Dynamo Complete: Graph Breaks#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Making Dynamo Complete: Graph Breaks#", "content": "a local variable calledgraph_out_0.graph_out_0is a tuple - L8-18. Leave the stack as it would be at the point of the graph break - L20. Execute the code that caused the graph break - L22-32. Call the compiled continuation function (a+b) The code generation of the stack in Dynamo is delegated to VariableTracker subclasses. Every VariableTracker object in Dynamo has a reconstruct method that generates the necessary bytecode to create the python object it represents on the stack. Debugging tip. Graph breaks hamper performance, and as such, it is best to avoid them. Running a program with TORCH_LOGS=graph_breaks is a great way to find how many graph breaks did our program hit. The information it returns is in terms of VariableTracker objects, so the debugging tips above are sometimes also helpful to figure out what caused that graph break.", "prev_chunk_id": "chunk_859", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_861", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Conclusion#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Conclusion#", "content": "Conclusion# Dynamo is a complex piece of software. Once you sign up to implement a CPython interpreter you know you are in for a ride. That being said, we hope that this post helps demystify it a bit. Dynamo is (mostly) implemented in Python. We left plenty of links to the pieces of the code that we discussed. We hope that reading those pieces of code and grepping for the places that call them, or putting breakpoints on them and looking at the call stack helps understanding the rest of the code base. Of course, the best way to learn how a piece of software works is by extending it. In this case, the best way is to have a look at the open dynamo issues on github. Many of them require very minor changes in the code, once you find where you need to make those changes.", "prev_chunk_id": "chunk_860", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_862", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html", "title": "Footnotes#", "page_title": "Dynamo Deep-Dive — PyTorch 2.8 documentation", "breadcrumbs": "Footnotes#", "content": "Footnotes# Below are additional details and references for concepts mentioned in this document.", "prev_chunk_id": "chunk_861", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_863", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "Dynamo Overview#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "Dynamo Overview#", "content": "Dynamo Overview# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 Before you read this section, read torch.compiler. TorchDynamo (or simply Dynamo) is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. Dynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an FX Graph which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds — usability and performance. Dynamo makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator torch._dynamo.optimize() which is wrapped for convenience by torch.compile() The following diagram demonstrates how PyTorch works with torch.compile and without it: TorchInductor is one of the backends supported by Dynamo Graph into Triton for GPUs or C++/OpenMP for CPUs. We have a training performance dashboard that provides performance comparison for different training backends. You can read more in the TorchInductor post on PyTorch dev-discuss. For an in-depth overview, read the sections below, watch the deep-dive video, and check out the dev-discuss topics. - Dynamo deep-dive video - dev-discuss topics", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_864", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "Dynamo Internals#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "Dynamo Internals#", "content": "Dynamo Internals# Author: Jason Ansel and Kaichao You This section will go over some of the Dynamo internals and will demonstrate how Dynamo works under the hood.", "prev_chunk_id": "chunk_863", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_865", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "What is a guard?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "What is a guard?#", "content": "What is a guard?# Dynamo operates just-in-time and specializes graphs based on dynamic properties. Below is a basic example of how to use Dynamo. One can decorate a function or a method using torchdynamo.optimize to enable Dynamo optimization: from typing import List import torch from torch import _dynamo as torchdynamo def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]): print(\"my_compiler() called with FX graph:\") gm.graph.print_tabular() return gm.forward # return a python callable @torchdynamo.optimize(my_compiler) def toy_example(a, b): x = a / (torch.abs(a) + 1) if b.sum() < 0: b = b * -1 return x * b for _ in range(100): toy_example(torch.randn(10), torch.randn(10)) For example, the first graph above has the following guards: GUARDS: hasattr(L['a'], '_dynamo_dynamic_indices') == False hasattr(L['b'], '_dynamo_dynamic_indices') == False utils_device.CURRENT_DEVICE == None ___skip_backend_check() or ___current_backend() == ___lookup_backend(140355900538256) check_tensor(L['a'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1]) check_tensor(L['b'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1]) If any of those guards fail, the graph will be recaptured and recompiled. The interesting guard there is check_tensor, which checks the following torch.Tensor properties: - Python class of the tensor (tensor subclassing, etc) - dtype - device - requires_grad - dispatch_key (with thread-local includes/excludes applied) - ndim - sizes* - strides* The full specialization mode allows the backend compiler to assume an entirely static graph. Unfortunately, most backends require this. Operators which return dynamic shapes will trigger a graph break when not in dynamic shape mode.", "prev_chunk_id": "chunk_864", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_866", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "What is Dynamo doing?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "What is Dynamo doing?#", "content": "What is Dynamo doing?# If you want to understand better what Dynamo is doing, you can run your code with: TORCH_LOGS=\"+dynamo,guards,bytecode\" If you are not familiar with Python bytecode, you can add a decompiler hook to decompile the bytecode into human-readable source code. One available tool is depyf. If you don’t have depyf already installed, run pip install depyf. Then, add the following code to install decompilation hooks before you run any code. import depyf depyf.install() This code triggers useful (but spammy) printouts. For example, the printouts for the first graph in the toy_example are: __compiled_fn_0 <eval_with_key>.1 opcode name target args kwargs ------------- ------- ------------------------------------------------------ ---------------- -------- placeholder a a () {} placeholder b b () {} call_function abs_1 <built-in method abs of type object at 0x7f9ca082f8a0> (a,) {} call_function add <built-in function add> (abs_1, 1) {} call_function truediv <built-in function truediv> (a, add) {} call_method sum_1 sum (b,) {} call_function lt <built-in function lt> (sum_1, 0) {} output output output ((truediv, lt),) {} ORIGINAL BYTECODE toy_example example.py line 12 14 0 LOAD_FAST 0 (a) 2 LOAD_GLOBAL 0 (torch) 4 LOAD_METHOD 1 (abs) 6 LOAD_FAST 0 (a) 8 CALL_METHOD 1 10 LOAD_CONST 1 (1) 12 BINARY_ADD 14 BINARY_TRUE_DIVIDE 16 STORE_FAST 2 (x) 15 18 LOAD_FAST 1 (b) 20 LOAD_METHOD 2 (sum) 22 CALL_METHOD 0 24 LOAD_CONST 2 (0) 26 COMPARE_OP 0 (<) 28 POP_JUMP_IF_FALSE 19 (to 38) 16 30 LOAD_FAST 1 (b) 32 LOAD_CONST 3 (-1) 34 BINARY_MULTIPLY 36 STORE_FAST 1 (b) 17 >> 38 LOAD_FAST 2 (x) 40 LOAD_FAST 1 (b) 42 BINARY_MULTIPLY 44 RETURN_VALUE MODIFIED BYTECODE toy_example example.py line 12 12 0 LOAD_GLOBAL 3 (__compiled_fn_0) 2 LOAD_FAST 0 (a) 4 LOAD_FAST 1 (b) 6 CALL_FUNCTION 2 8 UNPACK_SEQUENCE 2 10 STORE_FAST 2 (x) 12 POP_JUMP_IF_FALSE 12 (to 24) 14 LOAD_GLOBAL 4 (__resume_at_30_1) 16 LOAD_FAST 1 (b) 18 LOAD_FAST", "prev_chunk_id": "chunk_865", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_867", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "What is Dynamo doing?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "What is Dynamo doing?#", "content": "2 (x) 20 CALL_FUNCTION 2 22 RETURN_VALUE >> 24 LOAD_GLOBAL 5 (__resume_at_38_2) 26 LOAD_FAST 1 (b) 28 LOAD_FAST 2 (x) 30 CALL_FUNCTION 2 32 RETURN_VALUE possible source code: def toy_example(a, b): __temp_1 = __compiled_fn_0(a, b) x = __temp_1[0] if __temp_1[1]: return __resume_at_30_1(b, x) return __resume_at_38_2(b, x) If you find the decompiled code is wrong,please submit an issue at https://github.com/youkaichao/depyf/issues. At the top you can see the FX graph. Next, you see the original bytecode of the function, followed by the modified bytecode generated by Dynamo, and the decompiled source code for reference. Finally, you see the guards which we covered above. In the modified bytecode, __compiled_fn_0 is the return value of my_compiler() (the compiled graph). __resume_at_30_1 and __resume_at_38_2 are both generated continuation functions that pick up execution after a graph break (at bytecode offsets 30 and 38). Each of these functions take the form: __resume_at_<offset>: ... restore stack state if needed ... JUMP_ABSOLUTE <offset> into toy_example ... original bytecode of toy_example ... By generating this resume_at function, we force the remainder of the function to be executed in a new Python frame which recursively triggers Dynamo to restart its capture once execution reaches that point for the first time.", "prev_chunk_id": "chunk_866", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_868", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "How to inspect artifacts generated by Dynamo?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "How to inspect artifacts generated by Dynamo?#", "content": "How to inspect artifacts generated by Dynamo?# To inspect the artifacts generated by Dynamo, there is an API torch._dynamo.eval_frame._debug_get_cache_entry_list that retrieves compiled code and guards out of a function’s __code__ object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and a types.CodeType object to keep the code to be executed if the guarding conditions are satisfied. from torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn cache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example)) cache_entry = cache_entries[0] guard, code = cache_entry.check_fn, cache_entry.code # the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered. import dis dis.dis(guard) dis.dis(code) If you know Python bytecode, you can understand the above output. For the guard function, there is no need to inspect the bytecode. We can directly access its guarding conditions: for code_part in guard.code_parts: print(code_part) The output is: ___guarded_code.valid ___check_global_state() hasattr(L['a'], '_dynamo_dynamic_indices') == False hasattr(L['b'], '_dynamo_dynamic_indices') == False utils_device.CURRENT_DEVICE == None ___skip_backend_check() or ___current_backend() == ___lookup_backend(140215810860528) ___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names) Only when all the conditions are satisfied, the guard function returns true, and the compiled code is executed. For the compiled code, we cannot directly access its source but have to decompile it. from depyf import decompile print(decompile(code)) The output is: def toy_example(a, b): __temp_1 = __compiled_fn_0(a, b) x = __temp_1[0] if __temp_1[1]: return __resume_at_30_1(b, x) return __resume_at_38_2(b, x) Some names referenced in the code are: - Compiled functions, stored in the global namespace of the module containing the original functiontoy_example. These include names like__compiled_fn_0/__resume_at_30_1/__resume_at_38_2. - Closure variables used for checking guards. The names can be accessed fromguard.__code__.co_freevars, and the values are stored inguard.__closure__. These include names like___guarded_code/___is_grad_enabled/___are_deterministic_algorithms_enabled/___is_torch_function_enabled/utils_device/___check_tensors/tensor_check_names. - ArgumentLof theguardfunction. This is a dict mapping the name of arguments oftoy_exampleto its values. This is only available when the function is called, where the frame", "prev_chunk_id": "chunk_867", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_869", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "How to inspect artifacts generated by Dynamo?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "How to inspect artifacts generated by Dynamo?#", "content": "evaluation API comes into play. In short,Lis adictwith structure of{'a':value_a,'b':value_b}. Therefore, you can see the code usesL['a']to refer to the input variablea. The graph break is shown in the code of compiled toy_example, where we have to use Python interpreter to select the following graph to execute. Note that we pass a simple my_compiler function as the backend compiler, therefore the subgraph code __resume_at_38_2, __resume_at_30_1, and __compiled_fn_0 remain Python code. This can also be inspected (please ignore the function name, and only use the function signature and function body code): print(\"source code of __compiled_fn_0:\") print(innermost_fn(__compiled_fn_0).__self__.code) print(\"=\" * 60) print(\"source code of __resume_at_30_1:\") print(decompile(__resume_at_30_1)) print(\"=\" * 60) print(\"source code of __resume_at_38_2:\") print(decompile(__resume_at_38_2)) source code of __compiled_fn_0: def forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor): l_a_ = L_a_ l_b_ = L_b_ abs_1 = torch.abs(l_a_) add = abs_1 + 1; abs_1 = None truediv = l_a_ / add; l_a_ = add = None sum_1 = l_b_.sum(); l_b_ = None lt = sum_1 < 0; sum_1 = None return (truediv, lt) # To see more debug info, please use ``graph_module.print_readable()`` ============================================================ source code of __resume_at_30_1: def <resume in toy_example>(b, x): b = b * -1 return x * b ============================================================ source code of __resume_at_38_2: def <resume in toy_example>(b, x): return x * b However, if we use other backends like the built-in inductor, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU. To summarize, the compiled code is conceptually equivalent to the code below: def compiled_example(a, b): L = {'a': a, 'b': b} for guard, code in get_cache_entries(): if guard(L): return code(a, b) recompile_and_add_another_cache_entry() The following diagram demonstrates how torch.compile transforms and optimizes user-written code: it first extracts computation graphs from the user-written function, and compiles these graphs into optimized functions, then assembles them into a new function,", "prev_chunk_id": "chunk_868", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_870", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html", "title": "How to inspect artifacts generated by Dynamo?#", "page_title": "Dynamo Overview — PyTorch 2.8 documentation", "breadcrumbs": "How to inspect artifacts generated by Dynamo?#", "content": "which is functionally equivalent to the user-written code but optimized to have a good computation speed. To learn more about how all this is implemented internally, see Dynamo Deep-Dive.", "prev_chunk_id": "chunk_869", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_871", "url": "https://docs.pytorch.org/docs/stable/cond.html", "title": "Control Flow - Cond#", "page_title": "Control Flow - Cond — PyTorch 2.8 documentation", "breadcrumbs": "Control Flow - Cond#", "content": "Control Flow - Cond# Created On: Oct 03, 2023 | Last Updated On: Jun 13, 2025 torch.cond is a structured control flow operator. It can be used to specify if-else like control flow and can logically be seen as implemented as follows. def cond( pred: Union[bool, torch.Tensor], true_fn: Callable, false_fn: Callable, operands: Tuple[torch.Tensor] ): if pred: return true_fn(*operands) else: return false_fn(*operands) Its unique power lies in its ability of expressing data-dependent control flow: it lowers to a conditional operator (torch.ops.higher_order.cond), which preserves predicate, true function and false functions. This unlocks great flexibility in writing and deploying models that change model architecture based on the value or shape of inputs or intermediate outputs of tensor operations.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_872", "url": "https://docs.pytorch.org/docs/stable/cond.html", "title": "Examples#", "page_title": "Control Flow - Cond — PyTorch 2.8 documentation", "breadcrumbs": "Examples#", "content": "Examples# Below is an example that uses cond to branch based on input shape: import torch def true_fn(x: torch.Tensor): return x.cos() + x.sin() def false_fn(x: torch.Tensor): return x.sin() class DynamicShapeCondPredicate(torch.nn.Module): \"\"\" A basic usage of cond based on dynamic shape predicate. \"\"\" def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x.cos() def false_fn(x: torch.Tensor): return x.sin() return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,)) dyn_shape_mod = DynamicShapeCondPredicate() We can eagerly run the model and expect the results vary based on input shape: inp = torch.randn(3) inp2 = torch.randn(5) assert torch.equal(dyn_shape_mod(inp), false_fn(inp)) assert torch.equal(dyn_shape_mod(inp2), true_fn(inp2)) We can export the model for further transformations and deployment: inp = torch.randn(4, 3) dim_batch = torch.export.Dim(\"batch\", min=2) ep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={\"x\": {0: dim_batch}}) print(ep) This gives us an exported program as shown below: class GraphModule(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): sym_size: Sym(s0) = torch.ops.aten.sym_size.int(arg0_1, 0) gt: Sym(s0 > 4) = sym_size > 4; sym_size = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]); gt = true_graph_0 = false_graph_0 = arg0_1 = None return (conditional,) class <lambda>(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1) sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1); arg0_1 = None add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin); cos = sin = None return add class <lambda>(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1); arg0_1 = None return sin Notice that torch.cond is lowered to torch.ops.higher_order.cond, its predicate becomes a Symbolic expression over the shape of input, and branch functions becomes two sub-graph attributes of the top level graph module. Here is another example that showcases how to express a data-dependent control flow: class DataDependentCondPredicate(torch.nn.Module): \"\"\" A basic usage of cond based on data dependent predicate. \"\"\" def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return", "prev_chunk_id": "chunk_871", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_873", "url": "https://docs.pytorch.org/docs/stable/cond.html", "title": "Examples#", "page_title": "Control Flow - Cond — PyTorch 2.8 documentation", "breadcrumbs": "Examples#", "content": "torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,)) The exported program we get after export: class GraphModule(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): sum_1: f32[] = torch.ops.aten.sum.default(arg0_1) gt: b8[] = torch.ops.aten.gt.Scalar(sum_1, 4.0); sum_1 = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]); gt = true_graph_0 = false_graph_0 = arg0_1 = None return (conditional,) class <lambda>(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1) sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1); arg0_1 = None add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin); cos = sin = None return add class <lambda>(torch.nn.Module): def forward(self, arg0_1: f32[s0, 3]): sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1); arg0_1 = None return sin", "prev_chunk_id": "chunk_872", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_874", "url": "https://docs.pytorch.org/docs/stable/cond.html", "title": "Invariants of torch.ops.higher_order.cond#", "page_title": "Control Flow - Cond — PyTorch 2.8 documentation", "breadcrumbs": "Invariants of torch.ops.higher_order.cond#", "content": "Invariants of torch.ops.higher_order.cond# There are several useful invariants for torch.ops.higher_order.cond: - For predicate:Dynamicness of predicate is preserved (e.g.gtshown in the above example)If the predicate in user-program is constant (e.g. a python bool constant), thepredof the operator will be a constant. - For branches:The input and output signature will be a flattened tuple.They aretorch.fx.GraphModule.Closures in original function becomes explicit inputs. No closures.No mutations on inputs or globals are allowed. - For operands:It will also be a flat tuple. - Nesting oftorch.condin user program becomes nested graph modules.", "prev_chunk_id": "chunk_873", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_875", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.mutation.html", "title": "user_input_mutation#", "page_title": "torch.mutation — PyTorch 2.8 documentation", "breadcrumbs": "user_input_mutation#", "content": "user_input_mutation# Original source code: # mypy: allow-untyped-defs import torch class UserInputMutation(torch.nn.Module): \"\"\" Directly mutate user input in forward \"\"\" def forward(self, x): x.mul_(2) return x.cos() example_args = (torch.randn(3, 2),) tags = {\"torch.mutation\"} model = UserInputMutation() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul_: \"f32[3, 2]\" = torch.ops.aten.mul_.Tensor(x, 2); x = None cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(mul_); mul_ = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_876", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.operator.html", "title": "unsupported_operator#", "page_title": "torch.operator — PyTorch 2.8 documentation", "breadcrumbs": "unsupported_operator#", "content": "unsupported_operator# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class TorchSymMin(torch.nn.Module): \"\"\" torch.sym_min operator is not supported in export. \"\"\" def forward(self, x): return x.sum() + torch.sym_min(x.size(0), 100) example_args = (torch.randn(3, 2),) tags = {\"torch.operator\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = TorchSymMin() torch.export.export(model, example_args) Result: Unsupported: torch.* op returned non-Tensor", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_877", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.context-manager.html", "title": "null_context_manager#", "page_title": "python.context-manager — PyTorch 2.8 documentation", "breadcrumbs": "null_context_manager#", "content": "null_context_manager# Original source code: # mypy: allow-untyped-defs import contextlib import torch class NullContextManager(torch.nn.Module): \"\"\" Null context manager in Python will be traced out. \"\"\" def forward(self, x): \"\"\" Null context manager in Python will be traced out. \"\"\" ctx = contextlib.nullcontext() with ctx: return x.sin() + x.cos() example_args = (torch.randn(3, 2),) tags = {\"python.context-manager\"} model = NullContextManager() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): sin: \"f32[3, 2]\" = torch.ops.aten.sin.default(x) cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(x); x = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(sin, cos); sin = cos = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_878", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.object-model.html", "title": "model_attr_mutation#", "page_title": "python.object-model — PyTorch 2.8 documentation", "breadcrumbs": "model_attr_mutation#", "content": "model_attr_mutation# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class ModelAttrMutation(torch.nn.Module): \"\"\" Attribute mutation is not supported. \"\"\" def __init__(self) -> None: super().__init__() self.attr_list = [torch.randn(3, 2), torch.randn(3, 2)] def recreate_list(self): return [torch.zeros(3, 2), torch.zeros(3, 2)] def forward(self, x): self.attr_list = self.recreate_list() return x.sum() + self.attr_list[0].sum() example_args = (torch.randn(3, 2),) tags = {\"python.object-model\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = ModelAttrMutation() torch.export.export(model, example_args) Result: AssertionError: Mutating module attribute attr_list during export.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_879", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.object-model.html", "title": "optional_input#", "page_title": "python.object-model — PyTorch 2.8 documentation", "breadcrumbs": "optional_input#", "content": "optional_input# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class OptionalInput(torch.nn.Module): \"\"\" Tracing through optional input is not supported yet \"\"\" def forward(self, x, y=torch.randn(2, 3)): if y is not None: return x + y return x example_args = (torch.randn(2, 3),) tags = {\"python.object-model\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = OptionalInput() torch.export.export(model, example_args) Result: Unsupported: Tracing through optional input is not supported yet", "prev_chunk_id": "chunk_878", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_880", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.builtin.html", "title": "dynamic_shape_round#", "page_title": "python.builtin — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_round#", "content": "dynamic_shape_round# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel from torch.export import Dim class DynamicShapeRound(torch.nn.Module): \"\"\" Calling round on dynamic shapes is not supported. \"\"\" def forward(self, x): return x[: round(x.shape[0] / 2)] x = torch.randn(3, 2) dim0_x = Dim(\"dim0_x\") example_args = (x,) tags = {\"torch.dynamic-shape\", \"python.builtin\"} support_level = SupportLevel.NOT_SUPPORTED_YET dynamic_shapes = {\"x\": {0: dim0_x}} model = DynamicShapeRound() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: Unsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_881", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.builtin.html", "title": "tensor_setattr#", "page_title": "python.builtin — PyTorch 2.8 documentation", "breadcrumbs": "tensor_setattr#", "content": "tensor_setattr# Original source code: # mypy: allow-untyped-defs import torch class TensorSetattr(torch.nn.Module): \"\"\" setattr() call onto tensors is not supported. \"\"\" def forward(self, x, attr): setattr(x, attr, torch.randn(3, 2)) return x + 4 example_args = (torch.randn(3, 2), \"attr\") tags = {\"python.builtin\"} model = TensorSetattr() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", attr): randn: \"f32[3, 2]\" = torch.ops.aten.randn.default([3, 2], device = device(type='cpu'), pin_memory = False); randn = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4); x = None return (add,) Graph signature: # inputs x: USER_INPUT attr: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_880", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_882", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.builtin.html", "title": "type_reflection_method#", "page_title": "python.builtin — PyTorch 2.8 documentation", "breadcrumbs": "type_reflection_method#", "content": "type_reflection_method# Original source code: # mypy: allow-untyped-defs import torch class A: @classmethod def func(cls, x): return 1 + x class TypeReflectionMethod(torch.nn.Module): \"\"\" type() calls on custom objects followed by attribute accesses are not allowed due to its overly dynamic nature. \"\"\" def forward(self, x): a = A() return type(a).func(x) example_args = (torch.randn(3, 4),) tags = {\"python.builtin\"} model = TypeReflectionMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 4]\"): add: \"f32[3, 4]\" = torch.ops.aten.add.Tensor(x, 1); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_881", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_883", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.map.html", "title": "dynamic_shape_map#", "page_title": "torch.map — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_map#", "content": "dynamic_shape_map# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import map class DynamicShapeMap(torch.nn.Module): \"\"\" functorch map() maps a function over the first tensor dimension. \"\"\" def forward(self, xs, y): def body(x, y): return x + y return map(body, xs, y) example_args = (torch.randn(3, 2), torch.randn(2)) tags = {\"torch.dynamic-shape\", \"torch.map\"} model = DynamicShapeMap() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"): body_graph_0 = self.body_graph_0 map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]); body_graph_0 = xs = y = None getitem: \"f32[3, 2]\" = map_impl[0]; map_impl = None return (getitem,) class body_graph_0(torch.nn.Module): def forward(self, xs: \"f32[2]\", y: \"f32[2]\"): add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y); xs = y = None return (add,) Graph signature: # inputs xs: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_884", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.control-flow.html", "title": "dynamic_shape_if_guard#", "page_title": "python.control-flow — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_if_guard#", "content": "dynamic_shape_if_guard# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeIfGuard(torch.nn.Module): \"\"\" `if` statement with backed dynamic shape predicate will be specialized into one particular branch and generate a guard. However, export will fail if the the dimension is marked as dynamic shape from higher level API. \"\"\" def forward(self, x): if x.shape[0] == 3: return x.cos() return x.sin() example_args = (torch.randn(3, 2, 2),) tags = {\"torch.dynamic-shape\", \"python.control-flow\"} model = DynamicShapeIfGuard() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2, 2]\"): cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x); x = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_885", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.control-flow.html", "title": "list_unpack#", "page_title": "python.control-flow — PyTorch 2.8 documentation", "breadcrumbs": "list_unpack#", "content": "list_unpack# Original source code: # mypy: allow-untyped-defs import torch class ListUnpack(torch.nn.Module): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" def forward(self, args: list[torch.Tensor]): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" x, *y = args return x + y[0] example_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],) tags = {\"python.control-flow\", \"python.data-structure\"} model = ListUnpack() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1); args_0 = args_1 = None return (add,) Graph signature: # inputs args_0: USER_INPUT args_1: USER_INPUT args_2: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_884", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_886", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.control-flow.html", "title": "static_for_loop#", "page_title": "python.control-flow — PyTorch 2.8 documentation", "breadcrumbs": "static_for_loop#", "content": "static_for_loop# Original source code: # mypy: allow-untyped-defs import torch class StaticForLoop(torch.nn.Module): \"\"\" A for loop with constant number of iterations should be unrolled in the exported graph. \"\"\" def forward(self, x): # constant ret = [i + x for i in range(10)] return ret example_args = (torch.randn(3, 2),) tags = {\"python.control-flow\"} model = StaticForLoop() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 0) add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1) add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 2) add_3: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 3) add_4: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4) add_5: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 5) add_6: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 6) add_7: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 7) add_8: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 8) add_9: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 9); x = None return (add, add_1, add_2, add_3, add_4, add_5, add_6, add_7, add_8, add_9) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT add_1: USER_OUTPUT add_2: USER_OUTPUT add_3: USER_OUTPUT add_4: USER_OUTPUT add_5: USER_OUTPUT add_6: USER_OUTPUT add_7: USER_OUTPUT add_8: USER_OUTPUT add_9: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_885", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_887", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.control-flow.html", "title": "static_if#", "page_title": "python.control-flow — PyTorch 2.8 documentation", "breadcrumbs": "static_if#", "content": "static_if# Original source code: # mypy: allow-untyped-defs import torch class StaticIf(torch.nn.Module): \"\"\" `if` statement with static predicate value should be traced through with the taken branch. \"\"\" def forward(self, x): if len(x.shape) == 3: return x + torch.ones(1, 1, 1) return x example_args = (torch.randn(3, 2, 2),) tags = {\"python.control-flow\"} model = StaticIf() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2, 2]\"): ones: \"f32[1, 1, 1]\" = torch.ops.aten.ones.default([1, 1, 1], device = device(type='cpu'), pin_memory = False) add: \"f32[3, 2, 2]\" = torch.ops.aten.add.Tensor(x, ones); x = ones = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_886", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_888", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.assert.html", "title": "dynamic_shape_assert#", "page_title": "python.assert — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_assert#", "content": "dynamic_shape_assert# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeAssert(torch.nn.Module): \"\"\" A basic usage of python assertion. \"\"\" def forward(self, x): # assertion with error message assert x.shape[0] > 2, f\"{x.shape[0]} is greater than 2\" # assertion without error message assert x.shape[0] > 1 return x example_args = (torch.randn(3, 2),) tags = {\"python.assert\"} model = DynamicShapeAssert() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): return (x,) Graph signature: # inputs x: USER_INPUT # outputs x: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_889", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.assert.html", "title": "list_contains#", "page_title": "python.assert — PyTorch 2.8 documentation", "breadcrumbs": "list_contains#", "content": "list_contains# Original source code: # mypy: allow-untyped-defs import torch class ListContains(torch.nn.Module): \"\"\" List containment relation can be checked on a dynamic shape or constants. \"\"\" def forward(self, x): assert x.size(-1) in [6, 2] assert x.size(0) not in [4, 5, 6] assert \"monkey\" not in [\"cow\", \"pig\"] return x + x example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"} model = ListContains() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_888", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_890", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.data-structure.html", "title": "dictionary#", "page_title": "python.data-structure — PyTorch 2.8 documentation", "breadcrumbs": "dictionary#", "content": "dictionary# Original source code: # mypy: allow-untyped-defs import torch class Dictionary(torch.nn.Module): \"\"\" Dictionary structures are inlined and flattened along tracing. \"\"\" def forward(self, x, y): elements = {} elements[\"x2\"] = x * x y = y * elements[\"x2\"] return {\"y\": y} example_args = (torch.randn(3, 2), torch.tensor(4)) tags = {\"python.data-structure\"} model = Dictionary() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x); x = None mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul); y = mul = None return (mul_1,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs mul_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_891", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.data-structure.html", "title": "fn_with_kwargs#", "page_title": "python.data-structure — PyTorch 2.8 documentation", "breadcrumbs": "fn_with_kwargs#", "content": "fn_with_kwargs# Original source code: # mypy: allow-untyped-defs import torch class FnWithKwargs(torch.nn.Module): \"\"\" Keyword arguments are not supported at the moment. \"\"\" def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs): out = pos0 for arg in tuple0: out = out * arg for arg in myargs: out = out * arg out = out * mykw0 out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"] return out example_args = ( torch.randn(4), (torch.randn(4), torch.randn(4)), *[torch.randn(4), torch.randn(4)] ) example_kwargs = { \"mykw0\": torch.randn(4), \"input0\": torch.randn(4), \"input1\": torch.randn(4), } tags = {\"python.data-structure\"} model = FnWithKwargs() torch.export.export(model, example_args, example_kwargs) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"): mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0); pos0 = tuple0_0 = None mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1); mul = tuple0_1 = None mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0); mul_1 = myargs_0 = None mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1); mul_2 = myargs_1 = None mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0); mul_3 = mykw0 = None mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0); mul_4 = input0 = None mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1); mul_5 = input1 = None return (mul_6,) Graph signature: # inputs pos0: USER_INPUT tuple0_0: USER_INPUT tuple0_1: USER_INPUT myargs_0: USER_INPUT myargs_1: USER_INPUT mykw0: USER_INPUT input0: USER_INPUT input1: USER_INPUT # outputs mul_6: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_890", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_892", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.data-structure.html", "title": "list_contains#", "page_title": "python.data-structure — PyTorch 2.8 documentation", "breadcrumbs": "list_contains#", "content": "list_contains# Original source code: # mypy: allow-untyped-defs import torch class ListContains(torch.nn.Module): \"\"\" List containment relation can be checked on a dynamic shape or constants. \"\"\" def forward(self, x): assert x.size(-1) in [6, 2] assert x.size(0) not in [4, 5, 6] assert \"monkey\" not in [\"cow\", \"pig\"] return x + x example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"} model = ListContains() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_891", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_893", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.data-structure.html", "title": "list_unpack#", "page_title": "python.data-structure — PyTorch 2.8 documentation", "breadcrumbs": "list_unpack#", "content": "list_unpack# Original source code: # mypy: allow-untyped-defs import torch class ListUnpack(torch.nn.Module): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" def forward(self, args: list[torch.Tensor]): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" x, *y = args return x + y[0] example_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],) tags = {\"python.control-flow\", \"python.data-structure\"} model = ListUnpack() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1); args_0 = args_1 = None return (add,) Graph signature: # inputs args_0: USER_INPUT args_1: USER_INPUT args_2: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_892", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_894", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-value.html", "title": "constrain_as_size_example#", "page_title": "torch.dynamic-value — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_size_example#", "content": "constrain_as_size_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsSizeExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check_is_size is used for values that NEED to be used for constructing tensor. \"\"\" def forward(self, x): a = x.item() torch._check_is_size(a) torch._check(a <= 5) return torch.zeros((a, 5)) example_args = (torch.tensor(4),) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsSizeExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item); sym_constrain_range_for_size_default = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5 _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False); item = None return (zeros,) Graph signature: # inputs x: USER_INPUT # outputs zeros: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_895", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-value.html", "title": "constrain_as_value_example#", "page_title": "torch.dynamic-value — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_value_example#", "content": "constrain_as_value_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsValueExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check is used for values that don't need to be used for constructing tensor. \"\"\" def forward(self, x, y): a = x.item() torch._check(a >= 0) torch._check(a <= 5) if a < 6: return y.sin() return y.cos() example_args = (torch.tensor(4), torch.randn(5, 5)) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsValueExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5; item = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y); y = None return (sin,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": "chunk_894", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_896", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.closure.html", "title": "cond_closed_over_variable#", "page_title": "python.closure — PyTorch 2.8 documentation", "breadcrumbs": "cond_closed_over_variable#", "content": "cond_closed_over_variable# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondClosedOverVariable(torch.nn.Module): \"\"\" torch.cond() supports branches closed over arbitrary variables. \"\"\" def forward(self, pred, x): def true_fn(val): return x * 2 def false_fn(val): return x - 2 return cond(pred, true_fn, false_fn, [x + 1]) example_args = (torch.tensor(True), torch.randn(3, 2)) tags = {\"torch.cond\", \"python.closure\"} model = CondClosedOverVariable() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1); add = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,)); pred = true_graph_0 = false_graph_0 = x = None getitem: \"f32[3, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2); x = None return (mul,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2); x = None return (sub,) Graph signature: # inputs pred: USER_INPUT x: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_897", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/python.closure.html", "title": "nested_function#", "page_title": "python.closure — PyTorch 2.8 documentation", "breadcrumbs": "nested_function#", "content": "nested_function# Original source code: # mypy: allow-untyped-defs import torch class NestedFunction(torch.nn.Module): \"\"\" Nested functions are traced through. Side effects on global captures are not supported though. \"\"\" def forward(self, a, b): x = a + b z = a - b def closure(y): nonlocal x x += 1 return x * y + z return closure(x) example_args = (torch.randn(3, 2), torch.randn(2)) tags = {\"python.closure\"} model = NestedFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b) sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b); a = b = None add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1); add = None mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_); add_ = None add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub); mul = sub = None return (add_1,) Graph signature: # inputs a: USER_INPUT b: USER_INPUT # outputs add_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_896", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_898", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_branch_class_method#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_class_method#", "content": "cond_branch_class_method# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class MySubModule(torch.nn.Module): def foo(self, x): return x.cos() def forward(self, x): return self.foo(x) class CondBranchClassMethod(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using class method in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def __init__(self) -> None: super().__init__() self.subm = MySubModule() def bar(self, x): return x.sin() def forward(self, x): return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchClassMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): sin: \"f32[3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_899", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_branch_nested_function#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nested_function#", "content": "cond_branch_nested_function# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNestedFunction(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using nested function in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): def true_fn(x): def inner_true_fn(y): return x + y return inner_true_fn(x) def false_fn(x): def inner_false_fn(y): return x - y return inner_false_fn(x) return cond(x.shape[0] < 10, true_fn, false_fn, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNestedFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_898", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_900", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_branch_nonlocal_variables#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "cond_branch_nonlocal_variables# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNonlocalVariables(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions. The code below will not work because capturing closure variables is not supported. ``` my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(y): nonlocal my_tensor_var, my_primitive_var return y + my_tensor_var + my_primitive_var def false_fn(y): nonlocal my_tensor_var, my_primitive_var return y - my_tensor_var - my_primitive_var return cond(x.shape[0] > 5, true_fn, false_fn, [x]) ``` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(x, y, z): return x + y + z def false_fn(x, y, z): return x - y - z return cond( x.shape[0] > 5, true_fn, false_fn, [x, my_tensor_var, torch.tensor(my_primitive_var)], ) example_args = (torch.randn(6),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNonlocalVariables() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"): add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100) lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0); c_lifted_tensor_0 = None detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy); lift_fresh_copy = None add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add); x = add = None add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_); add_1 = detach_ = None return (add_2,) Graph signature: # inputs c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0' x: USER_INPUT #", "prev_chunk_id": "chunk_899", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_901", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_branch_nonlocal_variables#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "outputs add_2: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_900", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_902", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_closed_over_variable#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_closed_over_variable#", "content": "cond_closed_over_variable# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondClosedOverVariable(torch.nn.Module): \"\"\" torch.cond() supports branches closed over arbitrary variables. \"\"\" def forward(self, pred, x): def true_fn(val): return x * 2 def false_fn(val): return x - 2 return cond(pred, true_fn, false_fn, [x + 1]) example_args = (torch.tensor(True), torch.randn(3, 2)) tags = {\"torch.cond\", \"python.closure\"} model = CondClosedOverVariable() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1); add = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,)); pred = true_graph_0 = false_graph_0 = x = None getitem: \"f32[3, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2); x = None return (mul,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2); x = None return (sub,) Graph signature: # inputs pred: USER_INPUT x: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_901", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_903", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_operands#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_operands#", "content": "cond_operands# Original source code: # mypy: allow-untyped-defs import torch from torch.export import Dim x = torch.randn(3, 2) y = torch.randn(2) dim0_x = Dim(\"dim0_x\") class CondOperands(torch.nn.Module): \"\"\" The operands passed to cond() must be: - a list of tensors - match arguments of `true_fn` and `false_fn` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x, y): def true_fn(x, y): return x + y def false_fn(x, y): return x - y return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y]) example_args = (x, y) tags = { \"torch.cond\", \"torch.dynamic-shape\", } extra_inputs = (torch.randn(2, 2), torch.randn(2)) dynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None} model = CondOperands() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): # sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0) gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2; sym_size_int_1 = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y)); gt = true_graph_0 = false_graph_0 = x = y = None getitem: \"f32[s77, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y); x = y = None return (add,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y); x = y = None return (sub,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {s77: VR[0, int_oo]}", "prev_chunk_id": "chunk_902", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_904", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.cond.html", "title": "cond_predicate#", "page_title": "torch.cond — PyTorch 2.8 documentation", "breadcrumbs": "cond_predicate#", "content": "cond_predicate# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondPredicate(torch.nn.Module): \"\"\" The conditional statement (aka predicate) passed to cond() must be one of the following: - torch.Tensor with a single element - boolean expression NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): pred = x.dim() > 2 and x.shape[2] > 10 return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x]) example_args = (torch.randn(6, 4, 3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondPredicate() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[6, 4, 3]\"): sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_903", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_905", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_branch_class_method#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_class_method#", "content": "cond_branch_class_method# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class MySubModule(torch.nn.Module): def foo(self, x): return x.cos() def forward(self, x): return self.foo(x) class CondBranchClassMethod(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using class method in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def __init__(self) -> None: super().__init__() self.subm = MySubModule() def bar(self, x): return x.sin() def forward(self, x): return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchClassMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): sin: \"f32[3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_906", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_branch_nested_function#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nested_function#", "content": "cond_branch_nested_function# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNestedFunction(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using nested function in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): def true_fn(x): def inner_true_fn(y): return x + y return inner_true_fn(x) def false_fn(x): def inner_false_fn(y): return x - y return inner_false_fn(x) return cond(x.shape[0] < 10, true_fn, false_fn, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNestedFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_905", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_907", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_branch_nonlocal_variables#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "cond_branch_nonlocal_variables# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNonlocalVariables(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions. The code below will not work because capturing closure variables is not supported. ``` my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(y): nonlocal my_tensor_var, my_primitive_var return y + my_tensor_var + my_primitive_var def false_fn(y): nonlocal my_tensor_var, my_primitive_var return y - my_tensor_var - my_primitive_var return cond(x.shape[0] > 5, true_fn, false_fn, [x]) ``` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(x, y, z): return x + y + z def false_fn(x, y, z): return x - y - z return cond( x.shape[0] > 5, true_fn, false_fn, [x, my_tensor_var, torch.tensor(my_primitive_var)], ) example_args = (torch.randn(6),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNonlocalVariables() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"): add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100) lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0); c_lifted_tensor_0 = None detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy); lift_fresh_copy = None add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add); x = add = None add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_); add_1 = detach_ = None return (add_2,) Graph signature: # inputs c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0' x: USER_INPUT #", "prev_chunk_id": "chunk_906", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_908", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_branch_nonlocal_variables#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "outputs add_2: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_907", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_909", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_operands#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_operands#", "content": "cond_operands# Original source code: # mypy: allow-untyped-defs import torch from torch.export import Dim x = torch.randn(3, 2) y = torch.randn(2) dim0_x = Dim(\"dim0_x\") class CondOperands(torch.nn.Module): \"\"\" The operands passed to cond() must be: - a list of tensors - match arguments of `true_fn` and `false_fn` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x, y): def true_fn(x, y): return x + y def false_fn(x, y): return x - y return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y]) example_args = (x, y) tags = { \"torch.cond\", \"torch.dynamic-shape\", } extra_inputs = (torch.randn(2, 2), torch.randn(2)) dynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None} model = CondOperands() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): # sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0) gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2; sym_size_int_1 = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y)); gt = true_graph_0 = false_graph_0 = x = y = None getitem: \"f32[s77, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y); x = y = None return (add,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y); x = y = None return (sub,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {s77: VR[0, int_oo]}", "prev_chunk_id": "chunk_908", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_910", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "cond_predicate#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "cond_predicate#", "content": "cond_predicate# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondPredicate(torch.nn.Module): \"\"\" The conditional statement (aka predicate) passed to cond() must be one of the following: - torch.Tensor with a single element - boolean expression NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): pred = x.dim() > 2 and x.shape[2] > 10 return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x]) example_args = (torch.randn(6, 4, 3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondPredicate() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[6, 4, 3]\"): sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_909", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_911", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_constructor#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_constructor#", "content": "dynamic_shape_constructor# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeConstructor(torch.nn.Module): \"\"\" Tensor constructors should be captured with dynamic shape inputs rather than being baked in with static shape. \"\"\" def forward(self, x): return torch.zeros(x.shape[0] * 2) example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeConstructor() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): zeros: \"f32[6]\" = torch.ops.aten.zeros.default([6], device = device(type='cpu'), pin_memory = False) return (zeros,) Graph signature: # inputs x: USER_INPUT # outputs zeros: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_910", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_912", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_if_guard#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_if_guard#", "content": "dynamic_shape_if_guard# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeIfGuard(torch.nn.Module): \"\"\" `if` statement with backed dynamic shape predicate will be specialized into one particular branch and generate a guard. However, export will fail if the the dimension is marked as dynamic shape from higher level API. \"\"\" def forward(self, x): if x.shape[0] == 3: return x.cos() return x.sin() example_args = (torch.randn(3, 2, 2),) tags = {\"torch.dynamic-shape\", \"python.control-flow\"} model = DynamicShapeIfGuard() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2, 2]\"): cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x); x = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_911", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_913", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_map#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_map#", "content": "dynamic_shape_map# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import map class DynamicShapeMap(torch.nn.Module): \"\"\" functorch map() maps a function over the first tensor dimension. \"\"\" def forward(self, xs, y): def body(x, y): return x + y return map(body, xs, y) example_args = (torch.randn(3, 2), torch.randn(2)) tags = {\"torch.dynamic-shape\", \"torch.map\"} model = DynamicShapeMap() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"): body_graph_0 = self.body_graph_0 map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]); body_graph_0 = xs = y = None getitem: \"f32[3, 2]\" = map_impl[0]; map_impl = None return (getitem,) class body_graph_0(torch.nn.Module): def forward(self, xs: \"f32[2]\", y: \"f32[2]\"): add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y); xs = y = None return (add,) Graph signature: # inputs xs: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_912", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_914", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_round#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_round#", "content": "dynamic_shape_round# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel from torch.export import Dim class DynamicShapeRound(torch.nn.Module): \"\"\" Calling round on dynamic shapes is not supported. \"\"\" def forward(self, x): return x[: round(x.shape[0] / 2)] x = torch.randn(3, 2) dim0_x = Dim(\"dim0_x\") example_args = (x,) tags = {\"torch.dynamic-shape\", \"python.builtin\"} support_level = SupportLevel.NOT_SUPPORTED_YET dynamic_shapes = {\"x\": {0: dim0_x}} model = DynamicShapeRound() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: Unsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".", "prev_chunk_id": "chunk_913", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_915", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_slicing#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_slicing#", "content": "dynamic_shape_slicing# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeSlicing(torch.nn.Module): \"\"\" Slices with dynamic shape arguments should be captured into the graph rather than being baked in. \"\"\" def forward(self, x): return x[: x.shape[0] - 2, x.shape[1] - 1 :: 2] example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeSlicing() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): slice_1: \"f32[1, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 1); x = None slice_2: \"f32[1, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 1, 9223372036854775807, 2); slice_1 = None return (slice_2,) Graph signature: # inputs x: USER_INPUT # outputs slice_2: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_914", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_916", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "dynamic_shape_view#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_view#", "content": "dynamic_shape_view# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeView(torch.nn.Module): \"\"\" Dynamic shapes should be propagated to view arguments instead of being baked into the exported graph. \"\"\" def forward(self, x): new_x_shape = x.size()[:-1] + (2, 5) x = x.view(*new_x_shape) return x.permute(0, 2, 1) example_args = (torch.randn(10, 10),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeView() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[10, 10]\"): view: \"f32[10, 2, 5]\" = torch.ops.aten.view.default(x, [10, 2, 5]); x = None permute: \"f32[10, 5, 2]\" = torch.ops.aten.permute.default(view, [0, 2, 1]); view = None return (permute,) Graph signature: # inputs x: USER_INPUT # outputs permute: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_915", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_917", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "list_contains#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "list_contains#", "content": "list_contains# Original source code: # mypy: allow-untyped-defs import torch class ListContains(torch.nn.Module): \"\"\" List containment relation can be checked on a dynamic shape or constants. \"\"\" def forward(self, x): assert x.size(-1) in [6, 2] assert x.size(0) not in [4, 5, 6] assert \"monkey\" not in [\"cow\", \"pig\"] return x + x example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"} model = ListContains() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_916", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_918", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html", "title": "scalar_output#", "page_title": "torch.dynamic-shape — PyTorch 2.8 documentation", "breadcrumbs": "scalar_output#", "content": "scalar_output# Original source code: # mypy: allow-untyped-defs import torch from torch.export import Dim x = torch.randn(3, 2) dim1_x = Dim(\"dim1_x\") class ScalarOutput(torch.nn.Module): \"\"\" Returning scalar values from the graph is supported, in addition to Tensor outputs. Symbolic shapes are captured and rank is specialized. \"\"\" def __init__(self) -> None: super().__init__() def forward(self, x): return x.shape[1] + 1 example_args = (x,) tags = {\"torch.dynamic-shape\"} dynamic_shapes = {\"x\": {1: dim1_x}} model = ScalarOutput() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, s27]\"): # sym_size_int_1: \"Sym(s27)\" = torch.ops.aten.sym_size.int(x, 1); x = None add: \"Sym(s27 + 1)\" = sym_size_int_1 + 1; sym_size_int_1 = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {s27: VR[0, int_oo]}", "prev_chunk_id": "chunk_917", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_919", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.escape-hatch.html", "title": "assume_constant_result#", "page_title": "torch.escape-hatch — PyTorch 2.8 documentation", "breadcrumbs": "assume_constant_result#", "content": "assume_constant_result# Original source code: # mypy: allow-untyped-defs import torch import torch._dynamo as torchdynamo class AssumeConstantResult(torch.nn.Module): \"\"\" Applying `assume_constant_result` decorator to burn make non-tracable code as constant. \"\"\" @torchdynamo.assume_constant_result def get_item(self, y): return y.int().item() def forward(self, x, y): return x[: self.get_item(y)] example_args = (torch.randn(3, 2), torch.tensor(4)) tags = {\"torch.escape-hatch\"} model = AssumeConstantResult() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"): slice_1: \"f32[3, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 4); x = None return (slice_1,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs slice_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_920", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.escape-hatch.html", "title": "constrain_as_size_example#", "page_title": "torch.escape-hatch — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_size_example#", "content": "constrain_as_size_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsSizeExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check_is_size is used for values that NEED to be used for constructing tensor. \"\"\" def forward(self, x): a = x.item() torch._check_is_size(a) torch._check(a <= 5) return torch.zeros((a, 5)) example_args = (torch.tensor(4),) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsSizeExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item); sym_constrain_range_for_size_default = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5 _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False); item = None return (zeros,) Graph signature: # inputs x: USER_INPUT # outputs zeros: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": "chunk_919", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_921", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/torch.escape-hatch.html", "title": "constrain_as_value_example#", "page_title": "torch.escape-hatch — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_value_example#", "content": "constrain_as_value_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsValueExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check is used for values that don't need to be used for constructing tensor. \"\"\" def forward(self, x, y): a = x.item() torch._check(a >= 0) torch._check(a <= 5) if a < 6: return y.sin() return y.cos() example_args = (torch.tensor(4), torch.randn(5, 5)) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsValueExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5; item = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y); y = None return (sin,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": "chunk_920", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_922", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "ExportDB#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "ExportDB#", "content": "ExportDB# ExportDB is a centralized dataset of supported and unsupported export cases. It is targeted towards users who want to understand specifically what types of code are supported, the subtleties of export, and how to modify their existing code to be compatible with export. Note that this is not an exhaustive set of everything that is supported by exportdb, but it covers the most common and confusing use cases that users will run into. If you have a feature that you think needs a stronger guarantee from us to support in export please create an issue in the pytorch/pytorch repo with a module:export tag.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_923", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "assume_constant_result#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "assume_constant_result#", "content": "assume_constant_result# Original source code: # mypy: allow-untyped-defs import torch import torch._dynamo as torchdynamo class AssumeConstantResult(torch.nn.Module): \"\"\" Applying `assume_constant_result` decorator to burn make non-tracable code as constant. \"\"\" @torchdynamo.assume_constant_result def get_item(self, y): return y.int().item() def forward(self, x, y): return x[: self.get_item(y)] example_args = (torch.randn(3, 2), torch.tensor(4)) tags = {\"torch.escape-hatch\"} model = AssumeConstantResult() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"): slice_1: \"f32[3, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 4); x = None return (slice_1,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs slice_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_922", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_924", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "autograd_function#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "autograd_function#", "content": "autograd_function# Original source code: # mypy: allow-untyped-defs import torch class MyAutogradFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): return x.clone() @staticmethod def backward(ctx, grad_output): return grad_output + 1 class AutogradFunction(torch.nn.Module): \"\"\" TorchDynamo does not keep track of backward() on autograd functions. We recommend to use `allow_in_graph` to mitigate this problem. \"\"\" def forward(self, x): return MyAutogradFunction.apply(x) example_args = (torch.randn(3, 2),) model = AutogradFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): clone: \"f32[3, 2]\" = torch.ops.aten.clone.default(x); x = None return (clone,) Graph signature: # inputs x: USER_INPUT # outputs clone: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_923", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_925", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "class_method#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "class_method#", "content": "class_method# Original source code: # mypy: allow-untyped-defs import torch class ClassMethod(torch.nn.Module): \"\"\" Class methods are inlined during tracing. \"\"\" @classmethod def method(cls, x): return x + 1 def __init__(self) -> None: super().__init__() self.linear = torch.nn.Linear(4, 2) def forward(self, x): x = self.linear(x) return self.method(x) * self.__class__.method(x) * type(self).method(x) example_args = (torch.randn(3, 4),) model = ClassMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_linear_weight: \"f32[2, 4]\", p_linear_bias: \"f32[2]\", x: \"f32[3, 4]\"): linear: \"f32[3, 2]\" = torch.ops.aten.linear.default(x, p_linear_weight, p_linear_bias); x = p_linear_weight = p_linear_bias = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1) add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1) mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add, add_1); add = add_1 = None add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1); linear = None mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(mul, add_2); mul = add_2 = None return (mul_1,) Graph signature: # inputs p_linear_weight: PARAMETER target='linear.weight' p_linear_bias: PARAMETER target='linear.bias' x: USER_INPUT # outputs mul_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_924", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_926", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_branch_class_method#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_class_method#", "content": "cond_branch_class_method# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class MySubModule(torch.nn.Module): def foo(self, x): return x.cos() def forward(self, x): return self.foo(x) class CondBranchClassMethod(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using class method in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def __init__(self) -> None: super().__init__() self.subm = MySubModule() def bar(self, x): return x.sin() def forward(self, x): return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchClassMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): sin: \"f32[3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_925", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_927", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_branch_nested_function#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nested_function#", "content": "cond_branch_nested_function# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNestedFunction(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates using nested function in cond(). NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): def true_fn(x): def inner_true_fn(y): return x + y return inner_true_fn(x) def false_fn(x): def inner_false_fn(y): return x - y return inner_false_fn(x) return cond(x.shape[0] < 10, true_fn, false_fn, [x]) example_args = (torch.randn(3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNestedFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3]\"): add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_926", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_928", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_branch_nonlocal_variables#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "cond_branch_nonlocal_variables# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondBranchNonlocalVariables(torch.nn.Module): \"\"\" The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules: - both branches must take the same args, which must also match the branch args passed to cond. - both branches must return a single tensor - returned tensor must have the same tensor metadata, e.g. shape and dtype - branch function can be free function, nested function, lambda, class methods - branch function can not have closure variables - no inplace mutations on inputs or global variables This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions. The code below will not work because capturing closure variables is not supported. ``` my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(y): nonlocal my_tensor_var, my_primitive_var return y + my_tensor_var + my_primitive_var def false_fn(y): nonlocal my_tensor_var, my_primitive_var return y - my_tensor_var - my_primitive_var return cond(x.shape[0] > 5, true_fn, false_fn, [x]) ``` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): my_tensor_var = x + 100 my_primitive_var = 3.14 def true_fn(x, y, z): return x + y + z def false_fn(x, y, z): return x - y - z return cond( x.shape[0] > 5, true_fn, false_fn, [x, my_tensor_var, torch.tensor(my_primitive_var)], ) example_args = (torch.randn(6),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondBranchNonlocalVariables() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"): add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100) lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0); c_lifted_tensor_0 = None detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy); lift_fresh_copy = None add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add); x = add = None add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_); add_1 = detach_ = None return (add_2,) Graph signature: # inputs c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0' x: USER_INPUT #", "prev_chunk_id": "chunk_927", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_929", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_branch_nonlocal_variables#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_branch_nonlocal_variables#", "content": "outputs add_2: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_928", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_930", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_closed_over_variable#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_closed_over_variable#", "content": "cond_closed_over_variable# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondClosedOverVariable(torch.nn.Module): \"\"\" torch.cond() supports branches closed over arbitrary variables. \"\"\" def forward(self, pred, x): def true_fn(val): return x * 2 def false_fn(val): return x - 2 return cond(pred, true_fn, false_fn, [x + 1]) example_args = (torch.tensor(True), torch.randn(3, 2)) tags = {\"torch.cond\", \"python.closure\"} model = CondClosedOverVariable() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1); add = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,)); pred = true_graph_0 = false_graph_0 = x = None getitem: \"f32[3, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2); x = None return (mul,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2); x = None return (sub,) Graph signature: # inputs pred: USER_INPUT x: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_929", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_931", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_operands#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_operands#", "content": "cond_operands# Original source code: # mypy: allow-untyped-defs import torch from torch.export import Dim x = torch.randn(3, 2) y = torch.randn(2) dim0_x = Dim(\"dim0_x\") class CondOperands(torch.nn.Module): \"\"\" The operands passed to cond() must be: - a list of tensors - match arguments of `true_fn` and `false_fn` NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x, y): def true_fn(x, y): return x + y def false_fn(x, y): return x - y return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y]) example_args = (x, y) tags = { \"torch.cond\", \"torch.dynamic-shape\", } extra_inputs = (torch.randn(2, 2), torch.randn(2)) dynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None} model = CondOperands() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): # sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0) gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2; sym_size_int_1 = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y)); gt = true_graph_0 = false_graph_0 = x = y = None getitem: \"f32[s77, 2]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y); x = y = None return (add,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"): sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y); x = y = None return (sub,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {s77: VR[0, int_oo]}", "prev_chunk_id": "chunk_930", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_932", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "cond_predicate#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "cond_predicate#", "content": "cond_predicate# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import cond class CondPredicate(torch.nn.Module): \"\"\" The conditional statement (aka predicate) passed to cond() must be one of the following: - torch.Tensor with a single element - boolean expression NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized. \"\"\" def forward(self, x): pred = x.dim() > 2 and x.shape[2] > 10 return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x]) example_args = (torch.randn(6, 4, 3),) tags = { \"torch.cond\", \"torch.dynamic-shape\", } model = CondPredicate() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[6, 4, 3]\"): sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) Graph signature: # inputs x: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_931", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_933", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "constrain_as_size_example#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_size_example#", "content": "constrain_as_size_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsSizeExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check_is_size is used for values that NEED to be used for constructing tensor. \"\"\" def forward(self, x): a = x.item() torch._check_is_size(a) torch._check(a <= 5) return torch.zeros((a, 5)) example_args = (torch.tensor(4),) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsSizeExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item); sym_constrain_range_for_size_default = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5 _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False); item = None return (zeros,) Graph signature: # inputs x: USER_INPUT # outputs zeros: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": "chunk_932", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_934", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "constrain_as_value_example#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "constrain_as_value_example#", "content": "constrain_as_value_example# Original source code: # mypy: allow-untyped-defs import torch class ConstrainAsValueExample(torch.nn.Module): \"\"\" If the value is not known at tracing time, you can provide hint so that we can trace further. Please look at torch._check and torch._check_is_size APIs. torch._check is used for values that don't need to be used for constructing tensor. \"\"\" def forward(self, x, y): a = x.item() torch._check(a >= 0) torch._check(a <= 5) if a < 6: return y.sin() return y.cos() example_args = (torch.tensor(4), torch.randn(5, 5)) tags = { \"torch.dynamic-value\", \"torch.escape-hatch\", } model = ConstrainAsValueExample() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"): item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_1: \"Sym(u0 >= 0)\" = item >= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\"); ge_1 = _assert_scalar_default = None le_1: \"Sym(u0 <= 5)\" = item <= 5; item = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\"); le_1 = _assert_scalar_default_1 = None sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y); y = None return (sin,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs sin: USER_OUTPUT Range constraints: {u0: VR[0, 5], u1: VR[0, 5]}", "prev_chunk_id": "chunk_933", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_935", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "decorator#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "decorator#", "content": "decorator# Original source code: # mypy: allow-untyped-defs import functools import torch def test_decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) + 1 return wrapper class Decorator(torch.nn.Module): \"\"\" Decorators calls are inlined into the exported function during tracing. \"\"\" @test_decorator def forward(self, x, y): return x + y example_args = (torch.randn(3, 2), torch.randn(3, 2)) model = Decorator() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", y: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, y); x = y = None add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(add, 1); add = None return (add_1,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs add_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_934", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_936", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dictionary#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dictionary#", "content": "dictionary# Original source code: # mypy: allow-untyped-defs import torch class Dictionary(torch.nn.Module): \"\"\" Dictionary structures are inlined and flattened along tracing. \"\"\" def forward(self, x, y): elements = {} elements[\"x2\"] = x * x y = y * elements[\"x2\"] return {\"y\": y} example_args = (torch.randn(3, 2), torch.tensor(4)) tags = {\"python.data-structure\"} model = Dictionary() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x); x = None mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul); y = mul = None return (mul_1,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs mul_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_935", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_937", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_assert#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_assert#", "content": "dynamic_shape_assert# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeAssert(torch.nn.Module): \"\"\" A basic usage of python assertion. \"\"\" def forward(self, x): # assertion with error message assert x.shape[0] > 2, f\"{x.shape[0]} is greater than 2\" # assertion without error message assert x.shape[0] > 1 return x example_args = (torch.randn(3, 2),) tags = {\"python.assert\"} model = DynamicShapeAssert() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): return (x,) Graph signature: # inputs x: USER_INPUT # outputs x: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_936", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_938", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_constructor#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_constructor#", "content": "dynamic_shape_constructor# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeConstructor(torch.nn.Module): \"\"\" Tensor constructors should be captured with dynamic shape inputs rather than being baked in with static shape. \"\"\" def forward(self, x): return torch.zeros(x.shape[0] * 2) example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeConstructor() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): zeros: \"f32[6]\" = torch.ops.aten.zeros.default([6], device = device(type='cpu'), pin_memory = False) return (zeros,) Graph signature: # inputs x: USER_INPUT # outputs zeros: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_937", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_939", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_if_guard#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_if_guard#", "content": "dynamic_shape_if_guard# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeIfGuard(torch.nn.Module): \"\"\" `if` statement with backed dynamic shape predicate will be specialized into one particular branch and generate a guard. However, export will fail if the the dimension is marked as dynamic shape from higher level API. \"\"\" def forward(self, x): if x.shape[0] == 3: return x.cos() return x.sin() example_args = (torch.randn(3, 2, 2),) tags = {\"torch.dynamic-shape\", \"python.control-flow\"} model = DynamicShapeIfGuard() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2, 2]\"): cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x); x = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_938", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_940", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_map#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_map#", "content": "dynamic_shape_map# Original source code: # mypy: allow-untyped-defs import torch from functorch.experimental.control_flow import map class DynamicShapeMap(torch.nn.Module): \"\"\" functorch map() maps a function over the first tensor dimension. \"\"\" def forward(self, xs, y): def body(x, y): return x + y return map(body, xs, y) example_args = (torch.randn(3, 2), torch.randn(2)) tags = {\"torch.dynamic-shape\", \"torch.map\"} model = DynamicShapeMap() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"): body_graph_0 = self.body_graph_0 map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]); body_graph_0 = xs = y = None getitem: \"f32[3, 2]\" = map_impl[0]; map_impl = None return (getitem,) class body_graph_0(torch.nn.Module): def forward(self, xs: \"f32[2]\", y: \"f32[2]\"): add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y); xs = y = None return (add,) Graph signature: # inputs xs: USER_INPUT y: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_939", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_941", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_slicing#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_slicing#", "content": "dynamic_shape_slicing# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeSlicing(torch.nn.Module): \"\"\" Slices with dynamic shape arguments should be captured into the graph rather than being baked in. \"\"\" def forward(self, x): return x[: x.shape[0] - 2, x.shape[1] - 1 :: 2] example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeSlicing() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): slice_1: \"f32[1, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 1); x = None slice_2: \"f32[1, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 1, 9223372036854775807, 2); slice_1 = None return (slice_2,) Graph signature: # inputs x: USER_INPUT # outputs slice_2: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_940", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_942", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_view#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_view#", "content": "dynamic_shape_view# Original source code: # mypy: allow-untyped-defs import torch class DynamicShapeView(torch.nn.Module): \"\"\" Dynamic shapes should be propagated to view arguments instead of being baked into the exported graph. \"\"\" def forward(self, x): new_x_shape = x.size()[:-1] + (2, 5) x = x.view(*new_x_shape) return x.permute(0, 2, 1) example_args = (torch.randn(10, 10),) tags = {\"torch.dynamic-shape\"} model = DynamicShapeView() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[10, 10]\"): view: \"f32[10, 2, 5]\" = torch.ops.aten.view.default(x, [10, 2, 5]); x = None permute: \"f32[10, 5, 2]\" = torch.ops.aten.permute.default(view, [0, 2, 1]); view = None return (permute,) Graph signature: # inputs x: USER_INPUT # outputs permute: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_941", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_943", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "fn_with_kwargs#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "fn_with_kwargs#", "content": "fn_with_kwargs# Original source code: # mypy: allow-untyped-defs import torch class FnWithKwargs(torch.nn.Module): \"\"\" Keyword arguments are not supported at the moment. \"\"\" def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs): out = pos0 for arg in tuple0: out = out * arg for arg in myargs: out = out * arg out = out * mykw0 out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"] return out example_args = ( torch.randn(4), (torch.randn(4), torch.randn(4)), *[torch.randn(4), torch.randn(4)] ) example_kwargs = { \"mykw0\": torch.randn(4), \"input0\": torch.randn(4), \"input1\": torch.randn(4), } tags = {\"python.data-structure\"} model = FnWithKwargs() torch.export.export(model, example_args, example_kwargs) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"): mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0); pos0 = tuple0_0 = None mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1); mul = tuple0_1 = None mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0); mul_1 = myargs_0 = None mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1); mul_2 = myargs_1 = None mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0); mul_3 = mykw0 = None mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0); mul_4 = input0 = None mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1); mul_5 = input1 = None return (mul_6,) Graph signature: # inputs pos0: USER_INPUT tuple0_0: USER_INPUT tuple0_1: USER_INPUT myargs_0: USER_INPUT myargs_1: USER_INPUT mykw0: USER_INPUT input0: USER_INPUT input1: USER_INPUT # outputs mul_6: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_942", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_944", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "list_contains#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "list_contains#", "content": "list_contains# Original source code: # mypy: allow-untyped-defs import torch class ListContains(torch.nn.Module): \"\"\" List containment relation can be checked on a dynamic shape or constants. \"\"\" def forward(self, x): assert x.size(-1) in [6, 2] assert x.size(0) not in [4, 5, 6] assert \"monkey\" not in [\"cow\", \"pig\"] return x + x example_args = (torch.randn(3, 2),) tags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"} model = ListContains() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_943", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_945", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "list_unpack#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "list_unpack#", "content": "list_unpack# Original source code: # mypy: allow-untyped-defs import torch class ListUnpack(torch.nn.Module): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" def forward(self, args: list[torch.Tensor]): \"\"\" Lists are treated as static construct, therefore unpacking should be erased after tracing. \"\"\" x, *y = args return x + y[0] example_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],) tags = {\"python.control-flow\", \"python.data-structure\"} model = ListUnpack() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1); args_0 = args_1 = None return (add,) Graph signature: # inputs args_0: USER_INPUT args_1: USER_INPUT args_2: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_944", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_946", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "nested_function#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "nested_function#", "content": "nested_function# Original source code: # mypy: allow-untyped-defs import torch class NestedFunction(torch.nn.Module): \"\"\" Nested functions are traced through. Side effects on global captures are not supported though. \"\"\" def forward(self, a, b): x = a + b z = a - b def closure(y): nonlocal x x += 1 return x * y + z return closure(x) example_args = (torch.randn(3, 2), torch.randn(2)) tags = {\"python.closure\"} model = NestedFunction() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b) sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b); a = b = None add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1); add = None mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_); add_ = None add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub); mul = sub = None return (add_1,) Graph signature: # inputs a: USER_INPUT b: USER_INPUT # outputs add_1: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_945", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_947", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "null_context_manager#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "null_context_manager#", "content": "null_context_manager# Original source code: # mypy: allow-untyped-defs import contextlib import torch class NullContextManager(torch.nn.Module): \"\"\" Null context manager in Python will be traced out. \"\"\" def forward(self, x): \"\"\" Null context manager in Python will be traced out. \"\"\" ctx = contextlib.nullcontext() with ctx: return x.sin() + x.cos() example_args = (torch.randn(3, 2),) tags = {\"python.context-manager\"} model = NullContextManager() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): sin: \"f32[3, 2]\" = torch.ops.aten.sin.default(x) cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(x); x = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(sin, cos); sin = cos = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_946", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_948", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "pytree_flatten#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "pytree_flatten#", "content": "pytree_flatten# Original source code: # mypy: allow-untyped-defs import torch from torch.utils import _pytree as pytree class PytreeFlatten(torch.nn.Module): \"\"\" Pytree from PyTorch can be captured by TorchDynamo. \"\"\" def forward(self, x): y, _spec = pytree.tree_flatten(x) return y[0] + 1 example_args = ({1: torch.randn(3, 2), 2: torch.randn(3, 2)},), model = PytreeFlatten() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x_0_1: \"f32[3, 2]\", x_0_2: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x_0_1, 1); x_0_1 = None return (add,) Graph signature: # inputs x_0_1: USER_INPUT x_0_2: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_947", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_949", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "scalar_output#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "scalar_output#", "content": "scalar_output# Original source code: # mypy: allow-untyped-defs import torch from torch.export import Dim x = torch.randn(3, 2) dim1_x = Dim(\"dim1_x\") class ScalarOutput(torch.nn.Module): \"\"\" Returning scalar values from the graph is supported, in addition to Tensor outputs. Symbolic shapes are captured and rank is specialized. \"\"\" def __init__(self) -> None: super().__init__() def forward(self, x): return x.shape[1] + 1 example_args = (x,) tags = {\"torch.dynamic-shape\"} dynamic_shapes = {\"x\": {1: dim1_x}} model = ScalarOutput() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, s27]\"): # sym_size_int_1: \"Sym(s27)\" = torch.ops.aten.sym_size.int(x, 1); x = None add: \"Sym(s27 + 1)\" = sym_size_int_1 + 1; sym_size_int_1 = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {s27: VR[0, int_oo]}", "prev_chunk_id": "chunk_948", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_950", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "specialized_attribute#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "specialized_attribute#", "content": "specialized_attribute# Original source code: # mypy: allow-untyped-defs from enum import Enum import torch class Animal(Enum): COW = \"moo\" class SpecializedAttribute(torch.nn.Module): \"\"\" Model attributes are specialized. \"\"\" def __init__(self) -> None: super().__init__() self.a = \"moo\" self.b = 4 def forward(self, x): if self.a == Animal.COW.value: return x * x + self.b else: raise ValueError(\"bad\") example_args = (torch.randn(3, 2),) model = SpecializedAttribute() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x); x = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, 4); mul = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_949", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_951", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "static_for_loop#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "static_for_loop#", "content": "static_for_loop# Original source code: # mypy: allow-untyped-defs import torch class StaticForLoop(torch.nn.Module): \"\"\" A for loop with constant number of iterations should be unrolled in the exported graph. \"\"\" def forward(self, x): # constant ret = [i + x for i in range(10)] return ret example_args = (torch.randn(3, 2),) tags = {\"python.control-flow\"} model = StaticForLoop() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 0) add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1) add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 2) add_3: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 3) add_4: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4) add_5: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 5) add_6: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 6) add_7: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 7) add_8: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 8) add_9: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 9); x = None return (add, add_1, add_2, add_3, add_4, add_5, add_6, add_7, add_8, add_9) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT add_1: USER_OUTPUT add_2: USER_OUTPUT add_3: USER_OUTPUT add_4: USER_OUTPUT add_5: USER_OUTPUT add_6: USER_OUTPUT add_7: USER_OUTPUT add_8: USER_OUTPUT add_9: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_950", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_952", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "static_if#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "static_if#", "content": "static_if# Original source code: # mypy: allow-untyped-defs import torch class StaticIf(torch.nn.Module): \"\"\" `if` statement with static predicate value should be traced through with the taken branch. \"\"\" def forward(self, x): if len(x.shape) == 3: return x + torch.ones(1, 1, 1) return x example_args = (torch.randn(3, 2, 2),) tags = {\"python.control-flow\"} model = StaticIf() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2, 2]\"): ones: \"f32[1, 1, 1]\" = torch.ops.aten.ones.default([1, 1, 1], device = device(type='cpu'), pin_memory = False) add: \"f32[3, 2, 2]\" = torch.ops.aten.add.Tensor(x, ones); x = ones = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_951", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_953", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "tensor_setattr#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "tensor_setattr#", "content": "tensor_setattr# Original source code: # mypy: allow-untyped-defs import torch class TensorSetattr(torch.nn.Module): \"\"\" setattr() call onto tensors is not supported. \"\"\" def forward(self, x, attr): setattr(x, attr, torch.randn(3, 2)) return x + 4 example_args = (torch.randn(3, 2), \"attr\") tags = {\"python.builtin\"} model = TensorSetattr() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\", attr): randn: \"f32[3, 2]\" = torch.ops.aten.randn.default([3, 2], device = device(type='cpu'), pin_memory = False); randn = None add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4); x = None return (add,) Graph signature: # inputs x: USER_INPUT attr: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_952", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_954", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "type_reflection_method#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "type_reflection_method#", "content": "type_reflection_method# Original source code: # mypy: allow-untyped-defs import torch class A: @classmethod def func(cls, x): return 1 + x class TypeReflectionMethod(torch.nn.Module): \"\"\" type() calls on custom objects followed by attribute accesses are not allowed due to its overly dynamic nature. \"\"\" def forward(self, x): a = A() return type(a).func(x) example_args = (torch.randn(3, 4),) tags = {\"python.builtin\"} model = TypeReflectionMethod() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 4]\"): add: \"f32[3, 4]\" = torch.ops.aten.add.Tensor(x, 1); x = None return (add,) Graph signature: # inputs x: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_953", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_955", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "user_input_mutation#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "user_input_mutation#", "content": "user_input_mutation# Original source code: # mypy: allow-untyped-defs import torch class UserInputMutation(torch.nn.Module): \"\"\" Directly mutate user input in forward \"\"\" def forward(self, x): x.mul_(2) return x.cos() example_args = (torch.randn(3, 2),) tags = {\"torch.mutation\"} model = UserInputMutation() torch.export.export(model, example_args) Result: ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 2]\"): mul_: \"f32[3, 2]\" = torch.ops.aten.mul_.Tensor(x, 2); x = None cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(mul_); mul_ = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {}", "prev_chunk_id": "chunk_954", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_956", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "dynamic_shape_round#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "dynamic_shape_round#", "content": "dynamic_shape_round# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel from torch.export import Dim class DynamicShapeRound(torch.nn.Module): \"\"\" Calling round on dynamic shapes is not supported. \"\"\" def forward(self, x): return x[: round(x.shape[0] / 2)] x = torch.randn(3, 2) dim0_x = Dim(\"dim0_x\") example_args = (x,) tags = {\"torch.dynamic-shape\", \"python.builtin\"} support_level = SupportLevel.NOT_SUPPORTED_YET dynamic_shapes = {\"x\": {0: dim0_x}} model = DynamicShapeRound() torch.export.export(model, example_args, dynamic_shapes=dynamic_shapes) Result: Unsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".", "prev_chunk_id": "chunk_955", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_957", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "model_attr_mutation#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "model_attr_mutation#", "content": "model_attr_mutation# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class ModelAttrMutation(torch.nn.Module): \"\"\" Attribute mutation is not supported. \"\"\" def __init__(self) -> None: super().__init__() self.attr_list = [torch.randn(3, 2), torch.randn(3, 2)] def recreate_list(self): return [torch.zeros(3, 2), torch.zeros(3, 2)] def forward(self, x): self.attr_list = self.recreate_list() return x.sum() + self.attr_list[0].sum() example_args = (torch.randn(3, 2),) tags = {\"python.object-model\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = ModelAttrMutation() torch.export.export(model, example_args) Result: AssertionError: Mutating module attribute attr_list during export.", "prev_chunk_id": "chunk_956", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_958", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "optional_input#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "optional_input#", "content": "optional_input# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class OptionalInput(torch.nn.Module): \"\"\" Tracing through optional input is not supported yet \"\"\" def forward(self, x, y=torch.randn(2, 3)): if y is not None: return x + y return x example_args = (torch.randn(2, 3),) tags = {\"python.object-model\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = OptionalInput() torch.export.export(model, example_args) Result: Unsupported: Tracing through optional input is not supported yet", "prev_chunk_id": "chunk_957", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_959", "url": "https://docs.pytorch.org/docs/stable/generated/exportdb/index.html", "title": "unsupported_operator#", "page_title": "ExportDB — PyTorch 2.8 documentation", "breadcrumbs": "unsupported_operator#", "content": "unsupported_operator# Original source code: # mypy: allow-untyped-defs import torch from torch._export.db.case import SupportLevel class TorchSymMin(torch.nn.Module): \"\"\" torch.sym_min operator is not supported in export. \"\"\" def forward(self, x): return x.sum() + torch.sym_min(x.size(0), 100) example_args = (torch.randn(3, 2),) tags = {\"torch.operator\"} support_level = SupportLevel.NOT_SUPPORTED_YET model = TorchSymMin() torch.export.export(model, example_args) Result: Unsupported: torch.* op returned non-Tensor", "prev_chunk_id": "chunk_958", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_960", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_ir.html", "title": "IRs#", "page_title": "IRs — PyTorch 2.8 documentation", "breadcrumbs": "IRs#", "content": "IRs# Created On: Dec 13, 2022 | Last Updated On: Jun 10, 2025 PyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_961", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_ir.html", "title": "Core Aten IR#", "page_title": "IRs — PyTorch 2.8 documentation", "breadcrumbs": "Core Aten IR#", "content": "Core Aten IR# Core aten ops is the core subset of aten operators that can be used to compose other operators. Core aten IR is fully functional, and there is no inplace or _out variants in this opset. In contrast to Prims IR, core aten ops reuses the existing aten ops in “native_functions.yaml”, and it doesn’t further decompose ops into explicit type promotion and broadcasting ops. This opset is designed to serve as the functional IR to interface with backends.", "prev_chunk_id": "chunk_960", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_962", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_ir.html", "title": "Prims IR#", "page_title": "IRs — PyTorch 2.8 documentation", "breadcrumbs": "Prims IR#", "content": "Prims IR# Prims IR is a set of primitive operators that can be used to compose other operators. Prims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit type promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim. This opset is designed to interface with compiler backends.", "prev_chunk_id": "chunk_961", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_963", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Writing Graph Transformations on ATen IR#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Writing Graph Transformations on ATen IR#", "content": "Writing Graph Transformations on ATen IR# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_964", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Passes#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Passes#", "content": "Passes# Since the ATen IR sits at the FX Graph/GraphModule level, any transformations written for FX Graphs can be easily applied onto the ATen IR. If you’re familiar with writing FX graph transformations, then this will be the same. The most direct way of writing transformations is by looping through the given graph and directly manipulating the nodes within the graph. For example, let’s say we want to replace torch.ops.aten.add.Tensor() calls with torch.ops.aten.mul.Tensor() calls: import torch def replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: for node in gm.graph.nodes: if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.mul.Tensor We can also delete and append new nodes through FX utility functions that can be found in the Graph documentation. For example, if we want to insert a torch.ops.aten.relu.default() after the add call: import torch def insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: for node in gm.graph.nodes: if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor: # Specifies the insertion point. Any nodes added to the graph within # this scope will be inserted after `node` with gm.graph.inserting_after(node): # Insert a new `call_function` node with op `torch.ops.aten.relu.default` new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,)) # Replace all the places that use `node` to now use the `new_relu_node` node.replace_all_uses_with(new_relu_node) In general, transformations can be roughly categorized into a couple of axis: Axis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating many-to-one mapping (eg. fusion) Axis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing backwards iteration (eg. dead code elimination) Axis C: 1. Dependent on local node information (eg. out-variant conversion) 2. Dependent on global graph information (eg. memory planning) Our projection on the frequency of these use cases are: 1. A.1, B.1, C.1 2. A.2 3. B.2, C.2 Although we can make all graph transformations through directly manipulating the graph, we also provide some helper utilities for some ease", "prev_chunk_id": "chunk_963", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_965", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Passes#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Passes#", "content": "of use for the level 1 and 2 use-cases.", "prev_chunk_id": "chunk_964", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_966", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Transformer#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Transformer#", "content": "Transformer# For level 1 uses cases (creating one-to-X mappings, doing forwards iterations, and looking at local node information), we can utilize the Transformer class to execute each node and recreate a graph, except with the transformations specified.", "prev_chunk_id": "chunk_965", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_967", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "One-to-One Pass#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "One-to-One Pass#", "content": "One-to-One Pass# An example for one-to-one mappings, if we wanted to replace an op A with another op B, we can run the GraphModule, and very time we see op A, return op B. An example is: class ReplaceAddWithMul(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target != torch.ops.aten.add.Tensor: return super().call_function(target, args, kwargs) return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs) transformed_graph_module = ReplaceAddWithMul(graph_module).transform() The super().call_function(target, args, kwargs, meta) call creates a call_function FX node, and returns the result of running the operator with the given arguments.", "prev_chunk_id": "chunk_966", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_968", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "One-to-X Pass#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "One-to-X Pass#", "content": "One-to-X Pass# If we wanted to do one-to-X mappings, like replacing op A with 2 other ops B and C, we would then make 2 calls to super().call_function to create 2 FX nodes, one with op B and another with op C, and return the result of running op C. For example: class ReplaceAddWithMulSub(torch.fx.Transformer): \"\"\" Original: def f(x, y): return x + y After pass: def f(x, y): z = x * y return z - y \"\"\" def call_function(self, target, args, kwargs): if target != torch.ops.aten.add.Tensor: return super().call_function(target, args, kwargs) x, y = args mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {}) return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {}) transformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()", "prev_chunk_id": "chunk_967", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_969", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "One-to-None Pass#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "One-to-None Pass#", "content": "One-to-None Pass# If we wanted to remove an op, we can just return the value passed into the function: class RemoveDetachPass(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target not in ( torch.ops.aten.detach.default, torch.ops.aten.detach_copy.default, ): return super().call_function(target, args, kwargs, meta) assert len(args) == 1 return args[0] transformed_graph_module = RemoveDetachPass(graph_module).transform()", "prev_chunk_id": "chunk_968", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_970", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Utilizing Local Information#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Utilizing Local Information#", "content": "Utilizing Local Information# An example of utilizing local node information is, if we wanted to convert all the scalars within the graph to tensors, we can run the given fx.GraphModule, and for every argument that contains a scalar, we convert it to a tensor. It might look something like: def args_map(target, fn, args, kwargs): assert isinstance(args, tuple) assert isinstance(kwargs, dict) args = list(args) kwargs = kwargs.copy() # Update the argument based on the function passed def update(key, args, schema): args[key] = fn(args[key], schema) # Update each argument in the schema for i, schema in enumerate(target._schema.arguments): if schema.name in kwargs: update(schema.name, kwargs, schema) elif not schema.kwarg_only and i < len(args): update(i, args, schema) return tuple(args), kwargs class ScalarToTensorPass(torch.fx.Transformer): def call_function(self, target, args, kwargs): breakpoint() def try_coerce(value, arg): return ( torch.tensor(value) if isinstance(value, (float, int, bool)) and type(arg.type) == torch.TensorType else value ) args, kwargs = args_map(target, try_coerce, args, kwargs) return super().call_function(target, args, kwargs) transformed_graph_module = ScalarToTensorPass(graph_module).transform()", "prev_chunk_id": "chunk_969", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_971", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Subgraph Rewriter#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Subgraph Rewriter#", "content": "Subgraph Rewriter# For creating many-to-one mappings, we can utilize FX’s subgraph rewriter. Given a pattern, it creates a subgraph of operators matching to the pattern, and then replaces each matched subgraph with the replacement. Note: This is an inplace operation. The pattern and replacement inputs must be callable functions or GraphModules containing the same operators that are used within the graph (ATen ops) so that the subgraph rewriter can find the correct pattern in the graph. Inputs to the pattern/replacement callables will be treated as wildcards when matching. An example: from torch.fx import subgraph_rewriter def replace_patterns(graph_module): def pattern(x, y): x = torch.ops.aten.add.Tensor(x, y) x = torch.ops.aten.mul.Tensor(x, y) return x def replacement(x, y): return torch.ops.aten.sub.Tensor(x, y) replaced_patterns = subgraph_rewriter.replace_pattern_with_filters( traced_module, pattern, replacement ) The subgraph rewriter returns a list of ReplacedPatterns: @dataclass class ReplacedPatterns: # Node from which the match was found anchor: Node # Maps nodes in the pattern subgraph to nodes in the larger graph nodes_map: Dict[Node, Node] # List of nodes that were added into the graph replacements: List[Node] Note: The nodes created by the subgraph rewriter will not have the metadata that is populated in the matched nodes, but you can use `ReplacedPatterns.nodes_map` to find the nodes in the original graph that were matched, and `ReplacedPatterns.replacements` to find the nodes that were replaced in the transformed graph.", "prev_chunk_id": "chunk_970", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_972", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Pass Manager#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Pass Manager#", "content": "Pass Manager# The PassManager is a class used to run multiple passes on a given graph module. When initializing a PassManager instance, we pass in a list of passes that we want to run and set a couple of flags. To run the collection of passes on a graph module, we can pass the graph module directly to the PassManager instance. An example: from torch.fx.passes.infra.pass_manager import PassManager pm = PassManager( passes=[replace_add_with_div, replace_div_with_mul], run_checks_after_each_pass=True, suppress_check_failures=False, ) graph_module_out = pm(graph_module) To add a common set of checks that are run after each pass, we can call the function set_checks(check: Callable) which takes in a callable function as input. If the run_checks_after_each_pass flag is set, the check will be called after each pass is run on the graph module. An example: pm = PassManager(passes=[replace_add_with_div, replace_div_with_mul]) def check_div_target(graph_module): for node in graph_module.graph.nodes: if node.op == \"call_function\" and node.target != torch.div: raise ValueError(\"Target should be div!\") pm.add_checks(check_div_target) pm(graph_module) # raises ValueError after replace_div_with_mul pass", "prev_chunk_id": "chunk_971", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_973", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Partitioner#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Partitioner#", "content": "Partitioner# There are a couple of common FX graph based partitioners we can use to partition the graph.", "prev_chunk_id": "chunk_972", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_974", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Subgraph Matcher#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Subgraph Matcher#", "content": "Subgraph Matcher# For finding subgraphs within a graph that match a specific pattern, we can utilize FX’s SubgraphMatcher. Class Attributes: - pattern(Graph): The targeted matching pattern. Placeholder nodes in the graph will be treated as wildcards when matching. - match_output(bool): If True, output node in the pattern graph will be treated as a part of the targeted pattern. If False, output node is ignored during match. - match_placeholder(bool): If True, placeholder node in the pattern graph will be treated as a part of the targeted pattern. If False, placeholder nodes will be used a wildcard. - remove_overlapping_matches(bool): If True, in the case of overlapping matches, only the first match will be returned. - ignore_literals(bool): If True, will not check if literals are equal and will instead treat them as wildcards. An example: from torch.fx.passes.utils.matcher_utils import SubgraphMatcher class LargeModel(torch.nn.Module): def __init__(self): super().__init__() self._weight = torch.nn.Parameter(torch.ones(3, 3)) self._bias = torch.nn.Parameter(torch.ones(3, 3)) def forward(self, x): return torch.ops.aten.addmm.default(self._bias, x, self._weight) large_model_graph = torch.export(LargeModel(), inputs).graph class PatternModel(torch.nn.Module): def __init__(self): super().__init__() self._weight_1 = torch.nn.Parameter(torch.ones(5, 5)) self._bias_1 = torch.nn.Parameter(torch.ones(5, 5)) def forward(self, x): return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1) pattern_graph = torch.export(PatternModel(), inputs).graph subgraph_matcher = SubgraphMatcher(pattern_graph) match_result = subgraph_matcher.match(large_model_graph) The match function returns a list of InternalMatch: @dataclass class InternalMatch(): # Nodes from which the match was found anchors: List[Node] # Maps nodes in the pattern subgraph to nodes in the larger graph nodes_map: Dict[Node, Node] = field(default_factory=dict) # Nodes in target graph that are matched placeholder in pattern placeholder_nodes: List[Node] = field(default_factory=list) # Nodes in matched subgraph returned by output returning_nodes: List[Node] = field(default_factory=list)", "prev_chunk_id": "chunk_973", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_975", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_transformations.html", "title": "Capability Based Partitioner#", "page_title": "Writing Graph Transformations on ATen IR — PyTorch 2.8 documentation", "breadcrumbs": "Capability Based Partitioner#", "content": "Capability Based Partitioner# To find the largest subgraphs of nodes that support a specific invariant, we can utilize FX’s CapabilityBasedPartitioner. Class Attributes - graph_module(torch.fx.GraphModule): The graph module we are partitioning on. - operator_support(OperatorSupportBase): The object used to determine if a node in the graph is supported in the partition. - allows_single_node_partition(bool): If True, allows single node partitions to be formed. - non_compute_ops(Optional[Sequence[str]]): A set of ops that are considered to be “non-compute” (extorch.ops.aten.viewand_operator.getitem, so that the partitioner will not create graphs that only contain these non-compute ops - allowed_single_node_partition_ops(Optional[Sequence[str]]): A set of ops that are allowed to be in a single node partition. The OperatorSupportBase class is used by the partitioner to determine if a specific node in the graph belongs in the partition. This is done by overriding the is_node_supported function. You can chain multiple OperatorSupportBase by using chain (which returns False if any of the OperatorSupportBase return False) and any_chain (which returns True if any of the OperatorSupportBase returns True). An example: from torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner from torch.fx.passes.operator_support import any_chain, OperatorSupportBase class AddMulOperatorSupport(OperatorSupportBase): def is_node_supported(self, submodules, node: torch.fx.Node) -> bool: return node.op == \"call_function\" and node.target in [ torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor, ] capability_partitioner = CapabilityBasedPartitioner( graph_module, op_support, ) # Returns a list of partitions (list of nodes that belong in each partition) partition_list = capability_partitioner.propose_partitions() # Fuses the partitions into graph modules and inserts `call_module` nodes in the graph fused_graph_module = capability_partitioner.fuse_partitions(partition_list)", "prev_chunk_id": "chunk_974", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_976", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html", "title": "AOTInductor Minifier#", "page_title": "AOTInductor Minifier — PyTorch 2.8 documentation", "breadcrumbs": "AOTInductor Minifier#", "content": "AOTInductor Minifier# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 If you encounter an error while using AOT Inductor APIs such as torch._inductor.aoti_compile_and_package, torch._indcutor.aoti_load_package, or running the loaded model of aoti_load_package on some inputs, you can use the AOTInductor Minifier to create a minimal nn.Module that reproduce the error by setting from torch._inductor import config; config.aot_inductor.dump_aoti_minifier = True. One a high-level, there are two steps in using the minifier: - Setfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=Trueor set the environment variableDUMP_AOTI_MINIFIER=1. Then running the script that errors would produce aminifier_launcher.pyscript. The output directory is configurable by settingtorch._dynamo.config.debug_dir_rootto a valid directory name. - Run theminifier_launcher.pyscript. If the minifier runs successfully, it generates runnable python code inrepro.pywhich reproduces the exact error.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_977", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html", "title": "Example Code#", "page_title": "AOTInductor Minifier — PyTorch 2.8 documentation", "breadcrumbs": "Example Code#", "content": "Example Code# Here is sample code which will generate an error because we injected an error on relu with torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\". import torch from torch._inductor import config as inductor_config class Model(torch.nn.Module): def __init__(self): super().__init__() self.fc1 = torch.nn.Linear(10, 16) self.relu = torch.nn.ReLU() self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.sigmoid(x) return x inductor_config.aot_inductor.dump_aoti_minifier = True torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\" with torch.no_grad(): model = Model().to(\"cuda\") example_inputs = (torch.randn(8, 10).to(\"cuda\"),) ep = torch.export.export(model, example_inputs) package_path = torch._inductor.aoti_compile_and_package(ep) compiled_model = torch._inductor.aoti_load_package(package_path) result = compiled_model(*example_inputs) The code above generates the following error: RuntimeError: Failed to import /tmp/torchinductor_shangdiy/fr/cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py SyntaxError: invalid syntax (cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py, line 29) This is because we injected an error on relu, and so the generated triton kernel looks like below. Note that we have compile error! instead if relu, so we get a SyntaxError. @triton.jit def triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr): xnumel = 128 xoffset = tl.program_id(0) * XBLOCK xindex = xoffset + tl.arange(0, XBLOCK)[:] xmask = xindex < xnumel x2 = xindex x0 = xindex % 16 tmp0 = tl.load(in_out_ptr0 + (x2), xmask) tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last') tmp2 = tmp0 + tmp1 tmp3 = compile error! tmp4 = tl.sigmoid(tmp3) tl.store(in_out_ptr0 + (x2), tmp4, xmask) Since we have torch._inductor.config.aot_inductor.dump_aoti_minifier=True, we also see an additional line indicating where minifier_launcher.py has been written to. The output directory is configurable by setting torch._dynamo.config.debug_dir_root to a valid directory name. W1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] Writing minified repro to: W1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_21_08_602433-pid_2861654/minifier/minifier_launcher.py", "prev_chunk_id": "chunk_976", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_978", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html", "title": "Minifier Launcher#", "page_title": "AOTInductor Minifier — PyTorch 2.8 documentation", "breadcrumbs": "Minifier Launcher#", "content": "Minifier Launcher# The minifier_launcher.py file has the following code. The exported_program contains the inputs to torch._inductor.aoti_compile_and_package. The command='minify' parameter means the script will run the minifier to create a minimal graph module that reproduce the error. Alternatively, you set use command='run' to just compile, load, and run the loaded model (without running the minifier). import torch import torch._inductor.inductor_prims import torch._dynamo.config import torch._inductor.config import torch._functorch.config import torch.fx.experimental._config torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error' torch._inductor.config.aot_inductor.dump_aoti_minifier = True isolate_fails_code_str = None # torch version: 2.6.0a0+gitcd9c6e9 # torch cuda version: 12.0 # torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84 # CUDA Info: # nvcc: NVIDIA (R) Cuda compiler driver # Copyright (c) 2005-2023 NVIDIA Corporation # Built on Fri_Jan__6_16:45:21_PST_2023 # Cuda compilation tools, release 12.0, V12.0.140 # Build cuda_12.0.r12.0/compiler.32267302_0 # GPU Hardware Info: # NVIDIA PG509-210 : 8 exported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints/exported_program.pt2') # print(exported_program.graph) config_patches={} if __name__ == '__main__': from torch._dynamo.repro.aoti import run_repro with torch.no_grad(): run_repro(exported_program, config_patches=config_patches, accuracy=False, command='minify', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints', check_str=None) Suppose we kept the command='minify' option, and run the script, we would get the following output: ... W1031 16:48:08.938000 3598491 torch/_dynamo/repro/aoti.py:89] Writing checkpoint with 3 nodes to /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_48_02_720863-pid_3598491/minifier/checkpoints/3.py W1031 16:48:08.975000 3598491 torch/_dynamo/repro/aoti.py:101] Copying repro file for convenience to /data/users/shangdiy/pytorch/repro.py Wrote minimal repro out to repro.py If you get an AOTIMinifierError when running minifier_launcher.py, please report a bug here.", "prev_chunk_id": "chunk_977", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_979", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html", "title": "Minified Result#", "page_title": "AOTInductor Minifier — PyTorch 2.8 documentation", "breadcrumbs": "Minified Result#", "content": "Minified Result# The repro.py looks like this. Notice that the exported program is printed at the top of the file, and it contains only the relu node. The minifier successfully reduced the graph to the op that raises the error. # from torch.nn import * # class Repro(torch.nn.Module): # def __init__(self) -> None: # super().__init__() # def forward(self, linear): # relu = torch.ops.aten.relu.default(linear); linear = None # return (relu,) import torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf import torch._inductor.inductor_prims import torch._dynamo.config import torch._inductor.config import torch._functorch.config import torch.fx.experimental._config torch._inductor.config.generate_intermediate_hooks = True torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error' torch._inductor.config.aot_inductor.dump_aoti_minifier = True isolate_fails_code_str = None # torch version: 2.6.0a0+gitcd9c6e9 # torch cuda version: 12.0 # torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84 # CUDA Info: # nvcc: NVIDIA (R) Cuda compiler driver # Copyright (c) 2005-2023 NVIDIA Corporation # Built on Fri_Jan__6_16:45:21_PST_2023 # Cuda compilation tools, release 12.0, V12.0.140 # Build cuda_12.0.r12.0/compiler.32267302_0 # GPU Hardware Info: # NVIDIA PG509-210 : 8 exported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints/exported_program.pt2') # print(exported_program.graph) config_patches={'aot_inductor.package': True} if __name__ == '__main__': from torch._dynamo.repro.aoti import run_repro with torch.no_grad(): run_repro(exported_program, config_patches=config_patches, accuracy=False, command='run', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints', check_str=None)", "prev_chunk_id": "chunk_978", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_980", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models#", "content": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models# Created On: Jun 13, 2025 | Last Updated On: Jun 18, 2025 AOTInductor is a specialized version of TorchInductor, designed to process exported PyTorch models, optimize them, and produce shared libraries as well as other relevant artifacts. These compiled artifacts are specifically crafted for deployment in non-Python environments, which are frequently employed for inference deployments on the server side. In this tutorial, you will gain insight into the process of taking a PyTorch model, exporting it, compiling it into an artifact, and conducting model predictions using C++.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_981", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "Model Compilation#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "Model Compilation#", "content": "Model Compilation# To compile a model using AOTInductor, we first need to use torch.export.export() to capture a given PyTorch model into a computational graph. torch.export provides soundness guarantees and a strict specification on the IR captured, which AOTInductor relies on. We will then use torch._inductor.aoti_compile_and_package() to compile the exported program using TorchInductor, and save the compiled artifacts into one package. import os import torch class Model(torch.nn.Module): def __init__(self): super().__init__() self.fc1 = torch.nn.Linear(10, 16) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(16, 1) self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) x = self.sigmoid(x) return x with torch.no_grad(): device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model = Model().to(device=device) example_inputs=(torch.randn(8, 10, device=device),) batch_dim = torch.export.Dim(\"batch\", min=1, max=1024) # [Optional] Specify the first dimension of the input x as dynamic. exported = torch.export.export(model, example_inputs, dynamic_shapes={\"x\": {0: batch_dim}}) # [Note] In this example we directly feed the exported module to aoti_compile_and_package. # Depending on your use case, e.g. if your training platform and inference platform # are different, you may choose to save the exported model using torch.export.save and # then load it back using torch.export.load on your inference platform to run AOT compilation. output_path = torch._inductor.aoti_compile_and_package( exported, # [Optional] Specify the generated shared library path. If not specified, # the generated artifact is stored in your system temp directory. package_path=os.path.join(os.getcwd(), \"model.pt2\"), ) In this illustrative example, the Dim parameter is employed to designate the first dimension of the input variable “x” as dynamic. Notably, the path and name of the compiled library remain unspecified, resulting in the shared library being stored in a temporary directory. To access this path from the C++ side, we save it to a file for later retrieval within the C++ code.", "prev_chunk_id": "chunk_980", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_982", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "Inference in Python#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "Inference in Python#", "content": "Inference in Python# There are multiple ways to deploy the compiled artifact for inference, and one of that is using Python. We have provided a convenient utility API in Python torch._inductor.aoti_load_package() for loading and running the artifact, as shown in the following example: import os import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), \"model.pt2\")) print(model(torch.randn(8, 10, device=device))) The input at inference time should have the same size, dtype, and stride as the input at export time.", "prev_chunk_id": "chunk_981", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_983", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "Inference in C++#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "Inference in C++#", "content": "Inference in C++# Next, we use the following example C++ file inference.cpp to load the compiled artifact, enabling us to conduct model predictions directly within a C++ environment. #include <iostream> #include <vector> #include <torch/torch.h> #include <torch/csrc/inductor/aoti_package/model_package_loader.h> int main() { c10::InferenceMode mode; torch::inductor::AOTIModelPackageLoader loader(\"model.pt2\"); // Assume running on CUDA std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)}; std::vector<torch::Tensor> outputs = loader.run(inputs); std::cout << \"Result from the first inference:\"<< std::endl; std::cout << outputs[0] << std::endl; // The second inference uses a different batch size and it works because we // specified that dimension as dynamic when compiling model.pt2. std::cout << \"Result from the second inference:\"<< std::endl; // Assume running on CUDA std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl; return 0; } For building the C++ file, you can make use of the provided CMakeLists.txt file, which automates the process of invoking python model.py for AOT compilation of the model and compiling inference.cpp into an executable binary named aoti_example. cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(aoti_example) find_package(Torch REQUIRED) add_executable(aoti_example inference.cpp model.pt2) add_custom_command( OUTPUT model.pt2 COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py DEPENDS model.py ) target_link_libraries(aoti_example \"${TORCH_LIBRARIES}\") set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17) Provided the directory structure resembles the following, you can execute the subsequent commands to construct the binary. It is essential to note that the CMAKE_PREFIX_PATH variable is crucial for CMake to locate the LibTorch library, and it should be set to an absolute path. Please be mindful that your path may vary from the one illustrated in this example. aoti_example/ CMakeLists.txt inference.cpp model.py $ mkdir build $ cd build $ CMAKE_PREFIX_PATH=/path/to/python/install/site-packages/torch/share/cmake cmake .. $ cmake --build . --config Release After the aoti_example binary has been generated in the build directory, executing it will display results akin to the following: $ ./aoti_example Result from the first inference: 0.4866 0.5184 0.4462 0.4611 0.4744 0.4811 0.4938 0.4193 [ CUDAFloatType{8,1} ] Result from the second", "prev_chunk_id": "chunk_982", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_984", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "Inference in C++#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "Inference in C++#", "content": "inference: 0.4883 0.4703 [ CUDAFloatType{2,1} ]", "prev_chunk_id": "chunk_983", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_985", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_aot_inductor.html", "title": "Troubleshooting#", "page_title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models — PyTorch 2.8 documentation", "breadcrumbs": "Troubleshooting#", "content": "Troubleshooting# Below are some useful tools for debugging AOT Inductor. To enable runtime checks on inputs, set the environment variable AOTI_RUNTIME_CHECK_INPUTS to 1. This will raise a RuntimeError if the inputs to the compiled model differ in size, data type, or strides from those used during export.", "prev_chunk_id": "chunk_984", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_986", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Dynamic Shapes#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Shapes#", "content": "Dynamic Shapes# Created On: May 19, 2023 | Last Updated On: Jun 10, 2025 Code: symbolic_shapes.py See also: The dynamic shapes manual", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_987", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Motivation#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Motivation#", "content": "Motivation# Deep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient: - Some dimensions, such as batch size or sequence length, may vary. For example, an inference service performing adaptive batching will execute inference requests with varying batch sizes depending on how many requests it received within its batching window. We may also want to consider padding out variable size sequences only to the maximum sequence length within a batch, which may vary from batch-to-batch. - Some models exhibit data-dependent output shapes, that is to say, the size of their outputs and intermediates may depend on the actual input data which may vary across runs. For example, detection models may first generate a variable number of potential bounding boxes before running a more expensive image recognition model to identify if the subject is in a bounding box. The number of bounding boxes is data dependent. - One particularly important case of data-dependent shapes occurs when dealing with sparse representations, such as sparse tensors, jagged tensors, and graph neural networks. In all of these cases, the amount of data to be processed depends on the sparse structure of the problem, which will typically vary in a data-dependent way. In supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.", "prev_chunk_id": "chunk_986", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_988", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Abridged public API#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Abridged public API#", "content": "Abridged public API# The default dynamic behavior in PyTorch 2.1 is: - PT2 assumes everything is static by default - If we recompile because a size changed, we will instead attempt to recompile that size as being dynamic (sizes that have changed are likely to change in the future). This generalization may fail (e.g., because user code does a conditional branch on the size in question or missing dynamic shapes support in PT2). If you are trying to understand why PT2 has overspecialized some code, run withTORCH_LOGS=dynamicand look for “eval” entries that say when guards are added and why. - If you know ahead of time something will be dynamic, you can skip the first recompile withtorch._dynamo.mark_dynamic(tensor,dim). If you know ahead of time theminandmaxvalue this dimension can take, you can specifytorch._dynamo.mark_dynamic(tensor,dim,min=min,max=max) - If you saytorch.compile(dynamic=False), we will turn off automatic dynamic shapes on recompiles and always recompile for each distinct size. Conversely, if you saytorch.compile(dynamic=True), we will try to make everything as dynamic as possible. This is mostly useful for small operators; if you try it on a big model it will (1) probably crash PT2 and (2) run slow for no good reason. - You can whitelist specific sources to be marked as dynamic using theTORCH_COMPILE_DYNAMIC_SOURCESenvironment variable or by settingtorch.compiler.config.dynamic_sources. This is particularly useful for large models with graph breaks, as you can maintain dynamism across graph breaks since source names stay consistent. You can also use this to mark integers as dynamic. The format is a comma-delimited list of source names, e.g.,\"L['x'],L['y']\". You can also use regexes, e.g.,\"L\\['x.*'\\],L\\['y.*'\\]\"). This whitelist takes precedence over other flags likedynamic=False,force_nn_module_property_static_shapes, andforce_parameter_static_shapes. - Sometimes it can be cumbersome to find the right inputs to mark as dynamic. If you’re willing to take a performance hit for the first batch, one other affordable option", "prev_chunk_id": "chunk_987", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_989", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Abridged public API#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Abridged public API#", "content": "we have are the eager_then_compile stances which derive dynamism for you. Seetorch.compiler.set_stancefor more details.", "prev_chunk_id": "chunk_988", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_990", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "The Guard Model#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "The Guard Model#", "content": "The Guard Model# When considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a “hint” for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take. This greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program: def f(x, y): z = torch.cat([x, y]) if z.size(0) > 2: return z.mul(2) else: return z.add(2) The final IR we will compile with TorchInductor will either be torch.cat([x, y]).add(2) or torch.cat([x, y]).mul(2) (with the condition flattened away), but to determine which branch we are in, we would need to know the size of z, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reduce z.size(0) as an expression in terms of the inputs, x.size(0) + y.size(0). This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.", "prev_chunk_id": "chunk_989", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_991", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Overall architecture#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Overall architecture#", "content": "Overall architecture# Symbolic shapes workflow: - When we start compiling a frame in Dynamo, we allocate a ShapeEnv (attached to FakeTensorMode) which keeps track of symbolic shapes state. - We allocate symbolic sizes for tensors on entry (what is static or dynamic is a policy decision, with some knobs). - We propagate the symbolic sizes through operators, maintaining both (1) FX IR so that we can faithfully export symbolic compute, and (2) Sympy expressions representing the size vars, so we can reason about them. - When we condition on symbolic sizes, either in Dynamo tracing or in Inductor optimization, we add guards based on the conditional. These can be induced from both Python and C++. - These guards can induce further simplifications on symbolic variables. For example, if you asserts0==4, we can now replace all occurrences ofs0with4. - When we’re done tracing and optimizing, we install all of these guards with the compiled code; the compiled code is only reusable if all the guards evaluate true. Important files: - C++ SymInt API:c10/core/SymInt.h,SymFloat.h,SymBool.h - Python SymInt API:torch/__init__.py(look forSymInt/SymFloat/SymBool) - C++ plumbing:c10/core/SymNodeImpl.h,torch/csrc/utils/python_symnode.h,torch/csrc/jit/python/init.cpp - Python infrastructure:torch/fx/experimental/symbolic_shapes.py - Other important files:torch/_subclasses/fake_tensor.py,torch/_meta_registrations.py, decomps, PrimTorch refs", "prev_chunk_id": "chunk_990", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_992", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Abridged internal API#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Abridged internal API#", "content": "Abridged internal API# Understanding the Python class hierarchy: - SymInt/SymFloat/SymBool: these are user-visible classes that simulate their int/float/bool counterparts. If you add two SymInts, we give you a new SymInt that symbolically tracks that the integer addition had occurred. - SymNode: this is the internal structure (accessible via e.g.,symint.node) which holds the actual symbolic tracking info. SymNode is type erased; this makes it more convenient to represent mixed-type operations. Note that technically you don’t have to call into Python SymNode from SymInt; for example, XLA’s C++SymNodeImplwould take the place of SymNode. - ShapeEnv: per-compile context state which keeps track of all the free symbols and guards we have accumulated so far. Every SymNode records its ShapeEnv (but not vice versa; SymNodes only get used if they participate in a guard). C++ is fairly similar: - c10::SymInt/SymFloat/SymBool: user-visible classes that simulate int/float/bool. - c10::SymNode/SymNodeImpl: analogous to SymNode - There is no ShapeEnv in C++; for ease of debugging, the entire symbolic reasoning apparatus is in Python. When you write code that is traceable with make_fx, it must be able to deal with SymInt/SymFloat/SymBool flowing through it. The dynamic shapes manual gives some guidance for how to do this.", "prev_chunk_id": "chunk_991", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_993", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "DimDynamic policy#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "DimDynamic policy#", "content": "DimDynamic policy# Symbolic reasoning: - Value ranges - Sympy usage notes - Constraints - DimDynamic/Constraint", "prev_chunk_id": "chunk_992", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_994", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Unbacked SymInts#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Unbacked SymInts#", "content": "Unbacked SymInts# To resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like .nonzero() or .item(). It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations. Naively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work: - On tensor creation, PyTorch precomputes a lot of data about a tensor; for example, if you useempty_stridedto create a tensor, we will eagerly sort the strides and determine if the tensor is non-overlapping and dense. Sorts produce a lot of guards. However, it is more common to produce a tensor directly with a higher-level API likeempty, which is guaranteed to produce a non-overlapping and dense tensor. We modified PyTorch to avoid needlessly recomputing these properties. - Even if nontrivial compute is needed, sometimes a property is never actually queried at all. Making these precomputed properties lazy allows us to avoid guarding on an unbacked symbolic integer unless it is actually needed. - The data in an integer tensor is generally not known to be non-negative. However, we provide an APIconstrain_rangewhereby a user can specify that a size is bounded above and below by known limits. Similar to the dynamic APIs, there are corresponding unbacked APIs: namely you can use mark_unbacked instead of mark_dynamic and TORCH_COMPILE_UNBACKED_SOURCES instead of TORCH_COMPILE_DYNAMIC_SOURCES to tell the compiler to mark an input as unbacked. In future versions of PT2 (beyond PT2.1), we will extend our reasoning system to infer that an unbacked", "prev_chunk_id": "chunk_993", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_995", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html", "title": "Unbacked SymInts#", "page_title": "Dynamic Shapes — PyTorch 2.8 documentation", "breadcrumbs": "Unbacked SymInts#", "content": "symbolic integer is size-like based on usage. For example, if you pass the result of an .item() call to a factory function like torch.empty, we will automatically infer that the result is a size (because if it was not, it would fail.) This assumption would get validated at runtime, raising an error if it was not fulfilled.", "prev_chunk_id": "chunk_994", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_996", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Fake tensor#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Fake tensor#", "content": "Fake tensor# Created On: May 19, 2023 | Last Updated On: Jun 13, 2025 Code: fake_tensor.py", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_997", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Motivation#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Motivation#", "content": "Motivation# When doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you’re doing a lot of compute) and take a lot of memory (it’s bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn’t actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries. Similarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta[‘val’]). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn’t have handled (e.g., aliasing relationships).", "prev_chunk_id": "chunk_996", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_998", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Related work#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Related work#", "content": "Related work# - A meta tensor is a tensor with device=’meta’. This is actually a lot of what you want for fake tensor, but meta tensors don’t model devices, and sometimes stride behavior varies depending on your device, so fake tensors really can get a lot more accurate info this way. Also, meta tensors are “global” (they exist on their own, similar to how a CPU/CUDA tensor exist on their own), whereas fake tensors are scoped to a FakeTensorMode. - A tensor subclass lets you subclass torch.Tensor and customize their behavior. Fake tensors are implemented as a tensor subclass; that means almost all of its implementation lives in Python! For more simple examples of tensor subclasses check outsubclass_zoo. - Dynamic shapes allow you to create tensors with symbolic sizes rather than only concrete sizes, and propagate these sizes symbolically through operations. Dynamic shapes maintain state in a ShapeEnv, which is always associated with a FakeTensorMode (so fake tensors also are responsible for managing symbolic sizes.) In general, whenever we compile a subgraph with PT2, there is a tracing context associated with this compilation, which contains, among other things, a FakeTensorMode and (possibly) a ShapeEnv.", "prev_chunk_id": "chunk_997", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_999", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Overall architecture#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Overall architecture#", "content": "Overall architecture# All fake tensors are associated with a FakeTensorMode. Because fake tensor’s primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you’ll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you’re running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again. A fake tensor is represented as a __torch_dispatch__ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you’d end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won’t work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is", "prev_chunk_id": "chunk_998", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1000", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Overall architecture#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Overall architecture#", "content": "like a real kernel, but all it does is allocate the outputs, it doesn’t do any data compute. A tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe: - Run the meta kernel on the input fake tensors, reinterpreting them as meta tensors. This is done via a magic context manager in_kernel_invocation_manager which instructs all of PyTorch to view fake tensors as their underlying meta tensors, rather than “unwrapping” fake tensors into meta tensors (a fake tensor is a meta tensor). Fake tensors are represented this way to avoid having to keep two sets of metadata in sync (the meta tensor’s metadata, and the fake tensor’s metadata); the “is a” relationship ensures there is only one canonical copy of metadata. - If you’re a factory function, you’ll instead call the underlying factory function with device=’meta’. - Convert the resulting meta tensor into a fake tensor, computing what the output device of the tensor should be (this is usually trivial, but sometimes it is not, e.g., cpu scalar promotion, or device-converting operations.)", "prev_chunk_id": "chunk_999", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1001", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "API: the important bits#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "API: the important bits#", "content": "API: the important bits# Non-PT2 usage (check out test/test_fake_tensor.py for more examples): # Create a fake mode from torch._subclasses.fake_tensor import FakeTensorMode fake_mode = FakeTensorMode() converter = fake_mode.fake_tensor_converter # Fakeify some real tensors fake_x = converter.from_real_tensor(fake_mode, x) with fake_mode: # Do some operations on the fake tensors fake_y = fake_x * 2 # Factory operations automatically get fakeified in the context manager fake_z = torch.empty(20) Q: Why do you have real tensors as inputs? A: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you’re compiling, you already have the “real” inputs, because you’re compiling while you’re executing the program. PT2 pre-AOTAutograd usage (this is unusual, you probably don’t want to do this): # Fake mode is not enabled! from torch._guards import detect_fake_mode fake_mode = detect_fake_mode(args) # if fake_mode isn't None converter = fake_mode.fake_tensor_converter fake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args] with fake_mode: ... # do stuff with the fake args, if needed ... detect_fake_mode will search a number of locations to try to find “the” fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context. PT2 post-AOTAutograd usage: # Fake mode is enabled! example_inputs is typically fake already # TODO: we probably want to change this # Still do this to access fake mode fake_mode = detect_fake_mode(example_inputs) # But in general you don't have to turn it on Other useful stuff: from torch._subclasses.fake_tensor import unset_fake_temporarily with unset_fake_temporarily(): ... # fake mode is disabled here, you can do real tensor compute When might you want to disable fake tensor mode? Usually you don’t want to do this. One niche case where we’ve found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual", "prev_chunk_id": "chunk_1000", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1002", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "API: the important bits#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "API: the important bits#", "content": "tensor computation even though we’re in a fake tensor mode. import FakeTensorProp from torch.fx.passes.fake_tensor_prop gm: GraphModule real_inputs: List[Tensor] FakeTensorProp(gm).propagate(*real_inputs) # This will populate meta['val'] on all the FX nodes with a fake tensor # or if you have a preexisting fake mode, you should use it FakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs) # There is also propagate_dont_convert_inputs if your inputs are already fake fake_inputs: List[FakeTensor] FakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)", "prev_chunk_id": "chunk_1001", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1003", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Details#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Details#", "content": "Details# Auto-convert or not? Originally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun: with FakeTensorMode(): real_tensor.t_() What should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn’t any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: “Invoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.” This error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode. Eventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode. Metadata mutation on fake tensor If you have a fake tensor, and you t_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata! In fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don’t have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta[‘val’]", "prev_chunk_id": "chunk_1002", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1004", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "About the tensor subclass#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "About the tensor subclass#", "content": "About the tensor subclass# Fake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.__torch_dispatch__ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn’t recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.", "prev_chunk_id": "chunk_1003", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1005", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "How is each individual operator implemented?#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "How is each individual operator implemented?#", "content": "How is each individual operator implemented?# Unfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about: - Tensor subclasses support limited constant propagation if the number of elements is very small (this helps deal with some cases where we immediately call item() on such tensors.) - We have some fastpath implementations for certain operators, which are done entirely in fake tensor, for performance reasons. - If you use @custom_op to generate a custom tensor, these will register impl_abstract directly to fake tensor. - Fake tensor itself has some hardcoded special cases for device-converting operations. - If there is no meta implementation nor any decomposition, we will generate real zero-filled tensors and attempt to run the operator directly to find out what the results will be. This can cause segfaults if the operator attempts to do indexing with data, so we don’t turn this on by default for custom ops.", "prev_chunk_id": "chunk_1004", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1006", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "How does the converter work?#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "How does the converter work?#", "content": "How does the converter work?# Because fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad’ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.", "prev_chunk_id": "chunk_1005", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1007", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Performance characteristics#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Performance characteristics#", "content": "Performance characteristics# You would think fake tensors are fast because they don’t do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice: - Pointwise ops don’t go through PrimTorch decomps, instead we’ve hand-coded their propagation rule. - If possible, we should.", "prev_chunk_id": "chunk_1006", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1008", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Fake tensor of fake tensor?#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Fake tensor of fake tensor?#", "content": "Fake tensor of fake tensor?# There is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn’t really supported right now, but maybe it would not be too difficult to do.", "prev_chunk_id": "chunk_1007", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1009", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Interaction with dynamic shapes#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Interaction with dynamic shapes#", "content": "Interaction with dynamic shapes# Every FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together. Because FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.", "prev_chunk_id": "chunk_1008", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1010", "url": "https://docs.pytorch.org/docs/stable/torch.compiler_fake_tensor.html", "title": "Other resources#", "page_title": "Fake tensor — PyTorch 2.8 documentation", "breadcrumbs": "Other resources#", "content": "Other resources# Colab Tutorial On Using FakeTensor To Determine Max Batch Size", "prev_chunk_id": "chunk_1009", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1011", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "Draft Export#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "Draft Export#", "content": "Draft Export# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 Draft-export is a new version of export, which is designed to consistently produce a graph, even if there are potential soundness issues, and to generate a report listing out all of the issues export encountered during tracing and providing additional debugging information. For custom operators that don’t have fake kernels, it will also generate a profile which you can register to automatically generate a fake kernel. Have you ever tried to export a model using torch.export.export(), only to encounter a data-dependent issue? You fix it, but then run into a missing fake kernel problem. And after resolving that, you get hit with another data-dependent issue. You wonder to yourself, I wish there was a way I could just get a graph to play around with, and be able to view all the issues in one place so that I can fix them later… draft_export to the rescue! draft_export is a version of export which will always successfully export a graph, even if there are potential soundness issues. These issues will then be compiled into a report for clearer visualization, which can be fixed later on.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1012", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "What sort of errors does it catch?#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "What sort of errors does it catch?#", "content": "What sort of errors does it catch?# Draft-export helps to catch and debug the following errors: - Guard on data-dependent errors - Constraint violation errors - Missing fake kernels - Incorrectly written fake kernels", "prev_chunk_id": "chunk_1011", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1013", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "How does it work?#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "How does it work?#", "content": "How does it work?# In normal export, we will convert the sample inputs into FakeTensors and use them to record operations and trace the program into a graph. Input tensor shapes that can change (which are marked through dynamic_shapes), or values within tensors (typically from an .item() call) will be represented as a symbolic shape (SymInt) instead of a concrete integer. However some issues may occur while tracing - we may run into guards that we cannot evaluate, like if we want to check if some item in a tensor is greater than 0 (u0 >= 0). Since the tracer doesn’t know anything about the value of u0, it will throw a data-dependent error. If the model uses a custom operator but a fake kernel hasn’t been defined for it, then we will error with fake_tensor.UnsupportedOperatorException because export doesn’t know how to apply this on FakeTensors. If a custom operator has a fake kernel implemented incorrectly, export will silently produce an incorrect graph that doesn’t match the eager behavior. To fix the above errors, draft-export uses real tensor tracing to guide us on how to proceed when tracing. As we trace the model with fake tensors, for every operation that happens on a fake tensor, draft-export will also run the operator on stored real tensors which come from the example inputs passed to export. This allows us to address the above errors: When we reach a guard that we cannot evaluate, like u0 >= 0, we will use the stored real tensor values to evaluate this guard. Runtime asserts will be added into the graph to ensure that the graph asserts the same guard that we assumed while tracing. If we run into a custom operator without a fake kernel, we will run the operator’s normal kernel with the stored", "prev_chunk_id": "chunk_1012", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1014", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "How does it work?#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "How does it work?#", "content": "real tensors, and return a fake tensor with the same rank but unbacked shapes. Since we have the real tensor output for every operation, we will compare this with the fake tensor output from the fake kernel. If the fake kernel is implemented incorrectly, we will then catch this behavior and generate a more correct fake kernel.", "prev_chunk_id": "chunk_1013", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1015", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "How can I use draft export?#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "How can I use draft export?#", "content": "How can I use draft export?# Let’s say you’re trying to export this piece of code: class M(torch.nn.Module): def forward(self, x, y, z): res = torch.ops.mylib.foo2(x, y) a = res.item() a = -a a = a // 3 a = a + 5 z = torch.cat([z, z]) torch._check_is_size(a) torch._check(a < z.shape[0]) return z[:a] inp = (torch.tensor(3), torch.tensor(4), torch.ones(3, 3)) ep = torch.export.export(M(), inp) This runs into a “missing fake kernel” error for mylib.foo2 and then a GuardOnDataDependentExpression because of the slicing of z with a, an unbacked symint. To call draft-export, we can replace the torch.export line with the following: ep = torch.export.draft_export(M(), inp) ep is a valid ExportedProgram which can now be passed through further environments!", "prev_chunk_id": "chunk_1014", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1016", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "Debugging with draft-export#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "Debugging with draft-export#", "content": "Debugging with draft-export# In the terminal output from draft-export, you should see the following message: ######################################################################################### WARNING: 2 issue(s) found during export, and it was not able to soundly produce a graph. To view the report of failures in an html page, please run the command: `tlparse /tmp/export_angelayi/dedicated_log_torch_trace_axpofwe2.log --export` Or, you can view the errors in python by inspecting `print(ep._report)`. ######################################################################################## Draft-export automatically dumps logs for tlparse. You can view the tracing errors by using print(ep._report), or you can pass the logs into tlparse to generate an html report. Running the tlparse command in the terminal will generate a tlparse HTML report. Here is an example of the tlparse report: Clicking into the Data Dependent Error, we will see the following page which contains information to help debug this error. Specifically, it contains: - The stacktrace at which this error occurs - A list of local variables and their shapes - Information for how this guard was created", "prev_chunk_id": "chunk_1015", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1017", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "The returned Exported Program#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "The returned Exported Program#", "content": "The returned Exported Program# Because draft-export specializes on code paths based on the example inputs, the exported program resulting from draft-export is guaranteed to be runnable and return correct results for at least the given example inputs. Other inputs can work, as long as they match the same guards that were taken when we were draft-exporting. For example, if we have a graph branching on if a value is greater than 5, if in draft-export our example inputs were greater than 5, then the returned ExportedProgram will specialize on that branch, and will assert that the value is greater than 5. This means that the program will succeed if you pass in another value greater than 5, but will fail if you pass in a value less than 5. This is more sound than torch.jit.trace, which will silently specialize on the branch. The proper way for torch.export to support both branches would be to rewrite the code using torch.cond, which will then capture both branches. Because of the runtime assertions in the graph, the returned exported-program is also retraceable with torch.export or torch.compile, with a minor addition in the case where a custom operator is missing a fake kernel.", "prev_chunk_id": "chunk_1016", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1018", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "Generating Fake Kernels#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "Generating Fake Kernels#", "content": "Generating Fake Kernels# If a custom operator does not contain a fake implementation, currently draft-export will use the real-tensor propagation to get an output for the operator and continue tracing. However, if we run the exported program with fake tensors or retrace the exported model, we will still fail because there is still no fake kernel implementation. To address this, after draft-export, we will generate an operator profile for each custom operator call that we encounter, and store this on the report attached to the exported program: ep._report.op_profiles. Users can then use the context manager torch._library.fake_profile.unsafe_generate_fake_kernels to generate and register a fake implementation based on these operator profiles. This way future fake tensor retracing will work. The workflow would look something like: class M(torch.nn.Module): def forward(self, a, b): res = torch.ops.mylib.foo(a, b) # no fake impl return res ep = draft_export(M(), (torch.ones(3, 4), torch.ones(3, 4))) with torch._library.fake_profile.unsafe_generate_fake_kernels(ep._report.op_profiles): decomp = ep.run_decompositions() new_inp = ( torch.ones(2, 3, 4), torch.ones(2, 3, 4), ) # Save the profile to a yaml and check it into a codebase save_op_profiles(ep._report.op_profiles, \"op_profile.yaml\") # Load the yaml loaded_op_profile = load_op_profiles(\"op_profile.yaml\") The operator profile is a dictionary mapping operator name to a set of profiles which describe the input and outputs of the operator, and could be manually written, saved into a yaml file, and checked into a codebase. Here’s an example of a profile for mylib.foo.default: \"mylib.foo.default\": { OpProfile( args_profile=( TensorMetadata( rank=2, dtype=torch.float32, device=torch.device(\"cpu\"), layout=torch.strided, ), TensorMetadata( rank=2, dtype=torch.float32, device=torch.device(\"cpu\"), layout=torch.strided, ), ), out_profile=TensorMetadata( rank=2, dtype=torch.float32, device=torch.device(\"cpu\"), layout=torch.strided, ), ) } mylib.foo.default’s profile contains only one profile, which says that for 2 input tensors of rank 2, dtype torch.float32, device cpu, we will return one tensor of rank 2, dtype torch.float32, and device cpu. Using the context manager, will then generate a fake kernel where given 2 input", "prev_chunk_id": "chunk_1017", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1019", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "Generating Fake Kernels#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "Generating Fake Kernels#", "content": "tensors of rank 2 (and the other tensor metadata), we will output one tensor of rank 2 (and the other tensor metadata). If the operator also supports other input ranks, then we can add the profile to this list of profiles, either by manually adding it into the existing profile or rerunning draft-export with new inputs to get new profiles, so that the generated fake kernel will support more input types. Otherwise it will error.", "prev_chunk_id": "chunk_1018", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1020", "url": "https://docs.pytorch.org/docs/stable/draft_export.html", "title": "Where to go from here?#", "page_title": "Draft Export — PyTorch 2.8 documentation", "breadcrumbs": "Where to go from here?#", "content": "Where to go from here?# Now that we have successfully created an ExportedProgram using draft-export, we can use further compilers such as AOTInductor to optimize its performance and produce a runnable artifact. This optimized version can then be used for deployment. In parallel, we can utilize the report generated by draft-export to identify and fix torch.export errors that were encountered so that the original model can be directly traceable with torch.export.", "prev_chunk_id": "chunk_1019", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1021", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "torch.export IR Specification#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "torch.export IR Specification#", "content": "torch.export IR Specification# Created On: Oct 05, 2023 | Last Updated On: Jun 13, 2025 Export IR is an intermediate representation (IR) for compilers, which bears similarities to MLIR and TorchScript. It is specifically designed to express the semantics of PyTorch programs. Export IR primarily represents computation in a streamlined list of operations, with limited support for dynamism such as control flows. To create an Export IR graph, a frontend can be used that soundly captures a PyTorch program via a trace-specializing mechanism. The resulting Export IR can then be optimized and executed by a backend. This can be done today through torch.export.export(). The key concepts that will be covered in this document include: - ExportedProgram: the data structure containing the Export IR program - Graph: which consists of a list of nodes. - Nodes: which represents operations, control flow, and metadata stored on this node. - Values are produced and consumed by nodes. - Types are associated with values and nodes. - The size and memory layout of values are also defined.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1022", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "Assumptions#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "Assumptions#", "content": "Assumptions# This doc assumes that the audience is sufficiently familiar with PyTorch, specifically with torch.fx and its related toolings. Thus it will stop describing contents present in torch.fx documentation and paper.", "prev_chunk_id": "chunk_1021", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1023", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "What is Export IR#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "What is Export IR#", "content": "What is Export IR# Export IR is a graph-based intermediate representation IR of PyTorch programs. Export IR is realized on top of torch.fx.Graph. In other words, all Export IR graphs are also valid FX graphs, and if interpreted using standard FX semantics, Export IR can be interpreted soundly. One implication is that an exported graph can be converted to a valid Python program via standard FX codegen. This documentation will primarily focus on highlighting areas where Export IR differs from FX in terms of its strictness, while skipping parts where it shares similarities with FX.", "prev_chunk_id": "chunk_1022", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1024", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "ExportedProgram#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "ExportedProgram#", "content": "ExportedProgram# The top-level Export IR construct is an torch.export.ExportedProgram class. It bundles the computational graph of a PyTorch model (which is usually a torch.nn.Module) with the parameters or weights that this model consumes. Some notable attributes of the torch.export.ExportedProgram class are: - graph_module(torch.fx.GraphModule): Data structure containing the flattened computational graph of the PyTorch model. The graph can be directly accessed throughExportedProgram.graph. - graph_signature(torch.export.ExportGraphSignature): The graph signature, which specifies the parameters and buffer names used and mutated within the graph. Instead of storing parameters and buffers as attributes of the graph, they are lifted as inputs to the graph. The graph_signature is utilized to keep track of additional information on these parameters and buffers. - state_dict(Dict[str,Union[torch.Tensor,torch.nn.Parameter]]): Data structure containing the parameters and buffers. - range_constraints(Dict[sympy.Symbol,RangeConstraint]): For programs that are exported with data dependent behavior, the metadata on each node will contain symbolic shapes (which look likes0,i0). This attribute maps the symbolic shapes to their lower/upper ranges.", "prev_chunk_id": "chunk_1023", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1025", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "Graph#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "Graph#", "content": "Graph# An Export IR Graph is a PyTorch program represented in the form of a DAG (directed acyclic graph). Each node in this graph represents a particular computation or operation, and edges of this graph consist of references between nodes. We can view Graph having this schema: class Graph: nodes: List[Node] In practice, Export IR’s graph is realized as torch.fx.Graph Python class. An Export IR graph contains the following nodes (Nodes will be described in more details in the next section): - 0 or more nodes of op typeplaceholder - 0 or more nodes of op typecall_function - exactly 1 node of op typeoutput Collorary: The smallest valid Graph will be of one node. i.e. nodes is never empty. Definition: The set of placeholder nodes of a Graph represents the inputs of the Graph of GraphModule. The output node of a Graph represents the outputs of the Graph of GraphModule. Example: import torch from torch import nn class MyModule(nn.Module): def forward(self, x, y): return x + y example_args = (torch.randn(1), torch.randn(1)) mod = torch.export.export(MyModule(), example_args) print(mod.graph) graph(): %x : [num_users=1] = placeholder[target=x] %y : [num_users=1] = placeholder[target=y] %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {}) return (add,) The above is the textual representation of a Graph, with each line being a node.", "prev_chunk_id": "chunk_1024", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1026", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "Node#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "Node#", "content": "Node# A Node represents a particular computation or operation and is represented in Python using the torch.fx.Node class. Edges between nodes are represented as direct references to other nodes via the args property of the Node class. Using the same FX machinery, we can represent the following operations that a computational graph typically needs, such as operator calls, placeholders (aka inputs), conditionals, and loops. The Node has the following schema: class Node: name: str # name of node op_name: str # type of operation # interpretation of the fields below depends on op_name target: [str|Callable] args: List[object] kwargs: Dict[str, object] meta: Dict[str, object] FX Text Format As in the example above, notice that each line has this format: %<name>:[...] = <op_name>[target=<target>](args = (%arg1, %arg2, arg3, arg4, …)), kwargs = {\"keyword\": arg5}) This format captures everything present in the Node class, with the exception of meta, in a compact format. Concretely: - is the name of the node as it would appear innode.name. - <op_name>is thenode.opfield, which must be one of these:<call_function>,<placeholder>,<get_attr>, or<output>. - is the target of the node asnode.target. The meaning of this field depends onop_name. - args1, … args 4…are what is listed in thenode.argstuple. If a value in the list is antorch.fx.Node, then it will be especially indicated with a leading%. For example, a call to the add operator would appear as: %add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {}) Where %x, %y are two other Nodes that have names x and y. Worth noting that the string torch.op.aten.add.Tensor represents the callable object that is actually stored in the target field, not merely its string name. The final line of this text format is: return [add] which is a Node with op_name = output, indicating that we are returning this one element.", "prev_chunk_id": "chunk_1025", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1027", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "call_function#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "call_function#", "content": "call_function# A call_function node represents a call to an operator. Definitions - Functional:We say a callable is “functional” if it satisfies all the following requirements:Non-mutating: The operator does not mutate the value of its input (for tensors, this includes both metadata and data).No side effects: The operator does not mutate states that are visible from outside, like changing values of module parameters. - Operator:is a functional callable with a predefined schema. Examples of such operators include functional ATen operators. Representation in FX %name = call_function[target = operator](args = (%x, %y, …), kwargs = {}) Differences from vanilla FX call_function - In FX graph, a call_function can refer to any callable, in Export IR, we restrict it to only a select subset of ATen operators, custom operators, and control flow operators. - In Export IR, constant arguments will be embedded within the graph. - In FX graph, a get_attr node can represent reading any attribute stored in the graph module. However, in Export IR this is restricted to reading only submodules as all parameters/buffers will be passed in as inputs to the graph module.", "prev_chunk_id": "chunk_1026", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1028", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "Metadata#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "Metadata#", "content": "Metadata# Node.meta is a dict attached to every FX node. However, the FX spec does not specify what metadata can or will be there. Export IR provides a stronger contract, specifically all call_function nodes will guarantee having and only having the following metadata fields: - node.meta[\"stack_trace\"]is a string containing the Python stack trace referencing the original Python source code. An example stack trace looks like:File\"my_module.py\",line19,inforwardreturnx+dummy_helper(y)File\"helper_utility.py\",line89,indummy_helperreturny+1 - node.meta[\"val\"]describes the output of running the operation. It can be of type<symint>,<FakeTensor>, aList[Union[FakeTensor,SymInt]], orNone. - node.meta[\"nn_module_stack\"]describes the “stacktrace” of thetorch.nn.Modulefrom which the node came, if it was from atorch.nn.Modulecall. For example, if a node containing theaddmmop called from atorch.nn.Linearmodule inside of atorch.nn.Sequentialmodule, thenn_module_stackwould look something like:{'self_linear': ('self.linear', <class 'torch.nn.Linear'>), 'self_sequential': ('self.sequential', <class 'torch.nn.Sequential'>)} - node.meta[\"source_fn_stack\"]contains the torch function or the leaftorch.nn.Moduleclass this node was called from before decomposition. For example, a node containing theaddmmop from atorch.nn.Linearmodule call would containtorch.nn.Linearin theirsource_fn, and a node containing theaddmmop from atorch.nn.functional.Linearmodule call would containtorch.nn.functional.Linearin theirsource_fn.", "prev_chunk_id": "chunk_1027", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1029", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "placeholder#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "placeholder#", "content": "placeholder# Placeholder represents an input to a graph. Its semantics are exactly the same as in FX. Placeholder nodes must be the first N nodes in the nodes list of a graph. N can be zero. Representation in FX %name = placeholder[target = name](args = ()) The target field is a string which is the name of input. args, if non-empty, should be of size 1 representing the default value of this input. Metadata Placeholder nodes also have meta[‘val’], like call_function nodes. The val field in this case represents the input shape/dtype that the graph is expected to receive for this input parameter.", "prev_chunk_id": "chunk_1028", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1030", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "output#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "output#", "content": "output# An output call represents a return statement in a function; it thus terminates the current graph. There is one and only one output node, and it will always be the last node of the graph. Representation in FX output[](args = (%something, …)) This has the exact semantics as in torch.fx. args represents the node to be returned. Metadata Output node has the same metadata as call_function nodes.", "prev_chunk_id": "chunk_1029", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1031", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "get_attr#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "get_attr#", "content": "get_attr# get_attr nodes represent reading a submodule from the encapsulating torch.fx.GraphModule. Unlike a vanilla FX graph from torch.fx.symbolic_trace() in which get_attr nodes are used to read attributes such as parameters and buffers from the top-level torch.fx.GraphModule, parameters and buffers are passed in as inputs to the graph module, and stored in the top-level torch.export.ExportedProgram. Representation in FX %name = get_attr[target = name](args = ()) Example Consider the following model: from functorch.experimental.control_flow import cond def true_fn(x): return x.sin() def false_fn(x): return x.cos() def f(x, y): return cond(y, true_fn, false_fn, [x]) Graph: graph(): %x_1 : [num_users=1] = placeholder[target=x_1] %y_1 : [num_users=1] = placeholder[target=y_1] %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0] %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0] %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {}) return conditional The line, %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0], reads the submodule true_graph_0 which contains the sin operator.", "prev_chunk_id": "chunk_1030", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1032", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "SymInt#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "SymInt#", "content": "SymInt# A SymInt is an object that can either be a literal integer or a symbol that represents an Integer (represented in Python by sympy.Symbol class). When SymInt is a symbol, it describes a variable of type integer that is unknown to the graph at compile time, that is, its value is only known at runtime.", "prev_chunk_id": "chunk_1031", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1033", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "FakeTensor#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "FakeTensor#", "content": "FakeTensor# A FakeTensor is an object that contains the metadata of a tensor. It can be viewed as having the following metadata. class FakeTensor: size: List[SymInt] dtype: torch.dtype device: torch.device dim_order: List[int] # This doesn't exist yet The size field of FakeTensor is a list of integers or SymInts. If SymInts are present, this means this tensor has a dynamic shape. If integers are present, it is assumed that the tensor will have that exact static shape. The rank of the TensorMeta is never dynamic. The dtype field represents the dtype of the output of that node. There are no implicit type promotions in Edge IR. There are no strides in FakeTensor. In other words: - If the operator in node.target returns a Tensor, thennode.meta['val']is a FakeTensor describing that tensor. - If the operator in node.target returns an n-tuple of Tensors, thennode.meta['val']is an n-tuple of FakeTensors describing each tensor. - If the operator in node.target returns an int/float/scalar that is known at compile time, thennode.meta['val']is None. - If the operator in node.target returns an int/float/scalar that is not known at compile time, thennode.meta['val']is of type SymInt. For example: - aten::addreturns a Tensor; so its spec will be a FakeTensor with dtype and size of the tensor returned by this operator. - aten::sym_sizereturns an integer; so its val will be a SymInt because its value is only available at runtime. - max_pool2d_with_indexesreturns a tuple of (Tensor, Tensor); so the spec will also be a 2-tuple of FakeTensor objects, the first TensorMeta describes the first element of the return value etc. Python code: def add_one(x): return torch.ops.aten(x, 1) Graph: graph(): %ph_0 : [#users=1] = placeholder[target=ph_0] %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {}) return [add_tensor] FakeTensor: FakeTensor(dtype=torch.int, size=[2,], device=CPU)", "prev_chunk_id": "chunk_1032", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1034", "url": "https://docs.pytorch.org/docs/stable/export.ir_spec.html", "title": "Pytree-able Types#", "page_title": "torch.export IR Specification — PyTorch 2.8 documentation", "breadcrumbs": "Pytree-able Types#", "content": "Pytree-able Types# We define a type “Pytree-able”, if it is either a leaf type or a container type that contains other Pytree-able types. Note: The following types are defined as leaf type: The following types are defined as container type:", "prev_chunk_id": "chunk_1033", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1035", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "torch.export Programming Model#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "torch.export Programming Model#", "content": "torch.export Programming Model# Created On: Dec 18, 2024 | Last Updated On: Jun 11, 2025 This document aims to explain the behaviors and capabilities of torch.export.export(). It is intended to help build your intuition for how torch.export.export() handles code.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1036", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Basics of Tracing#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Basics of Tracing#", "content": "Basics of Tracing# torch.export.export() captures a graph representing your model by tracing its execution on “example” inputs and recording the PyTorch operations and conditions observed along the traced path. This graph can then be run on different inputs as long as they satisfy the same conditions. The basic output of torch.export.export() is a single graph of PyTorch operations, with associated metadata. The exact format of this output is covered in the torch.export IR Specification.", "prev_chunk_id": "chunk_1035", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1037", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Strict vs. Non-Strict Tracing#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Strict vs. Non-Strict Tracing#", "content": "Strict vs. Non-Strict Tracing# torch.export.export() provides two modes of tracing. In non-strict mode, we trace through the program using the normal Python interpreter. Your code executes exactly as it would in eager mode; the only difference is that all Tensors are replaced by fake Tensors, which have shapes and other forms of metadata but no data, wrapped in Proxy objects that record all operations on them into a graph. We also capture conditions on Tensor shapes that guard the correctness of the generated code. In strict mode, we first trace through the program using TorchDynamo, a Python bytecode analysis engine. TorchDynamo does not actually execute your Python code. Instead, it symbolically analyzes it and builds a graph based on the results. On the one hand, this analysis allows torch.export.export() to provide additional guarantees on Python-level safety (beyond capturing conditions on Tensor shapes, as in non-strict mode). On the other hand, not all Python features are supported by this analysis. Although currently the default mode of tracing is strict, we strongly recommend using non-strict, which will soon become the default. For most models, conditions on Tensor shapes are enough for soundness, and the additional guarantees on Python-level safety have no impact; at the same time, the possibility of hitting unsupported Python features in TorchDynamo presents an unnecessary risk. In the rest of this document we assume we are tracing in non-strict mode; in particular, we assume that all Python features are supported.", "prev_chunk_id": "chunk_1036", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1038", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Values: Static vs. Dynamic#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Values: Static vs. Dynamic#", "content": "Values: Static vs. Dynamic# A key concept in understanding the behavior of torch.export.export() is the difference between static and dynamic values.", "prev_chunk_id": "chunk_1037", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1039", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Static Values#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Static Values#", "content": "Static Values# A static value is a value that is fixed at export time and cannot change between executions of the exported program. When the value is encountered during tracing, we treat it as a constant and hard-code it into the graph. When an operation is performed (e.g. x + y) and all inputs are static, the output of the operation is directly hard-coded into the graph and the operation does not show up (i.e. it gets “constant-folded”). When a value has been hard-coded into the graph, we say that the graph has been specialized to that value. For example: import torch class MyMod(torch.nn.Module): def forward(self, x, y): z = y + 7 return x + z m = torch.export.export(MyMod(), (torch.randn(1), 3)) print(m.graph_module.code) \"\"\" def forward(self, arg0_1, arg1_1): add = torch.ops.aten.add.Tensor(arg0_1, 10); arg0_1 = None return (add,) \"\"\" Here, we provide 3 as the traced value for y; it is treated as a static value and added to 7, burning in the static value 10 in the graph.", "prev_chunk_id": "chunk_1038", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1040", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Dynamic Values#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Values#", "content": "Dynamic Values# A dynamic value is one that can change from run to run. It behaves just like a “normal” function argument: you can pass different inputs and expect your function to do the right thing.", "prev_chunk_id": "chunk_1039", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1041", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Which values are static vs. dynamic?#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Which values are static vs. dynamic?#", "content": "Which values are static vs. dynamic?# Whether a value is static or dynamic depends on its type: - For Tensor:Tensordatais treated as dynamic.Tensorshapescan be treated by the system as static or dynamic.By default, shapes of all input Tensors are considered static. The user can override this behavior for any input Tensor by specifying adynamic shapefor it.Tensors that are part of module state, i.e., parameters and buffers, always have static shapes.Other forms of Tensormetadata(e.g.device,dtype) are static. - Pythonprimitives(int,float,bool,str,None) are static.There are dynamic variants for some primitive types (SymInt,SymFloat,SymBool). Typically users do not have to deal with them. - For Pythonstandard containers(list,tuple,dict,namedtuple):The structure (i.e., length forlistandtuplevalues, and key sequence fordictandnamedtuplevalues) is static.The contained elements have these rules applied to them recursively (basically thePyTreescheme) with leaves that are either Tensor or primitive types. - Otherclasses(including data classes) can be registered with PyTree (see below), and follow the same rules as the standard containers.", "prev_chunk_id": "chunk_1040", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1042", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Input types#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Input types#", "content": "Input types# Inputs will be treated as either static or dynamic, based on their type (as explained above). - A static input will get hard-coded into the graph, and passing a different value at run time will result in an error. Recall that these are mostly values of primitive types. - A dynamic input behaves like a “normal” function input. Recall that these are mostly values of Tensor types. By default, the types of inputs you can use for your program are: - Tensor - Python primitives (int,float,bool,str,None) - Python standard containers (list,tuple,dict,namedtuple)", "prev_chunk_id": "chunk_1041", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1043", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Custom Input Types#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Custom Input Types#", "content": "Custom Input Types# In addition, you can also define your own (custom) class and use it as an input type, but you will need to register such a class as a PyTree. Here’s an example of using an utility to register a dataclass that is used as an input type. @dataclass class Input: f: torch.Tensor p: torch.Tensor torch.export.register_dataclass(Input) class M(torch.nn.Module): def forward(self, x: Input): return x.f + 1 torch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))", "prev_chunk_id": "chunk_1042", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1044", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Optional input types#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Optional input types#", "content": "Optional input types# For optional inputs to the program that are not passed in, torch.export.export() will specialize to their default values. As a result, the exported program will require users to explicitly pass in all arguments, and will lose the defaulting behavior. For example: class M(torch.nn.Module): def forward(self, x, y=None): if y is not None: return y * x return x + x # Optional input is passed in ep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3))) print(ep) \"\"\" ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\", y: \"f32[3, 3]\"): # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(y, x); y = x = None return (mul,) \"\"\" # Optional input is not passed in ep = torch.export.export(M(), (torch.randn(3, 3),)) print(ep) \"\"\" ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\", y): # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, x); x = None return (add,) \"\"\"", "prev_chunk_id": "chunk_1043", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1045", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Control Flow: Static vs. Dynamic#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Control Flow: Static vs. Dynamic#", "content": "Control Flow: Static vs. Dynamic# Control flow is supported by torch.export.export(). The behavior of control flow depends on whether the value you are branching on is static or dynamic.", "prev_chunk_id": "chunk_1044", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1046", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Static Control Flow#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Static Control Flow#", "content": "Static Control Flow# Python control flow over static values is supported transparently. (Recall that static values include static shapes, so control flow over static shapes is also covered by this case.) As mentioned above, we “burn in” static values, so the exported graph will never see any control flow over static values. In the case of an if statement, we will continue tracing the branch taken at export time. In the case of a for or while statement, we will continue tracing by unrolling the loop.", "prev_chunk_id": "chunk_1045", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1047", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Dynamic Control Flow: Shape-Dependent vs. Data-Dependent#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Control Flow: Shape-Dependent vs. Data-Dependent#", "content": "Dynamic Control Flow: Shape-Dependent vs. Data-Dependent# When the value involved in a control flow is dynamic, it could depend on dynamic shapes or dynamic data. Given that the compiler traces with information on shapes rather than data, the implications on the programming model are different in these cases.", "prev_chunk_id": "chunk_1046", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1048", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Dynamic Shape-Dependent Control Flow#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Shape-Dependent Control Flow#", "content": "Dynamic Shape-Dependent Control Flow# When the value involved in a control flow is a dynamic shape, in most cases we will also know the concrete value of the dynamic shape during tracing: see the following section for more details on how the compiler tracks this information. In these cases we say that the control flow is shape-dependent. We use the concrete value of the dynamic shape to evaluate the condition to either True or False and continue tracing (as discussed above), additionally emitting a guard corresponding to the condition just evaluated. Otherwise the control flow is considered data-dependent. We cannot evaluate the condition to either True or False, so cannot continue tracing and have to raise an error at export time. See next section.", "prev_chunk_id": "chunk_1047", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1049", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Dynamic Data-Dependent Control Flow#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Data-Dependent Control Flow#", "content": "Dynamic Data-Dependent Control Flow# Data-dependent control flow over dynamic values is supported, but you must use one of PyTorch’s explicit operators to continue tracing. Using Python control flow statements over dynamic values is not permitted, because the compiler cannot evaluate the conditions necessary to continue tracing and thus an error must be raised at export time. We provide operators to express general conditionals and loops over dynamic values, e.g., torch.cond, torch.map. Note that you only need to use these if you truly want data-dependent control flow. Here’s an example of an if statement on a data-dependent condition, x.sum() > 0, where x is an input Tensor, rewritten using torch.cond. Instead of having to decide which branch to trace, now both branches are traced. class M_old(torch.nn.Module): def forward(self, x): if x.sum() > 0: return x.sin() else: return x.cos() class M_new(torch.nn.Module): def forward(self, x): return torch.cond( pred=x.sum() > 0, true_fn=lambda x: x.sin(), false_fn=lambda x: x.cos(), operands=(x,), ) A special case of data-dependent control flow is where it involves a data-dependent dynamic shape: typically, the shape of some intermediate Tensor that depends on input data rather than on input shapes (thus not shape-dependent). Instead of using a control flow operator, in this case you can provide an assertion that decides whether the condition is True or False. Given such an assertion, we can continue tracing, emitting a guard as above. We provide operators to express assertions on dynamic shapes, e.g., torch._check. Note that you only need to use this when there is control flow on data-dependent dynamic shapes. Here’s an example of an if statement on a condition involving a data-dependent dynamic shape, nz.shape[0] > 0, where nz is the result of calling torch.nonzero(), an operator whose output shape depends on input data. Instead of rewriting it, you can add an assertion using", "prev_chunk_id": "chunk_1048", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1050", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Dynamic Data-Dependent Control Flow#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Data-Dependent Control Flow#", "content": "torch._check to effectively decide which branch to trace. class M_old(torch.nn.Module): def forward(self, x): nz = x.nonzero() if nz.shape[0] > 0: return x.sin() else: return x.cos() class M_new(torch.nn.Module): def forward(self, x): nz = x.nonzero() torch._check(nz.shape[0] > 0) if nz.shape[0] > 0: return x.sin() else: return x.cos()", "prev_chunk_id": "chunk_1049", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1051", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Basics of Symbolic Shapes#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Basics of Symbolic Shapes#", "content": "Basics of Symbolic Shapes# During tracing, dynamic Tensor shapes and conditions over them are encoded as “symbolic expressions.” (In contrast, static Tensor shapes and conditions over them are simply int and bool values.) A symbol is like a variable; it describes a dynamic Tensor shape. As tracing proceeds, shapes of intermediate Tensors may be described by more general expressions, typically involving integer arithmetic operators. This is because for most PyTorch operators, shapes of output Tensors can be described as functions of shapes of input Tensors. For example, the shape of the output of torch.cat() is the sum of the shapes of its inputs. Moreover, as we encounter control flow in the program, we create boolean expressions, typically involving relational operators, describing conditions along the traced path. These expressions are evaluated to decide which path to trace through the program, and recorded in a shape environment to guard the correctness of the traced path and to evaluate subsequently created expressions. We briefly introduce these subsystems next.", "prev_chunk_id": "chunk_1050", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1052", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Fake Implementations of PyTorch Operators#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Fake Implementations of PyTorch Operators#", "content": "Fake Implementations of PyTorch Operators# Recall that during tracing, we are executing the program with fake Tensors, which have no data. In general we cannot call the actual implementations of PyTorch operators with fake Tensors. Thus each operator needs to have an additional fake (a.k.a. “meta”) implementation, which inputs and outputs fake Tensors, that matches the behavior of the actual implementation in terms of shapes and other forms of metadata carried by fake Tensors. For example, note how the fake implementation of torch.index_select() computes the shape of the output using the shape of the input (while ignoring input data and returning empty output data). def meta_index_select(self, dim, index): result_size = list(self.size()) if self.dim() > 0: result_size[dim] = index.numel() return self.new_empty(result_size)", "prev_chunk_id": "chunk_1051", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1053", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Shape Propagation: Backed vs. Unbacked Dynamic Shapes#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Shape Propagation: Backed vs. Unbacked Dynamic Shapes#", "content": "Shape Propagation: Backed vs. Unbacked Dynamic Shapes# Shapes are propagated using fake implementations of PyTorch operators. A key concept to understand the propagation of dynamic shapes in particular is the difference between backed and unbacked dynamic shapes: we know the concrete values of the former but not the latter. Propagation of shapes, including tracking backed and unbacked dynamic shapes, proceeds as follows: - The shapes of Tensors representing inputs can be static or dynamic. When dynamic, they are described by symbols; moreover,such symbols are backed since we also know their concrete values given the “real” example inputs provided by the user at export time. - The output shape of an operator is computed by its fake implementation, and is either static or dynamic. When dynamic, in general it is described by a symbolic expression. Moreover:If the output shape depends only on input shapes, it is either static or backed dynamic whenever the input shapes are all static or backed dynamic.On the other hand,if the output shape depends on input data, it is necessarily dynamic, and moreover,because we cannot know its concrete value it is unbacked.", "prev_chunk_id": "chunk_1052", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1054", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Control Flow: Guards and Assertions#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Control Flow: Guards and Assertions#", "content": "Control Flow: Guards and Assertions# When a condition on shapes is encountered, it either involves only static shapes, in which case it is a bool, or it involves dynamic shapes, in which case it is a symbolic boolean expression. For the latter: - When the condition involves only backed dynamic shapes, we can use the concrete values of those dynamic shapes to evaluate the condition toTrueorFalse. We can then add a guard to the shape environment that states that the corresponding symbolic boolean expression isTrueorFalse, and continue tracing. - Otherwise the condition involves unbacked dynamic shapes. In general we cannot evaluate such a condition without additional information; thus we cannot continue tracing, and we must raise an error at export time. The user is expected to use an explicit PyTorch operator for tracing to continue. This information is added as a guard in the shape environment, and can also possibly help evaluate other subsequently encountered conditions toTrueorFalse. Once the model is exported, any guards on backed dynamic shapes can be understood as conditions on input dynamic shapes. These are verified against a dynamic shape specification that must have been provided to export, describing conditions on dynamic shapes that not only example inputs but also all future inputs are expected to satisfy for the generated code to be correct. More precisely, the dynamic shape specification must logically imply the generated guards, otherwise an error is raised at export time (along with suggested fixes to the dynamic shape specification). On the other hand, when there are no generated guards on backed dynamic shapes (in particular, when all shapes are static) no dynamic shape specification needs to be provided to export. In general, the dynamic shape specification is converted to runtime assertions on the inputs of the generated code. Finally, any guards on", "prev_chunk_id": "chunk_1053", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1055", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Control Flow: Guards and Assertions#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Control Flow: Guards and Assertions#", "content": "unbacked dynamic shapes are converted to “inline” runtime assertions. These are added in the generated code at the locations where those unbacked dynamic shapes were created: typically, right after data-dependent operator calls.", "prev_chunk_id": "chunk_1054", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1056", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Allowed PyTorch operators#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Allowed PyTorch operators#", "content": "Allowed PyTorch operators# All PyTorch operators are permitted.", "prev_chunk_id": "chunk_1055", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1057", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Custom operators#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Custom operators#", "content": "Custom operators# In addition, you can define and use custom operators. Defining a custom operator includes defining a fake implementation for it, just like any other PyTorch operator (see previous section). Here’s an example of a custom sin operator that wraps NumPy, and its registered (trivial) fake implementation. @torch.library.custom_op(\"mylib::sin\", mutates_args=()) def sin(x: Tensor) -> Tensor: x_np = x.numpy() y_np = np.sin(x_np) return torch.from_numpy(y_np) @torch.library.register_fake(\"mylib::sin\") def _(x: Tensor) -> Tensor: return torch.empty_like(x) Sometimes your custom operator’s fake implementation will involve data-dependent shapes. Here’s how a fake implementation for a custom nonzero might look like. ... @torch.library.register_fake(\"mylib::custom_nonzero\") def _(x): nnz = torch.library.get_ctx().new_dynamic_size() shape = [nnz, x.dim()] return x.new_empty(shape, dtype=torch.int64)", "prev_chunk_id": "chunk_1056", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1058", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Module State: Reads vs. Updates#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Module State: Reads vs. Updates#", "content": "Module State: Reads vs. Updates# Module states include parameters, buffers, and regular attributes. - A regular attribute can be of any type. - On the other hand, parameters and buffers are always Tensors. Module states can be dynamic or static, based on their types as outlined above. For example, self.training is a bool, which means it is static; on the other hand, any parameter or buffer is dynamic. The shapes of any Tensors contained in module states cannot be dynamic, i.e., those shapes are fixed at export time, and cannot change between executions of the exported program.", "prev_chunk_id": "chunk_1057", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1059", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Access rules#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Access rules#", "content": "Access rules# All module states must be initialized. Accessing a module state that is not already initialized causes an error to be raised at export time. Reading module states is always permitted. Updating module states is possible, but must follow the rules below: - A static regular attribute(e.g., of primitive type)can be updated. Reads and updates can be freely interleaved, and as expected, any reads will always see the values of the latest updates. Because these attributes are static, we will also burn the values in, so the generated code will not have any instructions to actually “get” or “set” such attributes. - A dynamic regular attribute(e.g., of Tensor type)cannot be updated. To do so, it must be registered as a buffer during module initialization. - A buffer can be updated, where the updating can be in-place (e.g.,self.buffer[:]=...) or not (e.g.,self.buffer=...). - A parameter cannot be updated. Typically parameters are updated only during training, not during inference. We recommend exporting withtorch.no_grad()to avoid parameter updates at export time.", "prev_chunk_id": "chunk_1058", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1060", "url": "https://docs.pytorch.org/docs/stable/export.programming_model.html", "title": "Effects of functionalization#", "page_title": "torch.export Programming Model — PyTorch 2.8 documentation", "breadcrumbs": "Effects of functionalization#", "content": "Effects of functionalization# Any dynamic module state that is read and/or updated is “lifted” (respectively) as an input and/or output of the generated code. The exported program stores, along with the generated code, the initial values of parameters and buffers and the constant values of other Tensor attributes.", "prev_chunk_id": "chunk_1059", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1061", "url": "https://docs.pytorch.org/docs/stable/cuda._sanitizer.html", "title": "CUDA Stream Sanitizer#", "page_title": "CUDA Stream Sanitizer — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Stream Sanitizer#", "content": "CUDA Stream Sanitizer# Created On: Sep 09, 2022 | Last Updated On: Oct 31, 2022", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1062", "url": "https://docs.pytorch.org/docs/stable/cuda._sanitizer.html", "title": "Overview#", "page_title": "CUDA Stream Sanitizer — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# This module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different streams. It stores information on accesses to tensors to determine if they are synchronized or not. When enabled in a python program and a possible data race is detected, a detailed warning will be printed and the program will exit. It can be enabled either by importing this module and calling enable_cuda_sanitizer() or by exporting the TORCH_CUDA_SANITIZER environment variable.", "prev_chunk_id": "chunk_1061", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1063", "url": "https://docs.pytorch.org/docs/stable/cuda._sanitizer.html", "title": "Usage#", "page_title": "CUDA Stream Sanitizer — PyTorch 2.8 documentation", "breadcrumbs": "Usage#", "content": "Usage# Here is an example of a simple synchronization error in PyTorch: import torch a = torch.rand(4, 2, device=\"cuda\") with torch.cuda.stream(torch.cuda.Stream()): torch.mul(a, 5, out=a) The a tensor is initialized on the default stream and, without any synchronization methods, modified on a new stream. The two kernels will run concurrently on the same tensor, which might cause the second kernel to read uninitialized data before the first one was able to write it, or the first kernel might overwrite part of the result of the second. When this script is run on the commandline with: TORCH_CUDA_SANITIZER=1 python example_error.py the following output is printed by CSAN: ============================ CSAN detected a possible data race on tensor with data pointer 139719969079296 Access by stream 94646435460352 during kernel: aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!) writing to argument(s) self, out, and to the output With stack trace: File \"example_error.py\", line 6, in <module> torch.mul(a, 5, out=a) ... File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch stack_trace = traceback.StackSummary.extract( Previous access by stream 0 during kernel: aten::rand(int[] size, *, int? dtype=None, Device? device=None) -> Tensor writing to the output With stack trace: File \"example_error.py\", line 3, in <module> a = torch.rand(10000, device=\"cuda\") ... File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch stack_trace = traceback.StackSummary.extract( Tensor was allocated with stack trace: File \"example_error.py\", line 3, in <module> a = torch.rand(10000, device=\"cuda\") ... File \"pytorch/torch/cuda/_sanitizer.py\", line 420, in _handle_memory_allocation traceback.StackSummary.extract( This gives extensive insight into the origin of the error: - A tensor was incorrectly accessed from streams with ids: 0 (default stream) and 94646435460352 (new stream) - The tensor was allocated by invokinga=torch.rand(10000,device=\"cuda\") - The faulty accesses were caused by operatorsa=torch.rand(10000,device=\"cuda\")on stream 0torch.mul(a,5,out=a)on stream 94646435460352 - The error message also displays the schemas of the invoked operators, along with a note showing which arguments of the operators correspond to", "prev_chunk_id": "chunk_1062", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1064", "url": "https://docs.pytorch.org/docs/stable/cuda._sanitizer.html", "title": "Usage#", "page_title": "CUDA Stream Sanitizer — PyTorch 2.8 documentation", "breadcrumbs": "Usage#", "content": "the affected tensor.In the example, it can be seen that tensoracorresponds to argumentsself,outand theoutputvalue of the invoked operatortorch.mul. The bug can be fixed by forcing the new stream to wait for the default stream: with torch.cuda.stream(torch.cuda.Stream()): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) When the script is run again, there are no errors reported.", "prev_chunk_id": "chunk_1063", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1065", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "TunableOp#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "TunableOp#", "content": "TunableOp# Created On: Jun 03, 2024 | Last Updated On: Jun 13, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1066", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Overview#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# This module exposes a TunableOp interface. Some operations, such as GEMMs, could be implemented using more than one library or more than one technique. For example, a GEMM could be implemented for CUDA or ROCm using either the blas or blasLt libraries. Further, ROCm’s rocblas and hipblaslt libraries allow the user to query for all possible algorithms and then choose one. How does one know which implementation is the fastest and should be chosen? That’s what TunableOp provides.", "prev_chunk_id": "chunk_1065", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1067", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Enabling TunableOp and Tuning Separately#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Enabling TunableOp and Tuning Separately#", "content": "Enabling TunableOp and Tuning Separately# The TunableOp feature is enabled separately from enabling the tuning phase itself. Enabling TunableOp means that PyTorch will replace any standard operators with their Tunable implementations. Any call to a TunableOp first checks whether it has already been tuned for the given operator inputs. If so, it will immediately call the tuned operation; no further tuning will take place even when the tuning setting is enabled. Instead if no tuning result is found, and tuning is enabled, the TunableOp will benchmark every registered implementation of that operator for the given set of inputs and select the fastest.", "prev_chunk_id": "chunk_1066", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1068", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "File Input and Output#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "File Input and Output#", "content": "File Input and Output# The first time any TunableOp is invoked, the internal database of tuned operations will be prepared by attempting to read the results from the given file. The default filename is ‘tunableop_results.csv’. To support tuning when multiple GPUs are used across multiple processes, the GPU device ordinal is automatically inserted into the filename to avoid multiple processes overwriting the same file. If tuning is enabled and new tunings are discovered during the course of your workload, it will also write out to this same filename with all tunings, both the ones it read in at startup as well as the new ones found at runtime. This can be used, for example, to build up a tunings file across many workloads by reusing the same file. The output file is automatically created when the application terminates. This behavior can be controlled by the C++ and Python APIs but not the environment variables. Assuming you specified a filename, you’ll end up with a CSV file with contents like so: Validator,PT_VERSION,2.2.0 Validator,ROCM_VERSION,6.0.0.0-12969-1544e39 Validator,HIPBLASLT_VERSION,0.6.0-a9c5cc7 Validator,ROCBLAS_VERSION,4.0.0-72e57364-dirty GemmTunableOp_float_NT,nt_25088_4096_64,Gemm_Hipblaslt_1219,1.262 GemmTunableOp_float_NT,nt_4096_4096_64,Gemm_Rocblas_1216,0.033 Note the “Validator” lines. If you change a library version, or ROCm version, or PyTorch version, TunableOp will detect this and reject the tunings file because the prior tunings are likely affected by other software changes. The remaining lines are the tuned solutions for each TunableOp encountered during your execution. Each line consists of 4 comma-separated fields: operator name, operator parameters, solution name, and average execution time. The execution time is an optional field. The CSV file can be edited, but with caution. For example, the solution name (field 3) can be changed to “Default” and it will fall back to the original PyTorch untuned implementation. Or, in the case of ROCm’s hipBLAS or hipBLASLt libraries, if you know the specific solution index you", "prev_chunk_id": "chunk_1067", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1069", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "File Input and Output#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "File Input and Output#", "content": "can override the solution that TunableOp selected by replacing the value. The operator name and parameters (fields 1 and 2) are internally named and should not be modified. In the case of GemmTunableOp, field 1 indicates the datatype and whether the inputs are transposed (T) or not (N) and field 2 indicates the M, N, K input shapes. There is an option to enable verbose output but it is only recommended for debugging purposes. This will produce a lot of diagnostic messages but may be useful to see if TunableOp is being used at all. Otherwise, TunableOp is completely silent, besides file output, unless there is a warning or error during its use. The verbose option is only available by setting the environment variable PYTORCH_TUNABLEOP_VEROBSE=1.", "prev_chunk_id": "chunk_1068", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1070", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "A Note on Tuning Behavior, Warmup, and Cache Effects#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "A Note on Tuning Behavior, Warmup, and Cache Effects#", "content": "A Note on Tuning Behavior, Warmup, and Cache Effects# Tuning an operator consists of iterating through the list or registered implementations and profiling each one. The profile is established by running a single implementation in a loop multiple times and taking the average execution time. There is also an optional warmup phase prior to tuning that can help with reaching stable power states by the hardware. During tuning of a workload the various hardware caches will more likely produce hits than when not tuning. There are options for flushing the instruction cache and rotate the input tensors which might help produce a more faithful profile of the tuned operator as if the operator were run within a larger workload instead of in a tight, repetitive loop. By default, each possible solution for a given operator will be run for either 100 iterations or as many iterations that can be run within 30ms, whichever is smaller, and its average execution will be calculated. The fastest solution among all that were successfully profiled will be chosen. A profile might fail if the given solution doesn’t achieve the same accuracy as the default implementation or if the solution returns an error code.", "prev_chunk_id": "chunk_1069", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1071", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "TunableGemm for ROCm#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "TunableGemm for ROCm#", "content": "TunableGemm for ROCm# Currently only a TunableGemm for ROCm is implemented. Note that CUDA builds of PyTorch will function correctly when using TunableOp but the only solution available to CUDA builds is the ‘Default’ implementation i.e. the original cuBLAS default, now called through TunableOp. Any call to at::cuda::blas::gemm() or ::bgemm() will be routed through TunableOp when enabled. Calling gemm() for a given set of input arguments (transa, transb, m, n, k) will attempt to use the fastest available implementation across both rocblas and hipblaslt.", "prev_chunk_id": "chunk_1070", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1072", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Motivation#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Motivation#", "content": "Motivation# There are several use cases for offline tuning. One use case involves a workload with a high-memory utilization, where regular tuning might lead to running out of memory. Another use case is for compute-intensive workloads. In such cases, it is more resource-efficient to collect the GEMMs for the workload once and then tune repeatedly with different tuning parameters or libraries.", "prev_chunk_id": "chunk_1071", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1073", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Workflow#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Workflow#", "content": "Workflow# There are basically two steps: 1) Set the environment variables to collect the untuned GEMM and this will generate tunableop_untuned0.csv: PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_TUNING=0 PYTORCH_TUNABLEOP_RECORD_UNTUNED=1 ... - Run a Python script that reads thetunableop_untuned0.csvand generates thetunableop_results0.csv, like this: import torch.cuda.tunable as tunable import os os.putenv('PYTORCH_TUNABLEOP_ENABLED', '1') os.putenv('PYTORCH_TUNABLEOP_TUNING', '1') os.putenv('PYTORCH_TUNABLEOP_RECORD_UNTUNED', '0') tunable.tune_gemm_in_file(\"tunableop_untuned0.csv\") It is also possible to take multiple untuned files and distribute the GEMMs for tuning to multiple GPUs within a single node. In the first step, the GEMMs are first gathered and duplicate GEMMs are eliminated. Next, the GEMMs are distributed to different GPUs for tuning. After all GEMMs are tuned, the results from all the GPUs are then gathered into a single file whose base filename has _full0 appended to it (for example tunableop_results_full0.csv). Finally, this new file, containing the gathered results, will be duplicated N times, once for each GPU as convenience to the user will run the workload with the tuned configuration on N GPUs. if __name__ == \"__main__\": num_gpus = 8 # number of GPUs that will be used during the tuning process tunable.mgpu_tune_gemm_in_file(\"tunableop_untuned?.csv\", num_gpus) Note that the usage of the mgpu_tune_gemm_in_file API is different from its single GPU counterpart (tune_gemm_in_file). The body of the Python script that calls the API must be wrapped in main() as shown due to the use of concurrent futures module. The argument to mgpu_tune_gemm_in_file must contain a wild card expression (? or *) to generate the list of untuned files containing the GEMMs to be processed. The num_gpus must between 1 and the total number of GPUs available.", "prev_chunk_id": "chunk_1072", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1074", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Tuning Context#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Tuning Context#", "content": "Tuning Context# The behavior of TunableOp is currently manipulated through environment variables, the C++ interface of at::cuda::tunable::getTuningContext(), or the torch.cuda.tunable python interfaces. The environment variables take precedence over any setting you manipulate using the C++ or Python APIs.", "prev_chunk_id": "chunk_1073", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1075", "url": "https://docs.pytorch.org/docs/stable/cuda.tunable.html", "title": "Environment Variable Interface#", "page_title": "TunableOp — PyTorch 2.8 documentation", "breadcrumbs": "Environment Variable Interface#", "content": "Environment Variable Interface# Environment variables are cached the first time they are read. You cannot use the environment variable interface programmatically since the settings become fixed. Use the C++ or Python APIs instead.", "prev_chunk_id": "chunk_1074", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1076", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "_", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "_", "content": "_ __create_chunk_list__() (torch.distributed.tensor.DTensor method) __getstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState method) __init__() (torch.distributed.FileStore method) (torch.distributed.HashStore method) (torch.distributed.PrefixStore method) (torch.distributed.Store method) (torch.distributed.TCPStore method) (torch.fx.Graph method) (torch.fx.GraphModule method) (torch.monitor.Event method) (torch.monitor.Stat method) (torch.monitor.TensorboardEventHandler method) (torch.package.PackageExporter method) (torch.package.PackageImporter method) (torch.Tensor method) (torch.utils.tensorboard.writer.SummaryWriter method) __setstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState method) _assert() (in module torch) _assign_worker_ranks() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _create_jit_fn() (in module torch.cuda.jiterator) _create_multi_output_jit_fn() (in module torch.cuda.jiterator) _dump_snapshot() (in module torch.cuda.memory) _exit_barrier() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _foreach_abs() (in module torch) _foreach_abs_() (in module torch) _foreach_acos() (in module torch) _foreach_acos_() (in module torch) _foreach_asin() (in module torch) _foreach_asin_() (in module torch) _foreach_atan() (in module torch) _foreach_atan_() (in module torch) _foreach_ceil() (in module torch) _foreach_ceil_() (in module torch) _foreach_cos() (in module torch) _foreach_cos_() (in module torch) _foreach_cosh() (in module torch) _foreach_cosh_() (in module torch) _foreach_erf() (in module torch) _foreach_erf_() (in module torch) _foreach_erfc() (in module torch) _foreach_erfc_() (in module torch) _foreach_exp() (in module torch) _foreach_exp_() (in module torch) _foreach_expm1() (in module torch) | _foreach_expm1_() (in module torch) _foreach_floor() (in module torch) _foreach_floor_() (in module torch) _foreach_frac() (in module torch) _foreach_frac_() (in module torch) _foreach_lgamma() (in module torch) _foreach_lgamma_() (in module torch) _foreach_log() (in module torch) _foreach_log10() (in module torch) _foreach_log10_() (in module torch) _foreach_log1p() (in module torch) _foreach_log1p_() (in module torch) _foreach_log2() (in module torch) _foreach_log2_() (in module torch) _foreach_log_() (in module torch) _foreach_neg() (in module torch) _foreach_neg_() (in module torch) _foreach_reciprocal() (in module torch) _foreach_reciprocal_() (in module torch) _foreach_round() (in module torch) _foreach_round_() (in module torch) _foreach_sigmoid() (in module torch) _foreach_sigmoid_() (in module torch) _foreach_sin() (in module torch) _foreach_sin_() (in module torch) _foreach_sinh() (in module torch) _foreach_sinh_() (in module torch) _foreach_sqrt() (in module torch) _foreach_sqrt_() (in module torch) _foreach_tan() (in module torch) _foreach_tan_() (in module torch) _foreach_trunc() (in module torch) _foreach_trunc_() (in module torch) _foreach_zero_() (in module torch) _initialize_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _KinetoProfile (class in torch.profiler) _monitor_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _record_memory_history() (in module torch.cuda.memory) _rendezvous() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _restart_workers()", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1077", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "_", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "_", "content": "(torch.distributed.elastic.agent.server.SimpleElasticAgent method) _shutdown() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _snapshot() (in module torch.cuda.memory) _start_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method) _stop_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent method)", "prev_chunk_id": "chunk_1076", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1078", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "A", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "A", "content": "A abs() (in module torch) (torch.Tensor method) abs_() (torch.Tensor method) absolute() (in module torch) (torch.Tensor method) absolute_() (torch.Tensor method) AbsTransform (class in torch.distributions.transforms) AcceleratorError acos() (in module torch) (torch.Tensor method) acos_() (torch.Tensor method) acosh() (in module torch) (torch.Tensor method) acosh_() (torch.Tensor method) acquire() (torch.distributed.elastic.timer.TimerClient method) Adadelta (class in torch.optim) Adafactor (class in torch.optim) Adagrad (class in torch.optim) Adam (class in torch.optim) Adamax (class in torch.optim) AdamW (class in torch.optim) adapt() (torch.export.unflatten.FlatArgsAdapter method) adaptive_autorange() (torch.utils.benchmark.Timer method) adaptive_avg_pool1d() (in module torch.nn.functional) adaptive_avg_pool2d (class in torch.ao.nn.quantized.functional) adaptive_avg_pool2d() (in module torch.nn.functional) adaptive_avg_pool3d (class in torch.ao.nn.quantized.functional) adaptive_avg_pool3d() (in module torch.nn.functional) adaptive_max_pool1d() (in module torch.nn.functional) adaptive_max_pool2d() (in module torch.nn.functional) adaptive_max_pool3d() (in module torch.nn.functional) AdaptiveAvgPool1d (class in torch.nn) AdaptiveAvgPool2d (class in torch.nn) AdaptiveAvgPool3d (class in torch.nn) AdaptiveLogSoftmaxWithLoss (class in torch.nn) AdaptiveMaxPool1d (class in torch.nn) AdaptiveMaxPool2d (class in torch.nn) AdaptiveMaxPool3d (class in torch.nn) add() (in module torch) (torch.ao.ns._numeric_suite.Shadow method) (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method) (torch.distributed.Store method) (torch.export.dynamic_shapes.AdditionalInputs method) (torch.fx.experimental.symbolic_shapes.DimConstraints method) (torch.monitor.Stat method) (torch.Tensor method) add_() (torch.Tensor method) add_audio() (torch.utils.tensorboard.writer.SummaryWriter method) add_custom_scalars() (torch.utils.tensorboard.writer.SummaryWriter method) add_dependency() (torch.package.PackageExporter method) add_done_callback() (torch.futures.Future method) add_dtype_config() (torch.ao.quantization.backend_config.BackendPatternConfig method) add_embedding() (torch.utils.tensorboard.writer.SummaryWriter method) add_equality() (torch.fx.experimental.symbolic_shapes.DimConstraints method) add_figure() (torch.utils.tensorboard.writer.SummaryWriter method) add_graph() (torch.utils.tensorboard.writer.SummaryWriter method) add_histogram() (torch.utils.tensorboard.writer.SummaryWriter method) add_hparams() (torch.utils.tensorboard.writer.SummaryWriter method) add_image() (torch.utils.tensorboard.writer.SummaryWriter method) add_images() (torch.utils.tensorboard.writer.SummaryWriter method) add_loggers() (in module torch.ao.ns._numeric_suite_fx) add_mesh() (torch.utils.tensorboard.writer.SummaryWriter method) add_metadata() (torch.profiler._KinetoProfile method) add_metadata_json() (torch.profiler._KinetoProfile method) add_module() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) add_param_group() (torch.distributed.optim.ZeroRedundancyOptimizer method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) add_pr_curve() (torch.utils.tensorboard.writer.SummaryWriter method) add_pruning_method() (torch.nn.utils.prune.PruningContainer method) add_quant_dequant (class in torch.ao.quantization) add_relu() (torch.ao.ns._numeric_suite.Shadow method) add_safe_globals() (in module torch.serialization) add_scalar() (torch.ao.ns._numeric_suite.Shadow method) (torch.utils.tensorboard.writer.SummaryWriter method) add_scalars() (torch.utils.tensorboard.writer.SummaryWriter method) add_shadow_loggers() (in module torch.ao.ns._numeric_suite_fx) add_submodule() (torch.fx.GraphModule method) add_text() (torch.utils.tensorboard.writer.SummaryWriter method) add_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) add_video() (torch.utils.tensorboard.writer.SummaryWriter method) addbmm() (in module torch) (torch.Tensor method) addbmm_() (torch.Tensor method) addcdiv() (in", "prev_chunk_id": "chunk_1077", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1079", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "A", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "A", "content": "module torch) (torch.Tensor method) addcdiv_() (torch.Tensor method) addcmul() (in module torch) (torch.Tensor method) addcmul_() (torch.Tensor method) AdditionalInputs (class in torch.export.dynamic_shapes) addmm() (in module torch) (in module torch.sparse) (torch.Tensor method) addmm_() (torch.Tensor method) addmv() (in module torch) (torch.Tensor method) addmv_() (torch.Tensor method) addr() (in module torch) (torch.Tensor method) addr_() (torch.Tensor method) adjoint() (in module torch) (torch.Tensor method) affine_grid() (in module torch.nn.functional) AffineQuantizedObserverBase (class in torch.ao.quantization.observer) AffineTransform (class in torch.distributions.transforms) Aggregation (class in torch.monitor) airy_ai() (in module torch.special) align_as() (torch.Tensor method) align_to() (torch.Tensor method) all() (in module torch) (torch.Tensor method) all_gather() (in module torch.distributed) all_gather_into_tensor() (in module torch.distributed) all_gather_object() (in module torch.distributed) all_input_nodes (torch.fx.Node property) all_paths() (torch.package.PackageExporter method) all_reduce() (in module torch.distributed) all_to_all() (in module torch.distributed) all_to_all_single() (in module torch.distributed) allclose() (in module torch) (torch.Tensor method) allocator (torch.cuda.memory.MemPool property) allow_bf16_reduced_precision_reduction (in module torch.backends.cuda.matmul) allow_fp16_bf16_reduction_math_sdp() (in module torch.backends.cuda) allow_fp16_reduced_precision_reduction (in module torch.backends.cuda.matmul) allow_in_graph() (in module torch.compiler) allow_mutation_on_saved_tensors (class in torch.autograd.graph) allow_tf32 (in module torch.backends.cuda.matmul) (in module torch.backends.cudnn) allreduce_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks) alpha_dropout() (in module torch.nn.functional) AlphaDropout (class in torch.nn) amax() (in module torch) (torch.Tensor method) amin() (in module torch) (torch.Tensor method) aminmax() (in module torch) (torch.Tensor method) and_masks() (in module torch.nn.attention.flex_attention) angle() (in module torch) (torch.Tensor method) annotate() (in module torch.jit) any() (in module torch) (torch.Tensor method) | aoti_compile_and_package() (in module torch._inductor) aoti_load_package() (in module torch._inductor) append() (torch.distributed.Store method) (torch.fx.Node method) (torch.nn.ModuleList method) (torch.nn.ParameterList method) (torch.nn.Sequential method) apply() (torch.autograd.function.BackwardCFunction method) (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.nn.utils.prune.BasePruningMethod class method) (torch.nn.utils.prune.CustomFromMask class method) (torch.nn.utils.prune.Identity class method) (torch.nn.utils.prune.L1Unstructured class method) (torch.nn.utils.prune.LnStructured class method) (torch.nn.utils.prune.PruningContainer class method) (torch.nn.utils.prune.RandomStructured class method) (torch.nn.utils.prune.RandomUnstructured class method) (torch.optim.swa_utils.AveragedModel method) apply_() (torch.Tensor method) apply_jvp() (torch.autograd.function.BackwardCFunction method) apply_mask() (torch.nn.utils.prune.BasePruningMethod method) (torch.nn.utils.prune.CustomFromMask method) (torch.nn.utils.prune.Identity method) (torch.nn.utils.prune.L1Unstructured method) (torch.nn.utils.prune.LnStructured method) (torch.nn.utils.prune.PruningContainer method) (torch.nn.utils.prune.RandomStructured method) (torch.nn.utils.prune.RandomUnstructured method) apply_weights() (torch.onnx.ONNXProgram method) arange() (in module torch) arccos() (in module torch) (torch.Tensor method) arccos_() (torch.Tensor method) arccosh() (in", "prev_chunk_id": "chunk_1078", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1080", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "A", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "A", "content": "module torch) (torch.Tensor method) arccosh_() (torch.Tensor method) arcsin() (in module torch) (torch.Tensor method) arcsin_() (torch.Tensor method) arcsinh() (in module torch) (torch.Tensor method) arcsinh_() (torch.Tensor method) arctan() (in module torch) (torch.Tensor method) arctan2() (in module torch) (torch.Tensor method) arctan2_() (torch.Tensor method) arctan_() (torch.Tensor method) arctanh() (in module torch) (torch.Tensor method) arctanh_() (torch.Tensor method) are_deterministic_algorithms_enabled() (in module torch) arg_constraints (torch.distributions.bernoulli.Bernoulli attribute) (torch.distributions.beta.Beta attribute) (torch.distributions.binomial.Binomial attribute) (torch.distributions.categorical.Categorical attribute) (torch.distributions.cauchy.Cauchy attribute) (torch.distributions.chi2.Chi2 attribute) (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute) (torch.distributions.dirichlet.Dirichlet attribute) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential attribute) (torch.distributions.fishersnedecor.FisherSnedecor attribute) (torch.distributions.gamma.Gamma attribute) (torch.distributions.generalized_pareto.GeneralizedPareto attribute) (torch.distributions.geometric.Geometric attribute) (torch.distributions.gumbel.Gumbel attribute) (torch.distributions.half_cauchy.HalfCauchy attribute) (torch.distributions.half_normal.HalfNormal attribute) (torch.distributions.independent.Independent attribute) (torch.distributions.inverse_gamma.InverseGamma attribute) (torch.distributions.kumaraswamy.Kumaraswamy attribute) (torch.distributions.laplace.Laplace attribute) (torch.distributions.lkj_cholesky.LKJCholesky attribute) (torch.distributions.log_normal.LogNormal attribute) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute) (torch.distributions.mixture_same_family.MixtureSameFamily attribute) (torch.distributions.multinomial.Multinomial attribute) (torch.distributions.multivariate_normal.MultivariateNormal attribute) (torch.distributions.negative_binomial.NegativeBinomial attribute) (torch.distributions.normal.Normal attribute) (torch.distributions.one_hot_categorical.OneHotCategorical attribute) (torch.distributions.pareto.Pareto attribute) (torch.distributions.poisson.Poisson attribute) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli attribute) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute) (torch.distributions.studentT.StudentT attribute) (torch.distributions.transformed_distribution.TransformedDistribution attribute) (torch.distributions.uniform.Uniform property) (torch.distributions.von_mises.VonMises attribute) (torch.distributions.weibull.Weibull attribute) (torch.distributions.wishart.Wishart property) argmax() (in module torch) (torch.Tensor method) argmin() (in module torch) (torch.Tensor method) args (torch.fx.Node property) argsort() (in module torch) (torch.Tensor method) argwhere() (in module torch) (torch.Tensor method) as_integer_ratio() (torch.SymFloat method) (torch.SymInt method) as_nested_tensor() (in module torch.nested) as_sparse_gradcheck() (in module torch.sparse) as_standardized() (torch.utils.benchmark.CallgrindStats method) as_strided() (in module torch) (torch.Tensor method) as_subclass() (torch.Tensor method) as_tensor() (in module torch) as_tuple() (torch.nn.attention.flex_attention.BlockMask method) asarray() (in module torch) asdict() (torch.onnx.verification.VerificationInfo method) ASGD (class in torch.optim) asin() (in module torch) (torch.Tensor method) asin_() (torch.Tensor method) asinh() (in module torch) (torch.Tensor method) asinh_() (torch.Tensor method) assert_allclose() (in module torch.testing) assert_close() (in module torch.testing) assume_constant_result() (in module torch.compiler) async_execution() (in module torch.distributed.rpc.functions) async_save() (in module torch.distributed.checkpoint.state_dict_saver) AsyncCheckpointerType (class in torch.distributed.checkpoint.state_dict_saver) AsyncStager (class in torch.distributed.checkpoint.staging) atan() (in module torch) (torch.Tensor method) atan2() (in module torch) (torch.Tensor method) atan2_() (torch.Tensor method) atan_() (torch.Tensor method) atanh() (in module torch) (torch.Tensor method) atanh_() (torch.Tensor method) aten_decompositions() (in module torch.onnx.ops) atleast_1d() (in module torch) atleast_2d() (in module torch) atleast_3d() (in module torch) attach_out_of_memory_observer() (in", "prev_chunk_id": "chunk_1079", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1081", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "A", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "A", "content": "module torch.mtia) attention() (in module torch.onnx.ops) Attribute (class in torch.jit) autocast (class in torch) (class in torch.cpu.amp) (class in torch.cuda.amp) AveragedModel (class in torch.optim.swa_utils) avg_pool1d() (in module torch.nn.functional) avg_pool2d (class in torch.ao.nn.quantized.functional) avg_pool2d() (in module torch.nn.functional) avg_pool3d (class in torch.ao.nn.quantized.functional) avg_pool3d() (in module torch.nn.functional) AvgPool1d (class in torch.nn) AvgPool2d (class in torch.nn) AvgPool3d (class in torch.nn)", "prev_chunk_id": "chunk_1080", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1082", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "B", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "B", "content": "B Backend (class in torch.distributed) BackendConfig (class in torch.ao.quantization.backend_config) BackendPatternConfig (class in torch.ao.quantization.backend_config) BackendType (class in torch.distributed.rpc) backward() (in module torch.autograd) (in module torch.distributed.autograd) (torch.autograd.Function static method) (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction method) (torch.distributed.rpc.PyRRef method) (torch.Tensor method) backward_extended() (torch.autograd.function.NestedIOFunction method) BackwardCFunction (class in torch.autograd.function) BackwardPrefetch (class in torch.distributed.fsdp) baddbmm() (in module torch) (torch.Tensor method) baddbmm_() (torch.Tensor method) barrier() (in module torch.distributed) bartlett() (in module torch.signal.windows) bartlett_window() (in module torch) base_dist (torch.distributions.half_cauchy.HalfCauchy attribute) (torch.distributions.half_normal.HalfNormal attribute) (torch.distributions.independent.Independent attribute) (torch.distributions.inverse_gamma.InverseGamma attribute) (torch.distributions.log_normal.LogNormal attribute) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute) BasePruningMethod (class in torch.nn.utils.prune) batch_isend_irecv() (in module torch.distributed) batch_norm() (in module torch.nn.functional) batch_shape (torch.distributions.distribution.Distribution property) batch_sizes (torch.nn.utils.rnn.PackedSequence attribute) batched_powerSGD_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook) BatchNorm1d (class in torch.nn) BatchNorm2d (class in torch.ao.nn.quantized) (class in torch.nn) BatchNorm3d (class in torch.ao.nn.quantized) (class in torch.nn) BatchSampler (class in torch.utils.data) BCELoss (class in torch.nn) BCEWithLogitsLoss (class in torch.nn) benchmark (in module torch.backends.cudnn) benchmark_limit (in module torch.backends.cudnn) Bernoulli (class in torch.distributions.bernoulli) bernoulli() (in module torch) (torch.Tensor method) bernoulli_() (torch.Tensor method) bessel_j0() (in module torch.special) bessel_j1() (in module torch.special) bessel_y0() (in module torch.special) bessel_y1() (in module torch.special) Beta (class in torch.distributions.beta) bf16_compress_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks) bf16_compress_wrapper() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks) bfloat16() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) BFloat16Storage (class in torch) Bilinear (class in torch.nn) bilinear() (in module torch.nn.functional) binary_cross_entropy() (in module torch.nn.functional) binary_cross_entropy_with_logits() (in module torch.nn.functional) bincount() (in module torch) (torch.Tensor method) | bind_symbols() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) Binomial (class in torch.distributions.binomial) bitwise_and() (in module torch) (torch.Tensor method) bitwise_and_() (torch.Tensor method) bitwise_left_shift() (in module torch) (torch.Tensor method) bitwise_left_shift_() (torch.Tensor method) bitwise_not() (in module torch) (torch.Tensor method) bitwise_not_() (torch.Tensor method) bitwise_or() (in module torch) (torch.Tensor method) bitwise_or_() (torch.Tensor method) bitwise_right_shift() (in module torch) (torch.Tensor method) bitwise_right_shift_() (torch.Tensor method) bitwise_xor() (in module torch) (torch.Tensor method) bitwise_xor_() (torch.Tensor method) blackman() (in module torch.signal.windows) blackman_window() (in module torch) block_diag() (in module torch) BLOCK_SIZE (torch.nn.attention.flex_attention.BlockMask attribute)", "prev_chunk_id": "chunk_1081", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1083", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "B", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "B", "content": "blocked_autorange() (torch.utils.benchmark.Timer method) BlockingAsyncStager (class in torch.distributed.checkpoint.staging) BlockMask (class in torch.nn.attention.flex_attention) bmm() (in module torch) (torch.Tensor method) BNReLU2d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.quantized) BNReLU3d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.quantized) bool() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) BoolStorage (class in torch) bound_sympy() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) boxed() (torch.distributed.Work method) boxed_run() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) breakpoint() (in module torch.distributed) broadcast() (in module torch.cuda.comm) (in module torch.distributed) broadcast_coalesced() (in module torch.cuda.comm) broadcast_object_list() (in module torch.distributed) broadcast_shapes() (in module torch) broadcast_tensors() (in module torch) broadcast_to() (in module torch) (torch.Tensor method) BroadcastingTorchSaveReader (class in torch.distributed.checkpoint.format_utils) bucketize() (in module torch) Buffer (class in torch.nn.parameter) buffer() (in module torch.distributed.GradBucket) buffers() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) build() (torch.distributed.elastic.rendezvous.api.RendezvousStoreInfo static method) build_stage() (in module torch.distributed.pipelining.stage) BuildExtension() (in module torch.utils.cpp_extension) byte() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) BytesIOWriteData (class in torch.distributed.checkpoint.planner) ByteStorage (class in torch) byteswap() (torch.UntypedStorage method)", "prev_chunk_id": "chunk_1082", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1084", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "C", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "C", "content": "C C10dRendezvousBackend (class in torch.distributed.elastic.rendezvous.c10d_rendezvous_backend) cached() (in module torch.nn.utils.parametrize) caching_allocator_alloc() (in module torch.cuda.memory) caching_allocator_delete() (in module torch.cuda.memory) caching_allocator_enable() (in module torch.cuda.memory) calculate_gain() (in module torch.nn.init) calculate_qparams() (torch.ao.quantization.observer.AffineQuantizedObserverBase method) (torch.ao.quantization.observer.MinMaxObserver method) call_function() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) (torch.fx.Transformer method) call_method() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) call_module() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) (torch.fx.Tracer method) (torch.fx.Transformer method) CallgrindStats (class in torch.utils.benchmark) CallMethodKey (class in torch.fx.experimental.symbolic_shapes) can_cast() (in module torch) can_device_access_peer() (in module torch.cuda) can_use_cudnn_attention() (in module torch.backends.cuda) can_use_efficient_attention() (in module torch.backends.cuda) can_use_flash_attention() (in module torch.backends.cuda) canonicalize_bool_expr() (in module torch.fx.experimental.symbolic_shapes) capture_begin() (torch.cuda.CUDAGraph method) capture_end() (torch.cuda.CUDAGraph method) cartesian_prod() (in module torch) cat (in module torch.distributions.constraints) cat() (in module torch) (torch.ao.ns._numeric_suite.Shadow method) Categorical (class in torch.distributions.categorical) CatTransform (class in torch.distributions.transforms) Cauchy (class in torch.distributions.cauchy) cauchy_() (torch.Tensor method) causal_lower_right() (in module torch.nn.attention.bias) causal_upper_left() (in module torch.nn.attention.bias) CausalBias (class in torch.nn.attention.bias) CausalVariant (class in torch.nn.attention.bias) ccol_indices() (torch.Tensor method) cdf() (torch.distributions.cauchy.Cauchy method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.distribution.Distribution method) (torch.distributions.exponential.Exponential method) (torch.distributions.gamma.Gamma method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.half_cauchy.HalfCauchy method) (torch.distributions.half_normal.HalfNormal method) (torch.distributions.laplace.Laplace method) (torch.distributions.mixture_same_family.MixtureSameFamily method) (torch.distributions.normal.Normal method) (torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.uniform.Uniform method) cdist() (in module torch) cdouble() (torch.Tensor method) ceil() (in module torch) (torch.Tensor method) ceil_() (torch.Tensor method) celu (class in torch.ao.nn.quantized.functional) CELU (class in torch.nn) celu() (in module torch.nn.functional) cfloat() (torch.Tensor method) chain_matmul() (in module torch) ChainDataset (class in torch.utils.data) ChainedScheduler (class in torch.optim.lr_scheduler) chalf() (torch.Tensor method) change_current_allocator() (in module torch.cuda.memory) ChannelShuffle (class in torch.nn) char() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) CharStorage (class in torch) chebyshev_polynomial_t() (in module torch.special) chebyshev_polynomial_u() (in module torch.special) chebyshev_polynomial_v() (in module torch.special) chebyshev_polynomial_w() (in module torch.special) check() (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method) (torch.distributed.Store method) (torch.distributions.constraints.Constraint method) (torch.distributions.constraints.MixtureSameFamilyConstraint method) check_consistent() (in module torch.fx.experimental.symbolic_shapes) check_equal() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) check_export_model_diff (class in torch.onnx.verification) check_is_root() (torch.distributed.fsdp.FullyShardedDataParallel method) check_sparse_tensor_invariants (class in torch.sparse) checkpoint() (in module torch.utils.checkpoint) checkpoint_id (torch.distributed.checkpoint.FileSystemReader property) checkpoint_sequential() (in module torch.utils.checkpoint) CheckpointPolicy (class in torch.utils.checkpoint) Chi2 (class in torch.distributions.chi2) ChildFailedError (class in", "prev_chunk_id": "chunk_1083", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1085", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "C", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "C", "content": "torch.distributed.elastic.multiprocessing.errors) children() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) cholesky() (in module torch) (in module torch.linalg) (torch.Tensor method) cholesky_ex() (in module torch.linalg) cholesky_inverse() (in module torch) (torch.Tensor method) cholesky_solve() (in module torch) (torch.Tensor method) chunk() (in module torch) (torch.Tensor method) CircularPad1d (class in torch.nn) CircularPad2d (class in torch.nn) CircularPad3d (class in torch.nn) clamp (class in torch.ao.nn.quantized.functional) clamp() (in module torch) (torch.Tensor method) clamp_() (torch.Tensor method) cleanup() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) clear() (in module torch.backends.cuda.cufft_plan_cache) (torch.autograd.profiler_util.StringTable method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) clear_safe_globals() (in module torch.serialization) clear_timers() (torch.distributed.elastic.timer.TimerServer method) clip() (in module torch) (torch.Tensor method) clip_() (torch.Tensor method) clip_grad_norm() (in module torch.nn.utils) clip_grad_norm_() (in module torch.nn.utils) (torch.distributed.fsdp.FullyShardedDataParallel method) clip_grad_value_() (in module torch.nn.utils) clip_grads_with_norm_() (in module torch.nn.utils) clock_rate() (in module torch.cuda) clone() (in module torch) (torch.autograd.grad_mode.inference_mode method) (torch.autograd.grad_mode.set_grad_enabled method) (torch.autograd.grad_mode.set_multithreading_enabled method) (torch.distributed.Store method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) clone_state() (torch.Generator method) close (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property) close() (torch.package.PackageExporter method) (torch.utils.tensorboard.writer.SummaryWriter method) cls_to_become (torch.nn.LazyBatchNorm1d attribute) (torch.nn.LazyBatchNorm2d attribute) (torch.nn.LazyBatchNorm3d attribute) (torch.nn.LazyConv1d attribute) (torch.nn.LazyConv2d attribute) (torch.nn.LazyConv3d attribute) (torch.nn.LazyConvTranspose1d attribute) (torch.nn.LazyConvTranspose2d attribute) (torch.nn.LazyConvTranspose3d attribute) (torch.nn.LazyInstanceNorm1d attribute) (torch.nn.LazyInstanceNorm2d attribute) (torch.nn.LazyInstanceNorm3d attribute) (torch.nn.LazyLinear attribute) (torch.nn.parameter.UninitializedParameter attribute) coalesce() (torch.Tensor method) code (torch.fx.GraphModule property) (torch.jit.ScriptModule property) code_with_constants (torch.jit.ScriptModule property) col_indices() (torch.Tensor method) collate() (in module torch.utils.data._utils.collate) collect_all() (in module torch.futures) collect_callgrind() (torch.utils.benchmark.Timer method) colorize() (torch.utils.benchmark.Compare method) column_stack() (in module torch) ColwiseParallel (class in torch.distributed.tensor.parallel) combinations() (in module torch) CommDebugMode (class in torch.distributed.tensor.debug) commit_tensor() (torch.distributed.checkpoint.LoadPlanner method) Compare (class in torch.utils.benchmark) compare_model_outputs() (in module torch.ao.ns._numeric_suite) compare_model_stub() (in module torch.ao.ns._numeric_suite) compare_results (class in torch.ao.quantization) compare_set() (torch.distributed.Store method) compare_weights() (in module torch.ao.ns._numeric_suite) compile() (in module torch) (in module torch.compiler) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) compile_shader() (in module torch.mps) compiled_with_cxx11_abi() (in module torch) complex() (in module torch) complex_double() (torch.TypedStorage method) (torch.UntypedStorage method) complex_float() (torch.TypedStorage method) (torch.UntypedStorage method) ComplexDoubleStorage (class in torch) ComplexFloatStorage (class in torch) component_distribution (torch.distributions.mixture_same_family.MixtureSameFamily property) ComposeTransform (class in torch.distributions.transforms) compute_cosine_similarity() (in module torch.ao.ns.fx.utils) compute_mask() (torch.nn.utils.prune.BasePruningMethod method)", "prev_chunk_id": "chunk_1084", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1086", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "C", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "C", "content": "(torch.nn.utils.prune.LnStructured method) (torch.nn.utils.prune.PruningContainer method) (torch.nn.utils.prune.RandomStructured method) compute_normalized_l2_error() (in module torch.ao.ns.fx.utils) compute_sqnr() (in module torch.ao.ns.fx.utils) compute_unbacked_bindings() (in module torch.fx.experimental.symbolic_shapes) compute_values() (torch.onnx.ONNXProgram method) concat() (in module torch) ConcatDataset (class in torch.utils.data) concatenate() (in module torch) concentration (torch.distributions.inverse_gamma.InverseGamma property) concentration0 (torch.distributions.beta.Beta property) concentration1 (torch.distributions.beta.Beta property) cond() (in module torch) (in module torch._higher_order_ops.cond) (in module torch.linalg) configs (torch.ao.quantization.backend_config.BackendConfig property) configure() (in module torch.distributed.elastic.metrics) (in module torch.distributed.elastic.timer) | confirmed_by_owner() (torch.distributed.rpc.PyRRef method) conj() (in module torch) (torch.Tensor method) conj_physical() (in module torch) (torch.Tensor method) conj_physical_() (torch.Tensor method) conjugate() (torch.SymFloat method) ConsoleMetricHandler (class in torch.distributed.elastic.metrics.api) consolidate_state_dict() (torch.distributed.optim.ZeroRedundancyOptimizer method) constant_() (in module torch.nn.init) ConstantLR (class in torch.optim.lr_scheduler) ConstantPad1d (class in torch.nn) ConstantPad2d (class in torch.nn) ConstantPad3d (class in torch.nn) constants (torch.export.ExportedProgram attribute) constrain_range() (in module torch.fx.experimental.symbolic_shapes) constrain_unify() (in module torch.fx.experimental.symbolic_shapes) Constraint (class in torch.distributions.constraints) ConstraintRegistry (class in torch.distributions.constraint_registry) construct_and_record_rdzv_event() (in module torch.distributed.elastic.events) context (class in torch.distributed.autograd) context_parallel() (in module torch.distributed.tensor.experimental) contiguous() (torch.Tensor method) ContinuousBernoulli (class in torch.distributions.continuous_bernoulli) Conv1d (class in torch.ao.nn.quantized) conv1d (class in torch.ao.nn.quantized.functional) Conv1d (class in torch.nn) conv1d() (in module torch.nn.functional) Conv2d (class in torch.ao.nn.qat) (class in torch.ao.nn.quantized) conv2d (class in torch.ao.nn.quantized.functional) Conv2d (class in torch.nn) conv2d() (in module torch.nn.functional) Conv3d (class in torch.ao.nn.qat) (class in torch.ao.nn.quantized) conv3d (class in torch.ao.nn.quantized.functional) Conv3d (class in torch.nn) conv3d() (in module torch.nn.functional) conv_transpose1d() (in module torch.nn.functional) conv_transpose2d() (in module torch.nn.functional) conv_transpose3d() (in module torch.nn.functional) ConvBn1d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) ConvBn2d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) ConvBn3d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) ConvBnReLU1d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) ConvBnReLU2d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) ConvBnReLU3d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) convert (class in torch.ao.quantization) convert() (torch.ao.quantization.observer.AffineQuantizedObserverBase method) convert_conv2d_weight_memory_format() (in module torch.nn.utils) convert_conv3d_weight_memory_format() (in module torch.nn.utils) convert_fx (class in torch.ao.quantization.quantize_fx) convert_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx) convert_sync_batchnorm() (torch.nn.SyncBatchNorm class method) ConvertCustomConfig (class in torch.ao.quantization.fx.custom_config) ConvertIntKey (class in torch.fx.experimental.symbolic_shapes) ConvReLU1d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.quantized) ConvReLU2d", "prev_chunk_id": "chunk_1085", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1087", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "C", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "C", "content": "(class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) (class in torch.ao.nn.intrinsic.quantized) ConvReLU3d (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) (class in torch.ao.nn.intrinsic.quantized) ConvTranspose1d (class in torch.ao.nn.quantized) (class in torch.nn) ConvTranspose2d (class in torch.ao.nn.quantized) (class in torch.nn) ConvTranspose3d (class in torch.ao.nn.quantized) (class in torch.nn) copy() (torch.autograd.profiler_util.StringTable method) (torch.export.decomp_utils.CustomDecompTable method) (torch.nn.ParameterDict method) copy_() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) copysign() (in module torch) (torch.Tensor method) copysign_() (torch.Tensor method) CorrCholeskyTransform (class in torch.distributions.transforms) corrcoef() (in module torch) (torch.Tensor method) cos() (in module torch) (torch.Tensor method) cos_() (torch.Tensor method) cosh() (in module torch) (torch.Tensor method) cosh_() (torch.Tensor method) cosine() (in module torch.signal.windows) cosine_embedding_loss() (in module torch.nn.functional) cosine_similarity() (in module torch.nn.functional) CosineAnnealingLR (class in torch.optim.lr_scheduler) CosineAnnealingWarmRestarts (class in torch.optim.lr_scheduler) CosineEmbeddingLoss (class in torch.nn) CosineSimilarity (class in torch.nn) count (torch.monitor.Stat property) count() (torch.autograd.forward_ad.UnpackedDualTensor method) (torch.autograd.profiler_util.Kernel method) (torch.jit.Attribute method) (torch.nn.utils.rnn.PackedSequence method) (torch.Size method) count_nonzero() (in module torch) (torch.Tensor method) counts() (torch.utils.benchmark.CallgrindStats method) cov() (in module torch) (torch.Tensor method) covariance_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.wishart.Wishart property) CppExtension() (in module torch.utils.cpp_extension) cpu() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) CPUOffload (class in torch.distributed.fsdp) CPUOffloadPolicy (class in torch.distributed.fsdp) create_arg() (torch.fx.Tracer method) create_args_for_root() (torch.fx.Tracer method) create_backend() (in module torch.distributed.elastic.rendezvous.c10d_rendezvous_backend) (in module torch.distributed.elastic.rendezvous.etcd_rendezvous_backend) create_block_mask() (in module torch.nn.attention.flex_attention) create_global_plan() (torch.distributed.checkpoint.LoadPlanner method) (torch.distributed.checkpoint.SavePlanner method) create_handler() (in module torch.distributed.elastic.rendezvous.dynamic_rendezvous) create_healthcheck_server() (in module torch.distributed.elastic.agent.server.health_check_server) create_local_plan() (torch.distributed.checkpoint.LoadPlanner method) (torch.distributed.checkpoint.SavePlanner method) create_mask() (in module torch.nn.attention.flex_attention) create_nested_block_mask() (in module torch.nn.attention.flex_attention) create_node() (torch.fx.Graph method) (torch.fx.Tracer method) create_proxy() (torch.fx.Tracer method) create_selective_checkpoint_contexts() (in module torch.utils.checkpoint) create_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_symbolic_sizes_strides_storage_offset() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_symboolnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_symfloatnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_symintnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_unbacked_symbool() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_unbacked_symfloat() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_unbacked_symint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_unspecified_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) create_unspecified_symint_and_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) cross() (in module torch) (in module torch.linalg) (torch.Tensor method) cross_entropy() (in module torch.nn.functional) CrossEntropyLoss (class in torch.nn) crow_indices() (torch.Tensor method) ctc_loss() (in module torch.nn.functional) CTCLoss (class in torch.nn) cuda() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method)", "prev_chunk_id": "chunk_1086", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1088", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "C", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "C", "content": "(torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) CUDAExtension() (in module torch.utils.cpp_extension) CUDAGraph (class in torch.cuda) cudagraph_mark_step_begin() (in module torch.compiler) CUDAPluggableAllocator (class in torch.cuda.memory) cudart() (in module torch.cuda) cudnn_sdp_enabled() (in module torch.backends.cuda) cufft_plan_cache (in module torch.backends.cuda) cummax() (in module torch) (torch.Tensor method) cummin() (in module torch) (torch.Tensor method) cumprod() (in module torch) (torch.Tensor method) cumprod_() (torch.Tensor method) cumsum() (in module torch) (torch.Tensor method) cumsum_() (torch.Tensor method) cumulative_trapezoid() (in module torch) CumulativeDistributionTransform (class in torch.distributions.transforms) current_accelerator() (in module torch.accelerator) current_allocated_memory() (in module torch.mps) current_blas_handle() (in module torch.cuda) current_device() (in module torch.cpu) (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) current_device_idx() (in module torch.accelerator) current_device_index() (in module torch.accelerator) current_step() (torch.autograd.profiler.KinetoStepTracker class method) current_stream() (in module torch.accelerator) (in module torch.cpu) (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) custom_bwd() (in module torch.amp) (in module torch.cuda.amp) custom_from_mask() (in module torch.nn.utils.prune) custom_fwd() (in module torch.amp) (in module torch.cuda.amp) CUSTOM_KEY (in module torch.ao.quantization) custom_op() (in module torch.library) CustomDecompTable (class in torch.export.decomp_utils) CustomFromMask (class in torch.nn.utils.prune) CustomObjArgument (class in torch.export.graph_signature) CustomOpDef (class in torch._library.custom_ops) CyclicLR (class in torch.optim.lr_scheduler)", "prev_chunk_id": "chunk_1087", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1089", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "D", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "D", "content": "D data (torch.monitor.Event property) (torch.nn.utils.rnn.PackedSequence attribute) data_parallel() (in module torch.nn.parallel) data_ptr() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) data_value_t (class in torch.monitor) DataLoader (class in torch.utils.data) DataParallel (class in torch.nn) Dataset (class in torch.utils.data) dcp_to_torch_save() (in module torch.distributed.checkpoint.format_utils) debug_dump() (torch.cuda.CUDAGraph method) debug_unwrap() (in module torch.func) default_activation_only_qconfig (in module torch.ao.quantization.qconfig) default_collate() (in module torch.utils.data) default_convert() (in module torch.utils.data) default_debug_observer (in module torch.ao.quantization.observer) default_debug_qconfig (in module torch.ao.quantization.qconfig) default_decompositions() (in module torch.export.exported_program) default_dynamic_qconfig (in module torch.ao.quantization.qconfig) default_dynamic_quant_observer (in module torch.ao.quantization.observer) default_eval_fn (class in torch.ao.quantization) default_factory (torch.autograd.profiler_util.StringTable attribute) default_fake_quant (in module torch.ao.quantization.fake_quantize) default_float_qparams_observer (in module torch.ao.quantization.observer) default_fused_act_fake_quant (in module torch.ao.quantization.fake_quantize) default_fused_per_channel_wt_fake_quant (in module torch.ao.quantization.fake_quantize) default_fused_wt_fake_quant (in module torch.ao.quantization.fake_quantize) default_generator (torch.torch attribute) default_histogram_fake_quant (in module torch.ao.quantization.fake_quantize) default_histogram_observer (in module torch.ao.quantization.observer) default_observer (in module torch.ao.quantization.observer) default_per_channel_qconfig (in module torch.ao.quantization.qconfig) default_per_channel_weight_fake_quant (in module torch.ao.quantization.fake_quantize) default_per_channel_weight_observer (in module torch.ao.quantization.observer) default_placeholder_observer (in module torch.ao.quantization.observer) default_qat_qconfig (in module torch.ao.quantization.qconfig) default_qat_qconfig_v2 (in module torch.ao.quantization.qconfig) default_qconfig (in module torch.ao.quantization.qconfig) default_stream() (in module torch.cuda) (in module torch.mtia) default_weight_fake_quant (in module torch.ao.quantization.fake_quantize) default_weight_observer (in module torch.ao.quantization.observer) default_weight_only_qconfig (in module torch.ao.quantization.qconfig) DefaultLoadPlanner (class in torch.distributed.checkpoint) DefaultLogsSpecs (class in torch.distributed.elastic.multiprocessing.api) DefaultSavePlanner (class in torch.distributed.checkpoint) DeferredMtiaCallError define() (in module torch.library) (torch.library.Library method) deg2rad() (in module torch) (torch.Tensor method) delete_all_unused_submodules() (torch.fx.GraphModule method) delete_key() (torch.distributed.Store method) delete_submodule() (torch.fx.GraphModule method) delta() (torch.utils.benchmark.CallgrindStats method) denied_modules() (torch.package.PackageExporter method) denoise() (torch.utils.benchmark.FunctionCounts method) dense_dim() (torch.Tensor method) deny() (torch.package.PackageExporter method) dependency_graph_string() (torch.package.PackageExporter method) dependent_property (in module torch.distributions.constraints) dequantize() (in module torch) (torch.ao.nn.quantizable.MultiheadAttention method) (torch.Tensor method) DeQuantStub (class in torch.ao.quantization) deregister_handle() (torch.cuda.gds.GdsFile method) deserialize_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) det() (in module torch) (in module torch.linalg) (torch.Tensor method) detach() (torch.Tensor method) detach_() (torch.Tensor method) detect_anomaly (class in torch.autograd) deterministic (in module torch.backends.cudnn) device (class in torch) (class in torch.cuda) (class in torch.mtia) (class in torch.xpu) (torch.autograd.profiler_util.Kernel attribute) (torch.Generator attribute) (torch.Tensor attribute) (torch.TypedStorage property) (torch.UntypedStorage attribute) device_count() (in module torch.accelerator) (in module torch.cpu) (in module torch.cuda) (in module torch.mps) (in module", "prev_chunk_id": "chunk_1088", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1090", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "D", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "D", "content": "torch.mtia) (in module torch.xpu) device_index (class in torch.accelerator) device_maps (torch.distributed.rpc.TensorPipeRpcBackendOptions property) device_memory_used() (in module torch.cuda) device_mesh (torch.distributed.tensor.DTensor property) device_of (class in torch.cuda) (class in torch.xpu) DeviceMesh (class in torch.distributed.device_mesh) devices (torch.distributed.rpc.TensorPipeRpcBackendOptions property) df (torch.distributions.chi2.Chi2 property) diag() (in module torch) (torch.Tensor method) diag_embed() (in module torch) (torch.Tensor method) | diagflat() (in module torch) (torch.Tensor method) diagonal() (in module torch) (in module torch.linalg) (torch.Tensor method) diagonal_scatter() (in module torch) (torch.Tensor method) diff() (in module torch) (torch.Tensor method) digamma() (in module torch) (in module torch.special) (torch.Tensor method) digamma_() (torch.Tensor method) Dim (class in torch.export.dynamic_shapes) dim (torch.distributed.tensor.placement_types.Shard attribute) dim() (torch.Tensor method) dim_order() (torch.Tensor method) DimConstraints (class in torch.fx.experimental.symbolic_shapes) DimDynamic (class in torch.fx.experimental.symbolic_shapes) dirac_() (in module torch.nn.init) Directory (class in torch.package) Dirichlet (class in torch.distributions.dirichlet) disable() (in module torch.compiler) (torch.sparse.check_sparse_tensor_invariants static method) disable_fake_quant (class in torch.ao.quantization.fake_quantize) disable_observer (class in torch.ao.quantization.fake_quantize) disable_saved_tensors_hooks (class in torch.autograd.graph) dist() (in module torch) (torch.Tensor method) DistBackendError (class in torch.distributed) DistError (class in torch.distributed) DistNetworkError (class in torch.distributed) distribute_module() (in module torch.distributed.tensor) distribute_tensor() (in module torch.distributed.tensor) DistributedDataParallel (class in torch.nn.parallel) DistributedOptimizer (class in torch.distributed.optim) DistributedSampler (class in torch.utils.data.distributed) Distribution (class in torch.distributions.distribution) DistStoreError (class in torch.distributed) div() (in module torch) (torch.Tensor method) div_() (torch.Tensor method) divide() (in module torch) (torch.Tensor method) divide_() (torch.Tensor method) DivideByKey (class in torch.fx.experimental.symbolic_shapes) done() (torch.futures.Future method) dot() (in module torch) (torch.Tensor method) double() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) DoubleStorage (class in torch) download_url_to_file() (in module torch.hub) draft_export() (in module torch.export) draw() (torch.quasirandom.SobolEngine method) draw_base2() (torch.quasirandom.SobolEngine method) driver_allocated_memory() (in module torch.mps) Dropout (class in torch.nn) dropout() (in module torch.nn.functional) Dropout1d (class in torch.nn) dropout1d() (in module torch.nn.functional) Dropout2d (class in torch.nn) dropout2d() (in module torch.nn.functional) Dropout3d (class in torch.nn) dropout3d() (in module torch.nn.functional) dsplit() (in module torch) (torch.Tensor method) dstack() (in module torch) DTensor (class in torch.distributed.tensor) dtype (class in", "prev_chunk_id": "chunk_1089", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1091", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "D", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "D", "content": "torch) (torch.BFloat16Storage attribute) (torch.BoolStorage attribute) (torch.ByteStorage attribute) (torch.CharStorage attribute) (torch.ComplexDoubleStorage attribute) (torch.ComplexFloatStorage attribute) (torch.DoubleStorage attribute) (torch.FloatStorage attribute) (torch.HalfStorage attribute) (torch.IntStorage attribute) (torch.LongStorage attribute) (torch.QInt32Storage attribute) (torch.QInt8Storage attribute) (torch.QUInt2x4Storage attribute) (torch.QUInt4x2Storage attribute) (torch.QUInt8Storage attribute) (torch.ShortStorage attribute) (torch.TypedStorage attribute) dtype() (torch.onnx.JitScalarType method) DTypeConfig (class in torch.ao.quantization.backend_config) DTypeWithConstraints (class in torch.ao.quantization.backend_config) dual_level (class in torch.autograd.forward_ad) duration (torch.autograd.profiler_util.Kernel attribute) dynamic_shapes() (torch.export.dynamic_shapes.AdditionalInputs method) (torch.export.dynamic_shapes.ShapesCollection method) DynamicMetaLoadPlanner (class in torch.distributed.checkpoint.format_utils) DynamicRendezvousHandler (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous) dynamo_export() (in module torch.onnx)", "prev_chunk_id": "chunk_1090", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1092", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "E", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "E", "content": "E eig() (in module torch.linalg) eigh() (in module torch.linalg) eigvals() (in module torch.linalg) eigvalsh() (in module torch.linalg) einsum() (in module torch) elapsed_time() (torch.cuda.Event method) (torch.Event method) (torch.mps.event.Event method) (torch.mtia.Event method) (torch.xpu.Event method) elapsed_us() (torch.autograd.profiler_util.Interval method) ElasticAgent (class in torch.distributed.elastic.agent.server) element_size() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) eliminate_dead_code() (torch.fx.Graph method) ELU (class in torch.ao.nn.quantized) elu (class in torch.ao.nn.quantized.functional) ELU (class in torch.nn) elu() (in module torch.nn.functional) elu_() (in module torch.nn.functional) Embedding (class in torch.ao.nn.quantized) (class in torch.nn) embedding() (in module torch.nn.functional) embedding_bag() (in module torch.nn.functional) EmbeddingBag (class in torch.ao.nn.quantized) (class in torch.nn) emit_itt (class in torch.autograd.profiler) emit_nvtx (class in torch.autograd.profiler) empty() (in module torch) (in module torch.distributed.tensor) empty_cache() (in module torch.cuda.memory) (in module torch.mps) (in module torch.mtia) (in module torch.xpu.memory) empty_like() (in module torch) empty_strided() (in module torch) EmptyMatchError (class in torch.package) enable() (in module torch.cuda.tunable) (torch.sparse.check_sparse_tensor_invariants static method) enable_cuda_sanitizer() (in module torch.cuda._sanitizer) enable_cudnn_sdp() (in module torch.backends.cuda) enable_debug_mode() (torch.cuda.CUDAGraph method) enable_fake_mode() (in module torch.onnx) enable_fake_quant (class in torch.ao.quantization.fake_quantize) enable_flash_sdp() (in module torch.backends.cuda) enable_grad (class in torch) enable_math_sdp() (in module torch.backends.cuda) enable_mem_efficient_sdp() (in module torch.backends.cuda) enable_observer (class in torch.ao.quantization.fake_quantize) enable_onednn_fusion() (in module torch.jit) enabled (in module torch.backends.cudnn) (in module torch.backends.opt_einsum) EnforceUnique (class in torch.autograd.profiler) enter_dual_level() (in module torch.autograd.forward_ad) entr() (in module torch.special) entropy() (torch.distributions.bernoulli.Bernoulli method) (torch.distributions.beta.Beta method) (torch.distributions.binomial.Binomial method) (torch.distributions.categorical.Categorical method) (torch.distributions.cauchy.Cauchy method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.dirichlet.Dirichlet method) (torch.distributions.distribution.Distribution method) (torch.distributions.exp_family.ExponentialFamily method) (torch.distributions.exponential.Exponential method) (torch.distributions.gamma.Gamma method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.geometric.Geometric method) (torch.distributions.gumbel.Gumbel method) (torch.distributions.half_cauchy.HalfCauchy method) (torch.distributions.half_normal.HalfNormal method) (torch.distributions.independent.Independent method) (torch.distributions.inverse_gamma.InverseGamma method) (torch.distributions.kumaraswamy.Kumaraswamy method) (torch.distributions.laplace.Laplace method) (torch.distributions.log_normal.LogNormal method) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method) (torch.distributions.multinomial.Multinomial method) (torch.distributions.multivariate_normal.MultivariateNormal method) (torch.distributions.normal.Normal method) (torch.distributions.one_hot_categorical.OneHotCategorical method) (torch.distributions.pareto.Pareto method) (torch.distributions.studentT.StudentT method) (torch.distributions.uniform.Uniform method) (torch.distributions.weibull.Weibull method) (torch.distributions.wishart.Wishart method) enumerate_support() (torch.distributions.bernoulli.Bernoulli method) (torch.distributions.binomial.Binomial method) (torch.distributions.categorical.Categorical method) (torch.distributions.distribution.Distribution method) (torch.distributions.independent.Independent method) (torch.distributions.one_hot_categorical.OneHotCategorical method) environment variable PYTORCH_JIT TORCH_COMPILE_JOB_ID eq() (in module torch) (torch.Tensor method) eq_() (torch.Tensor method) equal() (in module torch) (torch.Tensor method) EqualityConstraint (class in torch.fx.experimental.symbolic_shapes) erase_node()", "prev_chunk_id": "chunk_1091", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1093", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "E", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "E", "content": "(torch.fx.Graph method) erase_step_count() (torch.autograd.profiler.KinetoStepTracker class method) erf() (in module torch) (in module torch.special) (torch.Tensor method) erf_() (torch.Tensor method) erfc() (in module torch) (in module torch.special) (torch.Tensor method) erfc_() (torch.Tensor method) erfcx() (in module torch.special) erfinv() (in module torch) (in module torch.special) (torch.Tensor method) erfinv_() (torch.Tensor method) ErrorHandler (class in torch.distributed.elastic.multiprocessing.errors) EtcdRendezvousBackend (class in torch.distributed.elastic.rendezvous.etcd_rendezvous_backend) | EtcdRendezvousHandler (class in torch.distributed.elastic.rendezvous.etcd_rendezvous) EtcdServer (class in torch.distributed.elastic.rendezvous.etcd_server) EtcdStore (class in torch.distributed.elastic.rendezvous.etcd_store) eval() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) evaluate_expr() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) evaluate_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) evaluate_guards_for_args() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) evaluate_sym_node() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) evaluate_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) Event (class in torch) (class in torch.cuda) (class in torch.distributed.elastic.events.api) (class in torch.monitor) (class in torch.mps.event) (class in torch.mtia) (class in torch.xpu) event_shape (torch.distributions.distribution.Distribution property) EventHandlerHandle (class in torch.monitor) EventMetadataValue (in module torch.distributed.elastic.events.api) events() (torch.profiler._KinetoProfile method) EventSource (class in torch.distributed.elastic.events.api) example_inputs (torch.export.ExportedProgram attribute) exception() (torch.distributed.Work method) exit_dual_level() (in module torch.autograd.forward_ad) exp() (in module torch) (torch.Tensor method) exp2() (in module torch) (in module torch.special) exp_() (torch.Tensor method) expand() (torch.distributions.bernoulli.Bernoulli method) (torch.distributions.beta.Beta method) (torch.distributions.binomial.Binomial method) (torch.distributions.categorical.Categorical method) (torch.distributions.cauchy.Cauchy method) (torch.distributions.chi2.Chi2 method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.dirichlet.Dirichlet method) (torch.distributions.distribution.Distribution method) (torch.distributions.exponential.Exponential method) (torch.distributions.fishersnedecor.FisherSnedecor method) (torch.distributions.gamma.Gamma method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.geometric.Geometric method) (torch.distributions.gumbel.Gumbel method) (torch.distributions.half_cauchy.HalfCauchy method) (torch.distributions.half_normal.HalfNormal method) (torch.distributions.independent.Independent method) (torch.distributions.inverse_gamma.InverseGamma method) (torch.distributions.kumaraswamy.Kumaraswamy method) (torch.distributions.laplace.Laplace method) (torch.distributions.lkj_cholesky.LKJCholesky method) (torch.distributions.log_normal.LogNormal method) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method) (torch.distributions.mixture_same_family.MixtureSameFamily method) (torch.distributions.multinomial.Multinomial method) (torch.distributions.multivariate_normal.MultivariateNormal method) (torch.distributions.negative_binomial.NegativeBinomial method) (torch.distributions.normal.Normal method) (torch.distributions.one_hot_categorical.OneHotCategorical method) (torch.distributions.pareto.Pareto method) (torch.distributions.poisson.Poisson method) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli method) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical method) (torch.distributions.studentT.StudentT method) (torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.uniform.Uniform method) (torch.distributions.von_mises.VonMises method) (torch.distributions.weibull.Weibull method) (torch.distributions.wishart.Wishart method) (torch.Tensor method) expand_as() (torch.Tensor method) expires() (in module torch.distributed.elastic.timer) expit() (in module torch.special) expm1() (in module torch) (in module torch.special) (torch.Tensor method) expm1_() (torch.Tensor method) Exponential (class in torch.distributions.exponential) exponential() (in module torch.signal.windows) exponential_() (torch.Tensor method) ExponentialFamily (class in torch.distributions.exp_family) ExponentialLR (class in torch.optim.lr_scheduler) export() (in module torch.export) (in module torch.jit) (in module torch.onnx) export_chrome_trace() (torch.autograd.profiler.profile method) (torch.profiler._KinetoProfile method) export_memory_timeline() (torch.profiler._KinetoProfile method)", "prev_chunk_id": "chunk_1092", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1094", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "E", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "E", "content": "export_stacks() (torch.profiler._KinetoProfile method) ExportBackwardSignature (class in torch.export.graph_signature) ExportedProgram (class in torch.export) ExportGraphSignature (class in torch.export) (class in torch.export.graph_signature) ExportOptions (class in torch.onnx) ExpTransform (class in torch.distributions.transforms) extend() (torch.nn.ModuleList method) (torch.nn.ParameterList method) (torch.nn.Sequential method) extend_logger_results_with_comparison() (in module torch.ao.ns._numeric_suite_fx) extend_results() (torch.utils.benchmark.Compare method) extern() (torch.package.PackageExporter method) ExternalStream (class in torch.cuda) externed_modules() (torch.package.PackageExporter method) extra_repr() (torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.nn.modules.normalization.RMSNorm method) (torch.nn.RMSNorm method) (torch.optim.swa_utils.AveragedModel method) extract_logger_info() (in module torch.ao.ns._numeric_suite_fx) extract_results_from_loggers (class in torch.ao.quantization) extract_results_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx) extract_shadow_logger_info() (in module torch.ao.ns._numeric_suite_fx) extract_weights() (in module torch.ao.ns._numeric_suite_fx) eye() (in module torch) eye_() (in module torch.nn.init)", "prev_chunk_id": "chunk_1093", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1095", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "F", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "F", "content": "F fake_quantize_per_channel_affine() (in module torch) fake_quantize_per_tensor_affine() (in module torch) FakeQuantize (class in torch.ao.quantization.fake_quantize) FakeQuantizeBase (class in torch.ao.quantization.fake_quantize) fallback() (torch.library.Library method) fallthrough_kernel() (in module torch.library) fast_forward() (torch.quasirandom.SobolEngine method) feature_alpha_dropout() (in module torch.nn.functional) FeatureAlphaDropout (class in torch.nn) fetch_args_kwargs_from_env() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) fetch_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) fft() (in module torch.fft) fft2() (in module torch.fft) fftfreq() (in module torch.fft) fftn() (in module torch.fft) fftshift() (in module torch.fft) file_structure() (torch.package.PackageImporter method) filename (torch.TypedStorage property) (torch.UntypedStorage property) FileStore (class in torch.distributed) FileSystemReader (class in torch.distributed.checkpoint) FileSystemWriter (class in torch.distributed.checkpoint) FileTimerClient (class in torch.distributed.elastic.timer) FileTimerServer (class in torch.distributed.elastic.timer) fill_() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) fill_diagonal_() (torch.Tensor method) fill_uninitialized_memory (in module torch.utils.deterministic) filter() (torch.utils.benchmark.FunctionCounts method) find_mismatch() (in module torch.onnx.verification) find_nodes() (torch.fx.Graph method) finish() (torch.distributed.checkpoint.StorageWriter method) finish_plan() (torch.distributed.checkpoint.LoadPlanner method) (torch.distributed.checkpoint.SavePlanner method) FisherSnedecor (class in torch.distributions.fishersnedecor) fix() (in module torch) (torch.Tensor method) fix_() (torch.Tensor method) FixedQParamsFakeQuantize (class in torch.ao.quantization.fake_quantize) flags() (in module torch.backends.nnpack) flash_sdp_enabled() (in module torch.backends.cuda) FlatArgsAdapter (class in torch.export.unflatten) Flatten (class in torch.nn) flatten() (in module torch) (torch.Tensor method) flatten_parameters() (torch.nn.RNNBase method) flatten_sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) flex_attention() (in module torch.nn.attention.flex_attention) flip() (in module torch) (torch.Tensor method) fliplr() (in module torch) (torch.Tensor method) flipud() (in module torch) (torch.Tensor method) float() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) float16_dynamic_qconfig (in module torch.ao.quantization.qconfig) float16_static_qconfig (in module torch.ao.quantization.qconfig) float8_e4m3fn() (torch.TypedStorage method) (torch.UntypedStorage method) float8_e4m3fnuz() (torch.TypedStorage method) (torch.UntypedStorage method) float8_e5m2() (torch.TypedStorage method) (torch.UntypedStorage method) float8_e5m2fnuz() (torch.TypedStorage method) (torch.UntypedStorage method) float_power() (in module torch) (torch.Tensor method) float_power_() (torch.Tensor method) float_qparams_weight_only_qconfig (in module torch.ao.quantization.qconfig) FloatFunctional (class in torch.ao.nn.quantized) FloatStorage (class in torch) floor() (in module torch) (torch.Tensor method) floor_() (torch.Tensor method) floor_divide() (in module torch) (torch.Tensor method) floor_divide_() (torch.Tensor method) flush() (torch.utils.tensorboard.writer.SummaryWriter method) fmax() (in module torch) (torch.Tensor method) fmin() (in module torch) (torch.Tensor method) fmod() (in module torch) (torch.Tensor method) fmod_() (torch.Tensor method) Fold (class in", "prev_chunk_id": "chunk_1094", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1096", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "F", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "F", "content": "torch.nn) fold() (in module torch.nn.functional) forced_specializations() (torch.fx.experimental.symbolic_shapes.DimConstraints method) fork() (in module torch.jit) fork_rng() (in module torch.random) format_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) format_node() (torch.fx.Node method) forward() (torch.ao.nn.quantizable.MultiheadAttention method) (torch.ao.ns._numeric_suite.Logger method) (torch.ao.ns._numeric_suite.OutputLogger method) (torch.ao.ns._numeric_suite.Shadow method) (torch.ao.ns._numeric_suite.ShadowLogger method) (torch.ao.ns._numeric_suite_fx.OutputComparisonLogger method) (torch.ao.ns._numeric_suite_fx.OutputLogger method) (torch.ao.quantization.observer.AffineQuantizedObserverBase method) (torch.ao.quantization.observer.MinMaxObserver method) (torch.autograd.Function static method) (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction method) (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.nn.ConvTranspose2d method) (torch.nn.EmbeddingBag method) (torch.nn.Module method) (torch.nn.modules.normalization.RMSNorm method) (torch.nn.MultiheadAttention method) (torch.nn.RMSNorm method) (torch.nn.Transformer method) (torch.nn.TransformerDecoder method) (torch.nn.TransformerDecoderLayer method) (torch.nn.TransformerEncoder method) (torch.nn.TransformerEncoderLayer method) (torch.optim.swa_utils.AveragedModel method) | forward_extended() (torch.autograd.function.NestedIOFunction method) forward_shape() (torch.distributions.transforms.Transform method) fp16_bf16_reduction_math_sdp_allowed() (in module torch.backends.cuda) fp16_compress_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks) fp16_compress_wrapper() (in module torch.distributed.algorithms.ddp_comm_hooks.default_hooks) frac() (in module torch) (torch.Tensor method) frac_() (torch.Tensor method) fractional_max_pool2d() (in module torch.nn.functional) fractional_max_pool3d() (in module torch.nn.functional) FractionalMaxPool2d (class in torch.nn) FractionalMaxPool3d (class in torch.nn) freeze() (in module torch.jit) (torch.fx.experimental.symbolic_shapes.ShapeEnv method) freeze_bn_stats (class in torch.ao.nn.intrinsic.qat) freeze_runtime_asserts() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) frexp() (in module torch) (torch.Tensor method) from_backend() (torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler class method) from_buffer() (torch.TypedStorage class method) (torch.UntypedStorage static method) from_dict() (torch.ao.quantization.backend_config.BackendConfig class method) (torch.ao.quantization.backend_config.BackendPatternConfig class method) (torch.ao.quantization.backend_config.DTypeConfig class method) (torch.ao.quantization.fx.custom_config.ConvertCustomConfig class method) (torch.ao.quantization.fx.custom_config.FuseCustomConfig class method) (torch.ao.quantization.fx.custom_config.PrepareCustomConfig class method) (torch.ao.quantization.qconfig_mapping.QConfigMapping class method) from_dlpack() (in module torch) (in module torch.utils.dlpack) from_dtype() (torch.onnx.JitScalarType class method) from_file() (in module torch) (torch.TypedStorage class method) (torch.UntypedStorage static method) from_float() (torch.ao.nn.qat.Linear class method) (torch.ao.nn.quantized.Conv1d class method) (torch.ao.nn.quantized.Conv2d class method) (torch.ao.nn.quantized.Conv3d class method) (torch.ao.nn.quantized.dynamic.Linear class method) (torch.ao.nn.quantized.Embedding class method) (torch.ao.nn.quantized.EmbeddingBag class method) (torch.ao.nn.quantized.Linear class method) from_group() (torch.distributed.device_mesh.DeviceMesh static method) from_ipc_handle() (torch.cuda.Event class method) from_kv_blocks() (torch.nn.attention.flex_attention.BlockMask class method) from_local() (torch.distributed.tensor.DTensor static method) from_numpy() (in module torch) from_onnx_type() (torch.onnx.JitScalarType class method) from_pretrained() (torch.nn.Embedding class method) (torch.nn.EmbeddingBag class method) from_reference() (torch.ao.nn.quantized.dynamic.Linear class method) (torch.ao.nn.quantized.Linear class method) from_tensors() (torch.onnx.verification.VerificationInfo class method) from_value() (torch.onnx.JitScalarType class method) frombuffer() (in module torch) fromkeys() (torch.autograd.profiler_util.StringTable method) (torch.nn.ParameterDict method) fsdp_modules() (torch.distributed.fsdp.FullyShardedDataParallel static method) FSDPModule (class in torch.distributed.fsdp) full() (in module torch) (in module torch.distributed.tensor) full_kv_indices (torch.nn.attention.flex_attention.BlockMask attribute) full_kv_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute) full_like() (in module torch) full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static", "prev_chunk_id": "chunk_1095", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1097", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "F", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "F", "content": "method) full_q_indices (torch.nn.attention.flex_attention.BlockMask attribute) full_q_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute) full_tensor() (torch.distributed.tensor.DTensor method) FullOptimStateDictConfig (class in torch.distributed.fsdp) FullStateDictConfig (class in torch.distributed.fsdp) fully_shard() (in module torch.distributed.fsdp) FullyShardedDataParallel (class in torch.distributed.fsdp) Function (class in torch.autograd) functional_call() (in module torch.func) (in module torch.nn.utils.stateless) functionalize() (in module torch.func) FunctionCounts (class in torch.utils.benchmark) fuse_conv_bn_eval() (in module torch.nn.utils) fuse_conv_bn_weights() (in module torch.nn.utils) fuse_fx (class in torch.ao.quantization.quantize_fx) fuse_linear_bn_eval() (in module torch.nn.utils) fuse_linear_bn_weights() (in module torch.nn.utils) fuse_modules (class in torch.ao.quantization.fuse_modules) FuseCustomConfig (class in torch.ao.quantization.fx.custom_config) FusedMovingAvgObsFakeQuantize (class in torch.ao.quantization.fake_quantize) Future (class in torch.futures) FXFloatFunctional (class in torch.ao.nn.quantized)", "prev_chunk_id": "chunk_1096", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1098", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "G", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "G", "content": "G Gamma (class in torch.distributions.gamma) gammainc() (in module torch.special) gammaincc() (in module torch.special) gammaln() (in module torch.special) gather() (in module torch) (in module torch.cuda.comm) (in module torch.distributed) (torch.Tensor method) gather_object() (in module torch.distributed) gaussian() (in module torch.signal.windows) gaussian_nll_loss() (in module torch.nn.functional) GaussianNLLLoss (class in torch.nn) gcd() (in module torch) (torch.Tensor method) gcd_() (torch.Tensor method) gds_deregister_buffer() (in module torch.cuda.gds) gds_register_buffer() (in module torch.cuda.gds) GdsFile (class in torch.cuda.gds) ge() (in module torch) (torch.Tensor method) ge_() (torch.Tensor method) GELU (class in torch.nn) gelu() (in module torch.nn.functional) general_cosine() (in module torch.signal.windows) general_hamming() (in module torch.signal.windows) GeneralizedPareto (class in torch.distributions.generalized_pareto) generate_comm_debug_tracing_table() (torch.distributed.tensor.debug.CommDebugMode method) generate_json_dump() (torch.distributed.tensor.debug.CommDebugMode method) generate_methods_for_privateuse1_backend() (in module torch.utils) generate_numeric_debug_handle (class in torch.ao.quantization) generate_square_subsequent_mask() (torch.nn.Transformer static method) Generator (class in torch) Geometric (class in torch.distributions.geometric) geometric_() (torch.Tensor method) geqrf() (in module torch) (torch.Tensor method) ger() (in module torch) (torch.Tensor method) get() (torch.autograd.profiler_util.StringTable method) (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method) (torch.distributed.elastic.rendezvous.RendezvousParameters method) (torch.distributed.Store method) (torch.fx.experimental.symbolic_shapes.CallMethodKey method) (torch.fx.experimental.symbolic_shapes.ConvertIntKey method) (torch.fx.experimental.symbolic_shapes.DivideByKey method) (torch.fx.experimental.symbolic_shapes.InnerTensorKey method) (torch.monitor.Stat method) (torch.nn.ParameterDict method) get_all_groups() (torch.distributed.device_mesh.DeviceMesh method) get_all_sharing_strategies() (in module torch.multiprocessing) get_allocator_backend() (in module torch.cuda.memory) get_arch_list() (in module torch.cuda) (in module torch.xpu) get_as_bool() (torch.distributed.elastic.rendezvous.RendezvousParameters method) get_as_int() (torch.distributed.elastic.rendezvous.RendezvousParameters method) get_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) (torch.fx.Transformer method) get_axioms() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) get_backend() (in module torch.distributed) (torch.distributed.elastic.rendezvous.RendezvousHandler method) get_block_size (class in torch.ao.quantization.observer) get_buffer() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) get_comm_counts() (torch.distributed.tensor.debug.CommDebugMode method) get_compiler_abi_compatibility_and_version() (in module torch.utils.cpp_extension) get_coordinate() (torch.distributed.device_mesh.DeviceMesh method) get_cpp_backtrace() (in module torch.utils) get_cpu_capability() (in module torch.backends.cpu) get_crc32_options() (in module torch.serialization) get_ctx() (in module torch.library) get_debug_state() (torch.jit.ScriptFunction method) get_default_backend_for_device() (in module torch.distributed) get_default_device() (in module torch) get_default_dtype() (in module torch) get_default_load_endianness() (in module torch.serialization) get_default_mmap_options() (in module torch.serialization) get_default_qat_qconfig_mapping (class in torch.ao.quantization.qconfig_mapping) get_default_qconfig_mapping (class in torch.ao.quantization.qconfig_mapping) get_deterministic_debug_mode() (in module torch) get_device() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) get_device_capability() (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) get_device_module() (in module torch) get_device_name() (in module torch.cuda) (in module torch.xpu) get_device_properties()", "prev_chunk_id": "chunk_1097", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1099", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "G", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "G", "content": "(in module torch.cuda) (in module torch.xpu) get_dir() (in module torch.hub) get_ema_multi_avg_fn() (in module torch.optim.swa_utils) get_entrypoint_name() (torch.distributed.elastic.agent.server.WorkerSpec method) get_expired_timers() (torch.distributed.elastic.timer.TimerServer method) get_extra_state() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) get_fastpath_enabled() (in module torch.backends.mha) get_filename() (in module torch.cuda.tunable) get_float32_matmul_precision() (in module torch) get_fresh_qualname() (torch.fx.Tracer method) get_future() (torch.distributed.Work method) get_future_result() (torch.distributed.Work method) get_gencode_flags() (in module torch.cuda) (in module torch.xpu) get_global_rank() (in module torch.distributed) get_gradient_edge() (in module torch.autograd.graph) get_gradients() (in module torch.distributed.autograd) get_group() (torch.distributed.device_mesh.DeviceMesh method) get_group_rank() (in module torch.distributed) get_ignored_functions() (in module torch.overrides) get_implications() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) get_last_lr() (torch.optim.lr_scheduler.ChainedScheduler method) (torch.optim.lr_scheduler.ConstantLR method) (torch.optim.lr_scheduler.CosineAnnealingLR method) (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method) (torch.optim.lr_scheduler.CyclicLR method) (torch.optim.lr_scheduler.ExponentialLR method) (torch.optim.lr_scheduler.LambdaLR method) (torch.optim.lr_scheduler.LinearLR method) (torch.optim.lr_scheduler.LRScheduler method) (torch.optim.lr_scheduler.MultiplicativeLR method) (torch.optim.lr_scheduler.MultiStepLR method) (torch.optim.lr_scheduler.OneCycleLR method) (torch.optim.lr_scheduler.PolynomialLR method) (torch.optim.lr_scheduler.ReduceLROnPlateau method) (torch.optim.lr_scheduler.SequentialLR method) (torch.optim.lr_scheduler.StepLR method) (torch.optim.swa_utils.SWALR method) get_local_rank() (torch.distributed.device_mesh.DeviceMesh method) get_logger_dict() (in module torch.ao.ns._numeric_suite) get_logging_handler() (in module torch.distributed.elastic.events) get_lr() (torch.optim.lr_scheduler.ChainedScheduler method) (torch.optim.lr_scheduler.ConstantLR method) (torch.optim.lr_scheduler.CosineAnnealingLR method) (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method) (torch.optim.lr_scheduler.CyclicLR method) (torch.optim.lr_scheduler.ExponentialLR method) (torch.optim.lr_scheduler.LambdaLR method) (torch.optim.lr_scheduler.LinearLR method) (torch.optim.lr_scheduler.LRScheduler method) (torch.optim.lr_scheduler.MultiplicativeLR method) (torch.optim.lr_scheduler.MultiStepLR method) (torch.optim.lr_scheduler.OneCycleLR method) (torch.optim.lr_scheduler.PolynomialLR method) (torch.optim.lr_scheduler.ReduceLROnPlateau method) (torch.optim.lr_scheduler.SequentialLR method) (torch.optim.lr_scheduler.StepLR method) (torch.optim.swa_utils.SWALR method) | get_matching_activations() (in module torch.ao.ns._numeric_suite) get_max_tuning_duration() (in module torch.cuda.tunable) get_max_tuning_iterations() (in module torch.cuda.tunable) get_model_state_dict() (in module torch.distributed.checkpoint.state_dict) get_module_rref() (torch.distributed.nn.api.remote_module.RemoteModule method) get_nontrivial_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) get_num_interop_threads() (in module torch) get_num_threads() (in module torch) get_observer_state_dict (class in torch.ao.quantization.observer) get_opt_einsum() (in module torch.backends.opt_einsum) get_optimizer_state_dict() (in module torch.distributed.checkpoint.state_dict) get_overridable_functions() (in module torch.overrides) get_overwrite_module_params_on_conversion() (in module torch.__future__) get_parameter() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) get_parameter_info() (torch.distributed.tensor.debug.CommDebugMode method) get_per_process_memory_fraction() (in module torch.cuda.memory) get_process_group_ranks() (in module torch.distributed) get_proxy_mode() (in module torch.fx.experimental.proxy_tensor) get_pruned_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) get_rank() (in module torch.distributed) (torch.distributed.device_mesh.DeviceMesh method) get_rdeps() (torch.package.PackageExporter method) get_replace_hook() (torch.export.graph_signature.ExportGraphSignature method) get_results() (in module torch.cuda.tunable) get_rng_state() (in module torch) (in module torch.cuda) (in module torch.mps) (in module torch.mtia) (in module torch.random) (in module torch.xpu) get_rng_state_all() (in module torch.cuda) (in module torch.xpu) get_rotating_buffer_size() (in module torch.cuda.tunable) get_run_id() (torch.distributed.elastic.rendezvous.RendezvousHandler method) get_safe_globals() (in module torch.serialization) get_sharding_info() (torch.distributed.tensor.debug.CommDebugMode method) get_sharing_strategy() (in module torch.multiprocessing) get_state() (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "prev_chunk_id": "chunk_1098", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1100", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "G", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "G", "content": "method) (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend method) (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend method) (torch.Generator method) get_state_dict() (in module torch.distributed.checkpoint.state_dict) get_state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method) get_stream_from_external() (in module torch.cuda) (in module torch.xpu) get_submodule() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) get_subprocess_handler() (in module torch.distributed.elastic.multiprocessing.subprocess_handler.handlers) get_swap_module_params_on_conversion() (in module torch.__future__) get_sync_debug_mode() (in module torch.cuda) get_testing_overrides() (in module torch.overrides) get_total_counts() (torch.distributed.tensor.debug.CommDebugMode method) get_total_norm() (in module torch.nn.utils) get_trace_id() (torch.profiler.profile method) get_unique_id() (torch.package.PackageExporter method) get_unsafe_globals_in_checkpoint() (in module torch.serialization) get_validators() (in module torch.cuda.tunable) get_worker_group() (torch.distributed.elastic.agent.server.ElasticAgent method) get_worker_info() (in module torch.distributed.rpc) (in module torch.utils.data) get_world_size() (in module torch.distributed) getattr() (torch.fx.Tracer method) global_unstructured() (in module torch.nn.utils.prune) GLU (class in torch.nn) glu() (in module torch.nn.functional) grad (torch.Tensor attribute) grad() (in module torch.autograd) (in module torch.func) grad_and_value() (in module torch.func) GradBucket (class in torch.distributed) gradcheck() (in module torch.autograd.gradcheck) GradcheckError gradgradcheck() (in module torch.autograd.gradcheck) gradient() (in module torch) GradientEdge (class in torch.autograd.graph) gradients() (in module torch.distributed.GradBucket) GradScaler (class in torch.cpu.amp) (class in torch.cuda.amp) Granularity (class in torch.ao.quantization.observer) graph (class in torch.cuda) Graph (class in torch.fx) graph (torch.export.ExportedProgram attribute) (torch.fx.GraphModule property) (torch.jit.ScriptModule property) graph_copy() (torch.fx.Graph method) graph_pool_handle() (in module torch.cuda) graph_signature (torch.export.ExportedProgram attribute) GraphInfo (class in torch.onnx.verification) GraphInfoPrettyPrinter (class in torch.onnx.verification) GraphModule (class in torch.fx) graphsafe_get_state() (torch.Generator method) graphsafe_set_state() (torch.Generator method) greater() (in module torch) (torch.Tensor method) greater_() (torch.Tensor method) greater_equal() (in module torch) (torch.Tensor method) greater_equal_() (torch.Tensor method) greater_than (in module torch.distributions.constraints) greater_than_eq (in module torch.distributions.constraints) grid_sample() (in module torch.nn.functional) group_norm() (in module torch.nn.functional) GroupNorm (class in torch.ao.nn.quantized) (class in torch.nn) GRU (class in torch.ao.nn.quantized.dynamic) (class in torch.nn) GRUCell (class in torch.ao.nn.quantized.dynamic) (class in torch.nn) gt() (in module torch) (torch.Tensor method) gt_() (torch.Tensor method) guard_or_defer_runtime_assert() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) guard_or_false() (in module torch.fx.experimental.symbolic_shapes) guard_or_true() (in module torch.fx.experimental.symbolic_shapes) guard_size_oblivious() (in module torch.fx.experimental.symbolic_shapes) Gumbel (class in torch.distributions.gumbel) gumbel_softmax() (in module torch.nn.functional)", "prev_chunk_id": "chunk_1099", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1101", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "H", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "H", "content": "H H (torch.Tensor attribute) half() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) half_open_interval (in module torch.distributions.constraints) HalfCauchy (class in torch.distributions.half_cauchy) HalfNormal (class in torch.distributions.half_normal) HalfStorage (class in torch) hamming() (in module torch.signal.windows) hamming_window() (in module torch) handle_sym_dispatch() (in module torch.fx.experimental.proxy_tensor) handle_torch_function() (in module torch.overrides) hann() (in module torch.signal.windows) hann_window() (in module torch) Hardshrink (class in torch.nn) hardshrink() (in module torch.nn.functional) (torch.Tensor method) hardsigmoid (class in torch.ao.nn.quantized.functional) Hardsigmoid (class in torch.nn) hardsigmoid() (in module torch.nn.functional) Hardswish (class in torch.ao.nn.quantized) hardswish (class in torch.ao.nn.quantized.functional) Hardswish (class in torch.nn) hardswish() (in module torch.nn.functional) hardtanh (class in torch.ao.nn.quantized.functional) Hardtanh (class in torch.nn) hardtanh() (in module torch.nn.functional) hardtanh_() (in module torch.nn.functional) has_enumerate_support (torch.distributions.bernoulli.Bernoulli attribute) (torch.distributions.binomial.Binomial attribute) (torch.distributions.categorical.Categorical attribute) (torch.distributions.independent.Independent property) (torch.distributions.one_hot_categorical.OneHotCategorical attribute) has_extended_api() (torch.distributed.Store method) has_file() (torch.package.Directory method) has_free_symbols() (in module torch.fx.experimental.symbolic_shapes) has_free_unbacked_symbols() (in module torch.fx.experimental.symbolic_shapes) has_rsample (torch.distributions.beta.Beta attribute) (torch.distributions.cauchy.Cauchy attribute) (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute) (torch.distributions.dirichlet.Dirichlet attribute) (torch.distributions.exponential.Exponential attribute) (torch.distributions.fishersnedecor.FisherSnedecor attribute) (torch.distributions.gamma.Gamma attribute) (torch.distributions.generalized_pareto.GeneralizedPareto attribute) (torch.distributions.half_cauchy.HalfCauchy attribute) (torch.distributions.half_normal.HalfNormal attribute) (torch.distributions.independent.Independent property) (torch.distributions.inverse_gamma.InverseGamma attribute) (torch.distributions.kumaraswamy.Kumaraswamy attribute) (torch.distributions.laplace.Laplace attribute) (torch.distributions.log_normal.LogNormal attribute) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute) (torch.distributions.mixture_same_family.MixtureSameFamily attribute) (torch.distributions.multivariate_normal.MultivariateNormal attribute) (torch.distributions.normal.Normal attribute) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute) (torch.distributions.studentT.StudentT attribute) (torch.distributions.transformed_distribution.TransformedDistribution property) (torch.distributions.uniform.Uniform attribute) (torch.distributions.von_mises.VonMises attribute) (torch.distributions.wishart.Wishart attribute) | has_static_value() (in module torch.fx.experimental.symbolic_shapes) has_torch_function() (in module torch.overrides) has_uninitialized_params() (torch.nn.modules.lazy.LazyModuleMixin method) HashStore (class in torch.distributed) HealthCheckServer (class in torch.distributed.elastic.agent.server.health_check_server) heartbeat (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property) heaviside() (in module torch) (torch.Tensor method) help() (in module torch.hub) hermite_polynomial_h() (in module torch.special) hermite_polynomial_he() (in module torch.special) hessian() (in module torch.autograd.functional) (in module torch.func) hex() (torch.SymFloat method) hfft() (in module torch.fft) hfft2() (in module torch.fft) hfftn() (in module torch.fft) highlight_warnings() (torch.utils.benchmark.Compare method) hinge_embedding_loss() (in module torch.nn.functional) HingeEmbeddingLoss (class in torch.nn) hint_int() (in module torch.fx.experimental.symbolic_shapes) histc() (in module torch) (torch.Tensor method) histogram() (in module torch) (torch.Tensor method) histogramdd() (in module torch) HistogramObserver (class in torch.ao.quantization.observer) host (torch.distributed.TCPStore property) host_memory_stats() (in module torch.cuda.memory) host_memory_stats_as_nested_dict() (in module torch.cuda.memory) householder_product() (in", "prev_chunk_id": "chunk_1100", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1102", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "H", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "H", "content": "module torch.linalg) hpu() (torch.TypedStorage method) (torch.UntypedStorage method) hsplit() (in module torch) (torch.Tensor method) hspmm() (in module torch) hstack() (in module torch) huber_loss() (in module torch.nn.functional) HuberLoss (class in torch.nn) hvp() (in module torch.autograd.functional) hypot() (in module torch) (torch.Tensor method) hypot_() (torch.Tensor method)", "prev_chunk_id": "chunk_1101", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1103", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "I", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "I", "content": "I i0() (in module torch) (in module torch.special) (torch.Tensor method) i0_() (torch.Tensor method) i0e() (in module torch.special) i1() (in module torch.special) i1e() (in module torch.special) icdf() (torch.distributions.cauchy.Cauchy method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.distribution.Distribution method) (torch.distributions.exponential.Exponential method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.half_cauchy.HalfCauchy method) (torch.distributions.half_normal.HalfNormal method) (torch.distributions.laplace.Laplace method) (torch.distributions.normal.Normal method) (torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.uniform.Uniform method) id (torch.cuda.memory.MemPool property) (torch.distributed.rpc.WorkerInfo property) id() (torch.package.PackageImporter method) Identity (class in torch.nn) (class in torch.nn.utils.prune) identity() (in module torch.nn.utils.prune) ifft() (in module torch.fft) ifft2() (in module torch.fft) ifftn() (in module torch.fft) ifftshift() (in module torch.fft) igamma() (in module torch) (torch.Tensor method) igamma_() (torch.Tensor method) igammac() (in module torch) (torch.Tensor method) igammac_() (torch.Tensor method) ignore() (in module torch.jit) ignore_fresh_unbacked_symbols() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) ihfft() (in module torch.fft) ihfft2() (in module torch.fft) ihfftn() (in module torch.fft) imag (torch.Tensor attribute) imag() (in module torch) impl() (in module torch.library) (torch.library.Library method) impl_abstract() (in module torch.library) import_module() (torch.package.PackageImporter method) in_interval() (torch.autograd.profiler_util.MemRecordsAcc method) include_paths() (in module torch.utils.cpp_extension) increment_step() (torch.autograd.profiler.KinetoStepTracker class method) increment_version() (in module torch.autograd.graph) Independent (class in torch.distributions.independent) independent (in module torch.distributions.constraints) IndependentTransform (class in torch.distributions.transforms) index() (in module torch.distributed.GradBucket) (torch.autograd.forward_ad.UnpackedDualTensor method) (torch.autograd.profiler_util.Kernel method) (torch.jit.Attribute method) (torch.nn.utils.rnn.PackedSequence method) (torch.Size method) index_add() (in module torch) (torch.Tensor method) index_add_() (torch.Tensor method) index_copy() (in module torch) (torch.Tensor method) index_copy_() (torch.Tensor method) index_fill() (torch.Tensor method) index_fill_() (torch.Tensor method) index_put() (torch.Tensor method) index_put_() (torch.Tensor method) index_reduce() (in module torch) (torch.Tensor method) index_reduce_() (torch.Tensor method) index_select() (in module torch) (torch.Tensor method) indices() (torch.Tensor method) infer_schema() (in module torch.library) inference_mode (class in torch.autograd.grad_mode) init() (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) init_device_mesh() (in module torch.distributed.device_mesh) init_method (torch.distributed.rpc.RpcBackendOptions property) (torch.distributed.rpc.TensorPipeRpcBackendOptions property) init_process_group() (in module torch.distributed) init_rpc() (in module torch.distributed.rpc) init_step_count() (torch.autograd.profiler.KinetoStepTracker class method) initial_seed() (in module torch) (in module torch.cuda) (in module torch.random) (in module torch.xpu) (torch.Generator method) initialize_inference_session() (torch.onnx.ONNXProgram method) initialize_parameters() (torch.nn.modules.lazy.LazyModuleMixin method) inlined_graph (torch.jit.ScriptModule property) inner() (in module torch) (torch.Tensor method) InnerTensorKey (class", "prev_chunk_id": "chunk_1102", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1104", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "I", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "I", "content": "in torch.fx.experimental.symbolic_shapes) InplaceFunction (class in torch.autograd.function) INPUT_OUTPUT_NOT_OBSERVED (torch.ao.quantization.backend_config.ObservationType attribute) InputKind (class in torch.export.graph_signature) InputSpec (class in torch.export.graph_signature) insert() (torch.nn.ModuleList method) (torch.nn.Sequential method) insert_arg() (torch.fx.Node method) inserting_after() (torch.fx.Graph method) inserting_before() (torch.fx.Graph method) instance_norm() (in module torch.nn.functional) InstanceNorm1d (class in torch.ao.nn.quantized) (class in torch.nn) InstanceNorm2d (class in torch.ao.nn.quantized) (class in torch.nn) InstanceNorm3d (class in torch.ao.nn.quantized) (class in torch.nn) instantiate() (torch.cuda.CUDAGraph method) int() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) int_repr() (torch.Tensor method) integer_interval (in module torch.distributions.constraints) interface() (in module torch.jit) intern() (torch.package.PackageExporter method) interned_modules() (torch.package.PackageExporter method) interpolate (class in torch.ao.nn.quantized.functional) interpolate() (in module torch.nn.functional) Interpreter (class in torch.fx) InterpreterModule (class in torch.export.unflatten) InterpreterModuleDispatcher (class in torch.export.unflatten) Interval (class in torch.autograd.profiler_util) interval (in module torch.distributions.constraints) IntStorage (class in torch) inv (torch.distributions.transforms.Transform property) inv() (in module torch.linalg) inv_ex() (in module torch.linalg) inverse() (in module torch) (torch.Tensor method) inverse_shape() (torch.distributions.transforms.Transform method) InverseGamma (class in torch.distributions.inverse_gamma) ipc_collect() (in module torch.cuda) ipc_handle() (torch.cuda.Event method) | ipu() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) irecv() (in module torch.distributed) irfft() (in module torch.fft) irfft2() (in module torch.fft) irfftn() (in module torch.fft) is_accessor_node() (in module torch.fx.experimental.symbolic_shapes) is_autocast_available() (in module torch.amp.autocast_mode) is_available() (in module torch.accelerator) (in module torch.backends.cudnn) (in module torch.backends.cusparselt) (in module torch.backends.mkl) (in module torch.backends.mkldnn) (in module torch.backends.mps) (in module torch.backends.nnpack) (in module torch.backends.openmp) (in module torch.backends.opt_einsum) (in module torch.cpu) (in module torch.cuda) (in module torch.distributed) (in module torch.mtia) (in module torch.profiler.itt) (in module torch.xpu) is_built() (in module torch.backends.cuda) (in module torch.backends.mps) is_capturing_metal() (in module torch.mps.profiler) is_closed() (torch.distributed.elastic.rendezvous.RendezvousHandler method) is_coalesced() (torch.Tensor method) is_compiling() (in module torch.compiler) is_completed() (torch.distributed.Work method) is_complex() (in module torch) (torch.Tensor method) is_concrete_bool() (in module torch.fx.experimental.symbolic_shapes) is_concrete_float() (in module torch.fx.experimental.symbolic_shapes) is_concrete_int() (in module torch.fx.experimental.symbolic_shapes) is_conj() (in module torch) (torch.Tensor method) is_contiguous() (torch.Tensor method) is_cuda (torch.nn.utils.rnn.PackedSequence property) (torch.Tensor attribute) (torch.TypedStorage property) (torch.UntypedStorage property) is_current_stream_capturing() (in module torch.cuda) is_dependent() (in module torch.distributions.constraints) is_deterministic_algorithms_warn_only_enabled() (in module torch) is_dynamo_compiling() (in module torch.compiler)", "prev_chunk_id": "chunk_1103", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1105", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "I", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "I", "content": "is_enabled() (in module torch.cuda.tunable) (torch.sparse.check_sparse_tensor_invariants static method) is_exporting() (in module torch.compiler) is_flash_attention_available() (in module torch.backends.cuda) is_floating_point() (in module torch) (torch.Tensor method) is_gloo_available() (in module torch.distributed) is_grad_enabled() (in module torch) is_hpu (torch.TypedStorage property) (torch.UntypedStorage property) is_impure() (torch.fx.Node method) is_in_onnx_export() (in module torch.onnx) is_inference() (torch.Tensor method) is_inference_mode_enabled() (in module torch) is_initialized() (in module torch.cuda) (in module torch.distributed) (in module torch.mtia) (in module torch.xpu) is_integer() (torch.SymFloat method) is_last() (in module torch.distributed.GradBucket) is_leaf (torch.Tensor attribute) is_leaf_module() (torch.ao.ns._numeric_suite_fx.NSTracer method) (torch.fx.Tracer method) is_meta (torch.Tensor attribute) is_metal_capture_enabled() (in module torch.mps.profiler) is_mpi_available() (in module torch.distributed) is_nccl_available() (in module torch.distributed) is_ninja_available() (in module torch.utils.cpp_extension) is_nonzero() (in module torch) is_onnxrt_backend_supported() (in module torch.onnx) is_owner() (torch.distributed.rpc.PyRRef method) is_parametrized() (in module torch.nn.utils.parametrize) is_partial() (torch.distributed.tensor.placement_types.Placement method) is_pinned() (torch.nn.utils.rnn.PackedSequence method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) is_pruned() (in module torch.nn.utils.prune) is_quantized (torch.Tensor attribute) is_replicate() (torch.distributed.tensor.placement_types.Placement method) is_running() (torch.distributed.elastic.agent.server.WorkerState static method) is_scripting() (in module torch.jit) is_set_to() (torch.Tensor method) is_shard() (torch.distributed.tensor.placement_types.Placement method) is_shared() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) is_signed() (torch.Tensor method) is_sparse (torch.Tensor attribute) (torch.TypedStorage attribute) (torch.UntypedStorage attribute) is_sparse_csr (torch.Tensor attribute) (torch.UntypedStorage attribute) is_storage() (in module torch) is_success() (torch.distributed.Work method) is_symmetric (torch.cuda.memory.MemPool property) is_tensor() (in module torch) is_tensor_like() (in module torch.overrides) is_tensor_method_or_property() (in module torch.overrides) is_tf32_supported() (in module torch.cuda) is_torchelastic_launched() (in module torch.distributed) is_tracing() (in module torch.jit) is_unbacked_symint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) is_warn_always_enabled() (in module torch) is_xccl_available() (in module torch.distributed.distributed_c10d) isclose() (in module torch) (torch.Tensor method) isend() (in module torch.distributed) isfinite() (in module torch) (torch.Tensor method) isin() (in module torch) isinf() (in module torch) (torch.Tensor method) isinstance() (in module torch.jit) isnan() (in module torch) (torch.Tensor method) isneginf() (in module torch) (torch.Tensor method) isposinf() (in module torch) (torch.Tensor method) isreal() (in module torch) (torch.Tensor method) istft() (in module torch) (torch.Tensor method) item() (torch.Tensor method) items() (torch.autograd.profiler_util.StringTable method) (torch.export.decomp_utils.CustomDecompTable method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) itemsize (torch.Tensor attribute) iter() (torch.fx.Tracer method) IterableDataset (class in torch.utils.data)", "prev_chunk_id": "chunk_1104", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1106", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "J", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "J", "content": "J jacfwd() (in module torch.func) jacobian() (in module torch.autograd.functional) jacrev() (in module torch.func) JitScalarType (class in torch.onnx) job_id (in module torch.compiler.config) Join (class in torch.distributed.algorithms) join (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property) join() (torch.multiprocessing.SpawnContext method) (torch.nn.parallel.DistributedDataParallel method) join_device (torch.distributed.algorithms.Joinable property) (torch.distributed.optim.ZeroRedundancyOptimizer property) | join_hook() (torch.distributed.algorithms.Joinable method) (torch.distributed.optim.ZeroRedundancyOptimizer method) (torch.nn.parallel.DistributedDataParallel method) join_process_group (torch.distributed.algorithms.Joinable property) (torch.distributed.optim.ZeroRedundancyOptimizer property) Joinable (class in torch.distributed.algorithms) JoinHook (class in torch.distributed.algorithms) jvp() (in module torch.autograd.functional) (in module torch.func) (torch.autograd.Function static method) (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction static method)", "prev_chunk_id": "chunk_1105", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1107", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "K", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "K", "content": "K kaiming_normal_() (in module torch.nn.init) kaiming_uniform_() (in module torch.nn.init) kaiser() (in module torch.signal.windows) kaiser_window() (in module torch) keep_tensor_guards_unsafe() (in module torch.compiler) Kernel (class in torch.autograd.profiler_util) key_averages() (torch.autograd.profiler.profile method) (torch.profiler._KinetoProfile method) keys() (torch.autograd.profiler_util.StringTable method) (torch.export.decomp_utils.CustomDecompTable method) (torch.fx.Tracer method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) | KinetoStepTracker (class in torch.autograd.profiler) kl_div() (in module torch.nn.functional) kl_divergence() (in module torch.distributions.kl) KLDivLoss (class in torch.nn) kron() (in module torch) kthvalue() (in module torch) (torch.Tensor method) Kumaraswamy (class in torch.distributions.kumaraswamy) kv_indices (torch.nn.attention.flex_attention.BlockMask attribute) kv_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute) kwargs (torch.fx.Node property)", "prev_chunk_id": "chunk_1106", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1108", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "L", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "L", "content": "L l1_loss() (in module torch.nn.functional) l1_unstructured() (in module torch.nn.utils.prune) L1Loss (class in torch.nn) L1Unstructured (class in torch.nn.utils.prune) laguerre_polynomial_l() (in module torch.special) LambdaLR (class in torch.optim.lr_scheduler) Laplace (class in torch.distributions.laplace) last_call (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout property) layer_norm() (in module torch.nn.functional) LayerNorm (class in torch.ao.nn.quantized) (class in torch.nn) layout (class in torch) LazyBatchNorm1d (class in torch.nn) LazyBatchNorm2d (class in torch.nn) LazyBatchNorm3d (class in torch.nn) LazyConv1d (class in torch.nn) LazyConv2d (class in torch.nn) LazyConv3d (class in torch.nn) LazyConvTranspose1d (class in torch.nn) LazyConvTranspose2d (class in torch.nn) LazyConvTranspose3d (class in torch.nn) LazyInstanceNorm1d (class in torch.nn) LazyInstanceNorm2d (class in torch.nn) LazyInstanceNorm3d (class in torch.nn) LazyLinear (class in torch.nn) LazyModuleMixin (class in torch.nn.modules.lazy) LBFGS (class in torch.optim) lcm() (in module torch) (torch.Tensor method) lcm_() (torch.Tensor method) ldexp() (in module torch) (torch.Tensor method) ldexp_() (torch.Tensor method) ldl_factor() (in module torch.linalg) ldl_factor_ex() (in module torch.linalg) ldl_solve() (in module torch.linalg) le() (in module torch) (torch.Tensor method) le_() (torch.Tensor method) leaky_relu (class in torch.ao.nn.quantized.functional) leaky_relu() (in module torch.nn.functional) leaky_relu_() (in module torch.nn.functional) LeakyReLU (class in torch.ao.nn.quantized) (class in torch.nn) legendre_polynomial_p() (in module torch.special) lerp() (in module torch) (torch.Tensor method) lerp_() (torch.Tensor method) less() (in module torch) (torch.Tensor method) less_() (torch.Tensor method) less_equal() (in module torch) (torch.Tensor method) less_equal_() (torch.Tensor method) less_than (in module torch.distributions.constraints) lgamma() (in module torch) (torch.Tensor method) lgamma_() (torch.Tensor method) Library (class in torch.library) libuvBackend (torch.distributed.TCPStore property) Linear (class in torch.ao.nn.qat) (class in torch.ao.nn.qat.dynamic) (class in torch.ao.nn.quantized) (class in torch.ao.nn.quantized.dynamic) linear (class in torch.ao.nn.quantized.functional) Linear (class in torch.nn) linear() (in module torch.nn.functional) linearize() (in module torch.func) LinearLR (class in torch.optim.lr_scheduler) LinearReLU (class in torch.ao.nn.intrinsic) (class in torch.ao.nn.intrinsic.qat) (class in torch.ao.nn.intrinsic.quantized) (class in torch.ao.nn.intrinsic.quantized.dynamic) linspace() (in module torch) lint() (torch.fx.Graph method) list() (in module torch.hub) list_backends() (in module torch.compiler) list_gpu_processes() (in module torch.cuda.memory) LKJCholesky (class in torch.distributions.lkj_cholesky) ln_structured() (in module torch.nn.utils.prune) LnStructured (class in torch.nn.utils.prune) load() (in module torch) (in module torch.distributed.checkpoint.state_dict_loader)", "prev_chunk_id": "chunk_1107", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1109", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "L", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "L", "content": "(in module torch.export) (in module torch.hub) (in module torch.jit) (in module torch.utils.cpp_extension) load_binary() (torch.package.PackageImporter method) load_bytes() (torch.distributed.checkpoint.LoadPlanner method) load_inline() (in module torch.utils.cpp_extension) load_nvprof() (in module torch.autograd.profiler) load_observer_state_dict (class in torch.ao.quantization.observer) load_pickle() (torch.package.PackageImporter method) load_state_dict() (in module torch.distributed.checkpoint.state_dict_loader) (torch.distributed.checkpoint.stateful.Stateful method) (torch.distributed.optim.PostLocalSGDOptimizer method) (torch.distributed.optim.ZeroRedundancyOptimizer method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.lr_scheduler.ChainedScheduler method) (torch.optim.lr_scheduler.ConstantLR method) (torch.optim.lr_scheduler.CosineAnnealingLR method) (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method) (torch.optim.lr_scheduler.CyclicLR method) (torch.optim.lr_scheduler.ExponentialLR method) (torch.optim.lr_scheduler.LambdaLR method) (torch.optim.lr_scheduler.LinearLR method) (torch.optim.lr_scheduler.LRScheduler method) (torch.optim.lr_scheduler.MultiplicativeLR method) (torch.optim.lr_scheduler.MultiStepLR method) (torch.optim.lr_scheduler.OneCycleLR method) (torch.optim.lr_scheduler.PolynomialLR method) (torch.optim.lr_scheduler.ReduceLROnPlateau method) (torch.optim.lr_scheduler.SequentialLR method) (torch.optim.lr_scheduler.StepLR method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) (torch.optim.swa_utils.SWALR method) load_state_dict_from_url() (in module torch.hub) load_storage() (torch.cuda.gds.GdsFile method) load_text() (torch.package.PackageImporter method) load_url() (in module torch.utils.model_zoo) LoadPlan (class in torch.distributed.checkpoint) LoadPlanner (class in torch.distributed.checkpoint) lobpcg() (in module torch) loc (torch.distributions.log_normal.LogNormal property) local_map() (in module torch.distributed.tensor.experimental) local_response_norm() (in module torch.nn.functional) local_value() (torch.distributed.rpc.PyRRef method) LocalElasticAgent (class in torch.distributed.elastic.agent.server.local_elastic_agent) LocalOptimStateDictConfig (class in torch.distributed.fsdp) LocalResponseNorm (class in torch.nn) | LocalStateDictConfig (class in torch.distributed.fsdp) LocalTimerClient (class in torch.distributed.elastic.timer) LocalTimerServer (class in torch.distributed.elastic.timer) log() (in module torch) (torch.Tensor method) log10() (in module torch) (torch.Tensor method) log10_() (torch.Tensor method) log1p() (in module torch) (in module torch.special) (torch.Tensor method) log1p_() (torch.Tensor method) log2() (in module torch) (torch.Tensor method) log2_() (torch.Tensor method) log_() (torch.Tensor method) log_abs_det_jacobian() (torch.distributions.transforms.Transform method) log_cdf() (torch.distributions.generalized_pareto.GeneralizedPareto method) log_comm_debug_tracing_table_to_file() (torch.distributed.tensor.debug.CommDebugMode method) log_debug_info_for_expired_timers() (in module torch.distributed.elastic.timer.debug_info_logging) log_event() (in module torch.monitor) log_ndtr() (in module torch.special) log_normal_() (torch.Tensor method) log_prob() (torch.distributions.bernoulli.Bernoulli method) (torch.distributions.beta.Beta method) (torch.distributions.binomial.Binomial method) (torch.distributions.categorical.Categorical method) (torch.distributions.cauchy.Cauchy method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.dirichlet.Dirichlet method) (torch.distributions.distribution.Distribution method) (torch.distributions.exponential.Exponential method) (torch.distributions.fishersnedecor.FisherSnedecor method) (torch.distributions.gamma.Gamma method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.geometric.Geometric method) (torch.distributions.gumbel.Gumbel method) (torch.distributions.half_cauchy.HalfCauchy method) (torch.distributions.half_normal.HalfNormal method) (torch.distributions.independent.Independent method) (torch.distributions.laplace.Laplace method) (torch.distributions.lkj_cholesky.LKJCholesky method) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method) (torch.distributions.mixture_same_family.MixtureSameFamily method) (torch.distributions.multinomial.Multinomial method) (torch.distributions.multivariate_normal.MultivariateNormal method) (torch.distributions.negative_binomial.NegativeBinomial method) (torch.distributions.normal.Normal method) (torch.distributions.one_hot_categorical.OneHotCategorical method) (torch.distributions.poisson.Poisson method) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method) (torch.distributions.studentT.StudentT method)", "prev_chunk_id": "chunk_1108", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1110", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "L", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "L", "content": "(torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.uniform.Uniform method) (torch.distributions.von_mises.VonMises method) (torch.distributions.wishart.Wishart method) (torch.nn.AdaptiveLogSoftmaxWithLoss method) log_softmax() (in module torch.nn.functional) (in module torch.sparse) (in module torch.special) log_survival_function() (torch.distributions.generalized_pareto.GeneralizedPareto method) logaddexp() (in module torch) (torch.Tensor method) logaddexp2() (in module torch) (torch.Tensor method) logcumsumexp() (in module torch) (torch.Tensor method) logdet() (in module torch) (torch.Tensor method) Logger (class in torch.ao.ns._numeric_suite) loggers_set_enabled() (in module torch.ao.ns._numeric_suite_fx) loggers_set_save_activations() (in module torch.ao.ns._numeric_suite_fx) logical_and() (in module torch) (torch.Tensor method) logical_and_() (torch.Tensor method) logical_not() (in module torch) (torch.Tensor method) logical_not_() (torch.Tensor method) logical_or() (in module torch) (torch.Tensor method) logical_or_() (torch.Tensor method) logical_xor() (in module torch) (torch.Tensor method) logical_xor_() (torch.Tensor method) logit() (in module torch) (in module torch.special) (torch.Tensor method) logit_() (torch.Tensor method) LogitRelaxedBernoulli (class in torch.distributions.relaxed_bernoulli) logits (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.geometric.Geometric property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property) LogNormal (class in torch.distributions.log_normal) LogsDest (class in torch.distributed.elastic.multiprocessing.api) LogSigmoid (class in torch.nn) logsigmoid() (in module torch.nn.functional) LogSoftmax (class in torch.nn) logspace() (in module torch) LogsSpecs (class in torch.distributed.elastic.multiprocessing.api) logsumexp() (in module torch) (in module torch.special) (torch.Tensor method) long() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) LongStorage (class in torch) lookup_object() (torch.distributed.checkpoint.DefaultSavePlanner method) lookup_tensor() (torch.distributed.checkpoint.DefaultLoadPlanner method) loss_parallel() (in module torch.distributed.tensor.parallel) lower_pt2e_quantized_to_x86 (class in torch.ao.quantization.pt2e.lowering) LowerCholeskyTransform (class in torch.distributions.transforms) LowRankMultivariateNormal (class in torch.distributions.lowrank_multivariate_normal) lp_pool1d() (in module torch.nn.functional) lp_pool2d() (in module torch.nn.functional) lp_pool3d() (in module torch.nn.functional) LPPool1d (class in torch.nn) LPPool2d (class in torch.nn) LPPool3d (class in torch.nn) LRScheduler (class in torch.optim.lr_scheduler) lru_cache() (in module torch.fx.experimental.symbolic_shapes) LSTM (class in torch.ao.nn.quantizable) (class in torch.ao.nn.quantized.dynamic) (class in torch.nn) LSTMCell (class in torch.ao.nn.quantized.dynamic) (class in torch.nn) lstsq() (in module torch.linalg) lt() (in module torch) (torch.Tensor method) lt_() (torch.Tensor method) lu() (in module torch) (in module torch.linalg) (torch.Tensor method) lu_factor() (in module torch.linalg) lu_factor_ex() (in module torch.linalg) lu_solve() (in module torch) (in module torch.linalg) (torch.Tensor method) lu_unpack() (in module torch)", "prev_chunk_id": "chunk_1109", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1111", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "M main_hook() (torch.distributed.algorithms.JoinHook method) make_dual() (in module torch.autograd.forward_ad) make_fx() (in module torch.fx.experimental.proxy_tensor) make_graphed_callables() (in module torch.cuda) make_tensor() (in module torch.testing) manual_seed() (in module torch) (in module torch.cuda) (in module torch.mps) (in module torch.random) (in module torch.xpu) (torch.Generator method) manual_seed_all() (in module torch.cuda) (in module torch.xpu) map_() (torch.Tensor method) map_nodes_to_values() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) MappingType (class in torch.ao.quantization.observer) margin_ranking_loss() (in module torch.nn.functional) MarginRankingLoss (class in torch.nn) mark() (in module torch.cuda.nvtx) (in module torch.profiler.itt) mark_dirty() (torch.autograd.function.BackwardCFunction method) (torch.autograd.function.FunctionCtx method) (torch.autograd.function.InplaceFunction method) (torch.autograd.function.NestedIOFunction method) mark_non_differentiable() (torch.autograd.function.BackwardCFunction method) (torch.autograd.function.FunctionCtx method) (torch.autograd.function.InplaceFunction method) (torch.autograd.function.NestedIOFunction method) mask_mod (torch.nn.attention.flex_attention.BlockMask attribute) masked_fill() (torch.Tensor method) masked_fill_() (torch.Tensor method) masked_scatter() (torch.Tensor method) masked_scatter_() (torch.Tensor method) masked_select() (in module torch) (in module torch.nested) (torch.Tensor method) materialize() (torch.export.decomp_utils.CustomDecompTable method) math_sdp_enabled() (in module torch.backends.cuda) matmul() (in module torch) (in module torch.linalg) (torch.Tensor method) matrix_exp() (in module torch) (in module torch.linalg) (torch.Tensor method) matrix_norm() (in module torch.linalg) matrix_power() (in module torch) (in module torch.linalg) (torch.Tensor method) matrix_rank() (in module torch.linalg) max() (in module torch) (torch.Tensor method) max_memory_allocated() (in module torch.cuda.memory) (in module torch.xpu.memory) max_memory_cached() (in module torch.cuda.memory) max_memory_reserved() (in module torch.cuda.memory) (in module torch.xpu.memory) max_pool1d (class in torch.ao.nn.quantized.functional) max_pool1d() (in module torch.nn.functional) max_pool2d (class in torch.ao.nn.quantized.functional) max_pool2d() (in module torch.nn.functional) max_pool3d() (in module torch.nn.functional) max_size (in module torch.backends.cuda.cufft_plan_cache) max_unpool1d() (in module torch.nn.functional) max_unpool2d() (in module torch.nn.functional) max_unpool3d() (in module torch.nn.functional) maximum() (in module torch) (torch.Tensor method) MaxPool1d (class in torch.nn) MaxPool2d (class in torch.nn) MaxPool3d (class in torch.nn) MaxUnpool1d (class in torch.nn) MaxUnpool2d (class in torch.nn) MaxUnpool3d (class in torch.nn) maybe_disable_thunkify() (in module torch.fx.experimental.proxy_tensor) maybe_enable_thunkify() (in module torch.fx.experimental.proxy_tensor) mean (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.beta.Beta property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.cauchy.Cauchy property) (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.dirichlet.Dirichlet property) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential property) (torch.distributions.fishersnedecor.FisherSnedecor property) (torch.distributions.gamma.Gamma property) (torch.distributions.generalized_pareto.GeneralizedPareto property) (torch.distributions.geometric.Geometric property) (torch.distributions.gumbel.Gumbel property) (torch.distributions.half_cauchy.HalfCauchy property) (torch.distributions.half_normal.HalfNormal property) (torch.distributions.independent.Independent property) (torch.distributions.inverse_gamma.InverseGamma property) (torch.distributions.kumaraswamy.Kumaraswamy property) (torch.distributions.laplace.Laplace property) (torch.distributions.log_normal.LogNormal property) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.mixture_same_family.MixtureSameFamily", "prev_chunk_id": "chunk_1110", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1112", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.normal.Normal property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.pareto.Pareto property) (torch.distributions.poisson.Poisson property) (torch.distributions.studentT.StudentT property) (torch.distributions.uniform.Uniform property) (torch.distributions.von_mises.VonMises property) (torch.distributions.weibull.Weibull property) (torch.distributions.wishart.Wishart property) mean() (in module torch) (torch.Tensor method) Measurement (class in torch.utils.benchmark) median() (in module torch) (torch.Tensor method) mem_efficient_sdp_enabled() (in module torch.backends.cuda) mem_get_info() (in module torch.cuda.memory) (in module torch.xpu.memory) memory_allocated() (in module torch.cuda.memory) (in module torch.xpu.memory) memory_cached() (in module torch.cuda.memory) memory_format (class in torch) memory_reserved() (in module torch.cuda.memory) (in module torch.xpu.memory) memory_snapshot() (in module torch.cuda.memory) memory_stats() (in module torch.cuda.memory) (in module torch.mtia) (in module torch.mtia.memory) (in module torch.xpu.memory) memory_stats_as_nested_dict() (in module torch.cuda.memory) (in module torch.xpu.memory) memory_summary() (in module torch.cuda.memory) memory_usage() (in module torch.cuda) MemPool (class in torch.cuda.memory) MemRecordsAcc (class in torch.autograd.profiler_util) merge() (torch.utils.benchmark.Measurement static method) merge_chunks() (in module torch.distributed.pipelining.microbatch) merge_masks() (torch.nn.MultiheadAttention method) meshgrid() (in module torch) metadata() (torch.autograd.graph.Node method) metal_capture() (in module torch.mps.profiler) MetricHandler (class in torch.distributed.elastic.metrics.api) mgpu_tune_gemm_in_file() (in module torch.cuda.tunable) mH (torch.Tensor attribute) min() (in module torch) (torch.Tensor method) minimum() (in module torch) (torch.Tensor method) MinMaxObserver (class in torch.ao.quantization.observer) Mish (class in torch.nn) mish() (in module torch.nn.functional) MixedPrecision (class in torch.distributed.fsdp) MixedPrecisionPolicy (class in torch.distributed.fsdp) mixture_distribution (torch.distributions.mixture_same_family.MixtureSameFamily property) MixtureSameFamily (class in torch.distributions.mixture_same_family) MixtureSameFamilyConstraint (class in torch.distributions.constraints) mm() (in module torch) (in module torch.sparse) (torch.Tensor method) mock() (torch.package.PackageExporter method) mocked_modules() (torch.package.PackageExporter method) mode (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.beta.Beta property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.cauchy.Cauchy property) (torch.distributions.dirichlet.Dirichlet property) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential property) (torch.distributions.fishersnedecor.FisherSnedecor property) (torch.distributions.gamma.Gamma property) (torch.distributions.generalized_pareto.GeneralizedPareto property) (torch.distributions.geometric.Geometric property) (torch.distributions.gumbel.Gumbel property) (torch.distributions.half_cauchy.HalfCauchy property) (torch.distributions.half_normal.HalfNormal property) (torch.distributions.independent.Independent property) (torch.distributions.inverse_gamma.InverseGamma property) (torch.distributions.kumaraswamy.Kumaraswamy property) (torch.distributions.laplace.Laplace property) (torch.distributions.log_normal.LogNormal property) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.normal.Normal property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.pareto.Pareto property) (torch.distributions.poisson.Poisson property) (torch.distributions.studentT.StudentT property) (torch.distributions.uniform.Uniform property) (torch.distributions.von_mises.VonMises property) (torch.distributions.weibull.Weibull property) (torch.distributions.wishart.Wishart property) mode() (in module torch) (torch.Tensor method) model_is_exported (class in torch.ao.quantization.pt2e.export_utils) model_proto (torch.onnx.ONNXProgram property) modified_bessel_i0() (in module torch.special) modified_bessel_i1() (in module torch.special) modified_bessel_k0() (in module torch.special) modified_bessel_k1() (in module torch.special) module", "prev_chunk_id": "chunk_1111", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1113", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "torch torch.__config__ torch.__future__ torch._logging torch.accelerator torch.amp torch.amp.autocast_mode torch.amp.grad_scaler torch.ao torch.ao.nn torch.ao.nn.intrinsic torch.ao.nn.intrinsic.modules torch.ao.nn.intrinsic.modules.fused torch.ao.nn.intrinsic.qat torch.ao.nn.intrinsic.qat.modules torch.ao.nn.intrinsic.qat.modules.conv_fused torch.ao.nn.intrinsic.qat.modules.linear_fused torch.ao.nn.intrinsic.qat.modules.linear_relu torch.ao.nn.intrinsic.quantized torch.ao.nn.intrinsic.quantized.dynamic torch.ao.nn.intrinsic.quantized.dynamic.modules torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu torch.ao.nn.intrinsic.quantized.modules torch.ao.nn.intrinsic.quantized.modules.bn_relu torch.ao.nn.intrinsic.quantized.modules.conv_add torch.ao.nn.intrinsic.quantized.modules.conv_relu torch.ao.nn.intrinsic.quantized.modules.linear_relu torch.ao.nn.qat torch.ao.nn.qat.dynamic torch.ao.nn.qat.dynamic.modules torch.ao.nn.qat.dynamic.modules.linear torch.ao.nn.qat.modules torch.ao.nn.qat.modules.conv torch.ao.nn.qat.modules.embedding_ops torch.ao.nn.qat.modules.linear torch.ao.nn.quantizable torch.ao.nn.quantizable.modules torch.ao.nn.quantizable.modules.activation torch.ao.nn.quantizable.modules.rnn torch.ao.nn.quantized torch.ao.nn.quantized.dynamic torch.ao.nn.quantized.dynamic.modules torch.ao.nn.quantized.dynamic.modules.conv torch.ao.nn.quantized.dynamic.modules.linear torch.ao.nn.quantized.dynamic.modules.rnn torch.ao.nn.quantized.functional torch.ao.nn.quantized.modules torch.ao.nn.quantized.modules.activation torch.ao.nn.quantized.modules.batchnorm torch.ao.nn.quantized.modules.conv torch.ao.nn.quantized.modules.dropout torch.ao.nn.quantized.modules.embedding_ops torch.ao.nn.quantized.modules.functional_modules torch.ao.nn.quantized.modules.linear torch.ao.nn.quantized.modules.normalization torch.ao.nn.quantized.modules.rnn torch.ao.nn.quantized.modules.utils torch.ao.nn.quantized.reference torch.ao.nn.quantized.reference.modules torch.ao.nn.quantized.reference.modules.conv torch.ao.nn.quantized.reference.modules.linear torch.ao.nn.quantized.reference.modules.rnn torch.ao.nn.quantized.reference.modules.sparse torch.ao.nn.quantized.reference.modules.utils torch.ao.nn.sparse torch.ao.nn.sparse.quantized torch.ao.nn.sparse.quantized.dynamic torch.ao.nn.sparse.quantized.dynamic.linear torch.ao.nn.sparse.quantized.linear torch.ao.nn.sparse.quantized.utils torch.ao.ns torch.ao.ns._numeric_suite torch.ao.ns._numeric_suite_fx torch.ao.ns.fx torch.ao.ns.fx.graph_matcher torch.ao.ns.fx.graph_passes torch.ao.ns.fx.mappings torch.ao.ns.fx.n_shadows_utils torch.ao.ns.fx.ns_types torch.ao.ns.fx.pattern_utils torch.ao.ns.fx.qconfig_multi_mapping torch.ao.ns.fx.utils torch.ao.ns.fx.weight_utils torch.ao.pruning torch.ao.pruning.scheduler torch.ao.pruning.scheduler.base_scheduler torch.ao.pruning.scheduler.cubic_scheduler torch.ao.pruning.scheduler.lambda_scheduler torch.ao.pruning.sparsifier torch.ao.pruning.sparsifier.base_sparsifier torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier torch.ao.pruning.sparsifier.utils torch.ao.pruning.sparsifier.weight_norm_sparsifier torch.ao.quantization torch.ao.quantization.backend_config torch.ao.quantization.backend_config.backend_config torch.ao.quantization.backend_config.executorch torch.ao.quantization.backend_config.fbgemm torch.ao.quantization.backend_config.native torch.ao.quantization.backend_config.observation_type torch.ao.quantization.backend_config.onednn torch.ao.quantization.backend_config.qnnpack torch.ao.quantization.backend_config.tensorrt torch.ao.quantization.backend_config.utils torch.ao.quantization.backend_config.x86 torch.ao.quantization.fake_quantize torch.ao.quantization.fuse_modules torch.ao.quantization.fuser_method_mappings torch.ao.quantization.fx torch.ao.quantization.fx.convert torch.ao.quantization.fx.custom_config torch.ao.quantization.fx.fuse torch.ao.quantization.fx.fuse_handler torch.ao.quantization.fx.graph_module torch.ao.quantization.fx.lower_to_fbgemm torch.ao.quantization.fx.lower_to_qnnpack torch.ao.quantization.fx.lstm_utils torch.ao.quantization.fx.match_utils torch.ao.quantization.fx.pattern_utils torch.ao.quantization.fx.prepare torch.ao.quantization.fx.qconfig_mapping_utils torch.ao.quantization.fx.quantize_handler torch.ao.quantization.fx.tracer torch.ao.quantization.fx.utils torch.ao.quantization.observer torch.ao.quantization.pt2e torch.ao.quantization.pt2e.duplicate_dq_pass torch.ao.quantization.pt2e.export_utils torch.ao.quantization.pt2e.graph_utils torch.ao.quantization.pt2e.lowering torch.ao.quantization.pt2e.port_metadata_pass torch.ao.quantization.pt2e.prepare torch.ao.quantization.pt2e.qat_utils torch.ao.quantization.pt2e.representation torch.ao.quantization.pt2e.representation.rewrite torch.ao.quantization.pt2e.utils torch.ao.quantization.qconfig torch.ao.quantization.qconfig_mapping torch.ao.quantization.quant_type torch.ao.quantization.quantization_mappings torch.ao.quantization.quantize_fx torch.ao.quantization.quantize_jit torch.ao.quantization.quantize_pt2e torch.ao.quantization.quantizer torch.ao.quantization.quantizer.composable_quantizer torch.ao.quantization.quantizer.embedding_quantizer torch.ao.quantization.quantizer.quantizer torch.ao.quantization.quantizer.utils torch.ao.quantization.quantizer.x86_inductor_quantizer torch.ao.quantization.quantizer.xnnpack_quantizer torch.ao.quantization.quantizer.xnnpack_quantizer_utils torch.ao.quantization.quantizer.xpu_inductor_quantizer torch.ao.quantization.stubs torch.ao.quantization.utils torch.autograd torch.autograd.anomaly_mode torch.autograd.forward_ad torch.autograd.function torch.autograd.functional torch.autograd.grad_mode torch.autograd.gradcheck torch.autograd.graph torch.autograd.profiler torch.autograd.profiler_legacy torch.autograd.profiler_util torch.autograd.variable torch.backends torch.backends.cpu torch.backends.cuda torch.backends.cudnn torch.backends.cudnn.rnn torch.backends.cusparselt torch.backends.kleidiai torch.backends.mha torch.backends.mkl torch.backends.mkldnn torch.backends.mps torch.backends.nnpack torch.backends.openmp torch.backends.opt_einsum torch.backends.quantized torch.backends.xeon torch.backends.xeon.run_cpu torch.backends.xnnpack torch.compiler torch.compiler.config torch.contrib torch.cpu torch.cpu.amp torch.cpu.amp.autocast_mode torch.cpu.amp.grad_scaler torch.cuda torch.cuda._sanitizer torch.cuda.amp torch.cuda.amp.autocast_mode torch.cuda.amp.common torch.cuda.amp.grad_scaler torch.cuda.comm torch.cuda.error torch.cuda.gds torch.cuda.graphs torch.cuda.jiterator torch.cuda.memory torch.cuda.nccl torch.cuda.nvtx torch.cuda.profiler torch.cuda.random torch.cuda.sparse torch.cuda.streams torch.cuda.tunable torch.distributed torch.distributed.algorithms torch.distributed.algorithms.ddp_comm_hooks torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks torch.distributed.algorithms.ddp_comm_hooks.default_hooks torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks torch.distributed.algorithms.join torch.distributed.algorithms.model_averaging torch.distributed.algorithms.model_averaging.averagers torch.distributed.algorithms.model_averaging.hierarchical_model_averager torch.distributed.algorithms.model_averaging.utils torch.distributed.argparse_util torch.distributed.autograd torch.distributed.c10d_logger torch.distributed.checkpoint torch.distributed.checkpoint.api torch.distributed.checkpoint.default_planner torch.distributed.checkpoint.filesystem torch.distributed.checkpoint.format_utils torch.distributed.checkpoint.hf_storage torch.distributed.checkpoint.logger torch.distributed.checkpoint.logging_handlers torch.distributed.checkpoint.metadata torch.distributed.checkpoint.optimizer torch.distributed.checkpoint.planner torch.distributed.checkpoint.planner_helpers torch.distributed.checkpoint.resharding torch.distributed.checkpoint.staging torch.distributed.checkpoint.state_dict torch.distributed.checkpoint.state_dict_loader torch.distributed.checkpoint.state_dict_saver torch.distributed.checkpoint.stateful torch.distributed.checkpoint.storage torch.distributed.checkpoint.utils torch.distributed.collective_utils torch.distributed.constants torch.distributed.device_mesh torch.distributed.distributed_c10d torch.distributed.elastic torch.distributed.elastic.agent torch.distributed.elastic.agent.server torch.distributed.elastic.agent.server.api torch.distributed.elastic.agent.server.health_check_server torch.distributed.elastic.agent.server.local_elastic_agent torch.distributed.elastic.control_plane torch.distributed.elastic.events torch.distributed.elastic.events.api torch.distributed.elastic.events.handlers torch.distributed.elastic.metrics torch.distributed.elastic.metrics.api torch.distributed.elastic.multiprocessing torch.distributed.elastic.multiprocessing.api torch.distributed.elastic.multiprocessing.errors torch.distributed.elastic.multiprocessing.errors.error_handler torch.distributed.elastic.multiprocessing.errors.handlers torch.distributed.elastic.multiprocessing.redirects torch.distributed.elastic.multiprocessing.subprocess_handler torch.distributed.elastic.multiprocessing.subprocess_handler.handlers torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler torch.distributed.elastic.multiprocessing.tail_log torch.distributed.elastic.rendezvous torch.distributed.elastic.rendezvous.api torch.distributed.elastic.rendezvous.c10d_rendezvous_backend torch.distributed.elastic.rendezvous.dynamic_rendezvous torch.distributed.elastic.rendezvous.etcd_rendezvous torch.distributed.elastic.rendezvous.etcd_rendezvous_backend torch.distributed.elastic.rendezvous.etcd_server torch.distributed.elastic.rendezvous.etcd_store torch.distributed.elastic.rendezvous.registry torch.distributed.elastic.rendezvous.static_tcp_rendezvous torch.distributed.elastic.rendezvous.utils torch.distributed.elastic.timer torch.distributed.elastic.timer.api torch.distributed.elastic.timer.debug_info_logging torch.distributed.elastic.timer.file_based_local_timer torch.distributed.elastic.timer.local_timer torch.distributed.elastic.utils torch.distributed.elastic.utils.api torch.distributed.elastic.utils.data torch.distributed.elastic.utils.data.cycling_iterator torch.distributed.elastic.utils.data.elastic_distributed_sampler torch.distributed.elastic.utils.distributed torch.distributed.elastic.utils.log_level torch.distributed.elastic.utils.logging torch.distributed.elastic.utils.store", "prev_chunk_id": "chunk_1112", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1114", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "torch.distributed.fsdp torch.distributed.fsdp.api torch.distributed.fsdp.fully_sharded_data_parallel torch.distributed.fsdp.sharded_grad_scaler torch.distributed.fsdp.wrap torch.distributed.launch torch.distributed.launcher torch.distributed.launcher.api torch.distributed.logging_handlers torch.distributed.nn torch.distributed.nn.api torch.distributed.nn.api.remote_module torch.distributed.nn.functional torch.distributed.nn.jit torch.distributed.nn.jit.instantiator torch.distributed.nn.jit.templates torch.distributed.nn.jit.templates.remote_module_template torch.distributed.optim torch.distributed.optim.apply_optimizer_in_backward torch.distributed.optim.functional_adadelta torch.distributed.optim.functional_adagrad torch.distributed.optim.functional_adam torch.distributed.optim.functional_adamax torch.distributed.optim.functional_adamw torch.distributed.optim.functional_rmsprop torch.distributed.optim.functional_rprop torch.distributed.optim.functional_sgd torch.distributed.optim.named_optimizer torch.distributed.optim.optimizer torch.distributed.optim.post_localSGD_optimizer torch.distributed.optim.utils torch.distributed.optim.zero_redundancy_optimizer torch.distributed.pipelining torch.distributed.pipelining.microbatch torch.distributed.pipelining.schedules torch.distributed.pipelining.stage torch.distributed.remote_device torch.distributed.rendezvous torch.distributed.rpc torch.distributed.rpc.api torch.distributed.rpc.backend_registry torch.distributed.rpc.constants torch.distributed.rpc.functions torch.distributed.rpc.internal torch.distributed.rpc.options torch.distributed.rpc.rref_proxy torch.distributed.rpc.server_process_global_profiler torch.distributed.run torch.distributed.tensor torch.distributed.tensor.debug torch.distributed.tensor.device_mesh torch.distributed.tensor.experimental torch.distributed.tensor.parallel torch.distributed.tensor.parallel.api torch.distributed.tensor.parallel.ddp torch.distributed.tensor.parallel.fsdp torch.distributed.tensor.parallel.input_reshard torch.distributed.tensor.parallel.loss torch.distributed.tensor.parallel.style torch.distributed.tensor.placement_types torch.distributed.utils torch.distributions torch.distributions.bernoulli torch.distributions.beta torch.distributions.binomial torch.distributions.categorical torch.distributions.cauchy torch.distributions.chi2 torch.distributions.constraint_registry torch.distributions.constraints torch.distributions.continuous_bernoulli torch.distributions.dirichlet torch.distributions.distribution torch.distributions.exp_family torch.distributions.exponential torch.distributions.fishersnedecor torch.distributions.gamma torch.distributions.generalized_pareto torch.distributions.geometric torch.distributions.gumbel torch.distributions.half_cauchy torch.distributions.half_normal torch.distributions.independent torch.distributions.inverse_gamma torch.distributions.kl torch.distributions.kumaraswamy torch.distributions.laplace torch.distributions.lkj_cholesky torch.distributions.log_normal torch.distributions.logistic_normal torch.distributions.lowrank_multivariate_normal torch.distributions.mixture_same_family torch.distributions.multinomial torch.distributions.multivariate_normal torch.distributions.negative_binomial torch.distributions.normal torch.distributions.one_hot_categorical torch.distributions.pareto torch.distributions.poisson torch.distributions.relaxed_bernoulli torch.distributions.relaxed_categorical torch.distributions.studentT torch.distributions.transformed_distribution torch.distributions.transforms torch.distributions.uniform torch.distributions.utils torch.distributions.von_mises torch.distributions.weibull torch.distributions.wishart torch.export torch.export.custom_obj torch.export.custom_ops torch.export.decomp_utils torch.export.dynamic_shapes torch.export.experimental torch.export.exported_program torch.export.graph_signature torch.export.passes torch.export.pt2_archive torch.export.pt2_archive.constants torch.export.unflatten torch.fft torch.func torch.functional torch.futures torch.fx torch.fx.annotate torch.fx.config torch.fx.experimental torch.fx.experimental.accelerator_partitioner torch.fx.experimental.const_fold torch.fx.experimental.debug torch.fx.experimental.graph_gradual_typechecker torch.fx.experimental.merge_matmul torch.fx.experimental.meta_tracer torch.fx.experimental.migrate_gradual_types torch.fx.experimental.migrate_gradual_types.constraint torch.fx.experimental.migrate_gradual_types.constraint_generator torch.fx.experimental.migrate_gradual_types.constraint_transformation torch.fx.experimental.migrate_gradual_types.operation torch.fx.experimental.migrate_gradual_types.transform_to_z3 torch.fx.experimental.migrate_gradual_types.util torch.fx.experimental.migrate_gradual_types.z3_types torch.fx.experimental.normalize torch.fx.experimental.optimization torch.fx.experimental.partitioner_utils torch.fx.experimental.proxy_tensor torch.fx.experimental.recording torch.fx.experimental.refinement_types torch.fx.experimental.rewriter torch.fx.experimental.schema_type_annotation torch.fx.experimental.sym_node torch.fx.experimental.symbolic_shapes torch.fx.experimental.unification torch.fx.experimental.unification.core torch.fx.experimental.unification.dispatch torch.fx.experimental.unification.match torch.fx.experimental.unification.more torch.fx.experimental.unification.multipledispatch torch.fx.experimental.unification.multipledispatch.conflict torch.fx.experimental.unification.multipledispatch.core torch.fx.experimental.unification.multipledispatch.dispatcher torch.fx.experimental.unification.multipledispatch.utils torch.fx.experimental.unification.multipledispatch.variadic torch.fx.experimental.unification.unification_tools torch.fx.experimental.unification.utils torch.fx.experimental.unification.variable torch.fx.experimental.unify_refinements torch.fx.experimental.validator torch.fx.graph torch.fx.graph_module torch.fx.immutable_collections torch.fx.interpreter torch.fx.node torch.fx.operator_schemas torch.fx.passes torch.fx.passes.annotate_getitem_nodes torch.fx.passes.backends torch.fx.passes.backends.cudagraphs torch.fx.passes.dialect torch.fx.passes.dialect.common torch.fx.passes.dialect.common.cse_pass torch.fx.passes.fake_tensor_prop torch.fx.passes.graph_drawer torch.fx.passes.graph_manipulation torch.fx.passes.graph_transform_observer torch.fx.passes.infra torch.fx.passes.infra.partitioner torch.fx.passes.infra.pass_base torch.fx.passes.infra.pass_manager torch.fx.passes.net_min_base torch.fx.passes.operator_support torch.fx.passes.param_fetch torch.fx.passes.pass_manager torch.fx.passes.reinplace torch.fx.passes.runtime_assert torch.fx.passes.shape_prop torch.fx.passes.split_module torch.fx.passes.split_utils torch.fx.passes.splitter_base torch.fx.passes.tests torch.fx.passes.tests.test_pass_manager torch.fx.passes.tools_common torch.fx.passes.utils torch.fx.passes.utils.common torch.fx.passes.utils.fuser_utils torch.fx.passes.utils.matcher_utils torch.fx.passes.utils.matcher_with_name_node_map_utils torch.fx.passes.utils.source_matcher_utils torch.fx.proxy torch.fx.subgraph_rewriter torch.fx.tensor_type torch.fx.traceback torch.hub torch.jit torch.jit.annotations torch.jit.frontend torch.jit.generate_bytecode torch.jit.mobile torch.jit.quantized torch.jit.supported_ops torch.jit.unsupported_tensor_ops torch.library torch.linalg torch.masked torch.masked.maskedtensor torch.masked.maskedtensor.binary torch.masked.maskedtensor.core torch.masked.maskedtensor.creation torch.masked.maskedtensor.passthrough torch.masked.maskedtensor.reductions torch.masked.maskedtensor.unary torch.monitor torch.mps torch.mps.event torch.mps.profiler torch.mtia torch.mtia.memory torch.multiprocessing torch.multiprocessing.pool torch.multiprocessing.queue torch.multiprocessing.reductions torch.multiprocessing.spawn torch.nested torch.nn torch.nn.attention torch.nn.attention.bias torch.nn.attention.experimental torch.nn.attention.flex_attention torch.nn.backends torch.nn.backends.thnn torch.nn.common_types torch.nn.cpp torch.nn.functional torch.nn.grad torch.nn.init torch.nn.intrinsic torch.nn.intrinsic.modules torch.nn.intrinsic.modules.fused torch.nn.intrinsic.qat torch.nn.intrinsic.qat.modules torch.nn.intrinsic.qat.modules.conv_fused torch.nn.intrinsic.qat.modules.linear_fused torch.nn.intrinsic.qat.modules.linear_relu torch.nn.intrinsic.quantized torch.nn.intrinsic.quantized.dynamic torch.nn.intrinsic.quantized.dynamic.modules torch.nn.intrinsic.quantized.dynamic.modules.linear_relu torch.nn.intrinsic.quantized.modules torch.nn.intrinsic.quantized.modules.bn_relu torch.nn.intrinsic.quantized.modules.conv_relu torch.nn.intrinsic.quantized.modules.linear_relu torch.nn.modules torch.nn.modules.activation torch.nn.modules.adaptive torch.nn.modules.batchnorm torch.nn.modules.channelshuffle torch.nn.modules.container torch.nn.modules.conv torch.nn.modules.distance torch.nn.modules.dropout torch.nn.modules.flatten torch.nn.modules.fold torch.nn.modules.instancenorm torch.nn.modules.lazy torch.nn.modules.linear torch.nn.modules.loss torch.nn.modules.module torch.nn.modules.normalization torch.nn.modules.padding torch.nn.modules.pixelshuffle torch.nn.modules.pooling torch.nn.modules.rnn torch.nn.modules.sparse torch.nn.modules.transformer torch.nn.modules.upsampling torch.nn.modules.utils torch.nn.parallel torch.nn.parallel.comm torch.nn.parallel.distributed", "prev_chunk_id": "chunk_1113", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1115", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "torch.nn.parallel.parallel_apply torch.nn.parallel.replicate torch.nn.parallel.scatter_gather torch.nn.parameter torch.nn.qat torch.nn.qat.dynamic torch.nn.qat.dynamic.modules torch.nn.qat.dynamic.modules.linear torch.nn.qat.modules torch.nn.qat.modules.conv torch.nn.qat.modules.embedding_ops torch.nn.qat.modules.linear torch.nn.quantizable torch.nn.quantizable.modules torch.nn.quantizable.modules.activation torch.nn.quantizable.modules.rnn torch.nn.quantized torch.nn.quantized.dynamic torch.nn.quantized.dynamic.modules torch.nn.quantized.dynamic.modules.conv torch.nn.quantized.dynamic.modules.linear torch.nn.quantized.dynamic.modules.rnn torch.nn.quantized.functional torch.nn.quantized.modules torch.nn.quantized.modules.activation torch.nn.quantized.modules.batchnorm torch.nn.quantized.modules.conv torch.nn.quantized.modules.dropout torch.nn.quantized.modules.embedding_ops torch.nn.quantized.modules.functional_modules torch.nn.quantized.modules.linear torch.nn.quantized.modules.normalization torch.nn.quantized.modules.rnn torch.nn.quantized.modules.utils torch.nn.utils torch.nn.utils.clip_grad torch.nn.utils.convert_parameters torch.nn.utils.fusion torch.nn.utils.init torch.nn.utils.memory_format torch.nn.utils.parametrizations torch.nn.utils.parametrize torch.nn.utils.prune torch.nn.utils.rnn torch.nn.utils.stateless torch.onnx torch.onnx.errors torch.onnx.operators torch.onnx.ops torch.onnx.symbolic_caffe2 torch.onnx.symbolic_helper torch.onnx.symbolic_opset10 torch.onnx.symbolic_opset11 torch.onnx.symbolic_opset12 torch.onnx.symbolic_opset13 torch.onnx.symbolic_opset14 torch.onnx.symbolic_opset15 torch.onnx.symbolic_opset16 torch.onnx.symbolic_opset17 torch.onnx.symbolic_opset18 torch.onnx.symbolic_opset19 torch.onnx.symbolic_opset20 torch.onnx.symbolic_opset7 torch.onnx.symbolic_opset8 torch.onnx.symbolic_opset9 torch.onnx.utils torch.onnx.verification torch.optim torch.optim.adadelta torch.optim.adagrad torch.optim.adam torch.optim.adamax torch.optim.adamw torch.optim.asgd torch.optim.lbfgs torch.optim.lr_scheduler torch.optim.nadam torch.optim.optimizer torch.optim.radam torch.optim.rmsprop torch.optim.rprop torch.optim.sgd torch.optim.sparse_adam torch.optim.swa_utils torch.overrides torch.package torch.package.analyze torch.package.analyze.find_first_use_of_broken_modules torch.package.analyze.is_from_package torch.package.analyze.trace_dependencies torch.package.file_structure_representation torch.package.find_file_dependencies torch.package.glob_group torch.package.importer torch.package.package_exporter torch.package.package_importer torch.profiler torch.profiler.itt torch.profiler.profiler torch.profiler.python_tracer torch.quantization torch.quantization.fake_quantize torch.quantization.fuse_modules torch.quantization.fuser_method_mappings torch.quantization.fx torch.quantization.fx.convert torch.quantization.fx.fuse torch.quantization.fx.fusion_patterns torch.quantization.fx.graph_module torch.quantization.fx.match_utils torch.quantization.fx.pattern_utils torch.quantization.fx.prepare torch.quantization.fx.quantization_patterns torch.quantization.fx.quantization_types torch.quantization.fx.utils torch.quantization.observer torch.quantization.qconfig torch.quantization.quant_type torch.quantization.quantization_mappings torch.quantization.quantize torch.quantization.quantize_fx torch.quantization.quantize_jit torch.quantization.stubs torch.quantization.utils torch.quasirandom torch.random torch.return_types torch.serialization torch.signal torch.signal.windows torch.signal.windows.windows torch.sparse torch.sparse.semi_structured torch.special torch.storage torch.testing torch.torch_version torch.types torch.utils torch.utils.backcompat torch.utils.backend_registration torch.utils.benchmark torch.utils.benchmark.examples torch.utils.benchmark.examples.compare torch.utils.benchmark.examples.fuzzer torch.utils.benchmark.examples.op_benchmark torch.utils.benchmark.examples.simple_timeit torch.utils.benchmark.examples.spectral_ops_fuzz_test torch.utils.benchmark.op_fuzzers torch.utils.benchmark.op_fuzzers.binary torch.utils.benchmark.op_fuzzers.sparse_binary torch.utils.benchmark.op_fuzzers.sparse_unary torch.utils.benchmark.op_fuzzers.spectral torch.utils.benchmark.op_fuzzers.unary torch.utils.benchmark.utils torch.utils.benchmark.utils.common torch.utils.benchmark.utils.compare torch.utils.benchmark.utils.compile torch.utils.benchmark.utils.cpp_jit torch.utils.benchmark.utils.fuzzer torch.utils.benchmark.utils.sparse_fuzzer torch.utils.benchmark.utils.timer torch.utils.benchmark.utils.valgrind_wrapper torch.utils.benchmark.utils.valgrind_wrapper.timer_interface torch.utils.bottleneck torch.utils.bundled_inputs torch.utils.checkpoint torch.utils.collect_env torch.utils.cpp_backtrace torch.utils.cpp_extension torch.utils.data torch.utils.data.backward_compatibility torch.utils.data.dataloader torch.utils.data.datapipes torch.utils.data.datapipes.dataframe torch.utils.data.datapipes.dataframe.dataframe_wrapper torch.utils.data.datapipes.dataframe.dataframes torch.utils.data.datapipes.dataframe.datapipes torch.utils.data.datapipes.dataframe.structures torch.utils.data.datapipes.datapipe torch.utils.data.datapipes.gen_pyi torch.utils.data.datapipes.iter torch.utils.data.datapipes.iter.callable torch.utils.data.datapipes.iter.combinatorics torch.utils.data.datapipes.iter.combining torch.utils.data.datapipes.iter.filelister torch.utils.data.datapipes.iter.fileopener torch.utils.data.datapipes.iter.grouping torch.utils.data.datapipes.iter.routeddecoder torch.utils.data.datapipes.iter.selecting torch.utils.data.datapipes.iter.sharding torch.utils.data.datapipes.iter.streamreader torch.utils.data.datapipes.iter.utils torch.utils.data.datapipes.map torch.utils.data.datapipes.map.callable torch.utils.data.datapipes.map.combinatorics torch.utils.data.datapipes.map.combining torch.utils.data.datapipes.map.grouping torch.utils.data.datapipes.map.utils torch.utils.data.datapipes.utils torch.utils.data.datapipes.utils.common torch.utils.data.datapipes.utils.decoder torch.utils.data.datapipes.utils.snapshot torch.utils.data.dataset torch.utils.data.distributed torch.utils.data.graph torch.utils.data.graph_settings torch.utils.data.sampler torch.utils.deterministic torch.utils.dlpack torch.utils.file_baton torch.utils.flop_counter torch.utils.hipify torch.utils.hipify.constants torch.utils.hipify.cuda_to_hip_mappings torch.utils.hipify.hipify_python torch.utils.hipify.version torch.utils.hooks torch.utils.jit torch.utils.jit.log_extract torch.utils.mkldnn torch.utils.mobile_optimizer torch.utils.model_dump torch.utils.model_zoo torch.utils.module_tracker torch.utils.serialization torch.utils.serialization.config torch.utils.show_pickle torch.utils.tensorboard torch.utils.tensorboard.summary torch.utils.tensorboard.writer torch.utils.throughput_benchmark torch.utils.viz torch.utils.weak torch.version torch.xpu torch.xpu.memory torch.xpu.random torch.xpu.streams | Module (class in torch.nn) module (torch.distributed.fsdp.FullyShardedDataParallel property) module() (torch.export.ExportedProgram method) module_call_graph (torch.export.ExportedProgram attribute) module_load() (torch.Tensor method) ModuleCallEntry (class in torch.export) ModuleCallSignature (class in torch.export) ModuleDict (class in torch.nn) ModuleList (class in torch.nn) modules() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) ModuleTracker (class in torch.utils.module_tracker) monitored_barrier() (in module torch.distributed) move_to_device_pass() (in module torch.export.passes) moveaxis() (in module torch) (torch.Tensor method) movedim() (in module", "prev_chunk_id": "chunk_1114", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1116", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "M", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "M", "content": "torch) (torch.Tensor method) MovingAverageMinMaxObserver (class in torch.ao.quantization.observer) MovingAveragePerChannelMinMaxObserver (class in torch.ao.quantization.observer) mps() (torch.UntypedStorage method) mse_loss() (in module torch.nn.functional) MSELoss (class in torch.nn) msort() (in module torch) (torch.Tensor method) mT (torch.Tensor attribute) mtia() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) mul() (in module torch) (torch.ao.ns._numeric_suite.Shadow method) (torch.Tensor method) mul_() (torch.Tensor method) mul_scalar() (torch.ao.ns._numeric_suite.Shadow method) multi_dot() (in module torch.linalg) multi_get() (torch.distributed.Store method) multi_margin_loss() (in module torch.nn.functional) multi_set() (torch.distributed.Store method) multigammaln() (in module torch.special) MultiheadAttention (class in torch.ao.nn.quantizable) (class in torch.nn) multilabel_margin_loss() (in module torch.nn.functional) multilabel_soft_margin_loss() (in module torch.nn.functional) MultiLabelMarginLoss (class in torch.nn) MultiLabelSoftMarginLoss (class in torch.nn) MultiMarginLoss (class in torch.nn) Multinomial (class in torch.distributions.multinomial) multinomial (in module torch.distributions.constraints) multinomial() (in module torch) (torch.Tensor method) MultiplicativeLR (class in torch.optim.lr_scheduler) multiply() (in module torch) (torch.Tensor method) multiply_() (torch.Tensor method) MultiprocessContext (class in torch.distributed.elastic.multiprocessing.api) MultiStepLR (class in torch.optim.lr_scheduler) MultivariateNormal (class in torch.distributions.multivariate_normal) mv() (in module torch) (torch.Tensor method) mvlgamma() (in module torch) (torch.Tensor method) mvlgamma_() (torch.Tensor method)", "prev_chunk_id": "chunk_1115", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1117", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "N", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "N", "content": "N NAdam (class in torch.optim) name (torch.autograd.profiler_util.Kernel attribute) (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend property) (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend property) (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend property) (torch.distributed.rpc.WorkerInfo property) (torch.monitor.Aggregation property) (torch.monitor.Event property) (torch.monitor.Stat property) (torch.nn.attention.SDPBackend property) (torch.profiler.ProfilerActivity property) (torch.Tag property) name() (torch.autograd.graph.Node method) named_buffers() (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) named_children() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) named_modules() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) named_parameters() (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) NamedShape (torch.nn.Unflatten attribute) names (torch.Tensor attribute) nan_to_num() (in module torch) (torch.Tensor method) nan_to_num_() (torch.Tensor method) nanmean() (in module torch) (torch.Tensor method) nanmedian() (in module torch) (torch.Tensor method) nanquantile() (in module torch) (torch.Tensor method) nansum() (in module torch) (torch.Tensor method) narrow() (in module torch) (in module torch.nested) (torch.Tensor method) narrow_copy() (in module torch) (torch.Tensor method) nbytes (torch.Tensor attribute) nbytes() (torch.TypedStorage method) (torch.UntypedStorage method) ndim (torch.Tensor attribute) ndimension() (torch.Tensor method) ndtr() (in module torch.special) ndtri() (in module torch.special) ne() (in module torch) (torch.Tensor method) ne_() (torch.Tensor method) neg() (in module torch) (torch.Tensor method) neg_() (torch.Tensor method) negative() (in module torch) (torch.Tensor method) | negative_() (torch.Tensor method) NegativeBinomial (class in torch.distributions.negative_binomial) nelement() (torch.Tensor method) nested_compile_region() (in module torch.compiler) nested_tensor() (in module torch.nested) nested_tensor_from_jagged() (in module torch.nested) NestedIOFunction (class in torch.autograd.function) new() (torch.UntypedStorage method) new_empty() (torch.Tensor method) new_full() (torch.Tensor method) new_group() (in module torch.distributed) new_ones() (torch.Tensor method) new_tensor() (torch.Tensor method) new_zeros() (torch.Tensor method) next (torch.fx.Node property) next_functions (torch.autograd.graph.Node property) next_rendezvous() (torch.distributed.elastic.rendezvous.RendezvousHandler method) nextafter() (in module torch) (torch.Tensor method) nextafter_() (torch.Tensor method) nll_loss() (in module torch.nn.functional) NLLLoss (class in torch.nn) no_grad (class in torch) no_sync() (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.nn.parallel.DistributedDataParallel method) Node (class in torch.fx) node_copy() (torch.fx.Graph method) nodes (torch.fx.Graph property) nonzero() (in module torch) (torch.Tensor method) noop_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks) noop_mask() (in module torch.nn.attention.flex_attention) NoopObserver (class in torch.ao.quantization.observer) norm() (in module torch) (in module torch.linalg) (torch.Tensor method) Normal (class in torch.distributions.normal) normal() (in module torch) normal_() (in module torch.nn.init) (torch.Tensor method) normalize() (in", "prev_chunk_id": "chunk_1116", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1118", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "N", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "N", "content": "module torch.nn.functional) normalized_arguments() (torch.fx.Node method) not_equal() (in module torch) (torch.Tensor method) not_equal_() (torch.Tensor method) notify_join_context() (torch.distributed.algorithms.Join static method) NSTracer (class in torch.ao.ns._numeric_suite_fx) NullMetricHandler (class in torch.distributed.elastic.metrics.api) num_keys() (torch.distributed.Store method) num_nodes_waiting() (torch.distributed.elastic.rendezvous.RendezvousHandler method) num_worker_threads (torch.distributed.rpc.TensorPipeRpcBackendOptions property) numel() (in module torch) (torch.nn.attention.flex_attention.BlockMask method) (torch.Size method) (torch.Tensor method) NUMERIC_DEBUG_HANDLE_KEY (in module torch.ao.quantization) numpy() (torch.Tensor method) nuttall() (in module torch.signal.windows)", "prev_chunk_id": "chunk_1117", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1119", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "O", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "O", "content": "O ObservationType (class in torch.ao.quantization.backend_config) ObserverBase (class in torch.ao.quantization.observer) OffloadPolicy (class in torch.distributed.fsdp) on_generate_code() (torch.fx.Graph method) once_differentiable() (in module torch.autograd.function) one_hot() (in module torch.nn.functional) OneCycleLR (class in torch.optim.lr_scheduler) onednn_fusion_enabled() (in module torch.jit) OneHotCategorical (class in torch.distributions.one_hot_categorical) ones() (in module torch) (in module torch.distributed.tensor) ones_() (in module torch.nn.init) ones_like() (in module torch) onnx_compatible() (torch.onnx.JitScalarType method) onnx_type() (torch.onnx.JitScalarType method) OnnxBackend (class in torch.onnx.verification) OnnxExporterError (class in torch.onnx) ONNXProgram (class in torch.onnx) OnnxTestCaseRepro (class in torch.onnx.verification) opcheck() (in module torch.library) optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) optim_state_dict_to_load() (torch.distributed.fsdp.FullyShardedDataParallel static method) optimize() (torch.onnx.ONNXProgram method) optimize_for_inference() (in module torch.jit) optimize_for_mobile() (in module torch.utils.mobile_optimizer) | Optimizer (class in torch.optim) OptimStateDictConfig (class in torch.distributed.fsdp) or_masks() (in module torch.nn.attention.flex_attention) orgqr() (in module torch) (torch.Tensor method) ormqr() (in module torch) (torch.Tensor method) orthogonal() (in module torch.nn.utils.parametrizations) orthogonal_() (in module torch.nn.init) outer() (in module torch) (torch.Tensor method) OutOfMemoryError output() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) output_node() (torch.fx.Graph method) OUTPUT_SHARE_OBSERVER_WITH_INPUT (torch.ao.quantization.backend_config.ObservationType attribute) OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (torch.ao.quantization.backend_config.ObservationType attribute) OutputComparisonLogger (class in torch.ao.ns._numeric_suite_fx) OutputKind (class in torch.export.graph_signature) OutputLogger (class in torch.ao.ns._numeric_suite) (class in torch.ao.ns._numeric_suite_fx) OutputSpec (class in torch.export.graph_signature) owner() (torch.distributed.rpc.PyRRef method) owner_name() (torch.distributed.rpc.PyRRef method)", "prev_chunk_id": "chunk_1118", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1120", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "P", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "P", "content": "P P2POp (class in torch.distributed) pack_padded_sequence() (in module torch.nn.utils.rnn) pack_sequence() (in module torch.nn.utils.rnn) PackageExporter (class in torch.package) PackageImporter (class in torch.package) PackagingError (class in torch.package) PackedSequence (class in torch.nn.utils.rnn) pad() (in module torch.nn.functional) pad_packed_sequence() (in module torch.nn.utils.rnn) pad_sequence() (in module torch.nn.utils.rnn) pairwise_distance() (in module torch.nn.functional) PairwiseDistance (class in torch.nn) parallel_info() (in module torch.__config__) parallelize_module() (in module torch.distributed.tensor.parallel) param_shape (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property) Parameter (class in torch.nn.parameter) ParameterDict (class in torch.nn) ParameterList (class in torch.nn) parameters() (in module torch.distributed.GradBucket) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) parameters_to_vector() (in module torch.nn.utils) ParametrizationList (class in torch.nn.utils.parametrize) Pareto (class in torch.distributions.pareto) parse_nvprof_trace() (in module torch.autograd.profiler) Partial (class in torch.distributed.tensor.placement_types) patch_source_specialization() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) path (torch.distributed.FileStore property) path_of_module() (torch.fx.Tracer method) pca_lowrank() (in module torch) PContext (class in torch.distributed.elastic.multiprocessing.api) pdist() (in module torch.nn.functional) per_channel_dynamic_qconfig (in module torch.ao.quantization.qconfig) PerAxis (class in torch.ao.quantization.observer) PerBlock (class in torch.ao.quantization.observer) PerChannelMinMaxObserver (class in torch.ao.quantization.observer) PerGroup (class in torch.ao.quantization.observer) permute() (in module torch) (torch.Tensor method) perplexity() (torch.distributions.distribution.Distribution method) PerRow (class in torch.ao.quantization.observer) PerTensor (class in torch.ao.quantization.observer) PerToken (class in torch.ao.quantization.observer) pickle_storage_type() (torch.TypedStorage method) pin_memory() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) pinv() (in module torch.linalg) pinverse() (in module torch) (torch.Tensor method) Pipe (class in torch.distributed.pipelining) pipe_split() (in module torch.distributed.pipelining) pipeline() (in module torch.distributed.pipelining) PipelineScheduleMulti (class in torch.distributed.pipelining.schedules) PipelineScheduleSingle (class in torch.distributed.pipelining.schedules) PipelineStage (class in torch.distributed.pipelining.stage) pixel_shuffle() (in module torch.nn.functional) pixel_unshuffle() (in module torch.nn.functional) PixelShuffle (class in torch.nn) PixelUnshuffle (class in torch.nn) placeholder() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Graph method) (torch.fx.Interpreter method) (torch.fx.Transformer method) PlaceholderObserver (class in torch.ao.quantization.observer) Placement (class in torch.distributed.tensor.placement_types) placements (torch.distributed.tensor.DTensor property) Poisson (class in torch.distributions.poisson) poisson() (in module torch) poisson_nll_loss() (in module torch.nn.functional) PoissonNLLLoss (class in torch.nn) polar() (in module torch) polygamma() (in module torch) (in module torch.special) (torch.Tensor method) polygamma_() (torch.Tensor method) PolynomialLR (class in torch.optim.lr_scheduler) pool() (torch.cuda.CUDAGraph method) pop() (torch.autograd.profiler_util.StringTable method)", "prev_chunk_id": "chunk_1119", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1121", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "P", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "P", "content": "(torch.export.decomp_utils.CustomDecompTable method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) popitem() (torch.autograd.profiler_util.StringTable method) (torch.nn.ParameterDict method) port (torch.distributed.TCPStore property) | positive() (in module torch) (torch.Tensor method) PositiveDefiniteTransform (class in torch.distributions.transforms) post_hook() (torch.distributed.algorithms.JoinHook method) PostLocalSGDOptimizer (class in torch.distributed.optim) pow() (in module torch) (torch.Tensor method) pow_() (torch.Tensor method) power_draw() (in module torch.cuda) powerSGD_hook() (in module torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook) PowerSGDState (class in torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook) PowerTransform (class in torch.distributions.transforms) precision_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.wishart.Wishart property) predict() (torch.nn.AdaptiveLogSoftmaxWithLoss method) preferred_blas_library() (in module torch.backends.cuda) preferred_linalg_library() (in module torch.backends.cuda) preferred_rocm_fa_library() (in module torch.backends.cuda) PrefixStore (class in torch.distributed) PReLU (class in torch.nn) prelu() (in module torch.nn.functional) prepare (class in torch.ao.quantization) prepare_for_propagation_comparison (class in torch.ao.quantization) prepare_fx (class in torch.ao.quantization.quantize_fx) prepare_global_plan() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) (torch.distributed.checkpoint.StorageWriter method) prepare_local_plan() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) (torch.distributed.checkpoint.StorageWriter method) prepare_model_outputs() (in module torch.ao.ns._numeric_suite) prepare_model_with_stubs() (in module torch.ao.ns._numeric_suite) prepare_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx) prepare_qat (class in torch.ao.quantization) prepare_qat_fx (class in torch.ao.quantization.quantize_fx) PrepareCustomConfig (class in torch.ao.quantization.fx.custom_config) PrepareModuleInput (class in torch.distributed.tensor.parallel) PrepareModuleInputOutput (class in torch.distributed.tensor.parallel) PrepareModuleOutput (class in torch.distributed.tensor.parallel) prepend() (torch.fx.Node method) preset_metadata_json() (torch.profiler._KinetoProfile method) prettify_results() (torch.fx.experimental.symbolic_shapes.DimConstraints method) prev (torch.fx.Node property) primal (torch.autograd.forward_ad.UnpackedDualTensor attribute) print() (torch.utils.benchmark.Compare method) print_comparisons_n_shadows_model() (in module torch.ao.ns._numeric_suite_fx) print_readable() (torch.fx.GraphModule method) print_tabular() (torch.fx.Graph method) probs (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.geometric.Geometric property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli property) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property) process_inputs() (torch.fx.Graph method) process_outputs() (torch.fx.Graph method) ProcessFailure (class in torch.distributed.elastic.multiprocessing.errors) prod() (in module torch) (torch.Tensor method) produce_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) produce_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) produce_guards_verbose() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) prof() (in module torch.distributed.elastic.metrics) profile (class in torch.autograd.profiler) (class in torch.profiler) profile() (in module torch.mps.profiler) ProfilerAction (class in torch.profiler) ProfilerActivity (class in torch.profiler) promote_types() (in module torch) propagate_qconfig_ (class in torch.ao.quantization) PropagateUnbackedSymInts (class in torch.fx.experimental.symbolic_shapes) Proxy (class in torch.fx) proxy() (torch.fx.Tracer method) prune() (torch.nn.utils.prune.BasePruningMethod method) (torch.nn.utils.prune.CustomFromMask method) (torch.nn.utils.prune.Identity method) (torch.nn.utils.prune.L1Unstructured method) (torch.nn.utils.prune.LnStructured method) (torch.nn.utils.prune.PruningContainer method) (torch.nn.utils.prune.RandomStructured method) (torch.nn.utils.prune.RandomUnstructured method) PruningContainer (class in torch.nn.utils.prune) psi() (in module torch.special) put_() (torch.Tensor method) put_metric()", "prev_chunk_id": "chunk_1120", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1122", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "P", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "P", "content": "(in module torch.distributed.elastic.metrics) PyRRef (class in torch.distributed.rpc) python_code() (torch.fx.Graph method) python_version() (torch.package.PackageImporter method)", "prev_chunk_id": "chunk_1121", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1123", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "Q", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "Q", "content": "Q q_indices (torch.nn.attention.flex_attention.BlockMask attribute) q_num_blocks (torch.nn.attention.flex_attention.BlockMask attribute) q_per_channel_axis() (torch.Tensor method) q_per_channel_scales() (torch.Tensor method) q_per_channel_zero_points() (torch.Tensor method) q_scale() (torch.Tensor method) q_zero_point() (torch.Tensor method) QConfig (class in torch.ao.quantization.qconfig) QConfigMapping (class in torch.ao.quantization.qconfig_mapping) QFunctional (class in torch.ao.nn.quantized) QInt32Storage (class in torch) QInt8Storage (class in torch) qr() (in module torch) (in module torch.linalg) (torch.Tensor method) qscheme() (torch.Tensor method) quantile() (in module torch) (torch.Tensor method) quantize (class in torch.ao.quantization) quantize_dynamic (class in torch.ao.quantization) quantize_per_channel() (in module torch) quantize_per_tensor() (in module torch) | quantize_qat (class in torch.ao.quantization) quantized_batch_norm() (in module torch) quantized_max_pool1d() (in module torch) quantized_max_pool2d() (in module torch) QuantStub (class in torch.ao.quantization) QuantWrapper (class in torch.ao.quantization) query() (torch.cuda.Event method) (torch.cuda.ExternalStream method) (torch.cuda.Stream method) (torch.Event method) (torch.mps.event.Event method) (torch.mtia.Event method) (torch.mtia.Stream method) (torch.Stream method) (torch.xpu.Event method) (torch.xpu.Stream method) queue_len() (torch.distributed.Store method) queue_pop() (torch.distributed.Store method) queue_push() (torch.distributed.Store method) QUInt2x4Storage (class in torch) QUInt4x2Storage (class in torch) QUInt8Storage (class in torch)", "prev_chunk_id": "chunk_1122", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1124", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "R", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "R", "content": "R rad2deg() (in module torch) (torch.Tensor method) RAdam (class in torch.optim) rand() (in module torch) (in module torch.distributed.tensor) rand_like() (in module torch) randint() (in module torch) randint_like() (in module torch) randn() (in module torch) (in module torch.distributed.tensor) randn_like() (in module torch) random_() (torch.Tensor method) random_split() (in module torch.utils.data) random_structured() (in module torch.nn.utils.prune) random_unstructured() (in module torch.nn.utils.prune) RandomSampler (class in torch.utils.data) RandomStructured (class in torch.nn.utils.prune) RandomUnstructured (class in torch.nn.utils.prune) randperm() (in module torch) range() (in module torch) (in module torch.cuda.nvtx) range_constraints (torch.export.ExportedProgram attribute) range_pop() (in module torch.cuda.nvtx) (in module torch.profiler.itt) range_push() (in module torch.cuda.nvtx) (in module torch.profiler.itt) rate (torch.distributions.inverse_gamma.InverseGamma property) ravel() (in module torch) (torch.Tensor method) raw_cuda_graph() (torch.cuda.CUDAGraph method) read_data() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) read_file() (in module torch.cuda.tunable) read_metadata() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) ReadItem (class in torch.distributed.checkpoint) real (torch.Tensor attribute) real() (in module torch) rebind_unbacked() (in module torch.fx.experimental.symbolic_shapes) reciprocal() (in module torch) (torch.Tensor method) reciprocal_() (torch.Tensor method) recommended_max_memory() (in module torch.mps) recompile() (torch.fx.GraphModule method) record() (in module torch.distributed.elastic.events) (in module torch.distributed.elastic.multiprocessing.errors) (torch.cuda.Event method) (torch.Event method) (torch.mps.event.Event method) (torch.mtia.Event method) (torch.xpu.Event method) record_event() (torch.cuda.ExternalStream method) (torch.cuda.Stream method) (torch.mtia.Stream method) (torch.Stream method) (torch.xpu.Stream method) record_function (class in torch.autograd.profiler) record_memory_history() (in module torch.mtia) record_stream() (torch.Tensor method) record_untuned_enable() (in module torch.cuda.tunable) record_untuned_is_enabled() (in module torch.cuda.tunable) RecordingObserver (class in torch.ao.quantization.observer) recursive_undo() (torch.optim.lr_scheduler.SequentialLR method) recv() (in module torch.distributed) recv_object_list() (in module torch.distributed) redistribute() (torch.distributed.tensor.DTensor method) reduce() (in module torch.distributed) reduce_add() (in module torch.cuda.comm) reduce_add_coalesced() (in module torch.cuda.comm) reduce_op (class in torch.distributed) (torch.distributed.tensor.placement_types.Partial attribute) reduce_scatter() (in module torch.distributed) reduce_scatter_tensor() (in module torch.distributed) ReduceLROnPlateau (class in torch.optim.lr_scheduler) ReduceOp (class in torch.distributed) refine_dynamic_shapes_from_suggested_fixes() (in module torch.export.dynamic_shapes) refine_names() (torch.Tensor method) ReflectionPad1d (class in torch.nn) ReflectionPad2d (class in torch.nn) ReflectionPad3d (class in torch.nn) register() (torch.distributions.constraint_registry.ConstraintRegistry method) register_autocast() (in module torch.library) register_autograd() (in module torch.library) register_backend() (torch.distributed.Backend class method) register_backward_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_buffer() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method)", "prev_chunk_id": "chunk_1123", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1125", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "R", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "R", "content": "register_comm_hook() (torch.distributed.fsdp.FullyShardedDataParallel method) (torch.nn.parallel.DistributedDataParallel method) register_custom_op_symbolic() (in module torch.onnx) register_dataclass() (in module torch.export) register_event_handler() (in module torch.monitor) register_extern_hook() (torch.package.PackageExporter method) register_fake() (in module torch.library) register_forward_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_forward_pre_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_fsdp_forward_method() (in module torch.distributed.fsdp) register_full_backward_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_full_backward_pre_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_handle() (torch.cuda.gds.GdsFile method) register_hook() (torch.autograd.graph.Node method) (torch.Tensor method) register_intern_hook() (torch.package.PackageExporter method) register_kernel() (in module torch.library) register_kl() (in module torch.distributions.kl) register_load_state_dict_post_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) register_load_state_dict_pre_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) register_mock_hook() (torch.package.PackageExporter method) register_module() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_module_backward_hook() (in module torch.nn.modules.module) register_module_buffer_registration_hook() (in module torch.nn.modules.module) register_module_forward_hook() (in module torch.nn.modules.module) register_module_forward_pre_hook() (in module torch.nn.modules.module) register_module_full_backward_hook() (in module torch.nn.modules.module) register_module_full_backward_pre_hook() (in module torch.nn.modules.module) register_module_module_registration_hook() (in module torch.nn.modules.module) register_module_parameter_registration_hook() (in module torch.nn.modules.module) register_multi_grad_hook (class in torch.autograd.graph) register_package() (in module torch.serialization) register_parameter() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) register_parametrization() (in module torch.nn.utils.parametrize) register_post_accumulate_grad_hook() (torch.Tensor method) register_prehook() (torch.autograd.graph.Node method) register_sharding() (in module torch.distributed.tensor.experimental) register_state_dict_post_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) register_state_dict_pre_hook() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) register_step_post_hook() (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad", "prev_chunk_id": "chunk_1124", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1126", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "R", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "R", "content": "method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) | register_step_pre_hook() (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) register_timers() (torch.distributed.elastic.timer.TimerServer method) register_torch_dispatch() (in module torch.library) register_vmap() (in module torch.library) reify() (torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs method) (torch.distributed.elastic.multiprocessing.api.LogsSpecs method) rekey_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) RelaxedBernoulli (class in torch.distributions.relaxed_bernoulli) RelaxedOneHotCategorical (class in torch.distributions.relaxed_categorical) RelaxedUnspecConstraint (class in torch.fx.experimental.symbolic_shapes) release() (torch.distributed.elastic.timer.TimerClient method) (torch.onnx.ONNXProgram method) ReLU (class in torch.nn) relu() (in module torch.nn.functional) ReLU6 (class in torch.ao.nn.quantized) (class in torch.nn) relu6() (in module torch.nn.functional) relu_() (in module torch.nn.functional) remainder() (in module torch) (torch.Tensor method) remainder_() (torch.Tensor method) remote() (in module torch.distributed.rpc) (torch.distributed.rpc.PyRRef method) remote_parameters() (torch.distributed.nn.api.remote_module.RemoteModule method) RemoteModule (class in torch.distributed.nn.api.remote_module) remove() (in module torch.nn.utils.prune) (torch.nn.utils.prune.BasePruningMethod method) (torch.nn.utils.prune.CustomFromMask method) (torch.nn.utils.prune.Identity method) (torch.nn.utils.prune.L1Unstructured method) (torch.nn.utils.prune.LnStructured method) (torch.nn.utils.prune.PruningContainer method) (torch.nn.utils.prune.RandomStructured method) (torch.nn.utils.prune.RandomUnstructured method) remove_parametrizations() (in module torch.nn.utils.parametrize) remove_spectral_norm() (in module torch.nn.utils) remove_weight_norm() (in module torch.nn.utils) rename() (torch.Tensor method) rename_() (torch.Tensor method) rename_privateuse1_backend() (in module torch.utils) render() (torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint method) RendezvousBackend (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous) RendezvousClosedError (class in torch.distributed.elastic.rendezvous.api) RendezvousConnectionError (class in torch.distributed.elastic.rendezvous.api) RendezvousError (class in torch.distributed.elastic.rendezvous.api) RendezvousGracefulExitError (class in torch.distributed.elastic.rendezvous.api) RendezvousHandler (class in torch.distributed.elastic.rendezvous) RendezvousHandlerRegistry (class in torch.distributed.elastic.rendezvous) RendezvousInfo (class in torch.distributed.elastic.rendezvous) RendezvousParameters (class in torch.distributed.elastic.rendezvous) RendezvousStateError (class in torch.distributed.elastic.rendezvous.api) RendezvousStoreInfo (class in torch.distributed.elastic.rendezvous.api) RendezvousTimeout (class in torch.distributed.elastic.rendezvous.dynamic_rendezvous) RendezvousTimeoutError (class in torch.distributed.elastic.rendezvous.api) renorm() (in module torch) (torch.Tensor method) renorm_() (torch.Tensor method) repeat() (torch.Tensor method) repeat_interleave() (in module torch) (torch.Tensor method) replace() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) replace_all_batch_norm_modules_() (in module torch.func) replace_all_uses() (torch.export.graph_signature.ExportGraphSignature method) replace_all_uses_with() (torch.fx.Node method) replace_input_with() (torch.fx.Node method) replace_pattern() (in module torch.fx) replay() (torch.cuda.CUDAGraph method) Replicate (class in torch.distributed.tensor.placement_types) ReplicationPad1d (class in torch.nn) ReplicationPad2d (class in torch.nn) ReplicationPad3d (class in torch.nn) requires_grad (torch.Tensor attribute) requires_grad_() (torch.jit.ScriptModule method) (torch.nn.Module", "prev_chunk_id": "chunk_1125", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1127", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "R", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "R", "content": "method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) reset() (in module torch.compiler) (torch.cuda.CUDAGraph method) (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) (torch.distributed.checkpoint.StorageWriter method) (torch.quasirandom.SobolEngine method) reset_accumulated_host_memory_stats() (in module torch.cuda.memory) reset_accumulated_memory_stats() (in module torch.cuda.memory) (in module torch.xpu.memory) reset_max_memory_allocated() (in module torch.cuda.memory) reset_max_memory_cached() (in module torch.cuda.memory) reset_min_max_vals() (torch.ao.quantization.observer.MinMaxObserver method) (torch.ao.quantization.observer.PerChannelMinMaxObserver method) reset_parameters() (torch.nn.modules.normalization.RMSNorm method) (torch.nn.RMSNorm method) reset_peak_host_memory_stats() (in module torch.cuda.memory) reset_peak_memory_stats() (in module torch.cuda.memory) (in module torch.xpu.memory) reshape() (in module torch) (torch.Tensor method) reshape_as() (torch.Tensor method) ReshapeTransform (class in torch.distributions.transforms) reshard() (torch.distributed.fsdp.FSDPModule method) resizable() (torch.TypedStorage method) (torch.UntypedStorage method) resize_() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) resize_as_() (torch.Tensor method) resolve_bytes() (torch.distributed.checkpoint.LoadPlanner method) resolve_conj() (in module torch) (torch.Tensor method) resolve_data() (torch.distributed.checkpoint.SavePlanner method) resolve_name() (in module torch.overrides) resolve_neg() (in module torch) (torch.Tensor method) resolve_tensor() (torch.distributed.checkpoint.LoadPlanner method) resolve_unbacked_bindings() (in module torch.fx.experimental.symbolic_shapes) result() (torch.distributed.Work method) result_type() (in module torch) retain_grad() (torch.Tensor method) retains_grad (torch.Tensor attribute) rewrite_with_congruences() (torch.fx.experimental.symbolic_shapes.DimConstraints method) rfft() (in module torch.fft) rfft2() (in module torch.fft) rfftfreq() (in module torch.fft) rfftn() (in module torch.fft) right_inverse() (torch.nn.utils.parametrize.ParametrizationList method) rms_norm() (in module torch.nn.functional) RMSNorm (class in torch.nn) (class in torch.nn.modules.normalization) RMSprop (class in torch.optim) RNN (class in torch.nn) RNNBase (class in torch.nn) RNNCell (class in torch.ao.nn.quantized.dynamic) (class in torch.nn) roll() (in module torch) (torch.Tensor method) rot90() (in module torch) (torch.Tensor method) rotary_embedding() (in module torch.onnx.ops) round() (in module torch) (in module torch.special) (torch.Tensor method) round_() (torch.Tensor method) row_indices() (torch.Tensor method) row_stack() (in module torch) RowwiseParallel (class in torch.distributed.tensor.parallel) rpc_async() (in module torch.distributed.rpc) (torch.distributed.rpc.PyRRef method) rpc_sync() (in module torch.distributed.rpc) (torch.distributed.rpc.PyRRef method) rpc_timeout (torch.distributed.rpc.RpcBackendOptions property) (torch.distributed.rpc.TensorPipeRpcBackendOptions property) RpcBackendOptions (class in torch.distributed.rpc) Rprop (class in torch.optim) RReLU (class in torch.nn) rrelu() (in module torch.nn.functional) rrelu_() (in module torch.nn.functional) rsample() (torch.distributions.beta.Beta method) (torch.distributions.cauchy.Cauchy method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.dirichlet.Dirichlet method) (torch.distributions.distribution.Distribution method) (torch.distributions.exponential.Exponential method) (torch.distributions.fishersnedecor.FisherSnedecor method) (torch.distributions.gamma.Gamma method) (torch.distributions.generalized_pareto.GeneralizedPareto method) (torch.distributions.independent.Independent method) (torch.distributions.laplace.Laplace method) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal method) (torch.distributions.multivariate_normal.MultivariateNormal method) (torch.distributions.normal.Normal method) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli method) (torch.distributions.studentT.StudentT method) (torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.uniform.Uniform method) (torch.distributions.wishart.Wishart method) rsqrt()", "prev_chunk_id": "chunk_1126", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1128", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "R", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "R", "content": "(in module torch) (torch.Tensor method) rsqrt_() (torch.Tensor method) run() (torch.distributed.elastic.agent.server.ElasticAgent method) (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) run_decompositions() (torch.export.ExportedProgram method) run_node() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts method) (torch.fx.Interpreter method) RunProcsResult (class in torch.distributed.elastic.multiprocessing.api) RunResult (class in torch.distributed.elastic.agent.server.api)", "prev_chunk_id": "chunk_1127", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1129", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "S safe_globals (class in torch.serialization) sample() (torch.distributions.bernoulli.Bernoulli method) (torch.distributions.binomial.Binomial method) (torch.distributions.categorical.Categorical method) (torch.distributions.continuous_bernoulli.ContinuousBernoulli method) (torch.distributions.distribution.Distribution method) (torch.distributions.geometric.Geometric method) (torch.distributions.independent.Independent method) (torch.distributions.lkj_cholesky.LKJCholesky method) (torch.distributions.mixture_same_family.MixtureSameFamily method) (torch.distributions.multinomial.Multinomial method) (torch.distributions.negative_binomial.NegativeBinomial method) (torch.distributions.normal.Normal method) (torch.distributions.one_hot_categorical.OneHotCategorical method) (torch.distributions.poisson.Poisson method) (torch.distributions.transformed_distribution.TransformedDistribution method) (torch.distributions.von_mises.VonMises method) sample_n() (torch.distributions.distribution.Distribution method) sampled_addmm() (in module torch.sparse) Sampler (class in torch.utils.data) save() (in module torch) (in module torch.distributed.checkpoint.state_dict_saver) (in module torch.export) (in module torch.jit) (torch.jit.ScriptFunction method) (torch.jit.ScriptModule method) (torch.onnx.ONNXProgram method) save_binary() (torch.package.PackageExporter method) save_for_backward() (torch.autograd.function.BackwardCFunction method) (torch.autograd.function.FunctionCtx method) (torch.autograd.function.InplaceFunction method) (torch.autograd.function.NestedIOFunction method) save_for_forward() (torch.autograd.function.BackwardCFunction method) (torch.autograd.function.InplaceFunction method) (torch.autograd.function.NestedIOFunction method) save_module() (torch.package.PackageExporter method) save_on_cpu (class in torch.autograd.graph) save_pickle() (torch.package.PackageExporter method) save_source_file() (torch.package.PackageExporter method) save_source_string() (torch.package.PackageExporter method) save_state_dict() (in module torch.distributed.checkpoint.state_dict_saver) save_storage() (torch.cuda.gds.GdsFile method) save_text() (torch.package.PackageExporter method) save_to_buffer() (torch.jit.ScriptFunction method) saved_tensors (torch.autograd.function.NestedIOFunction property) saved_tensors_hooks (class in torch.autograd.graph) SavePlan (class in torch.distributed.checkpoint) SavePlanner (class in torch.distributed.checkpoint) scalar_name() (torch.onnx.JitScalarType method) scale (torch.distributions.half_cauchy.HalfCauchy property) (torch.distributions.half_normal.HalfNormal property) (torch.distributions.log_normal.LogNormal property) scale_fn() (torch.optim.lr_scheduler.CyclicLR method) scale_tril (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.wishart.Wishart property) scaled_dot_product_attention() (in module torch.nn.functional) scaled_modified_bessel_k0() (in module torch.special) scaled_modified_bessel_k1() (in module torch.special) scatter() (in module torch) (in module torch.cuda.comm) (in module torch.distributed) (torch.Tensor method) scatter_() (torch.Tensor method) scatter_add() (in module torch) (torch.Tensor method) scatter_add_() (torch.Tensor method) scatter_full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) scatter_object_list() (in module torch.distributed) scatter_reduce() (in module torch) (torch.Tensor method) scatter_reduce_() (torch.Tensor method) schedule() (in module torch.profiler) Schedule1F1B (class in torch.distributed.pipelining.schedules) ScheduleGPipe (class in torch.distributed.pipelining.schedules) ScheduleInterleaved1F1B (class in torch.distributed.pipelining.schedules) ScheduleInterleavedZeroBubble (class in torch.distributed.pipelining.schedules) ScheduleLoopedBFS (class in torch.distributed.pipelining.schedules) ScheduleZBVZeroBubble (class in torch.distributed.pipelining.schedules) script() (in module torch.jit) script_if_tracing() (in module torch.jit) ScriptFunction (class in torch.jit) ScriptModule (class in torch.jit) sdp_kernel() (in module torch.backends.cuda) sdpa_kernel() (in module torch.nn.attention) SDPAParams (class in torch.backends.cuda) SDPBackend (class in torch.nn.attention) searchsorted() (in module torch) see() (torch.autograd.profiler.EnforceUnique method) seed() (in module torch) (in module torch.cuda) (in module torch.mps) (in module torch.random) (in module torch.xpu) (torch.Generator method) seed_all() (in module torch.cuda) (in module torch.xpu) select() (in module torch)", "prev_chunk_id": "chunk_1128", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1130", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "(torch.Tensor method) select_model_mode_for_export() (in module torch.onnx) select_scatter() (in module torch) (torch.Tensor method) SelectiveCheckpointContext (class in torch.utils.checkpoint) self_cpu_time_total (torch.autograd.profiler.profile property) SELU (class in torch.nn) selu() (in module torch.nn.functional) send() (in module torch.distributed) send_object_list() (in module torch.distributed) seq_lengths (torch.nn.attention.flex_attention.BlockMask attribute) SequenceParallel (class in torch.distributed.tensor.parallel) Sequential (class in torch.nn) SequentialLR (class in torch.optim.lr_scheduler) SequentialSampler (class in torch.utils.data) set() (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method) (torch.distributed.Store method) set_() (torch.Tensor method) set_all_reduce_hook() (torch.distributed.fsdp.FSDPModule method) set_allocate_memory_from_process_group_for_comm() (torch.distributed.fsdp.FSDPModule method) set_backend_pattern_config() (torch.ao.quantization.backend_config.BackendConfig method) set_backend_pattern_configs() (torch.ao.quantization.backend_config.BackendConfig method) set_buffer() (in module torch.distributed.GradBucket) set_checkpoint_debug_enabled() (in module torch.utils.checkpoint) set_closed() (torch.distributed.elastic.rendezvous.RendezvousHandler method) set_codegen() (torch.fx.Graph method) set_crc32_options() (in module torch.serialization) set_custom_trace_id_callback() (torch.profiler.profile method) set_default_device() (in module torch) set_default_dtype() (in module torch) set_default_load_endianness() (in module torch.serialization) set_default_mmap_options() (in module torch.serialization) set_default_tensor_type() (in module torch) set_default_validate_args() (torch.distributions.distribution.Distribution static method) set_detect_anomaly (class in torch.autograd) set_deterministic_debug_mode() (in module torch) set_device() (in module torch.cpu) (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) set_device_idx() (in module torch.accelerator) set_device_index() (in module torch.accelerator) set_device_map() (torch.distributed.rpc.TensorPipeRpcBackendOptions method) set_devices() (torch.distributed.rpc.TensorPipeRpcBackendOptions method) set_dir() (in module torch.hub) set_dtype_configs() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_enable_guard_collectives() (in module torch.compiler) set_exception() (torch.futures.Future method) set_extra_state() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) set_fastpath_enabled() (in module torch.backends.mha) set_filename() (in module torch.cuda.tunable) set_flags() (in module torch.backends.nnpack) set_float32_matmul_precision() (in module torch) set_float_to_observed_mapping() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_flush_denormal() (in module torch) set_force_sum_reduction_for_comms() (torch.distributed.fsdp.FSDPModule method) set_fused_module() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_fuser_method() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_fusion_strategy() (in module torch.jit) set_global() (torch.ao.quantization.qconfig_mapping.QConfigMapping method) set_grad_enabled (class in torch.autograd.grad_mode) set_gradient_divide_factor() (torch.distributed.fsdp.FSDPModule method) set_input_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_is_last_backward() (torch.distributed.fsdp.FSDPModule method) set_kernel_enabled() (torch._library.custom_ops.CustomOpDef method) set_logs() (in module torch._logging) set_materialize_grads() (torch.autograd.function.BackwardCFunction method) (torch.autograd.function.FunctionCtx method) (torch.autograd.function.InplaceFunction method) (torch.autograd.function.NestedIOFunction method) set_max_tuning_duration() (in module torch.cuda.tunable) set_max_tuning_iterations() (in module torch.cuda.tunable) set_model_state_dict() (in module torch.distributed.checkpoint.state_dict) set_module() (in module torch.utils) set_module_name() (torch.ao.quantization.qconfig_mapping.QConfigMapping method) set_module_name_object_type_order() (torch.ao.quantization.qconfig_mapping.QConfigMapping method) set_module_name_regex() (torch.ao.quantization.qconfig_mapping.QConfigMapping method) set_modules_to_backward_prefetch() (torch.distributed.fsdp.FSDPModule method) set_modules_to_forward_prefetch() (torch.distributed.fsdp.FSDPModule method) set_multithreading_enabled (class in torch.autograd.grad_mode) set_name() (torch.ao.quantization.backend_config.BackendConfig method) set_non_traceable_module_classes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_non_traceable_module_names() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_num_interop_threads() (in module torch) set_num_threads() (in module torch) set_object_type() (torch.ao.quantization.qconfig_mapping.QConfigMapping method) set_observation_type() (torch.ao.quantization.backend_config.BackendPatternConfig", "prev_chunk_id": "chunk_1129", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1131", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "method) set_observed_to_quantized_mapping() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method) set_optimizer_state_dict() (in module torch.distributed.checkpoint.state_dict) set_output_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_overwrite_module_params_on_conversion() (in module torch.__future__) set_pattern() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_per_process_memory_fraction() (in module torch.cuda.memory) (in module torch.mps) set_post_optim_event() (torch.distributed.fsdp.FSDPModule method) set_preserved_attributes() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method) (torch.ao.quantization.fx.custom_config.FuseCustomConfig method) (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_printoptions() (in module torch) set_qat_module() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_reduce_scatter_divide_factor() (torch.distributed.fsdp.FSDPModule method) set_reference_quantized_module() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_requires_all_reduce() (torch.distributed.fsdp.FSDPModule method) set_requires_gradient_sync() (torch.distributed.fsdp.FSDPModule method) set_reshard_after_backward() (torch.distributed.fsdp.FSDPModule method) set_reshard_after_forward() (torch.distributed.fsdp.FSDPModule method) set_result() (torch.futures.Future method) set_rng_state() (in module torch) (in module torch.cuda) (in module torch.mps) (in module torch.mtia) (in module torch.random) (in module torch.xpu) set_rng_state_all() (in module torch.cuda) (in module torch.xpu) set_root_module() (torch.ao.quantization.backend_config.BackendPatternConfig method) set_rotating_buffer_size() (in module torch.cuda.tunable) set_sharing_strategy() (in module torch.multiprocessing) set_stance() (in module torch.compiler) set_standalone_module_class() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_standalone_module_name() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) set_state() (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend method) (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend method) (torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend method) (torch.Generator method) set_state_dict() (in module torch.distributed.checkpoint.state_dict) set_state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method) set_stream() (in module torch.accelerator) (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) set_submodule() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) set_swap_module_params_on_conversion() (in module torch.__future__) set_sync_debug_mode() (in module torch.cuda) set_timeout() (torch.distributed.Store method) set_unbacked_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) set_unshard_in_backward() (torch.distributed.fsdp.FSDPModule method) set_up_planner() (torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner method) (torch.distributed.checkpoint.LoadPlanner method) (torch.distributed.checkpoint.SavePlanner method) set_up_storage_reader() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader method) (torch.distributed.checkpoint.StorageReader method) set_up_storage_writer() (torch.distributed.checkpoint.StorageWriter method) set_warn_always() (in module torch) setdefault() (torch.autograd.profiler_util.StringTable method) (torch.nn.ParameterDict method) setup_context() (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction static method) SGD (class in torch.optim) sgn() (in module torch) (torch.Tensor method) sgn_() (torch.Tensor method) Shadow (class in torch.ao.ns._numeric_suite) ShadowLogger (class in torch.ao.ns._numeric_suite) shape (torch.nn.attention.flex_attention.BlockMask property) (torch.Tensor attribute) ShapeEnv (class in torch.fx.experimental.symbolic_shapes) ShapeEnvSettings (class in torch.fx.experimental.symbolic_shapes) ShapesCollection (class in torch.export.dynamic_shapes) Shard (class in torch.distributed.tensor.placement_types) shard_full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel static method) ShardedOptimStateDictConfig (class in torch.distributed.fsdp) ShardedStateDictConfig (class in torch.distributed.fsdp) ShardingStrategy (class in torch.distributed.fsdp) share_memory() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) share_memory_() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) shifted_chebyshev_polynomial_t() (in module torch.special) shifted_chebyshev_polynomial_u() (in module torch.special) shifted_chebyshev_polynomial_v() (in module torch.special) shifted_chebyshev_polynomial_w() (in module torch.special) short() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) ShortStorage (class in torch) should_synchronize_after_execute (torch.distributed.checkpoint.staging.AsyncStager", "prev_chunk_id": "chunk_1130", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1132", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "property) show() (in module torch.__config__) shutdown() (in module torch.distributed.rpc) (torch.distributed.elastic.rendezvous.RendezvousHandler method) Sigmoid (class in torch.ao.nn.quantized) (class in torch.nn) sigmoid() (in module torch) (in module torch.nn.functional) (torch.Tensor method) sigmoid_() (torch.Tensor method) SigmoidTransform (class in torch.distributions.transforms) sign (torch.distributions.transforms.Transform property) sign() (in module torch) (torch.Tensor method) sign_() (torch.Tensor method) signbit() (in module torch) (torch.Tensor method) significant_figures (torch.utils.benchmark.Measurement property) SiLU (class in torch.nn) silu() (in module torch.nn.functional) SimpleElasticAgent (class in torch.distributed.elastic.agent.server) simplify() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) sin() (in module torch) (torch.Tensor method) sin_() (torch.Tensor method) sinc() (in module torch) (in module torch.special) (torch.Tensor method) sinc_() (torch.Tensor method) sinh() (in module torch) (torch.Tensor method) sinh_() (torch.Tensor method) Size (class in torch) size (in module torch.backends.cuda.cufft_plan_cache) size() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) size_hint() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) skip_data (class in torch.serialization) skip_guard_on_all_nn_modules_unsafe() (in module torch.compiler) skip_guard_on_globals_unsafe() (in module torch.compiler) skip_guard_on_inbuilt_nn_modules_unsafe() (in module torch.compiler) skip_init() (in module torch.nn.utils) slice_scatter() (in module torch) (torch.Tensor method) slogdet() (in module torch) (in module torch.linalg) (torch.Tensor method) | smm() (in module torch) (torch.Tensor method) smooth_l1_loss() (in module torch.nn.functional) SmoothL1Loss (class in torch.nn) snapshot() (in module torch.mtia) (torch.cuda.memory.MemPool method) SobolEngine (class in torch.quasirandom) soft_margin_loss() (in module torch.nn.functional) SoftMarginLoss (class in torch.nn) Softmax (class in torch.nn) softmax() (in module torch) (in module torch.nn.functional) (in module torch.sparse) (in module torch.special) (torch.Tensor method) Softmax2d (class in torch.nn) SoftmaxTransform (class in torch.distributions.transforms) Softmin (class in torch.nn) softmin() (in module torch.nn.functional) Softplus (class in torch.nn) softplus() (in module torch.nn.functional) SoftplusTransform (class in torch.distributions.transforms) Softshrink (class in torch.nn) softshrink() (in module torch.nn.functional) Softsign (class in torch.nn) softsign() (in module torch.nn.functional) solve() (in module torch.linalg) (torch.fx.experimental.symbolic_shapes.DimConstraints method) solve_ex() (in module torch.linalg) solve_triangular() (in module torch.linalg) sort() (in module torch) (torch.Tensor method) sorted_indices (torch.nn.utils.rnn.PackedSequence attribute) source_rank() (torch.distributed.Work method) sparse_() (in module torch.nn.init) sparse_bsc_tensor() (in module torch) sparse_bsr_tensor() (in module torch) sparse_compressed_tensor() (in module torch) sparse_coo_tensor() (in module torch) sparse_csc_tensor() (in module torch) sparse_csr_tensor()", "prev_chunk_id": "chunk_1131", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1133", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "(in module torch) sparse_dim() (torch.Tensor method) sparse_mask() (torch.Tensor method) sparse_resize_() (torch.Tensor method) sparse_resize_and_clear_() (torch.Tensor method) SparseAdam (class in torch.optim) sparsity() (torch.nn.attention.flex_attention.BlockMask method) spawn() (in module torch.multiprocessing.spawn) SpawnContext (class in torch.multiprocessing) spdiags() (in module torch.sparse) Specialization (class in torch.fx.experimental.symbolic_shapes) spectral_norm() (in module torch.nn.utils) (in module torch.nn.utils.parametrizations) spherical_bessel_j0() (in module torch.special) split() (in module torch) (torch.Tensor method) split_args_kwargs_into_chunks() (in module torch.distributed.pipelining.microbatch) SplitPoint (class in torch.distributed.pipelining) spsolve() (in module torch.sparse) sqrt() (in module torch) (torch.Tensor method) sqrt_() (torch.Tensor method) square() (in module torch) (torch.Tensor method) square_() (torch.Tensor method) squeeze() (in module torch) (torch.Tensor method) squeeze_() (torch.Tensor method) sspaddmm() (in module torch) (torch.Tensor method) stack (in module torch.distributions.constraints) stack() (in module torch) stack_module_state() (in module torch.func) stack_trace (torch.fx.Node property) StackDataset (class in torch.utils.data) StackTransform (class in torch.distributions.transforms) stage() (torch.distributed.checkpoint.FileSystemWriter method) (torch.distributed.checkpoint.staging.AsyncStager method) (torch.distributed.checkpoint.staging.BlockingAsyncStager method) StandaloneModuleConfigEntry (class in torch.ao.quantization.fx.custom_config) start() (in module torch.mps.profiler) (torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer method) start_processes() (in module torch.distributed.elastic.multiprocessing) Stat (class in torch.monitor) state_dict (torch.export.ExportedProgram attribute) state_dict() (torch.distributed.checkpoint.stateful.Stateful method) (torch.distributed.optim.PostLocalSGDOptimizer method) (torch.distributed.optim.ZeroRedundancyOptimizer method) (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.lr_scheduler.ChainedScheduler method) (torch.optim.lr_scheduler.ConstantLR method) (torch.optim.lr_scheduler.CosineAnnealingLR method) (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method) (torch.optim.lr_scheduler.ExponentialLR method) (torch.optim.lr_scheduler.LambdaLR method) (torch.optim.lr_scheduler.LinearLR method) (torch.optim.lr_scheduler.LRScheduler method) (torch.optim.lr_scheduler.MultiplicativeLR method) (torch.optim.lr_scheduler.MultiStepLR method) (torch.optim.lr_scheduler.OneCycleLR method) (torch.optim.lr_scheduler.PolynomialLR method) (torch.optim.lr_scheduler.ReduceLROnPlateau method) (torch.optim.lr_scheduler.SequentialLR method) (torch.optim.lr_scheduler.StepLR method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) (torch.optim.swa_utils.SWALR method) state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel static method) StateDictConfig (class in torch.distributed.fsdp) StateDictOptions (class in torch.distributed.checkpoint.state_dict) StateDictSettings (class in torch.distributed.fsdp) Stateful (class in torch.distributed.checkpoint.stateful) StatefulSymbolicContext (class in torch.fx.experimental.symbolic_shapes) StatelessSymbolicContext (class in torch.fx.experimental.symbolic_shapes) statically_known_false() (in module torch.fx.experimental.symbolic_shapes) statically_known_true() (in module torch.fx.experimental.symbolic_shapes) stats() (torch.utils.benchmark.CallgrindStats method) std() (in module torch) (torch.Tensor method) std_mean() (in module torch) stddev (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential property) (torch.distributions.gumbel.Gumbel property) (torch.distributions.laplace.Laplace property) (torch.distributions.normal.Normal property) (torch.distributions.uniform.Uniform property) step() (torch.distributed.optim.DistributedOptimizer method) (torch.distributed.optim.PostLocalSGDOptimizer method) (torch.distributed.optim.ZeroRedundancyOptimizer method) (torch.distributed.pipelining.schedules.PipelineScheduleMulti method) (torch.distributed.pipelining.schedules.PipelineScheduleSingle", "prev_chunk_id": "chunk_1132", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1134", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.lr_scheduler.ChainedScheduler method) (torch.optim.lr_scheduler.ConstantLR method) (torch.optim.lr_scheduler.CosineAnnealingLR method) (torch.optim.lr_scheduler.CosineAnnealingWarmRestarts method) (torch.optim.lr_scheduler.CyclicLR method) (torch.optim.lr_scheduler.ExponentialLR method) (torch.optim.lr_scheduler.LambdaLR method) (torch.optim.lr_scheduler.LinearLR method) (torch.optim.lr_scheduler.LRScheduler method) (torch.optim.lr_scheduler.MultiplicativeLR method) (torch.optim.lr_scheduler.MultiStepLR method) (torch.optim.lr_scheduler.OneCycleLR method) (torch.optim.lr_scheduler.PolynomialLR method) (torch.optim.lr_scheduler.ReduceLROnPlateau method) (torch.optim.lr_scheduler.SequentialLR method) (torch.optim.lr_scheduler.StepLR method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.SWALR method) (torch.profiler.profile method) StepLR (class in torch.optim.lr_scheduler) stft() (in module torch) (torch.Tensor method) StickBreakingTransform (class in torch.distributions.transforms) stop() (in module torch.mps.profiler) (torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer method) storage() (torch.Tensor method) storage_meta() (torch.distributed.checkpoint.StorageWriter method) storage_offset() (torch.Tensor method) storage_type() (torch.Tensor method) StorageReader (class in torch.distributed.checkpoint) StorageWriter (class in torch.distributed.checkpoint) Store (class in torch.distributed) strategy (in module torch.backends.opt_einsum) Stream (class in torch) (class in torch.cpu) (class in torch.cuda) (class in torch.mtia) (class in torch.xpu) stream() (in module torch.cpu) (in module torch.cuda) (in module torch.mtia) (in module torch.xpu) StreamContext (class in torch.cpu) (class in torch.cuda) (class in torch.mtia) (class in torch.xpu) strict_fusion (class in torch.jit) StrictMinMaxConstraint (class in torch.fx.experimental.symbolic_shapes) stride() (torch.Tensor method) StringTable (class in torch.autograd.profiler_util) StudentT (class in torch.distributions.studentT) sub() (in module torch) (torch.Tensor method) sub_() (torch.Tensor method) SubclassSymbolicContext (class in torch.fx.experimental.symbolic_shapes) SubprocessContext (class in torch.distributed.elastic.multiprocessing.api) SubprocessHandler (class in torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler) Subset (class in torch.utils.data) SubsetRandomSampler (class in torch.utils.data) substitute_in_graph() (in module torch.compiler) subtract() (in module torch) (torch.Tensor method) subtract_() (torch.Tensor method) sum() (in module torch) (in module torch.sparse) (torch.Tensor method) sum_to_size() (torch.Tensor method) SummaryWriter (class in torch.utils.tensorboard.writer) summon_full_params() (torch.distributed.fsdp.FullyShardedDataParallel static method) support (torch.distributions.bernoulli.Bernoulli attribute) (torch.distributions.beta.Beta attribute) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.cauchy.Cauchy attribute) (torch.distributions.continuous_bernoulli.ContinuousBernoulli attribute) (torch.distributions.dirichlet.Dirichlet attribute) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential attribute) (torch.distributions.fishersnedecor.FisherSnedecor attribute) (torch.distributions.gamma.Gamma attribute) (torch.distributions.generalized_pareto.GeneralizedPareto property) (torch.distributions.geometric.Geometric attribute) (torch.distributions.gumbel.Gumbel attribute) (torch.distributions.half_cauchy.HalfCauchy attribute) (torch.distributions.half_normal.HalfNormal attribute) (torch.distributions.independent.Independent property) (torch.distributions.inverse_gamma.InverseGamma attribute) (torch.distributions.kumaraswamy.Kumaraswamy attribute) (torch.distributions.laplace.Laplace attribute) (torch.distributions.lkj_cholesky.LKJCholesky attribute) (torch.distributions.log_normal.LogNormal attribute) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal attribute) (torch.distributions.mixture_same_family.MixtureSameFamily property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.multivariate_normal.MultivariateNormal attribute) (torch.distributions.negative_binomial.NegativeBinomial attribute) (torch.distributions.normal.Normal attribute) (torch.distributions.one_hot_categorical.OneHotCategorical attribute) (torch.distributions.pareto.Pareto property) (torch.distributions.poisson.Poisson", "prev_chunk_id": "chunk_1133", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1135", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "S", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "S", "content": "attribute) (torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli attribute) (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute) (torch.distributions.studentT.StudentT attribute) (torch.distributions.transformed_distribution.TransformedDistribution property) (torch.distributions.uniform.Uniform property) (torch.distributions.von_mises.VonMises attribute) (torch.distributions.weibull.Weibull attribute) (torch.distributions.wishart.Wishart attribute) suppress_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv method) svd() (in module torch) (in module torch.linalg) (torch.Tensor method) svd_lowrank() (in module torch) svdvals() (in module torch.linalg) SWALR (class in torch.optim.swa_utils) swap_module (class in torch.ao.quantization) swap_tensors() (in module torch.utils) swapaxes() (in module torch) (torch.Tensor method) swapdims() (in module torch) (torch.Tensor method) SyclExtension() (in module torch.utils.cpp_extension) sym_and() (in module torch.fx.experimental.symbolic_shapes) sym_eq() (in module torch.fx.experimental.symbolic_shapes) sym_float() (in module torch) sym_fresh_size() (in module torch) sym_int() (in module torch) sym_ite() (in module torch) sym_max() (in module torch) sym_min() (in module torch) sym_not() (in module torch) sym_or() (in module torch.fx.experimental.symbolic_shapes) sym_sum() (in module torch) symbolic() (in module torch.onnx.ops) symbolic_multi_out() (in module torch.onnx.ops) symbolic_trace() (in module torch.fx) SymbolicContext (class in torch.fx.experimental.symbolic_shapes) SymBool (class in torch) SymBoolArgument (class in torch.export.graph_signature) SymFloat (class in torch) SymFloatArgument (class in torch.export.graph_signature) SymInt (class in torch) SymIntArgument (class in torch.export.graph_signature) SyncBatchNorm (class in torch.nn) synchronize() (in module torch.accelerator) (in module torch.cpu) (in module torch.cuda) (in module torch.mps) (in module torch.mtia) (in module torch.xpu) (torch.cuda.Event method) (torch.cuda.ExternalStream method) (torch.cuda.Stream method) (torch.distributed.Work method) (torch.Event method) (torch.mps.event.Event method) (torch.mtia.Event method) (torch.mtia.Stream method) (torch.Stream method) (torch.xpu.Event method) (torch.xpu.Stream method) synchronize_staging() (torch.distributed.checkpoint.staging.AsyncStager method) (torch.distributed.checkpoint.staging.BlockingAsyncStager method)", "prev_chunk_id": "chunk_1134", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1136", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "T T (torch.Tensor attribute) t() (in module torch) (torch.Tensor method) t_() (torch.Tensor method) Tag (class in torch) take() (in module torch) (torch.Tensor method) take_along_dim() (in module torch) (torch.Tensor method) tan() (in module torch) (torch.Tensor method) tan_() (torch.Tensor method) tangent (torch.autograd.forward_ad.UnpackedDualTensor attribute) Tanh (class in torch.nn) tanh() (in module torch) (in module torch.nn.functional) (torch.Tensor method) tanh_() (torch.Tensor method) Tanhshrink (class in torch.nn) tanhshrink() (in module torch.nn.functional) TanhTransform (class in torch.distributions.transforms) TCPStore (class in torch.distributed) temperature (torch.distributions.relaxed_bernoulli.RelaxedBernoulli property) (torch.distributions.relaxed_categorical.RelaxedOneHotCategorical property) temperature() (in module torch.cuda) Tensor (class in torch) tensor() (in module torch) tensor_split() (in module torch) (torch.Tensor method) tensor_storage_size() (torch.distributed.checkpoint.planner.WriteItem method) tensorboard_trace_handler() (in module torch.profiler) TensorboardEventHandler (class in torch.monitor) TensorChunkSpec (class in torch.distributed.pipelining.microbatch) TensorDataset (class in torch.utils.data) tensordot() (in module torch) tensorinv() (in module torch.linalg) TensorPipeRpcBackendOptions (class in torch.distributed.rpc) tensorsolve() (in module torch.linalg) then() (torch.futures.Future method) threshold (class in torch.ao.nn.quantized.functional) Threshold (class in torch.nn) threshold() (in module torch.nn.functional) threshold_() (in module torch.nn.functional) tile() (in module torch) (torch.Tensor method) timeit() (torch.utils.benchmark.Timer method) timeout (torch.distributed.Store property) Timer (class in torch.utils.benchmark) TimerClient (class in torch.distributed.elastic.timer) TimerRequest (class in torch.distributed.elastic.timer) TimerServer (class in torch.distributed.elastic.timer) timestamp (torch.monitor.Event property) to() (torch.jit.ScriptModule method) (torch.nn.attention.flex_attention.BlockMask method) (torch.nn.Module method) (torch.nn.utils.rnn.PackedSequence method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) to_bool() (torch.fx.Tracer method) to_dense() (torch.nn.attention.flex_attention.BlockMask method) (torch.Tensor method) to_dict() (torch.ao.quantization.backend_config.BackendConfig method) (torch.ao.quantization.backend_config.BackendPatternConfig method) (torch.ao.quantization.backend_config.DTypeConfig method) (torch.ao.quantization.fx.custom_config.ConvertCustomConfig method) (torch.ao.quantization.fx.custom_config.FuseCustomConfig method) (torch.ao.quantization.fx.custom_config.PrepareCustomConfig method) (torch.ao.quantization.qconfig_mapping.QConfigMapping method) to_dlpack() (in module torch.utils.dlpack) to_empty() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) to_folder() (torch.fx.GraphModule method) to_here() (torch.distributed.rpc.PyRRef method) to_local() (torch.distributed.tensor.DTensor method) to_mkldnn() (torch.Tensor method) to_padded_tensor() (in module torch.nested) to_sparse() (torch.Tensor method) to_sparse_bsc() (torch.Tensor method) to_sparse_bsr() (torch.Tensor method) to_sparse_coo() (torch.Tensor method) to_sparse_csc() (torch.Tensor method) to_sparse_csr() (torch.Tensor method) to_string() (torch.nn.attention.flex_attention.BlockMask method) toggle_collection_dynamic() (torch.profiler._KinetoProfile method) tolist() (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) topk() (in module torch) (torch.Tensor method) torch module torch.__config__ module torch.__future__ module torch._logging module torch.accelerator module torch.amp module torch.amp.autocast_mode module torch.amp.grad_scaler", "prev_chunk_id": "chunk_1135", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1137", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "module torch.ao module torch.ao.nn module torch.ao.nn.intrinsic module torch.ao.nn.intrinsic.modules module torch.ao.nn.intrinsic.modules.fused module torch.ao.nn.intrinsic.qat module torch.ao.nn.intrinsic.qat.modules module torch.ao.nn.intrinsic.qat.modules.conv_fused module torch.ao.nn.intrinsic.qat.modules.linear_fused module torch.ao.nn.intrinsic.qat.modules.linear_relu module torch.ao.nn.intrinsic.quantized module torch.ao.nn.intrinsic.quantized.dynamic module torch.ao.nn.intrinsic.quantized.dynamic.modules module torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu module torch.ao.nn.intrinsic.quantized.modules module torch.ao.nn.intrinsic.quantized.modules.bn_relu module torch.ao.nn.intrinsic.quantized.modules.conv_add module torch.ao.nn.intrinsic.quantized.modules.conv_relu module torch.ao.nn.intrinsic.quantized.modules.linear_relu module torch.ao.nn.qat module torch.ao.nn.qat.dynamic module torch.ao.nn.qat.dynamic.modules module torch.ao.nn.qat.dynamic.modules.linear module torch.ao.nn.qat.modules module torch.ao.nn.qat.modules.conv module torch.ao.nn.qat.modules.embedding_ops module torch.ao.nn.qat.modules.linear module torch.ao.nn.quantizable module torch.ao.nn.quantizable.modules module torch.ao.nn.quantizable.modules.activation module torch.ao.nn.quantizable.modules.rnn module torch.ao.nn.quantized module torch.ao.nn.quantized.dynamic module torch.ao.nn.quantized.dynamic.modules module torch.ao.nn.quantized.dynamic.modules.conv module torch.ao.nn.quantized.dynamic.modules.linear module torch.ao.nn.quantized.dynamic.modules.rnn module torch.ao.nn.quantized.functional module torch.ao.nn.quantized.modules module torch.ao.nn.quantized.modules.activation module torch.ao.nn.quantized.modules.batchnorm module torch.ao.nn.quantized.modules.conv module torch.ao.nn.quantized.modules.dropout module torch.ao.nn.quantized.modules.embedding_ops module torch.ao.nn.quantized.modules.functional_modules module torch.ao.nn.quantized.modules.linear module torch.ao.nn.quantized.modules.normalization module torch.ao.nn.quantized.modules.rnn module torch.ao.nn.quantized.modules.utils module torch.ao.nn.quantized.reference module torch.ao.nn.quantized.reference.modules module torch.ao.nn.quantized.reference.modules.conv module torch.ao.nn.quantized.reference.modules.linear module torch.ao.nn.quantized.reference.modules.rnn module torch.ao.nn.quantized.reference.modules.sparse module torch.ao.nn.quantized.reference.modules.utils module torch.ao.nn.sparse module torch.ao.nn.sparse.quantized module torch.ao.nn.sparse.quantized.dynamic module torch.ao.nn.sparse.quantized.dynamic.linear module torch.ao.nn.sparse.quantized.linear module torch.ao.nn.sparse.quantized.utils module torch.ao.ns module torch.ao.ns._numeric_suite module torch.ao.ns._numeric_suite_fx module torch.ao.ns.fx module torch.ao.ns.fx.graph_matcher module torch.ao.ns.fx.graph_passes module torch.ao.ns.fx.mappings module torch.ao.ns.fx.n_shadows_utils module torch.ao.ns.fx.ns_types module torch.ao.ns.fx.pattern_utils module torch.ao.ns.fx.qconfig_multi_mapping module torch.ao.ns.fx.utils module torch.ao.ns.fx.weight_utils module torch.ao.pruning module torch.ao.pruning.scheduler module torch.ao.pruning.scheduler.base_scheduler module torch.ao.pruning.scheduler.cubic_scheduler module torch.ao.pruning.scheduler.lambda_scheduler module torch.ao.pruning.sparsifier module torch.ao.pruning.sparsifier.base_sparsifier module torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier module torch.ao.pruning.sparsifier.utils module torch.ao.pruning.sparsifier.weight_norm_sparsifier module torch.ao.quantization module torch.ao.quantization.backend_config module torch.ao.quantization.backend_config.backend_config module torch.ao.quantization.backend_config.executorch module torch.ao.quantization.backend_config.fbgemm module torch.ao.quantization.backend_config.native module torch.ao.quantization.backend_config.observation_type module torch.ao.quantization.backend_config.onednn module torch.ao.quantization.backend_config.qnnpack module torch.ao.quantization.backend_config.tensorrt module torch.ao.quantization.backend_config.utils module torch.ao.quantization.backend_config.x86 module torch.ao.quantization.fake_quantize module torch.ao.quantization.fuse_modules module torch.ao.quantization.fuser_method_mappings module torch.ao.quantization.fx module torch.ao.quantization.fx.convert module torch.ao.quantization.fx.custom_config module torch.ao.quantization.fx.fuse module torch.ao.quantization.fx.fuse_handler module torch.ao.quantization.fx.graph_module module torch.ao.quantization.fx.lower_to_fbgemm module torch.ao.quantization.fx.lower_to_qnnpack module torch.ao.quantization.fx.lstm_utils module torch.ao.quantization.fx.match_utils module torch.ao.quantization.fx.pattern_utils module torch.ao.quantization.fx.prepare module torch.ao.quantization.fx.qconfig_mapping_utils module torch.ao.quantization.fx.quantize_handler module torch.ao.quantization.fx.tracer module torch.ao.quantization.fx.utils module torch.ao.quantization.observer module torch.ao.quantization.pt2e module torch.ao.quantization.pt2e.duplicate_dq_pass module torch.ao.quantization.pt2e.export_utils module torch.ao.quantization.pt2e.graph_utils module torch.ao.quantization.pt2e.lowering module torch.ao.quantization.pt2e.port_metadata_pass module torch.ao.quantization.pt2e.prepare module torch.ao.quantization.pt2e.qat_utils module torch.ao.quantization.pt2e.representation module torch.ao.quantization.pt2e.representation.rewrite module torch.ao.quantization.pt2e.utils module torch.ao.quantization.qconfig module torch.ao.quantization.qconfig_mapping module torch.ao.quantization.quant_type module torch.ao.quantization.quantization_mappings module torch.ao.quantization.quantize_fx module torch.ao.quantization.quantize_jit module torch.ao.quantization.quantize_pt2e module torch.ao.quantization.quantizer module torch.ao.quantization.quantizer.composable_quantizer module torch.ao.quantization.quantizer.embedding_quantizer module torch.ao.quantization.quantizer.quantizer module torch.ao.quantization.quantizer.utils module torch.ao.quantization.quantizer.x86_inductor_quantizer module torch.ao.quantization.quantizer.xnnpack_quantizer module torch.ao.quantization.quantizer.xnnpack_quantizer_utils module torch.ao.quantization.quantizer.xpu_inductor_quantizer module torch.ao.quantization.stubs module torch.ao.quantization.utils module torch.autograd module torch.autograd.anomaly_mode module torch.autograd.forward_ad module torch.autograd.function", "prev_chunk_id": "chunk_1136", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1138", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "module torch.autograd.functional module torch.autograd.grad_mode module torch.autograd.gradcheck module torch.autograd.graph module torch.autograd.profiler module torch.autograd.profiler_legacy module torch.autograd.profiler_util module torch.autograd.variable module torch.backends module torch.backends.cpu module torch.backends.cuda module torch.backends.cudnn module torch.backends.cudnn.rnn module torch.backends.cusparselt module torch.backends.kleidiai module torch.backends.mha module torch.backends.mkl module torch.backends.mkldnn module torch.backends.mps module torch.backends.nnpack module torch.backends.openmp module torch.backends.opt_einsum module torch.backends.quantized module torch.backends.xeon module torch.backends.xeon.run_cpu module torch.backends.xnnpack module torch.compiler module torch.compiler.config module torch.contrib module torch.cpu module torch.cpu.amp module torch.cpu.amp.autocast_mode module torch.cpu.amp.grad_scaler module torch.cuda module torch.cuda._sanitizer module torch.cuda.amp module torch.cuda.amp.autocast_mode module torch.cuda.amp.common module torch.cuda.amp.grad_scaler module torch.cuda.comm module torch.cuda.error module torch.cuda.gds module torch.cuda.graphs module torch.cuda.jiterator module torch.cuda.memory module torch.cuda.nccl module torch.cuda.nvtx module torch.cuda.profiler module torch.cuda.random module torch.cuda.sparse module torch.cuda.streams module torch.cuda.tunable module torch.distributed module torch.distributed.algorithms module torch.distributed.algorithms.ddp_comm_hooks module torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook module torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks module torch.distributed.algorithms.ddp_comm_hooks.default_hooks module torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks module torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks module torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook module torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook module torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks module torch.distributed.algorithms.join module torch.distributed.algorithms.model_averaging module torch.distributed.algorithms.model_averaging.averagers module torch.distributed.algorithms.model_averaging.hierarchical_model_averager module torch.distributed.algorithms.model_averaging.utils module torch.distributed.argparse_util module torch.distributed.autograd module torch.distributed.c10d_logger module torch.distributed.checkpoint module torch.distributed.checkpoint.api module torch.distributed.checkpoint.default_planner module torch.distributed.checkpoint.filesystem module torch.distributed.checkpoint.format_utils module torch.distributed.checkpoint.hf_storage module torch.distributed.checkpoint.logger module torch.distributed.checkpoint.logging_handlers module torch.distributed.checkpoint.metadata module torch.distributed.checkpoint.optimizer module torch.distributed.checkpoint.planner module torch.distributed.checkpoint.planner_helpers module torch.distributed.checkpoint.resharding module torch.distributed.checkpoint.staging module torch.distributed.checkpoint.state_dict module torch.distributed.checkpoint.state_dict_loader module torch.distributed.checkpoint.state_dict_saver module torch.distributed.checkpoint.stateful module torch.distributed.checkpoint.storage module torch.distributed.checkpoint.utils module torch.distributed.collective_utils module torch.distributed.constants module torch.distributed.device_mesh module torch.distributed.distributed_c10d module torch.distributed.elastic module torch.distributed.elastic.agent module torch.distributed.elastic.agent.server module torch.distributed.elastic.agent.server.api module torch.distributed.elastic.agent.server.health_check_server module torch.distributed.elastic.agent.server.local_elastic_agent module torch.distributed.elastic.control_plane module torch.distributed.elastic.events module torch.distributed.elastic.events.api module torch.distributed.elastic.events.handlers module torch.distributed.elastic.metrics module torch.distributed.elastic.metrics.api module torch.distributed.elastic.multiprocessing module torch.distributed.elastic.multiprocessing.api module torch.distributed.elastic.multiprocessing.errors module torch.distributed.elastic.multiprocessing.errors.error_handler module torch.distributed.elastic.multiprocessing.errors.handlers module torch.distributed.elastic.multiprocessing.redirects module torch.distributed.elastic.multiprocessing.subprocess_handler module torch.distributed.elastic.multiprocessing.subprocess_handler.handlers module torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler module torch.distributed.elastic.multiprocessing.tail_log module torch.distributed.elastic.rendezvous module torch.distributed.elastic.rendezvous.api module torch.distributed.elastic.rendezvous.c10d_rendezvous_backend module torch.distributed.elastic.rendezvous.dynamic_rendezvous module torch.distributed.elastic.rendezvous.etcd_rendezvous module torch.distributed.elastic.rendezvous.etcd_rendezvous_backend module torch.distributed.elastic.rendezvous.etcd_server module torch.distributed.elastic.rendezvous.etcd_store module torch.distributed.elastic.rendezvous.registry module torch.distributed.elastic.rendezvous.static_tcp_rendezvous module torch.distributed.elastic.rendezvous.utils module torch.distributed.elastic.timer module torch.distributed.elastic.timer.api module torch.distributed.elastic.timer.debug_info_logging module torch.distributed.elastic.timer.file_based_local_timer module torch.distributed.elastic.timer.local_timer module torch.distributed.elastic.utils module torch.distributed.elastic.utils.api module torch.distributed.elastic.utils.data module torch.distributed.elastic.utils.data.cycling_iterator module torch.distributed.elastic.utils.data.elastic_distributed_sampler module torch.distributed.elastic.utils.distributed module torch.distributed.elastic.utils.log_level module torch.distributed.elastic.utils.logging module torch.distributed.elastic.utils.store module torch.distributed.fsdp module torch.distributed.fsdp.api module torch.distributed.fsdp.fully_sharded_data_parallel module torch.distributed.fsdp.sharded_grad_scaler module torch.distributed.fsdp.wrap module torch.distributed.launch module torch.distributed.launcher module torch.distributed.launcher.api", "prev_chunk_id": "chunk_1137", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1139", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "module torch.distributed.logging_handlers module torch.distributed.nn module torch.distributed.nn.api module torch.distributed.nn.api.remote_module module torch.distributed.nn.functional module torch.distributed.nn.jit module torch.distributed.nn.jit.instantiator module torch.distributed.nn.jit.templates module torch.distributed.nn.jit.templates.remote_module_template module torch.distributed.optim module torch.distributed.optim.apply_optimizer_in_backward module torch.distributed.optim.functional_adadelta module torch.distributed.optim.functional_adagrad module torch.distributed.optim.functional_adam module torch.distributed.optim.functional_adamax module torch.distributed.optim.functional_adamw module torch.distributed.optim.functional_rmsprop module torch.distributed.optim.functional_rprop module torch.distributed.optim.functional_sgd module torch.distributed.optim.named_optimizer module torch.distributed.optim.optimizer module torch.distributed.optim.post_localSGD_optimizer module torch.distributed.optim.utils module torch.distributed.optim.zero_redundancy_optimizer module torch.distributed.pipelining module torch.distributed.pipelining.microbatch module torch.distributed.pipelining.schedules module torch.distributed.pipelining.stage module torch.distributed.remote_device module torch.distributed.rendezvous module torch.distributed.rpc module torch.distributed.rpc.api module torch.distributed.rpc.backend_registry module torch.distributed.rpc.constants module torch.distributed.rpc.functions module torch.distributed.rpc.internal module torch.distributed.rpc.options module torch.distributed.rpc.rref_proxy module torch.distributed.rpc.server_process_global_profiler module torch.distributed.run module torch.distributed.tensor module torch.distributed.tensor.debug module torch.distributed.tensor.device_mesh module torch.distributed.tensor.experimental module torch.distributed.tensor.parallel module torch.distributed.tensor.parallel.api module torch.distributed.tensor.parallel.ddp module torch.distributed.tensor.parallel.fsdp module torch.distributed.tensor.parallel.input_reshard module torch.distributed.tensor.parallel.loss module torch.distributed.tensor.parallel.style module torch.distributed.tensor.placement_types module torch.distributed.utils module torch.distributions module torch.distributions.bernoulli module torch.distributions.beta module torch.distributions.binomial module torch.distributions.categorical module torch.distributions.cauchy module torch.distributions.chi2 module torch.distributions.constraint_registry module torch.distributions.constraints module torch.distributions.continuous_bernoulli module torch.distributions.dirichlet module torch.distributions.distribution module torch.distributions.exp_family module torch.distributions.exponential module torch.distributions.fishersnedecor module torch.distributions.gamma module torch.distributions.generalized_pareto module torch.distributions.geometric module torch.distributions.gumbel module torch.distributions.half_cauchy module torch.distributions.half_normal module torch.distributions.independent module torch.distributions.inverse_gamma module torch.distributions.kl module torch.distributions.kumaraswamy module torch.distributions.laplace module torch.distributions.lkj_cholesky module torch.distributions.log_normal module torch.distributions.logistic_normal module torch.distributions.lowrank_multivariate_normal module torch.distributions.mixture_same_family module torch.distributions.multinomial module torch.distributions.multivariate_normal module torch.distributions.negative_binomial module torch.distributions.normal module torch.distributions.one_hot_categorical module torch.distributions.pareto module torch.distributions.poisson module torch.distributions.relaxed_bernoulli module torch.distributions.relaxed_categorical module torch.distributions.studentT module torch.distributions.transformed_distribution module torch.distributions.transforms module torch.distributions.uniform module torch.distributions.utils module torch.distributions.von_mises module torch.distributions.weibull module torch.distributions.wishart module torch.export module torch.export.custom_obj module torch.export.custom_ops module torch.export.decomp_utils module torch.export.dynamic_shapes module | torch.export.experimental module torch.export.exported_program module torch.export.graph_signature module torch.export.passes module torch.export.pt2_archive module torch.export.pt2_archive.constants module torch.export.unflatten module torch.fft module torch.finfo (class in torch) torch.func module torch.functional module torch.futures module torch.fx module torch.fx.annotate module torch.fx.config module torch.fx.experimental module torch.fx.experimental.accelerator_partitioner module torch.fx.experimental.const_fold module torch.fx.experimental.debug module torch.fx.experimental.graph_gradual_typechecker module torch.fx.experimental.merge_matmul module torch.fx.experimental.meta_tracer module torch.fx.experimental.migrate_gradual_types module torch.fx.experimental.migrate_gradual_types.constraint module torch.fx.experimental.migrate_gradual_types.constraint_generator module torch.fx.experimental.migrate_gradual_types.constraint_transformation module torch.fx.experimental.migrate_gradual_types.operation module torch.fx.experimental.migrate_gradual_types.transform_to_z3 module torch.fx.experimental.migrate_gradual_types.util module torch.fx.experimental.migrate_gradual_types.z3_types module torch.fx.experimental.normalize module torch.fx.experimental.optimization module torch.fx.experimental.partitioner_utils module torch.fx.experimental.proxy_tensor module torch.fx.experimental.recording module torch.fx.experimental.refinement_types module torch.fx.experimental.rewriter module torch.fx.experimental.schema_type_annotation module torch.fx.experimental.sym_node module torch.fx.experimental.symbolic_shapes module torch.fx.experimental.unification module torch.fx.experimental.unification.core module", "prev_chunk_id": "chunk_1138", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1140", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "torch.fx.experimental.unification.dispatch module torch.fx.experimental.unification.match module torch.fx.experimental.unification.more module torch.fx.experimental.unification.multipledispatch module torch.fx.experimental.unification.multipledispatch.conflict module torch.fx.experimental.unification.multipledispatch.core module torch.fx.experimental.unification.multipledispatch.dispatcher module torch.fx.experimental.unification.multipledispatch.utils module torch.fx.experimental.unification.multipledispatch.variadic module torch.fx.experimental.unification.unification_tools module torch.fx.experimental.unification.utils module torch.fx.experimental.unification.variable module torch.fx.experimental.unify_refinements module torch.fx.experimental.validator module torch.fx.graph module torch.fx.graph_module module torch.fx.immutable_collections module torch.fx.interpreter module torch.fx.node module torch.fx.operator_schemas module torch.fx.passes module torch.fx.passes.annotate_getitem_nodes module torch.fx.passes.backends module torch.fx.passes.backends.cudagraphs module torch.fx.passes.dialect module torch.fx.passes.dialect.common module torch.fx.passes.dialect.common.cse_pass module torch.fx.passes.fake_tensor_prop module torch.fx.passes.graph_drawer module torch.fx.passes.graph_manipulation module torch.fx.passes.graph_transform_observer module torch.fx.passes.infra module torch.fx.passes.infra.partitioner module torch.fx.passes.infra.pass_base module torch.fx.passes.infra.pass_manager module torch.fx.passes.net_min_base module torch.fx.passes.operator_support module torch.fx.passes.param_fetch module torch.fx.passes.pass_manager module torch.fx.passes.reinplace module torch.fx.passes.runtime_assert module torch.fx.passes.shape_prop module torch.fx.passes.split_module module torch.fx.passes.split_utils module torch.fx.passes.splitter_base module torch.fx.passes.tests module torch.fx.passes.tests.test_pass_manager module torch.fx.passes.tools_common module torch.fx.passes.utils module torch.fx.passes.utils.common module torch.fx.passes.utils.fuser_utils module torch.fx.passes.utils.matcher_utils module torch.fx.passes.utils.matcher_with_name_node_map_utils module torch.fx.passes.utils.source_matcher_utils module torch.fx.proxy module torch.fx.subgraph_rewriter module torch.fx.tensor_type module torch.fx.traceback module torch.hub module torch.iinfo (class in torch) torch.jit module torch.jit.annotations module torch.jit.frontend module torch.jit.generate_bytecode module torch.jit.mobile module torch.jit.quantized module torch.jit.supported_ops module torch.jit.unsupported_tensor_ops module torch.library module torch.linalg module torch.masked module torch.masked.maskedtensor module torch.masked.maskedtensor.binary module torch.masked.maskedtensor.core module torch.masked.maskedtensor.creation module torch.masked.maskedtensor.passthrough module torch.masked.maskedtensor.reductions module torch.masked.maskedtensor.unary module torch.monitor module torch.mps module torch.mps.event module torch.mps.profiler module torch.mtia module torch.mtia.memory module torch.multiprocessing module torch.multiprocessing.pool module torch.multiprocessing.queue module torch.multiprocessing.reductions module torch.multiprocessing.spawn module torch.nested module torch.nn module torch.nn.attention module torch.nn.attention.bias module torch.nn.attention.experimental module torch.nn.attention.flex_attention module torch.nn.backends module torch.nn.backends.thnn module torch.nn.common_types module torch.nn.cpp module torch.nn.functional module torch.nn.grad module torch.nn.init module torch.nn.intrinsic module torch.nn.intrinsic.modules module torch.nn.intrinsic.modules.fused module torch.nn.intrinsic.qat module torch.nn.intrinsic.qat.modules module torch.nn.intrinsic.qat.modules.conv_fused module torch.nn.intrinsic.qat.modules.linear_fused module torch.nn.intrinsic.qat.modules.linear_relu module torch.nn.intrinsic.quantized module torch.nn.intrinsic.quantized.dynamic module torch.nn.intrinsic.quantized.dynamic.modules module torch.nn.intrinsic.quantized.dynamic.modules.linear_relu module torch.nn.intrinsic.quantized.modules module torch.nn.intrinsic.quantized.modules.bn_relu module torch.nn.intrinsic.quantized.modules.conv_relu module torch.nn.intrinsic.quantized.modules.linear_relu module torch.nn.modules module torch.nn.modules.activation module torch.nn.modules.adaptive module torch.nn.modules.batchnorm module torch.nn.modules.channelshuffle module torch.nn.modules.container module torch.nn.modules.conv module torch.nn.modules.distance module torch.nn.modules.dropout module torch.nn.modules.flatten module torch.nn.modules.fold module torch.nn.modules.instancenorm module torch.nn.modules.lazy module torch.nn.modules.linear module torch.nn.modules.loss module torch.nn.modules.module module torch.nn.modules.normalization module torch.nn.modules.padding module torch.nn.modules.pixelshuffle module torch.nn.modules.pooling module torch.nn.modules.rnn module torch.nn.modules.sparse module torch.nn.modules.transformer module torch.nn.modules.upsampling module torch.nn.modules.utils module torch.nn.parallel module torch.nn.parallel.comm module torch.nn.parallel.distributed module torch.nn.parallel.parallel_apply module torch.nn.parallel.replicate module torch.nn.parallel.scatter_gather module", "prev_chunk_id": "chunk_1139", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1141", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "torch.nn.parameter module torch.nn.qat module torch.nn.qat.dynamic module torch.nn.qat.dynamic.modules module torch.nn.qat.dynamic.modules.linear module torch.nn.qat.modules module torch.nn.qat.modules.conv module torch.nn.qat.modules.embedding_ops module torch.nn.qat.modules.linear module torch.nn.quantizable module torch.nn.quantizable.modules module torch.nn.quantizable.modules.activation module torch.nn.quantizable.modules.rnn module torch.nn.quantized module torch.nn.quantized.dynamic module torch.nn.quantized.dynamic.modules module torch.nn.quantized.dynamic.modules.conv module torch.nn.quantized.dynamic.modules.linear module torch.nn.quantized.dynamic.modules.rnn module torch.nn.quantized.functional module torch.nn.quantized.modules module torch.nn.quantized.modules.activation module torch.nn.quantized.modules.batchnorm module torch.nn.quantized.modules.conv module torch.nn.quantized.modules.dropout module torch.nn.quantized.modules.embedding_ops module torch.nn.quantized.modules.functional_modules module torch.nn.quantized.modules.linear module torch.nn.quantized.modules.normalization module torch.nn.quantized.modules.rnn module torch.nn.quantized.modules.utils module torch.nn.utils module torch.nn.utils.clip_grad module torch.nn.utils.convert_parameters module torch.nn.utils.fusion module torch.nn.utils.init module torch.nn.utils.memory_format module torch.nn.utils.parametrizations module torch.nn.utils.parametrize module torch.nn.utils.prune module torch.nn.utils.rnn module torch.nn.utils.stateless module torch.onnx module torch.onnx.errors module torch.onnx.operators module torch.onnx.ops module torch.onnx.symbolic_caffe2 module torch.onnx.symbolic_helper module torch.onnx.symbolic_opset10 module torch.onnx.symbolic_opset11 module torch.onnx.symbolic_opset12 module torch.onnx.symbolic_opset13 module torch.onnx.symbolic_opset14 module torch.onnx.symbolic_opset15 module torch.onnx.symbolic_opset16 module torch.onnx.symbolic_opset17 module torch.onnx.symbolic_opset18 module torch.onnx.symbolic_opset19 module torch.onnx.symbolic_opset20 module torch.onnx.symbolic_opset7 module torch.onnx.symbolic_opset8 module torch.onnx.symbolic_opset9 module torch.onnx.utils module torch.onnx.verification module torch.optim module torch.optim.adadelta module torch.optim.adagrad module torch.optim.adam module torch.optim.adamax module torch.optim.adamw module torch.optim.asgd module torch.optim.lbfgs module torch.optim.lr_scheduler module torch.optim.nadam module torch.optim.optimizer module torch.optim.radam module torch.optim.rmsprop module torch.optim.rprop module torch.optim.sgd module torch.optim.sparse_adam module torch.optim.swa_utils module torch.overrides module torch.package module torch.package.analyze module torch.package.analyze.find_first_use_of_broken_modules module torch.package.analyze.is_from_package module torch.package.analyze.trace_dependencies module torch.package.file_structure_representation module torch.package.find_file_dependencies module torch.package.glob_group module torch.package.importer module torch.package.package_exporter module torch.package.package_importer module torch.profiler module torch.profiler.itt module torch.profiler.profiler module torch.profiler.python_tracer module torch.quantization module torch.quantization.fake_quantize module torch.quantization.fuse_modules module torch.quantization.fuser_method_mappings module torch.quantization.fx module torch.quantization.fx.convert module torch.quantization.fx.fuse module torch.quantization.fx.fusion_patterns module torch.quantization.fx.graph_module module torch.quantization.fx.match_utils module torch.quantization.fx.pattern_utils module torch.quantization.fx.prepare module torch.quantization.fx.quantization_patterns module torch.quantization.fx.quantization_types module torch.quantization.fx.utils module torch.quantization.observer module torch.quantization.qconfig module torch.quantization.quant_type module torch.quantization.quantization_mappings module torch.quantization.quantize module torch.quantization.quantize_fx module torch.quantization.quantize_jit module torch.quantization.stubs module torch.quantization.utils module torch.quasirandom module torch.random module torch.return_types module torch.serialization module torch.signal module torch.signal.windows module torch.signal.windows.windows module torch.sparse module torch.sparse.semi_structured module torch.special module torch.storage module torch.testing module torch.torch_version module torch.types module torch.utils module torch.utils.backcompat module torch.utils.backend_registration module torch.utils.benchmark module torch.utils.benchmark.examples module torch.utils.benchmark.examples.compare module torch.utils.benchmark.examples.fuzzer module torch.utils.benchmark.examples.op_benchmark module torch.utils.benchmark.examples.simple_timeit module torch.utils.benchmark.examples.spectral_ops_fuzz_test module torch.utils.benchmark.op_fuzzers module torch.utils.benchmark.op_fuzzers.binary module torch.utils.benchmark.op_fuzzers.sparse_binary module torch.utils.benchmark.op_fuzzers.sparse_unary module torch.utils.benchmark.op_fuzzers.spectral module", "prev_chunk_id": "chunk_1140", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1142", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "torch.utils.benchmark.op_fuzzers.unary module torch.utils.benchmark.utils module torch.utils.benchmark.utils.common module torch.utils.benchmark.utils.compare module torch.utils.benchmark.utils.compile module torch.utils.benchmark.utils.cpp_jit module torch.utils.benchmark.utils.fuzzer module torch.utils.benchmark.utils.sparse_fuzzer module torch.utils.benchmark.utils.timer module torch.utils.benchmark.utils.valgrind_wrapper module torch.utils.benchmark.utils.valgrind_wrapper.timer_interface module torch.utils.bottleneck module torch.utils.bundled_inputs module torch.utils.checkpoint module torch.utils.collect_env module torch.utils.cpp_backtrace module torch.utils.cpp_extension module torch.utils.data module torch.utils.data.backward_compatibility module torch.utils.data.dataloader module torch.utils.data.datapipes module torch.utils.data.datapipes.dataframe module torch.utils.data.datapipes.dataframe.dataframe_wrapper module torch.utils.data.datapipes.dataframe.dataframes module torch.utils.data.datapipes.dataframe.datapipes module torch.utils.data.datapipes.dataframe.structures module torch.utils.data.datapipes.datapipe module torch.utils.data.datapipes.gen_pyi module torch.utils.data.datapipes.iter module torch.utils.data.datapipes.iter.callable module torch.utils.data.datapipes.iter.combinatorics module torch.utils.data.datapipes.iter.combining module torch.utils.data.datapipes.iter.filelister module torch.utils.data.datapipes.iter.fileopener module torch.utils.data.datapipes.iter.grouping module torch.utils.data.datapipes.iter.routeddecoder module torch.utils.data.datapipes.iter.selecting module torch.utils.data.datapipes.iter.sharding module torch.utils.data.datapipes.iter.streamreader module torch.utils.data.datapipes.iter.utils module torch.utils.data.datapipes.map module torch.utils.data.datapipes.map.callable module torch.utils.data.datapipes.map.combinatorics module torch.utils.data.datapipes.map.combining module torch.utils.data.datapipes.map.grouping module torch.utils.data.datapipes.map.utils module torch.utils.data.datapipes.utils module torch.utils.data.datapipes.utils.common module torch.utils.data.datapipes.utils.decoder module torch.utils.data.datapipes.utils.snapshot module torch.utils.data.dataset module torch.utils.data.distributed module torch.utils.data.graph module torch.utils.data.graph_settings module torch.utils.data.sampler module torch.utils.deterministic module torch.utils.dlpack module torch.utils.file_baton module torch.utils.flop_counter module torch.utils.hipify module torch.utils.hipify.constants module torch.utils.hipify.cuda_to_hip_mappings module torch.utils.hipify.hipify_python module torch.utils.hipify.version module torch.utils.hooks module torch.utils.jit module torch.utils.jit.log_extract module torch.utils.mkldnn module torch.utils.mobile_optimizer module torch.utils.model_dump module torch.utils.model_zoo module torch.utils.module_tracker module torch.utils.serialization module torch.utils.serialization.config module torch.utils.show_pickle module torch.utils.tensorboard module torch.utils.tensorboard.summary module torch.utils.tensorboard.writer module torch.utils.throughput_benchmark module torch.utils.viz module torch.utils.weak module torch.version module torch.xpu module torch.xpu.memory module torch.xpu.random module torch.xpu.streams module TORCH_COMPILE_JOB_ID torch_name() (torch.onnx.JitScalarType method) torch_save_to_dcp() (in module torch.distributed.checkpoint.format_utils) TorchAODType (class in torch.ao.quantization.observer) total_average() (torch.autograd.profiler.profile method) total_count (torch.distributions.multinomial.Multinomial attribute) trace() (in module torch) (in module torch.jit) (torch.fx.Tracer method) (torch.Tensor method) trace_module() (in module torch.jit) Tracer (class in torch.fx) train() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) Transform (class in torch.distributions.transforms) transform() (torch.fx.Transformer method) (torch.utils.benchmark.FunctionCounts method) transform_object() (torch.distributed.checkpoint.DefaultSavePlanner method) transform_tensor() (torch.distributed.checkpoint.DefaultLoadPlanner method) TransformedDistribution (class in torch.distributions.transformed_distribution) Transformer (class in torch.fx) (class in torch.nn) TransformerDecoder (class in torch.nn) TransformerDecoderLayer (class in torch.nn) TransformerEncoder (class in torch.nn) TransformerEncoderLayer (class in torch.nn) transpose() (in module torch) (torch.Tensor method) transpose_() (torch.Tensor method) trapezoid() (in module torch) trapz() (in module torch) triangular_solve() (in module torch) (torch.Tensor method) tril() (in module torch) (torch.Tensor method) tril_() (torch.Tensor method) tril_indices() (in module torch) trim_significant_figures() (torch.utils.benchmark.Compare method) triplet_margin_loss() (in module", "prev_chunk_id": "chunk_1141", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1143", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "T", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "T", "content": "torch.nn.functional) triplet_margin_with_distance_loss() (in module torch.nn.functional) TripletMarginLoss (class in torch.nn) TripletMarginWithDistanceLoss (class in torch.nn) triton_op() (in module torch.library) triu() (in module torch) (torch.Tensor method) triu_() (torch.Tensor method) triu_indices() (in module torch) true_divide() (in module torch) (torch.Tensor method) true_divide_() (torch.Tensor method) trunc() (in module torch) (torch.Tensor method) trunc_() (torch.Tensor method) trunc_normal_() (in module torch.nn.init) tune_gemm_in_file() (in module torch.cuda.tunable) tuning_enable() (in module torch.cuda.tunable) tuning_is_enabled() (in module torch.cuda.tunable) type (torch.jit.Attribute attribute) type() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method) (torch.TypedStorage method) (torch.UntypedStorage method) type_as() (torch.Tensor method) TypedStorage (class in torch)", "prev_chunk_id": "chunk_1142", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1144", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "U", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "U", "content": "U unbind() (in module torch) (torch.Tensor method) unbox() (torch.distributed.Work static method) underlying_store (torch.distributed.PrefixStore property) Unflatten (class in torch.nn) unflatten() (in module torch) (in module torch.export.unflatten) (torch.Tensor method) Unfold (class in torch.nn) unfold() (in module torch.nn.functional) (torch.Tensor method) Uniform (class in torch.distributions.uniform) uniform_() (in module torch.nn.init) (torch.Tensor method) UninitializedBuffer (class in torch.nn.parameter) UninitializedParameter (class in torch.nn.parameter) unique() (in module torch) (torch.Tensor method) unique_consecutive() (in module torch) (torch.Tensor method) unpack_dual() (in module torch.autograd.forward_ad) unpack_sequence() (in module torch.nn.utils.rnn) UnpackedDualTensor (class in torch.autograd.forward_ad) unpad_sequence() (in module torch.nn.utils.rnn) unravel_index() (in module torch) unregister_custom_op_symbolic() (in module torch.onnx) unregister_event_handler() (in module torch.monitor) unshard() (torch.distributed.fsdp.FSDPModule method) UnshardHandle (class in torch.distributed.fsdp) unsorted_indices (torch.nn.utils.rnn.PackedSequence attribute) | unsqueeze() (in module torch) (torch.Tensor method) unsqueeze_() (torch.Tensor method) untyped() (torch.TypedStorage method) (torch.UntypedStorage method) untyped_storage() (torch.Tensor method) UntypedStorage (class in torch) unused() (in module torch.jit) update() (torch.autograd.profiler_util.StringTable method) (torch.export.decomp_utils.CustomDecompTable method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) update_arg() (torch.fx.Node method) update_bn() (in module torch.optim.swa_utils) update_bn_stats (class in torch.ao.nn.intrinsic.qat) update_kwarg() (torch.fx.Node method) update_parameters() (torch.optim.swa_utils.AveragedModel method) upsample (class in torch.ao.nn.quantized.functional) Upsample (class in torch.nn) upsample() (in module torch.nn.functional) upsample_bilinear (class in torch.ao.nn.quantized.functional) upsample_bilinear() (in module torch.nn.functional) upsample_nearest (class in torch.ao.nn.quantized.functional) upsample_nearest() (in module torch.nn.functional) UpsamplingBilinear2d (class in torch.nn) UpsamplingNearest2d (class in torch.nn) use_agent_store (torch.distributed.elastic.rendezvous.RendezvousHandler property) use_count() (torch.cuda.memory.MemPool method) use_deterministic_algorithms() (in module torch) use_mem_pool (class in torch.cuda) utilization() (in module torch.cuda)", "prev_chunk_id": "chunk_1143", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1145", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "V", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "V", "content": "V validate_checkpoint_id() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader class method) (torch.distributed.checkpoint.StorageReader class method) (torch.distributed.checkpoint.StorageWriter class method) value (torch.jit.Attribute attribute) value() (torch.futures.Future method) values() (torch.autograd.profiler_util.StringTable method) (torch.nn.ModuleDict method) (torch.nn.ParameterDict method) (torch.Tensor method) vander() (in module torch) (in module torch.linalg) var() (in module torch) (torch.Tensor method) var_mean() (in module torch) variance (torch.distributions.bernoulli.Bernoulli property) (torch.distributions.beta.Beta property) (torch.distributions.binomial.Binomial property) (torch.distributions.categorical.Categorical property) (torch.distributions.cauchy.Cauchy property) (torch.distributions.continuous_bernoulli.ContinuousBernoulli property) (torch.distributions.dirichlet.Dirichlet property) (torch.distributions.distribution.Distribution property) (torch.distributions.exponential.Exponential property) (torch.distributions.fishersnedecor.FisherSnedecor property) (torch.distributions.gamma.Gamma property) (torch.distributions.generalized_pareto.GeneralizedPareto property) (torch.distributions.geometric.Geometric property) (torch.distributions.gumbel.Gumbel property) (torch.distributions.half_cauchy.HalfCauchy property) (torch.distributions.half_normal.HalfNormal property) (torch.distributions.independent.Independent property) (torch.distributions.inverse_gamma.InverseGamma property) (torch.distributions.kumaraswamy.Kumaraswamy property) (torch.distributions.laplace.Laplace property) (torch.distributions.log_normal.LogNormal property) (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal property) (torch.distributions.mixture_same_family.MixtureSameFamily property) (torch.distributions.multinomial.Multinomial property) (torch.distributions.multivariate_normal.MultivariateNormal property) (torch.distributions.negative_binomial.NegativeBinomial property) (torch.distributions.normal.Normal property) (torch.distributions.one_hot_categorical.OneHotCategorical property) (torch.distributions.pareto.Pareto property) (torch.distributions.poisson.Poisson property) (torch.distributions.studentT.StudentT property) (torch.distributions.uniform.Uniform property) (torch.distributions.von_mises.VonMises property) (torch.distributions.weibull.Weibull property) (torch.distributions.wishart.Wishart property) | vdot() (in module torch) (torch.Tensor method) vecdot() (in module torch.linalg) vector_norm() (in module torch.linalg) vector_to_parameters() (in module torch.nn.utils) verbose (class in torch.backends.mkl) (class in torch.backends.mkldnn) VerificationInfo (class in torch.onnx.verification) VerificationOptions (class in torch.onnx.verification) verify() (in module torch.onnx.verification) (torch.export.dynamic_shapes.AdditionalInputs method) verify_aten_graph() (in module torch.onnx.verification) verify_ninja_availability() (in module torch.utils.cpp_extension) verify_onnx_program() (in module torch.onnx.verification) version() (in module torch.backends.cudnn) (in module torch.backends.cusparselt) vhp() (in module torch.autograd.functional) view() (torch.Tensor method) view_as() (torch.Tensor method) view_as_complex() (in module torch) view_as_real() (in module torch) visualize_sharding() (in module torch.distributed.tensor.debug) vjp() (in module torch.autograd.functional) (in module torch.func) (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction static method) vmap() (in module torch) (in module torch.func) (torch.autograd.Function static method) (torch.autograd.function.InplaceFunction static method) (torch.autograd.function.NestedIOFunction static method) VonMises (class in torch.distributions.von_mises) vsplit() (in module torch) (torch.Tensor method) vstack() (in module torch)", "prev_chunk_id": "chunk_1144", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1146", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "W", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "W", "content": "W wait() (in module torch.jit) (torch.cuda.Event method) (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore method) (torch.distributed.fsdp.UnshardHandle method) (torch.distributed.Store method) (torch.distributed.Work method) (torch.Event method) (torch.futures.Future method) (torch.mps.event.Event method) (torch.mtia.Event method) (torch.xpu.Event method) wait_all() (in module torch.futures) wait_event() (torch.cuda.ExternalStream method) (torch.cuda.Stream method) (torch.mtia.Stream method) (torch.Stream method) (torch.xpu.Stream method) wait_stream() (torch.cuda.ExternalStream method) (torch.cuda.Stream method) (torch.mtia.Stream method) (torch.Stream method) (torch.xpu.Stream method) Weibull (class in torch.distributions.weibull) | weight_norm() (in module torch.nn.utils) (in module torch.nn.utils.parametrizations) WeightedRandomSampler (class in torch.utils.data) where() (in module torch) (torch.Tensor method) Wishart (class in torch.distributions.wishart) with_args() (torch.ao.quantization.observer.AffineQuantizedObserverBase class method) (torch.ao.quantization.observer.ObserverBase class method) with_callable_args() (torch.ao.quantization.observer.ObserverBase class method) Work (class in torch.distributed) Worker (class in torch.distributed.elastic.agent.server) worker_main() (in module torch.distributed.elastic.control_plane) WorkerGroup (class in torch.distributed.elastic.agent.server) WorkerInfo (class in torch.distributed.rpc) WorkerSpec (class in torch.distributed.elastic.agent.server) WorkerState (class in torch.distributed.elastic.agent.server) wrap() (in module torch.fx) wrap_torch_function() (in module torch.overrides) wrap_triton() (in module torch.library) write_data() (torch.distributed.checkpoint.StorageWriter method) write_file() (in module torch.cuda.tunable) write_file_on_exit() (in module torch.cuda.tunable) WriteItem (class in torch.distributed.checkpoint.planner)", "prev_chunk_id": "chunk_1145", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1147", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "X", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "X", "content": "X xavier_normal_() (in module torch.nn.init) xavier_uniform_() (in module torch.nn.init) xlog1py() (in module torch.special) xlogy() (in module torch) (in module torch.special) (torch.Tensor method) | xlogy_() (torch.Tensor method) xpu() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.swa_utils.AveragedModel method) (torch.Tensor method)", "prev_chunk_id": "chunk_1146", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1148", "url": "https://docs.pytorch.org/docs/stable/genindex.html", "title": "Z", "page_title": "Index — PyTorch 2.8 documentation", "breadcrumbs": "Z", "content": "Z zero_() (torch.Tensor method) zero_grad() (torch.jit.ScriptModule method) (torch.nn.Module method) (torch.optim.Adadelta method) (torch.optim.Adafactor method) (torch.optim.Adagrad method) (torch.optim.Adam method) (torch.optim.Adamax method) (torch.optim.AdamW method) (torch.optim.ASGD method) (torch.optim.LBFGS method) (torch.optim.NAdam method) (torch.optim.Optimizer method) (torch.optim.RAdam method) (torch.optim.RMSprop method) (torch.optim.Rprop method) (torch.optim.SGD method) (torch.optim.SparseAdam method) (torch.optim.swa_utils.AveragedModel method) | ZeroPad1d (class in torch.nn) ZeroPad2d (class in torch.nn) ZeroPad3d (class in torch.nn) ZeroPointDomain (class in torch.ao.quantization.observer) ZeroRedundancyOptimizer (class in torch.distributed.optim) zeros() (in module torch) (in module torch.distributed.tensor) zeros_() (in module torch.nn.init) zeros_like() (in module torch) zeta() (in module torch.special)", "prev_chunk_id": "chunk_1147", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1149", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Windows FAQ#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Windows FAQ#", "content": "Windows FAQ# Created On: Apr 23, 2018 | Last Updated On: May 20, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1150", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Include optional components#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Include optional components#", "content": "Include optional components# There are two supported components for Windows PyTorch: MKL and MAGMA. Here are the steps to build with them. REM Make sure you have 7z and curl installed. REM Download MKL files curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O 7z x -aoa mkl_2020.2.254.7z -omkl REM Download MAGMA files REM version available: REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release) REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release) REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release) REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release) set \"CUDA_PREFIX=cuda102\" set \"CONFIG=release\" set \"HOST=https://s3.amazonaws.com/ossci-windows\" curl -k \"%HOST%/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z\" -o magma.7z 7z x -aoa magma.7z -omagma REM Setting essential environment variables set \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\" set \"LIB=%cd%\\mkl\\lib;%LIB%\" set \"MAGMA_HOME=%cd%\\magma\"", "prev_chunk_id": "chunk_1149", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1151", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Speeding CUDA build for Windows#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Speeding CUDA build for Windows#", "content": "Speeding CUDA build for Windows# Visual Studio doesn’t support parallel custom task currently. As an alternative, we can use Ninja to parallelize CUDA build tasks. It can be used by typing only a few lines of code. REM Let's install ninja first. pip install ninja REM Set it as the cmake generator set CMAKE_GENERATOR=Ninja", "prev_chunk_id": "chunk_1150", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1152", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "One key install script#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "One key install script#", "content": "One key install script# You can take a look at this set of scripts. It will lead the way for you.", "prev_chunk_id": "chunk_1151", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1153", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "CFFI Extension#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "CFFI Extension#", "content": "CFFI Extension# The support for CFFI Extension is very experimental. You must specify additional libraries in Extension object to make it build on Windows. ffi = create_extension( '_ext.my_lib', headers=headers, sources=sources, define_macros=defines, relative_to=__file__, with_cuda=with_cuda, extra_compile_args=[\"-std=c99\"], libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart )", "prev_chunk_id": "chunk_1152", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1154", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Cpp Extension#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Cpp Extension#", "content": "Cpp Extension# This type of extension has better support compared with the previous one. However, it still needs some manual configuration. First, you should open the x86_x64 Cross Tools Command Prompt for VS 2017. And then, you can start your compiling process.", "prev_chunk_id": "chunk_1153", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1155", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Package not found in win-32 channel.#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Package not found in win-32 channel.#", "content": "Package not found in win-32 channel.# Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels: - pytorch Current channels: - https://repo.continuum.io/pkgs/main/win-32 - https://repo.continuum.io/pkgs/main/noarch - https://repo.continuum.io/pkgs/free/win-32 - https://repo.continuum.io/pkgs/free/noarch - https://repo.continuum.io/pkgs/r/win-32 - https://repo.continuum.io/pkgs/r/noarch - https://repo.continuum.io/pkgs/pro/win-32 - https://repo.continuum.io/pkgs/pro/noarch - https://repo.continuum.io/pkgs/msys2/win-32 - https://repo.continuum.io/pkgs/msys2/noarch PyTorch doesn’t work on 32-bit system. Please use Windows and Python 64-bit version.", "prev_chunk_id": "chunk_1154", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1156", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Import error#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Import error#", "content": "Import error# from torch._C import * ImportError: DLL load failed: The specified module could not be found. The problem is caused by the missing of the essential files. For the wheels package, since we didn’t pack some libraries and VS2017 redistributable files in, please make sure you install them manually. The VS 2017 redistributable installer can be downloaded. And you should also pay attention to your installation of Numpy. Make sure it uses MKL instead of OpenBLAS. You may type in the following command. pip install numpy mkl intel-openmp mkl_fft", "prev_chunk_id": "chunk_1155", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1157", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Multiprocessing error without if-clause protection#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing error without if-clause protection#", "content": "Multiprocessing error without if-clause protection# RuntimeError: An attempt has been made to start a new process before the current process has finished its bootstrapping phase. This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module: if __name__ == '__main__': freeze_support() ... The \"freeze_support()\" line can be omitted if the program is not going to be frozen to produce an executable. The implementation of multiprocessing is different on Windows, which uses spawn instead of fork. So we have to wrap the code with an if-clause to protect the code from executing multiple times. Refactor your code into the following structure. import torch def main() for i, data in enumerate(dataloader): # do something here if __name__ == '__main__': main()", "prev_chunk_id": "chunk_1156", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1158", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Multiprocessing error “Broken pipe”#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing error “Broken pipe”#", "content": "Multiprocessing error “Broken pipe”# ForkingPickler(file, protocol).dump(obj) BrokenPipeError: [Errno 32] Broken pipe This issue happens when the child process ends before the parent process finishes sending data. There may be something wrong with your code. You can debug your code by reducing the num_worker of DataLoader to zero and see if the issue persists.", "prev_chunk_id": "chunk_1157", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1159", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "Multiprocessing error “driver shut down”#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing error “driver shut down”#", "content": "Multiprocessing error “driver shut down”# Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154 [windows] driver shut down Please update your graphics driver. If this persists, this may be that your graphics card is too old or the calculation is too heavy for your card. Please update the TDR settings according to this post.", "prev_chunk_id": "chunk_1158", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1160", "url": "https://docs.pytorch.org/docs/stable/notes/windows.html", "title": "CUDA IPC operations#", "page_title": "Windows FAQ — PyTorch 2.8 documentation", "breadcrumbs": "CUDA IPC operations#", "content": "CUDA IPC operations# THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS They are not supported on Windows. Something like doing multiprocessing on CUDA tensors cannot succeed, there are two alternatives for this. 1. Don’t use multiprocessing. Set the num_worker of DataLoader to zero. 2. Share CPU tensors instead. Make sure your custom DataSet returns CPU tensors.", "prev_chunk_id": "chunk_1159", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1161", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Serialization semantics#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Serialization semantics#", "content": "Serialization semantics# Created On: Feb 26, 2017 | Last Updated On: May 19, 2025 This note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1162", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Saving and loading tensors#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Saving and loading tensors#", "content": "Saving and loading tensors# torch.save() and torch.load() let you easily save and load tensors: >>> t = torch.tensor([1., 2.]) >>> torch.save(t, 'tensor.pt') >>> torch.load('tensor.pt') tensor([1., 2.]) By convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension. torch.save() and torch.load() use Python’s pickle by default, so you can also save multiple tensors as part of Python objects like tuples, lists, and dicts: >>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])} >>> torch.save(d, 'tensor_dict.pt') >>> torch.load('tensor_dict.pt') {'a': tensor([1., 2.]), 'b': tensor([3., 4.])} Custom data structures that include PyTorch tensors can also be saved if the data structure is pickle-able.", "prev_chunk_id": "chunk_1161", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1163", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Saving and loading tensors preserves views#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Saving and loading tensors preserves views#", "content": "Saving and loading tensors preserves views# Saving tensors preserves their view relationships: >>> numbers = torch.arange(1, 10) >>> evens = numbers[1::2] >>> torch.save([numbers, evens], 'tensors.pt') >>> loaded_numbers, loaded_evens = torch.load('tensors.pt') >>> loaded_evens *= 2 >>> loaded_numbers tensor([ 1, 4, 3, 8, 5, 12, 7, 16, 9]) Behind the scenes, these tensors share the same “storage.” See Tensor Views for more on views and storage. When PyTorch saves tensors it saves their storage objects and tensor metadata separately. This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors. In the above snippet, for example, only a single storage is written to ‘tensors.pt’. In some cases, however, saving the current storage objects may be unnecessary and create prohibitively large files. In the following snippet a storage much larger than the saved tensor is written to a file: >>> large = torch.arange(1, 1000) >>> small = large[0:5] >>> torch.save(small, 'small.pt') >>> loaded_small = torch.load('small.pt') >>> loaded_small.storage().size() 999 Instead of saving only the five values in the small tensor to ‘small.pt,’ the 999 values in the storage it shares with large were saved and loaded. When saving tensors with fewer elements than their storage objects, the size of the saved file can be reduced by first cloning the tensors. Cloning a tensor produces a new tensor with a new storage object containing only the values in the tensor: >>> large = torch.arange(1, 1000) >>> small = large[0:5] >>> torch.save(small.clone(), 'small.pt') # saves a clone of small >>> loaded_small = torch.load('small.pt') >>> loaded_small.storage().size() 5 Since the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors", "prev_chunk_id": "chunk_1162", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1164", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Saving and loading tensors preserves views#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Saving and loading tensors preserves views#", "content": "smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.", "prev_chunk_id": "chunk_1163", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1165", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Saving and loading torch.nn.Modules#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Saving and loading torch.nn.Modules#", "content": "Saving and loading torch.nn.Modules# See also: Tutorial: Saving and loading modules In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers: >>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True) >>> list(bn.named_parameters()) [('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)), ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))] >>> list(bn.named_buffers()) [('running_mean', tensor([0., 0., 0.])), ('running_var', tensor([1., 1., 1.])), ('num_batches_tracked', tensor(0))] >>> bn.state_dict() OrderedDict([('weight', tensor([1., 1., 1.])), ('bias', tensor([0., 0., 0.])), ('running_mean', tensor([0., 0., 0.])), ('running_var', tensor([1., 1., 1.])), ('num_batches_tracked', tensor(0))]) Instead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, load_state_dict(), to restore their states from a state dict: >>> torch.save(bn.state_dict(), 'bn.pt') >>> bn_state_dict = torch.load('bn.pt') >>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True) >>> new_bn.load_state_dict(bn_state_dict) <All keys matched successfully> Note that the state dict is first loaded from its file with torch.load() and the state then restored with load_state_dict(). Even custom modules and modules containing other modules have state dicts and can use this pattern: # A module with two linear layers >>> class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.l0 = torch.nn.Linear(4, 2) self.l1 = torch.nn.Linear(2, 1) def forward(self, input): out0 = self.l0(input) out0_relu = torch.nn.functional.relu(out0) return self.l1(out0_relu) >>> m = MyModule() >>> m.state_dict() OrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406], [-0.3289, 0.2827, 0.4588, 0.2031]])), ('l0.bias', tensor([ 0.0300, -0.1316])), ('l1.weight', tensor([[0.6533, 0.3413]])), ('l1.bias', tensor([-0.1112]))]) >>> torch.save(m.state_dict(), 'mymodule.pt') >>> m_state_dict = torch.load('mymodule.pt') >>> new_m = MyModule() >>> new_m.load_state_dict(m_state_dict) <All keys matched successfully>", "prev_chunk_id": "chunk_1164", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1166", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Serialized file format for torch.save#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Serialized file format for torch.save#", "content": "Serialized file format for torch.save# Since PyTorch 1.6.0, torch.save defaults to returning an uncompressed ZIP64 archive unless the user sets _use_new_zipfile_serialization=False. In this archive, the files are ordered as such checkpoint.pth ├── data.pkl ├── byteorder # added in PyTorch 2.1.0 ├── data/ │ ├── 0 │ ├── 1 │ ├── 2 │ └── … └── version When saving, PyTorch will ensure that the local file header of each file is padded to an offset that is a multiple of 64 bytes, ensuring that the offset of each file is 64-byte aligned.", "prev_chunk_id": "chunk_1165", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1167", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Layout Control#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Layout Control#", "content": "Layout Control# The mmap argument in torch.load() allows for lazy loading of tensor storages. In addition, there are some advanced features that allow for more fine-grained control and manipulation of a torch.save checkpoint. To inspect tensor metadata in a torch.save checkpoint without allocating memory for storage data, use torch.load within the FakeTensorMode context manager. On top of skipping loading storage data similar to skip_data above, it additionally tags storages with their offset within the checkpoint, enabling direct checkpoint manipulation. import torch.nn as nn from torch._subclasses.fake_tensor import FakeTensorMode m = nn.Linear(10, 10) torch.save(m.state_dict(), \"checkpoint.pt\") with FakeTensorMode() as mode: fake_sd = torch.load(\"checkpoint.pt\") for k, v in fake_sd.items(): print(f\"key={k}, dtype={v.dtype}, shape={v.shape}, stride={v.stride()}, storage_offset={v.storage_offset()}\") # offset of the storage in the checkpoint print(f\"key={k}, checkpoint_offset={v.untyped_storage()._checkpoint_offset}\") For more information, this tutorial offers a comprehensive example of using these features to manipulate a checkpoint.", "prev_chunk_id": "chunk_1166", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1168", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "torch.load with weights_only=True#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "torch.load with weights_only=True#", "content": "torch.load with weights_only=True# Starting in version 2.6, torch.load will use weights_only=True if the pickle_module argument is not passed. As discussed in the documentation for torch.load(), weights_only=True restricts the unpickler used in torch.load to only executing functions/building classes required for state_dicts of plain torch.Tensors as well as some other primitive types. Further, unlike the default Unpickler provided by the pickle module, the weights_only Unpickler is not allowed to dynamically import anything during unpickling. As mentioned above, saving a module’s state_dict is a best practice when using torch.save. If loading an old checkpoint that contains an nn.Module, we recommend weights_only=False. When loading a checkpoint that contains tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details. If the weights_only Unpickler encounters a function or class that is not allowlisted by default within the pickle file, you should see an actionable error like such _pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. 1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source. 2. Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message. WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global if you trust this class/function. Please follow the steps in the error message and allowlist the functions or classes only if you trust them. To get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use torch.serialization.get_unsafe_globals_in_checkpoint() which will return a list of strings of the", "prev_chunk_id": "chunk_1167", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1169", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "torch.load with weights_only=True#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "torch.load with weights_only=True#", "content": "form {__module__}.{__name__}. If you trust these functions/classes, you can import them and allowlist them per the error message either via torch.serialization.add_safe_globals() or the context manager torch.serialization.safe_globals. To access the list of user-allowlisted functions/classes you can use torch.serialization.get_safe_globals() and to clear the current list see torch.serialization.clear_safe_globals().", "prev_chunk_id": "chunk_1168", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1170", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Getting unsafe globals#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Getting unsafe globals#", "content": "Getting unsafe globals# A caveat is that torch.serialization.get_unsafe_globals_in_checkpoint() analyzes the checkpoint statically, some types might be built dynamically during the unpickling process and hence will not be reported by torch.serialization.get_unsafe_globals_in_checkpoint(). One such example is dtypes in numpy. In numpy < 1.25 after allowlisting all the functions/classes reported by torch.serialization.get_unsafe_globals_in_checkpoint() you might see an error like WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtype[float32]'> This can be allowlisted via {add_}safe_globals([type(np.dtype(np.float32))]). In numpy >=1.25 you would see WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float32DType'> This can be allowlisted via {add_}safe_globals([np.dtypes.Float32DType]).", "prev_chunk_id": "chunk_1169", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1171", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Environment Variables#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Environment Variables#", "content": "Environment Variables# There are two environment variables that will influence the behavior of torch.load. These can be helpful if one does not have access to the torch.load callsites. - TORCH_FORCE_WEIGHTS_ONLY_LOAD=1will override alltorch.loadcallsites to useweights_only=True. - TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1will maketorch.loadcallsites useweights_only=Falseonlyifweights_onlywas not passed as an argument.", "prev_chunk_id": "chunk_1170", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1172", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Serializing torch.nn.Modules and loading them in C++#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Serializing torch.nn.Modules and loading them in C++#", "content": "Serializing torch.nn.Modules and loading them in C++# See also: Tutorial: Loading a TorchScript Model in C++ ScriptModules can be serialized as a TorchScript program and loaded using torch.jit.load(). This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python). The distinction between torch.jit.save() and torch.save() may not be immediately clear. torch.save() saves Python objects with pickle. This is especially useful for prototyping, researching, and training. torch.jit.save(), on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models. To script, serialize and load a module in Python: >>> scripted_module = torch.jit.script(MyModule()) >>> torch.jit.save(scripted_module, 'mymodule.pt') >>> torch.jit.load('mymodule.pt') RecursiveScriptModule( original_name=MyModule (l0): RecursiveScriptModule(original_name=Linear) (l1): RecursiveScriptModule(original_name=Linear) ) Traced modules can also be saved with torch.jit.save(), with the caveat that only the traced code path is serialized. The following example demonstrates this: # A module with control flow >>> class ControlFlowModule(torch.nn.Module): def __init__(self): super().__init__() self.l0 = torch.nn.Linear(4, 2) self.l1 = torch.nn.Linear(2, 1) def forward(self, input): if input.dim() > 1: return torch.tensor(0) out0 = self.l0(input) out0_relu = torch.nn.functional.relu(out0) return self.l1(out0_relu) >>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4)) >>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt') >>> loaded = torch.jit.load('controlflowmodule_traced.pt') >>> loaded(torch.randn(2, 4))) tensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>) >>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4)) >>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt') >>> loaded = torch.jit.load('controlflowmodule_scripted.pt') >> loaded(torch.randn(2, 4)) tensor(0) The above module has an if statement that is not triggered by the traced inputs, and so is not part of the traced module and not serialized with it. The scripted module, however, contains the if statement and is serialized with it. See the TorchScript documentation for more on scripting and tracing. Finally, to load the module in", "prev_chunk_id": "chunk_1171", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1173", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Serializing torch.nn.Modules and loading them in C++#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Serializing torch.nn.Modules and loading them in C++#", "content": "C++: >>> torch::jit::script::Module module; >>> module = torch::jit::load('controlflowmodule_scripted.pt'); See the PyTorch C++ API documentation for details about how to use PyTorch modules in C++.", "prev_chunk_id": "chunk_1172", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1174", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Saving and loading ScriptModules across PyTorch versions#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Saving and loading ScriptModules across PyTorch versions#", "content": "Saving and loading ScriptModules across PyTorch versions# The PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly. In limited cases, detailed below, PyTorch will preserve the historic behavior of serialized ScriptModules so they do not require an update.", "prev_chunk_id": "chunk_1173", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1175", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "torch.div performing integer division#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "torch.div performing integer division#", "content": "torch.div performing integer division# In PyTorch 1.5 and earlier torch.div() would perform floor division when given two integer inputs: # PyTorch 1.5 (and earlier) >>> a = torch.tensor(5) >>> b = torch.tensor(3) >>> a / b tensor(1) In PyTorch 1.7, however, torch.div() will always perform a true division of its inputs, just like division in Python 3: # PyTorch 1.7 >>> a = torch.tensor(5) >>> b = torch.tensor(3) >>> a / b tensor(1.6667) The behavior of torch.div() is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.div() perform floor division when given two integer inputs even when loaded with newer versions of PyTorch. ScriptModules using torch.div() and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.", "prev_chunk_id": "chunk_1174", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1176", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "torch.full always inferring a float dtype#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "torch.full always inferring a float dtype#", "content": "torch.full always inferring a float dtype# In PyTorch 1.5 and earlier torch.full() always returned a float tensor, regardless of the fill value it’s given: # PyTorch 1.5 and earlier >>> torch.full((3,), 1) # Note the integer fill value... tensor([1., 1., 1.]) # ...but float tensor! In PyTorch 1.7, however, torch.full() will infer the returned tensor’s dtype from the fill value: # PyTorch 1.7 >>> torch.full((3,), 1) tensor([1, 1, 1]) >>> torch.full((3,), True) tensor([True, True, True]) >>> torch.full((3,), 1.) tensor([1., 1., 1.]) >>> torch.full((3,), 1 + 1j) tensor([1.+1.j, 1.+1.j, 1.+1.j]) The behavior of torch.full() is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.full return float tensors by default, even when given bool or integer fill values. ScriptModules using torch.full() and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.", "prev_chunk_id": "chunk_1175", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1177", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Utility functions#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Utility functions#", "content": "Utility functions# The following utility functions are related to serialization:", "prev_chunk_id": "chunk_1176", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1178", "url": "https://docs.pytorch.org/docs/stable/notes/serialization.html", "title": "Config#", "page_title": "Serialization semantics — PyTorch 2.8 documentation", "breadcrumbs": "Config#", "content": "Config# torch.utils.serialization.config provides a global config that can control the behavior of torch.save and torch.load. torch.utils.serialization.config.save contains options that control the behavior of torch.save. torch.utils.serialization.config.load contains options that control the behavior of torch.load.", "prev_chunk_id": "chunk_1177", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1179", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "Reproducibility#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "Reproducibility#", "content": "Reproducibility# Created On: Sep 11, 2018 | Last Updated On: Nov 26, 2024 Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds. However, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1180", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "PyTorch random number generator#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch random number generator#", "content": "PyTorch random number generator# You can use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA): import torch torch.manual_seed(0) Some PyTorch operations may use random numbers internally. torch.svd_lowrank() does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as torch.manual_seed() is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment. It is also possible to obtain identical results from an operation that uses random numbers by setting torch.manual_seed() to the same value between subsequent calls.", "prev_chunk_id": "chunk_1179", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1181", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "Python#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "Python#", "content": "Python# For custom operators, you might need to set python seed as well: import random random.seed(0)", "prev_chunk_id": "chunk_1180", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1182", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "Random number generators in other libraries#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "Random number generators in other libraries#", "content": "Random number generators in other libraries# If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with: import numpy as np np.random.seed(0) However, some applications and libraries may use NumPy Random Generator objects, not the global RNG (https://numpy.org/doc/stable/reference/random/generator.html), and those will need to be seeded consistently as well. If you are using any other libraries that use random number generators, refer to the documentation for those libraries to see how to set consistent seeds for them.", "prev_chunk_id": "chunk_1181", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1183", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "CUDA convolution benchmarking#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "CUDA convolution benchmarking#", "content": "CUDA convolution benchmarking# The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine. Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance. However, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with torch.backends.cudnn.benchmark = True. Note that this setting is different from the torch.backends.cudnn.deterministic setting discussed below.", "prev_chunk_id": "chunk_1182", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1184", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "Avoiding nondeterministic algorithms#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "Avoiding nondeterministic algorithms#", "content": "Avoiding nondeterministic algorithms# torch.use_deterministic_algorithms() lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative). Please check the documentation for torch.use_deterministic_algorithms() for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: pytorch/pytorch#issues For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_() will throw an error: >>> import torch >>> torch.use_deterministic_algorithms(True) >>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2)) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. ... When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used: >>> import torch >>> torch.use_deterministic_algorithms(True) >>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda()) tensor([[[ 1.1900, -2.3409], [ 0.4796, 0.8003]], [[ 0.1509, 1.8027], [ 0.0333, -1.1444]]], device='cuda:0') Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you should set the environment variable CUBLAS_WORKSPACE_CONFIG according to CUDA documentation: https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility", "prev_chunk_id": "chunk_1183", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1185", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "CUDA convolution determinism#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "CUDA convolution determinism#", "content": "CUDA convolution determinism# While disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either torch.use_deterministic_algorithms(True) or torch.backends.cudnn.deterministic = True is set. The latter setting controls only this behavior, unlike torch.use_deterministic_algorithms() which will make other PyTorch operations behave deterministically, too.", "prev_chunk_id": "chunk_1184", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1186", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "CUDA RNN and LSTM#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "CUDA RNN and LSTM#", "content": "CUDA RNN and LSTM# In some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior. See torch.nn.RNN() and torch.nn.LSTM() for details and workarounds.", "prev_chunk_id": "chunk_1185", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1187", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "Filling uninitialized memory#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "Filling uninitialized memory#", "content": "Filling uninitialized memory# Operations like torch.empty() and torch.Tensor.resize_() can return tensors with uninitialized memory that contain undefined values. Using such a tensor as an input to another operation is invalid if determinism is required, because the output will be nondeterministic. But there is nothing to actually prevent such invalid code from being run. So for safety, torch.utils.deterministic.fill_uninitialized_memory is set to True by default, which will fill the uninitialized memory with a known value if torch.use_deterministic_algorithms(True) is set. This will prevent the possibility of this kind of nondeterministic behavior. However, filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance.", "prev_chunk_id": "chunk_1186", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1188", "url": "https://docs.pytorch.org/docs/stable/notes/randomness.html", "title": "DataLoader#", "page_title": "Reproducibility — PyTorch 2.8 documentation", "breadcrumbs": "DataLoader#", "content": "DataLoader# DataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use worker_init_fn() and generator to preserve reproducibility: def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 numpy.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(0) DataLoader( train_dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, generator=g, )", "prev_chunk_id": "chunk_1187", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1189", "url": "https://docs.pytorch.org/docs/stable/notes/out.html", "title": "Out Notes#", "page_title": "Out Notes — PyTorch 2.8 documentation", "breadcrumbs": "Out Notes#", "content": "Out Notes# Created On: Apr 24, 2025 | Last Updated On: Apr 24, 2025 When a user passes one or more tensors to out= the contract is as follows: - if an out tensor has no elements it will be resized to the shape, stride, and memory format of the output of the computation. - if an out tensor has a different shape than the result of the computation an error is thrown OR the out tensor is resized to the same shape, stride, and memory format of the output computation, just like a tensor with no elements. (This resizing behavior is deprecated and PyTorch is updating its operators to consistently throw an error.) - passing out= tensors with the correct shape is numerically equivalent to performing the operation and “safe copying” its results to the (possibly resized) out tensor. In this case strides and memory format are preserved. - passing out= tensors with grad needed is not supported. - if multiple tensors are passed to out= then the above behavior applies to each independently. A “safe copy” is different from PyTorch’s regular copy. For operations that do not participate in type promotion the device and dtype of the source and destination tensors must match. For operations that do participate in type promotion the copy can be to a different dtype, but the destination of the copy cannot be a lower “type kind” than the source. PyTorch has four type kinds: boolean, integer, float, and complex, in that order. So, for example, an operation like add (which participates in type promotion) will throw a runtime error if given float inputs but an integer out= tensor. Note that while the numerics of out= for correctly shaped tensors are that the operation is performed and then its results are “safe copied,” behind", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1190", "url": "https://docs.pytorch.org/docs/stable/notes/out.html", "title": "Out Notes#", "page_title": "Out Notes — PyTorch 2.8 documentation", "breadcrumbs": "Out Notes#", "content": "the scenes operations may reuse the storage of out= tensors and fuse the copy for efficiency. Many operations, like add, perform these optimizations. Also, while PyTorch’s “out= contract” is specified above, many operations in PyTorch do not correctly implement the contract and need to be updated.", "prev_chunk_id": "chunk_1189", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1191", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Numerical accuracy#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Numerical accuracy#", "content": "Numerical accuracy# Created On: Oct 13, 2021 | Last Updated On: Sep 24, 2024 In modern computers, floating point numbers are represented using IEEE 754 standard. For more details on floating point arithmetic and IEEE 754 standard, please see Floating point arithmetic In particular, note that floating point provides limited accuracy (about 7 decimal digits for single precision floating point numbers, about 16 decimal digits for double precision floating point numbers) and that floating point addition and multiplication are not associative, so the order of the operations affects the results. Because of this, PyTorch is not guaranteed to produce bitwise identical results for floating point computations that are mathematically identical. Similarly, bitwise identical results are not guaranteed across PyTorch releases, individual commits, or different platforms. In particular, CPU and GPU results can be different even for bitwise-identical inputs and even after controlling for the sources of randomness.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1192", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Batched computations or slice computations#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Batched computations or slice computations#", "content": "Batched computations or slice computations# Many operations in PyTorch support batched computation, where the same operation is performed for the elements of the batches of inputs. An example of this is torch.mm() and torch.bmm(). It is possible to implement batched computation as a loop over batch elements, and apply the necessary math operations to the individual batch elements, for efficiency reasons we are not doing that, and typically perform computation for the whole batch. The mathematical libraries that we are calling, and PyTorch internal implementations of operations can produces slightly different results in this case, compared to non-batched computations. In particular, let A and B be 3D tensors with the dimensions suitable for batched matrix multiplication. Then (A@B)[0] (the first element of the batched result) is not guaranteed to be bitwise identical to A[0]@B[0] (the matrix product of the first elements of the input batches) even though mathematically it’s an identical computation. Similarly, an operation applied to a tensor slice is not guaranteed to produce results that are identical to the slice of the result of the same operation applied to the full tensor. E.g. let A be a 2-dimensional tensor. A.sum(-1)[0] is not guaranteed to be bitwise equal to A[:,0].sum().", "prev_chunk_id": "chunk_1191", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1193", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Extremal values#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Extremal values#", "content": "Extremal values# When inputs contain large values such that intermediate results may overflow the range of the used datatype, the end result may overflow too, even though it is representable in the original datatype. E.g.: import torch a=torch.tensor([1e20, 1e20]) # fp32 type by default a.norm() # produces tensor(inf) a.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32", "prev_chunk_id": "chunk_1192", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1194", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Non-finite values#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Non-finite values#", "content": "Non-finite values# The external libraries (backends) that torch.linalg uses provide no guarantees on their behaviour when the inputs have non-finite values like inf or NaN. As such, neither does PyTorch. The operations may return a tensor with non-finite values, or raise an exception, or even segfault. Consider using torch.isfinite() before calling these functions to detect this situation.", "prev_chunk_id": "chunk_1193", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1195", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Extremal values in linalg#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Extremal values in linalg#", "content": "Extremal values in linalg# Functions within torch.linalg have more Extremal Values than other PyTorch functions. Solvers and Inverses assume that the input matrix A is invertible. If it is close to being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return incorrect results. These matrices are said to be ill-conditioned. If provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different devices or when using different backends via the keyword driver. Spectral operations like svd, eig, and eigh may also return incorrect results (and their gradients may be infinite) when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions struggle to converge for these inputs. Running the computation in float64 (as NumPy does by default) often helps, but it does not solve these issues in all cases. Analyzing the spectrum of the inputs via torch.linalg.svdvals() or their condition number via torch.linalg.cond() may help to detect these issues.", "prev_chunk_id": "chunk_1194", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1196", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "TensorFloat-32(TF32) on Nvidia Ampere (and later) devices#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "TensorFloat-32(TF32) on Nvidia Ampere (and later) devices#", "content": "TensorFloat-32(TF32) on Nvidia Ampere (and later) devices# On Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions. When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read. This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input). By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32. We recommend enabling TF32 tensor cores for matrix multiplications with torch.backends.cuda.matmul.allow_tf32 = True if your network does not need full float32 precision. If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with torch.backends.cudnn.allow_tf32 = False. For more information see TensorFloat32.", "prev_chunk_id": "chunk_1195", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1197", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Reduced Precision Reduction for FP16 and BF16 GEMMs#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Reduced Precision Reduction for FP16 and BF16 GEMMs#", "content": "Reduced Precision Reduction for FP16 and BF16 GEMMs# Half-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., inf values when the final result should be be representable in half-precision). If reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False A similar flag exists for BF16 GEMM operations and is turned on by default. If BF16 reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False For more information see allow_fp16_reduced_precision_reduction and allow_bf16_reduced_precision_reduction", "prev_chunk_id": "chunk_1196", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1198", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)#", "content": "Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)# A naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul. For scenarios where reduced-precision reductions are preferred for speed, they can be enabled with the following setting: torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)", "prev_chunk_id": "chunk_1197", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1199", "url": "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html", "title": "Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices#", "page_title": "Numerical accuracy — PyTorch 2.8 documentation", "breadcrumbs": "Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices#", "content": "Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices# On AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior. rocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. When training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows: The following is the list of operations where rocBLAS may be used: - torch.addbmm - torch.addmm - torch.baddbmm - torch.bmm - torch.mm - torch.nn.GRUCell - torch.nn.LSTMCell - torch.nn.Linear - torch.sparse.addmm - the following torch._C._ConvBackend implementations:slowNdslowNd_transposedslowNd_dilatedslowNd_dilated_transposed The following is the list of operations where MIOpen may be used: - torch.nn.Conv[Transpose]Nd - the following torch._C._ConvBackend implementations:ConvBackend::MiopenConvBackend::MiopenDepthwiseConvBackend::MiopenTranspose", "prev_chunk_id": "chunk_1198", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1200", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Multiprocessing best practices#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Multiprocessing best practices#", "content": "Multiprocessing best practices# Created On: Jan 16, 2017 | Last Updated On: Jun 18, 2025 torch.multiprocessing is a drop in replacement for Python’s multiprocessing module. It supports the exact same operations, but extends it, so that all tensors sent through a multiprocessing.Queue, will have their data moved into shared memory and will only send a handle to another process. This allows to implement various training methods, like Hogwild, A3C, or any others that require asynchronous operation.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1201", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Poison fork in multiprocessing#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Poison fork in multiprocessing#", "content": "Poison fork in multiprocessing# When using multiprocessing with accelerators, a known issue called “poison fork” may occur. This happens when the accelerator’s runtime is not fork safe and is initialized before a process forks, leading to runtime errors in child processes.", "prev_chunk_id": "chunk_1200", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1202", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "CUDA in multiprocessing#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "CUDA in multiprocessing#", "content": "CUDA in multiprocessing# The CUDA runtime has the limitation described in Poison fork in multiprocessing when using the fork start method; either the spawn or forkserver start method are required to use CUDA in subprocesses. Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. It is implemented under the hood but requires users to follow the best practices for the program to run correctly. For example, the sending process must stay alive as long as the consumer process has references to the tensor, and the refcounting can not save you if the consumer process exits abnormally via a fatal signal. See this section. See also: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel", "prev_chunk_id": "chunk_1201", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1203", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Avoiding and fighting deadlocks#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Avoiding and fighting deadlocks#", "content": "Avoiding and fighting deadlocks# There are a lot of things that can go wrong when a new process is spawned, with the most common cause of deadlocks being background threads. If there’s any thread that holds a lock or imports a module, and fork is called, it’s very likely that the subprocess will be in a corrupted state and will deadlock or fail in a different way. Note that even if you don’t, Python built in libraries do - no need to look further than multiprocessing. multiprocessing.Queue is actually a very complex class, that spawns multiple threads used to serialize, send and receive objects, and they can cause aforementioned problems too. If you find yourself in such situation try using a SimpleQueue, that doesn’t use any additional threads. We’re trying our best to make it easy for you and ensure these deadlocks don’t happen but some things are out of our control. If you have any issues you can’t cope with for a while, try reaching out on forums, and we’ll see if it’s an issue we can fix.", "prev_chunk_id": "chunk_1202", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1204", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Reuse buffers passed through a Queue#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Reuse buffers passed through a Queue#", "content": "Reuse buffers passed through a Queue# Remember that each time you put a Tensor into a multiprocessing.Queue, it has to be moved into shared memory. If it’s already shared, it is a no-op, otherwise it will incur an additional memory copy that can slow down the whole process. Even if you have a pool of processes sending data to a single one, make it send the buffers back - this is nearly free and will let you avoid a copy when sending next batch.", "prev_chunk_id": "chunk_1203", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1205", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Asynchronous multiprocess training (e.g. Hogwild)#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Asynchronous multiprocess training (e.g. Hogwild)#", "content": "Asynchronous multiprocess training (e.g. Hogwild)# Using torch.multiprocessing, it is possible to train a model asynchronously, with parameters either shared all the time, or being periodically synchronized. In the first case, we recommend sending over the whole model object, while in the latter, we advise to only send the state_dict(). We recommend using multiprocessing.Queue for passing all kinds of PyTorch objects between processes. It is possible to e.g. inherit the tensors and storages already in shared memory, when using the fork start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.", "prev_chunk_id": "chunk_1204", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1206", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Hogwild#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Hogwild#", "content": "Hogwild# A concrete Hogwild implementation can be found in the examples repository, but to showcase the overall structure of the code, there’s also a minimal example below as well: import torch.multiprocessing as mp from model import MyModel def train(model): # Construct data_loader, optimizer, etc. for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() # This will update the shared parameters if __name__ == '__main__': num_processes = 4 model = MyModel() # NOTE: this is required for the ``fork`` method to work model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join()", "prev_chunk_id": "chunk_1205", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1207", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "CPU in multiprocessing#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "CPU in multiprocessing#", "content": "CPU in multiprocessing# Inappropriate multiprocessing can lead to CPU oversubscription, causing different processes to compete for CPU resources, resulting in low efficiency. This tutorial will explain what CPU oversubscription is and how to avoid it.", "prev_chunk_id": "chunk_1206", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1208", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "CPU oversubscription#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "CPU oversubscription#", "content": "CPU oversubscription# CPU oversubscription is a technical term that refers to a situation where the total number of vCPUs allocated to a system exceeds the total number of vCPUs available on the hardware. This leads to severe contention for CPU resources. In such cases, there is frequent switching between processes, which increases processes switching overhead and decreases overall system efficiency. See CPU oversubscription with the code examples in the Hogwild implementation found in the example repository. When running the training example with the following command on CPU using 4 processes: python main.py --num-processes 4 Assuming there are N vCPUs available on the machine, executing the above command will generate 4 subprocesses. Each subprocess will allocate N vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the machine only has N vCPUs available. Consequently, the different processes will compete for resources, leading to frequent process switching. The following observations indicate the presence of CPU over subscription: - High CPU Utilization: By using thehtopcommand, you can observe that the CPU utilization is consistently high, often reaching or exceeding its maximum capacity. This indicates that the demand for CPU resources exceeds the available physical cores, causing contention and competition among processes for CPU time. - Frequent Context Switching with Low System Efficiency: In an oversubscribed CPU scenario, processes compete for CPU time, and the operating system needs to rapidly switch between different processes to allocate resources fairly. This frequent context switching adds overhead and reduces the overall system efficiency.", "prev_chunk_id": "chunk_1207", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1209", "url": "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html", "title": "Avoid CPU oversubscription#", "page_title": "Multiprocessing best practices — PyTorch 2.8 documentation", "breadcrumbs": "Avoid CPU oversubscription#", "content": "Avoid CPU oversubscription# A good way to avoid CPU oversubscription is proper resource allocation. Ensure that the number of processes or threads running concurrently does not exceed the available CPU resources. In this case, a solution would be to specify the appropriate number of threads in the subprocesses. This can be achieved by setting the number of threads for each process using the torch.set_num_threads(int) function in subprocess. Assuming there are N vCPUs on the machine and M processes will be generated, the maximum num_threads value used by each process would be floor(N/M). To avoid CPU oversubscription in the mnist_hogwild example, the following changes are needed for the file train.py in example repository. def train(rank, args, model, device, dataset, dataloader_kwargs): torch.manual_seed(args.seed + rank) #### define the num threads used in current sub-processes torch.set_num_threads(floor(N/M)) train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs) optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) for epoch in range(1, args.epochs + 1): train_epoch(epoch, args, model, device, train_loader, optimizer) Set num_thread for each process using torch.set_num_threads(floor(N/M)). where you replace N with the number of vCPUs available and M with the chosen number of processes. The appropriate num_thread value will vary depending on the specific task at hand. However, as a general guideline, the maximum value for the num_thread should be floor(N/M) to avoid CPU oversubscription. In the mnist_hogwild training example, after avoiding CPU over subscription, you can achieve a 30x performance boost.", "prev_chunk_id": "chunk_1208", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1210", "url": "https://docs.pytorch.org/docs/stable/notes/mps.html", "title": "MPS backend#", "page_title": "MPS backend — PyTorch 2.8 documentation", "breadcrumbs": "MPS backend#", "content": "MPS backend# Created On: May 13, 2022 | Last Updated On: Jun 02, 2022 mps device enables high-performance training on GPU for MacOS devices with Metal programming framework. It introduces a new device to map Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively. The new MPS backend extends the PyTorch ecosystem and provides existing scripts capabilities to setup and run operations on GPU. To get started, simply move your Tensor and Module to the mps device: # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\"MPS not available because the current PyTorch install was not \" \"built with MPS enabled.\") else: print(\"MPS not available because the current MacOS version is not 12.3+ \" \"and/or you do not have an MPS-enabled device on this machine.\") else: mps_device = torch.device(\"mps\") # Create a Tensor directly on the mps device x = torch.ones(5, device=mps_device) # Or x = torch.ones(5, device=\"mps\") # Any operation happens on the GPU y = x * 2 # Move your model to mps just like any other device model = YourFavoriteNet() model.to(mps_device) # Now every call runs on the GPU pred = model(x)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1211", "url": "https://docs.pytorch.org/docs/stable/notes/libtorch_stable_abi.html", "title": "LibTorch Stable ABI#", "page_title": "LibTorch Stable ABI — PyTorch 2.8 documentation", "breadcrumbs": "LibTorch Stable ABI#", "content": "LibTorch Stable ABI# Created On: Mar 17, 2025 | Last Updated On: Jun 17, 2025 This note will eventually contain more details on how to use the APIs in torch/csrc/stable. For the moment, it contains a table of internal representations: - type in custom extension: type used within the end user custom library. - StableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner. - type in libtorch: type used within libtorch.so (or any code binary locked with libtorch). - Schema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library. Our confidently supported types are the ones in the table that have completed rows. You can rely on this subset proper ABI stability. For a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. These types are currently ABI-stable on best effort but might break in the future and thus should be used for short term testing only. You can always work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions by not introspecting into the StableIValue. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator with aoti_torch_call_dispatcher.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1212", "url": "https://docs.pytorch.org/docs/stable/notes/libtorch_stable_abi.html", "title": "How to use stack-based APIs#", "page_title": "LibTorch Stable ABI — PyTorch 2.8 documentation", "breadcrumbs": "How to use stack-based APIs#", "content": "How to use stack-based APIs# aoti_torch_call_dispatcher is what we consider a stack-based API because it takes as input a stack of StableIValues. Working with the dispatcher will likely bring you into proximity with stack-based APIs, so we are documenting some invariants: - The stack is populated left to right. a. For example, a stack representing argumentsarg0,arg1, andarg2will havearg0at index 0,arg1at index 1, andarg2at index 2. b. Returns are also populated left to right, e.g.,ret0will be at index 0 andret1will be at index 1, and so on. - The stack always has ownership of the objects it holds. a. When calling a stack-based API, you must give owning references to the calling stack and steal references from the returned stack. b. When registering your function to be called with a stack, you must steal references from your argument stack and push onto the stack new references.", "prev_chunk_id": "chunk_1211", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1213", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Features for large-scale deployments#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Features for large-scale deployments#", "content": "Features for large-scale deployments# Created On: Jul 24, 2019 | Last Updated On: May 11, 2020 This note talks about several extension points and tricks that might be useful when running PyTorch within a larger system or operating multiple systems using PyTorch in a larger organization. It doesn’t cover topics of deploying models to production. Check torch.jit or one of the corresponding tutorials. The note assumes that you either build PyTorch from source in your organization or have an ability to statically link additional code to be loaded when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that can be triggered once in a centralized place, e.g. in static initialization code.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1214", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Fleet-wide operator profiling#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Fleet-wide operator profiling#", "content": "Fleet-wide operator profiling# PyTorch comes with torch.autograd.profiler capable of measuring time taken by individual operators on demand. One can use the same mechanism to do “always ON” measurements for any process running PyTorch. It might be useful for gathering information about PyTorch workloads running in a given process or across the entire set of machines. New callbacks for any operator invocation can be added with torch::addGlobalCallback. Hooks will be called with torch::RecordFunction struct that describes invocation context (e.g. name). If enabled, RecordFunction::inputs() contains arguments of the function represented as torch::IValue variant type. Note, that inputs logging is relatively expensive and thus has to be enabled explicitly. The operator callbacks also have access to c10::ThreadLocalDebugInfo::get() interface that returns a pointer to the struct holding the debug information. This debug information can be set earlier by using at::DebugInfoGuard object. Debug information is propagated through the forward (including async fork tasks) and backward passes and can be useful for passing some extra information about execution environment (e.g. model id) from the higher layers of the application down to the operator callbacks. Invoking callbacks adds some overhead, so usually it’s useful to just randomly sample operator invocations. This can be enabled on per-callback basis with an optional sampling rate passed into torch::addGlobalCallback. Note, that addGlobalCallback is not thread-safe and can be called only when no PyTorch operator is running. Usually, it’s a good idea to call them once during initialization. Here’s an example: // Called somewhere in the program beginning void init() { // Sample one in a hundred operator runs randomly addGlobalCallback( RecordFunctionCallback( &onFunctionEnter, &onFunctionExit) .needsInputs(true) .samplingProb(0.01) ); // Note, to enable observers in the model calling thread, // call enableRecordFunction() in the thread before running a model } void onFunctionEnter(const RecordFunction& fn) { std::cerr << \"Before function \" << fn.name() << \"", "prev_chunk_id": "chunk_1213", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1215", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Fleet-wide operator profiling#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Fleet-wide operator profiling#", "content": "with \" << fn.inputs().size() << \" inputs\" << std::endl; } void onFunctionExit(const RecordFunction& fn) { std::cerr << \"After function \" << fn.name(); }", "prev_chunk_id": "chunk_1214", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1216", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "API usage logging#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "API usage logging#", "content": "API usage logging# When running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in one-off python scripts, the callback fires only once for a given process for each of the APIs. c10::SetAPIUsageHandler can be used to register API usage instrumentation handler. Passed argument is going to be an “api key” identifying used point, for example python.import for PyTorch extension import or torch.script.compile if TorchScript compilation was triggered. SetAPIUsageLogger([](const std::string& event_name) { std::cerr << \"API was used: \" << event_name << std::endl; }); Note for developers: new API trigger points can be added in code with C10_LOG_API_USAGE_ONCE(\"my_api\") in C++ or torch._C._log_api_usage_once(\"my.api\") in Python.", "prev_chunk_id": "chunk_1215", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1217", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Attaching metadata to saved TorchScript models#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Attaching metadata to saved TorchScript models#", "content": "Attaching metadata to saved TorchScript models# TorchScript modules can be saved as an archive file that bundles serialized parameters and module code as TorchScript (see torch.jit.save()). It’s often convenient to bundle additional information together with the model, for example, description of model producer or auxiliary artifacts. It can be achieved by passing the _extra_files argument to torch.jit.save() and torch::jit::load to store and retrieve arbitrary binary blobs during saving process. Since TorchScript files are regular ZIP archives, extra information gets stored as regular files inside archive’s extra/ directory. There’s also a global hook allowing to attach extra files to any TorchScript archive produced in the current process. It might be useful to tag models with producer metadata, akin to JPEG metadata produced by digital cameras. Example usage might look like: SetExportModuleExtraFilesHook([](const Module&) { ExtraFilesMap files; files[\"producer_info.json\"] = \"{\\\"user\\\": \\\"\" + getenv(\"USER\") + \"\\\"}\"; return files; });", "prev_chunk_id": "chunk_1216", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1218", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Build environment considerations#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Build environment considerations#", "content": "Build environment considerations# TorchScript’s compilation needs to have access to the original python files as it uses python’s inspect.getsource call. In certain production environments it might require explicitly deploying .py files along with precompiled .pyc.", "prev_chunk_id": "chunk_1217", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1219", "url": "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html", "title": "Common extension points#", "page_title": "Features for large-scale deployments — PyTorch 2.8 documentation", "breadcrumbs": "Common extension points#", "content": "Common extension points# PyTorch APIs are generally loosely coupled and it’s easy to replace a component with specialized version. Common extension points include: - Custom operators implemented in C++ - seetutorial for more details. - Custom data reading can be often integrated directly by invoking corresponding python library. Existing functionality oftorch.utils.datacan be utilized by extendingDatasetorIterableDataset.", "prev_chunk_id": "chunk_1218", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1220", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "HIP (ROCm) semantics#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "HIP (ROCm) semantics#", "content": "HIP (ROCm) semantics# Created On: May 12, 2021 | Last Updated On: Apr 27, 2025 ROCm™ is AMD’s open source software platform for GPU-accelerated high performance computing and machine learning. HIP is ROCm’s C++ dialect designed to ease conversion of CUDA applications to portable C++ code. HIP is used when converting existing CUDA applications like PyTorch to portable C++ and for new projects that require portability between AMD and NVIDIA.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1221", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "HIP Interfaces Reuse the CUDA Interfaces#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "HIP Interfaces Reuse the CUDA Interfaces#", "content": "HIP Interfaces Reuse the CUDA Interfaces# PyTorch for HIP intentionally reuses the existing torch.cuda interfaces. This helps to accelerate the porting of existing PyTorch code and models because very few code changes are necessary, if any. The example from CUDA semantics will work exactly the same for HIP: cuda = torch.device('cuda') # Default HIP device cuda0 = torch.device('cuda:0') # 'rocm' or 'hip' are not valid, use 'cuda' cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed) x = torch.tensor([1., 2.], device=cuda0) # x.device is device(type='cuda', index=0) y = torch.tensor([1., 2.]).cuda() # y.device is device(type='cuda', index=0) with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.tensor([1., 2.], device=cuda) # transfers a tensor from CPU to GPU 1 b = torch.tensor([1., 2.]).cuda() # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch.tensor([1., 2.]).to(device=cuda) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch.randn(2, device=cuda2) e = torch.randn(2).to(cuda2) f = torch.randn(2).cuda(cuda2) # d.device, e.device, and f.device are all device(type='cuda', index=2)", "prev_chunk_id": "chunk_1220", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1222", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "Checking for HIP#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "Checking for HIP#", "content": "Checking for HIP# Whether you are using PyTorch for CUDA or HIP, the result of calling is_available() will be the same. If you are using a PyTorch that has been built with GPU support, it will return True. If you must check which version of PyTorch you are using, refer to this example below: if torch.cuda.is_available() and torch.version.hip: # do something specific for HIP elif torch.cuda.is_available() and torch.version.cuda: # do something specific for CUDA", "prev_chunk_id": "chunk_1221", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1223", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "TensorFloat-32(TF32) on ROCm#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "TensorFloat-32(TF32) on ROCm#", "content": "TensorFloat-32(TF32) on ROCm# TF32 is not supported on ROCm.", "prev_chunk_id": "chunk_1222", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1224", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "Memory management#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "Memory management#", "content": "Memory management# PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in rocm-smi. You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch. For more advanced users, we offer more comprehensive memory benchmarking via memory_stats(). We also offer the capability to capture a complete snapshot of the memory allocator state via memory_snapshot(), which can help you understand the underlying allocation patterns produced by your code. To debug memory errors, set PYTORCH_NO_HIP_MEMORY_CACHING=1 in your environment to disable caching. PYTORCH_NO_CUDA_MEMORY_CACHING=1 is also accepted for ease of porting.", "prev_chunk_id": "chunk_1223", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1225", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "hipBLAS workspaces#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "hipBLAS workspaces#", "content": "hipBLAS workspaces# For each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that handle and stream combination executes a hipBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless torch._C._cuda_clearCublasWorkspaces() is called; note that it’s the same function for CUDA or HIP. The workspace size per allocation can be specified via the environment variable HIPBLAS_WORKSPACE_CONFIG with the format :[SIZE]:[COUNT]. As an example, the environment variable HIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8 specifies a total size of 2 * 4096 + 8 * 16 KiB or 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force hipBLAS to avoid using workspaces, set HIPBLAS_WORKSPACE_CONFIG=:0:0. For convenience, CUBLAS_WORKSPACE_CONFIG is also accepted.", "prev_chunk_id": "chunk_1224", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1226", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "hipFFT/rocFFT plan cache#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "hipFFT/rocFFT plan cache#", "content": "hipFFT/rocFFT plan cache# Setting the size of the cache for hipFFT/rocFFT plans is not supported.", "prev_chunk_id": "chunk_1225", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1227", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "torch.distributed backends#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "torch.distributed backends#", "content": "torch.distributed backends# Currently, only the “nccl” and “gloo” backends for torch.distributed are supported on ROCm.", "prev_chunk_id": "chunk_1226", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1228", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "CUDA API to HIP API mappings in C++#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "CUDA API to HIP API mappings in C++#", "content": "CUDA API to HIP API mappings in C++# Please refer: https://rocm.docs.amd.com/projects/HIP/en/latest/reference/api_syntax.html NOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks. For example: Instead of using #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 to implicitly exclude ROCm/HIP, use the following to not take the code path for ROCm/HIP: #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM) Alternatively, if it is desired to take the code path for ROCm/HIP: #if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM) Or if it is desired to take the code path for ROCm/HIP only for specific HIP versions: #if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)", "prev_chunk_id": "chunk_1227", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1229", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "Refer to CUDA Semantics doc#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "Refer to CUDA Semantics doc#", "content": "Refer to CUDA Semantics doc# For any sections not listed here, please refer to the CUDA semantics doc: CUDA semantics", "prev_chunk_id": "chunk_1228", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1230", "url": "https://docs.pytorch.org/docs/stable/notes/hip.html", "title": "Enabling kernel asserts#", "page_title": "HIP (ROCm) semantics — PyTorch 2.8 documentation", "breadcrumbs": "Enabling kernel asserts#", "content": "Enabling kernel asserts# Kernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled by recompiling the PyTorch from source. Please add below line as an argument to cmake command parameters: -DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON", "prev_chunk_id": "chunk_1229", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1231", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Gradcheck mechanics#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Gradcheck mechanics#", "content": "Gradcheck mechanics# Created On: Apr 27, 2021 | Last Updated On: Jun 18, 2025 This note presents an overview of how the gradcheck() and gradgradcheck() functions work. It will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where fast_mode=True argument is passed (referred to as fast gradcheck below).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1232", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Notations and background information#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Notations and background information#", "content": "Notations and background information# Throughout this note, we will use the following convention: - xxx,yyy,aaa,bbb,vvv,uuu,urururanduiuiuiare real-valued vectors andzzzis a complex-valued vector that can be rewritten in terms of two real-valued vectors asz=a+ibz = a + i bz=a+ib. - NNNandMMMare two integers that we will use for the dimension of the input and output space respectively. - f:RN→RMf: \\mathcal{R}^N \\to \\mathcal{R}^Mf:RN→RMis our basic real-to-real function such thaty=f(x)y = f(x)y=f(x). - g:CN→RMg: \\mathcal{C}^N \\to \\mathcal{R}^Mg:CN→RMis our basic complex-to-real function such thaty=g(z)y = g(z)y=g(z). For the simple real-to-real case, we write as JfJ_fJf​ the Jacobian matrix associated with fff of size M×NM \\times NM×N. This matrix contains all the partial derivatives such that the entry at position (i,j)(i, j)(i,j) contains ∂yi∂xj\\frac{\\partial y_i}{\\partial x_j}∂xj​∂yi​​. Backward mode AD is then computing, for a given vector vvv of size MMM, the quantity vTJfv^T J_fvTJf​. Forward mode AD on the other hand is computing, for a given vector uuu of size NNN, the quantity JfuJ_f uJf​u. For functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at Autograd for Complex Numbers. The constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called WWW below) and the Conjugate Wirtinger derivative (called CWCWCW below). Both WWW and CWCWCW need to be propagated because in general, despite their name, one is not the complex conjugate of the other. To avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a", "prev_chunk_id": "chunk_1231", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1233", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Notations and background information#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Notations and background information#", "content": "bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers). Under this assumption, using WWW and CWCWCW definitions, we can show that W=CW∗W = CW^*W=CW∗ (we use ∗*∗ to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered. To simplify internal computations, PyTorch uses 2∗CW2 * CW2∗CW as the value it backwards and returns when the user asks for gradients. Similarly to the real case, when the output is actually in RM\\mathcal{R}^MRM, backward mode AD does not compute 2∗CW2 * CW2∗CW but only vT(2∗CW)v^T (2 * CW)vT(2∗CW) for a given vector v∈RMv \\in \\mathcal{R}^Mv∈RM. For forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in R\\mathcal{R}R. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in R\\mathcal{R}R and in this case, using WWW and CWCWCW definitions, we can show that W=CWW = CWW=CW for the intermediary functions. To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes 2∗CW2 * CW2∗CW. Similarly to the real case, when the input is actually in RN\\mathcal{R}^NRN, forward mode AD does not compute 2∗CW2 * CW2∗CW but only (2∗CW)u(2 * CW) u(2∗CW)u for a given vector u∈RNu \\in \\mathcal{R}^Nu∈RN.", "prev_chunk_id": "chunk_1232", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1234", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Real-to-real functions#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Real-to-real functions#", "content": "Real-to-real functions# To test a function f:RN→RM,x→yf: \\mathcal{R}^N \\to \\mathcal{R}^M, x \\to yf:RN→RM,x→y, we reconstruct the full Jacobian matrix JfJ_fJf​ of size M×NM \\times NM×N in two ways: analytically and numerically. The analytical version uses our backward mode AD while the numerical version uses finite difference. The two reconstructed Jacobian matrices are then compared elementwise for equality.", "prev_chunk_id": "chunk_1233", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1235", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Default real input numerical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Default real input numerical evaluation#", "content": "Default real input numerical evaluation# If we consider the elementary case of a one-dimensional function (N=M=1N = M = 1N=M=1), then we can use the basic finite difference formula from the wikipedia article. We use the “central difference” for better numerical properties: This formula easily generalizes for multiple outputs (M>1M \\gt 1M>1) by having ∂y∂x\\frac{\\partial y}{\\partial x}∂x∂y​ be a column vector of size M×1M \\times 1M×1 like f(x+eps)f(x + eps)f(x+eps). In that case, the above formula can be reused as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely f(x+eps)f(x + eps)f(x+eps) and f(x−eps)f(x - eps)f(x−eps)). It is more computationally expensive to handle the case with multiple inputs (N>1N \\gt 1N>1). In this scenario, we loop over all the inputs one after the other and apply the epsepseps perturbation for each element of xxx one after the other. This allows us to reconstruct the JfJ_fJf​ matrix column by column.", "prev_chunk_id": "chunk_1234", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1236", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Default real input analytical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Default real input analytical evaluation#", "content": "Default real input analytical evaluation# For the analytical evaluation, we use the fact, as described above, that backward mode AD computes vTJfv^T J_fvTJf​. For functions with a single output, we simply use v=1v = 1v=1 to recover the full Jacobian matrix with a single backward pass. For functions with more than one output, we resort to a for-loop which iterates over the outputs where each vvv is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the JfJ_fJf​ matrix row by row.", "prev_chunk_id": "chunk_1235", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1237", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Complex-to-real functions#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Complex-to-real functions#", "content": "Complex-to-real functions# To test a function g:CN→RM,z→yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN→RM,z→y with z=a+ibz = a + i bz=a+ib, we reconstruct the (complex-valued) matrix that contains 2∗CW2 * CW2∗CW.", "prev_chunk_id": "chunk_1236", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1238", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Default complex input numerical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Default complex input numerical evaluation#", "content": "Default complex input numerical evaluation# Consider the elementary case where N=M=1N = M = 1N=M=1 first. We know from (chapter 3 of) this research paper that: Note that ∂y∂a\\frac{\\partial y}{\\partial a}∂a∂y​ and ∂y∂b\\frac{\\partial y}{\\partial b}∂b∂y​, in the above equation, are R→R\\mathcal{R} \\to \\mathcal{R}R→R derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. This allows us to compute the CWCWCW matrix and then multiply it by 222. Note that the code, as of time of writing, computes this value in a slightly convoluted way: # Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105 # Notation changes in this code block: # s here is y above # x, y here are a, b above ds_dx = compute_gradient(eps) ds_dy = compute_gradient(eps * 1j) # conjugate wirtinger derivative conj_w_d = 0.5 * (ds_dx + ds_dy * 1j) # wirtinger derivative w_d = 0.5 * (ds_dx - ds_dy * 1j) d[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj() # Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.", "prev_chunk_id": "chunk_1237", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1239", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Default complex input analytical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Default complex input analytical evaluation#", "content": "Default complex input analytical evaluation# Since backward mode AD computes exactly twice the CWCWCW derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.", "prev_chunk_id": "chunk_1238", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1240", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Functions with complex outputs#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Functions with complex outputs#", "content": "Functions with complex outputs# In this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. To solve this, we will replace the test of the function h:PN→CMh: \\mathcal{P}^N \\to \\mathcal{C}^Mh:PN→CM (where P\\mathcal{P}P can be either R\\mathcal{R}R or C\\mathcal{C}C), with two functions: hrhrhr and hihihi such that: where q∈Pq \\in \\mathcal{P}q∈P. We then do a basic gradcheck for both hrhrhr and hihihi using either the real-to-real or complex-to-real case described above, depending on P\\mathcal{P}P. Note that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the realrealreal or imagimagimag functions manually by passing the grad_out\\text{grad\\_out}grad_out arguments to the different functions. When grad_out=1\\text{grad\\_out} = 1grad_out=1, then we are considering hrhrhr. When grad_out=1j\\text{grad\\_out} = 1jgrad_out=1j, then we are considering hihihi.", "prev_chunk_id": "chunk_1239", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1241", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast backward mode gradcheck#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast backward mode gradcheck#", "content": "Fast backward mode gradcheck# While the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices. This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user. The high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.", "prev_chunk_id": "chunk_1240", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1242", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast gradcheck for real-to-real functions#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast gradcheck for real-to-real functions#", "content": "Fast gradcheck for real-to-real functions# The scalar quantity that we want to compute here is vTJfuv^T J_f uvTJf​u for a given random vector v∈RMv \\in \\mathcal{R}^Mv∈RM and a random unit norm vector u∈RNu \\in \\mathcal{R}^Nu∈RN. For the numerical evaluation, we can efficiently compute We then perform the dot product between this vector and vvv to get the scalar value of interest. For the analytical version, we can use backward mode AD to compute vTJfv^T J_fvTJf​ directly. We then perform the dot product with uuu to get the expected value.", "prev_chunk_id": "chunk_1241", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1243", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast gradcheck for complex-to-real functions#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast gradcheck for complex-to-real functions#", "content": "Fast gradcheck for complex-to-real functions# Similar to the real-to-real case, we want to perform a reduction of the full matrix. But the 2∗CW2 * CW2∗CW matrix is complex-valued and so in this case, we will compare to complex scalars. Due to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value: where v∈RMv \\in \\mathcal{R}^Mv∈RM, ur∈RNur \\in \\mathcal{R}^Nur∈RN and ui∈RNui \\in \\mathcal{R}^Nui∈RN.", "prev_chunk_id": "chunk_1242", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1244", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast complex input numerical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast complex input numerical evaluation#", "content": "Fast complex input numerical evaluation# We first consider how to compute sss with a numerical method. To do so, keeping in mind that we’re considering g:CN→RM,z→yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN→RM,z→y with z=a+ibz = a + i bz=a+ib, and that CW=12∗(∂y∂a+i∂y∂b)CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})CW=21​∗(∂a∂y​+i∂b∂y​), we rewrite it as follows: In this formula, we can see that ∂y∂aur\\frac{\\partial y}{\\partial a} ur∂a∂y​ur and ∂y∂bui\\frac{\\partial y}{\\partial b} ui∂b∂y​ui can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued vvv vector.", "prev_chunk_id": "chunk_1243", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1245", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast complex input analytical evaluation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast complex input analytical evaluation#", "content": "Fast complex input analytical evaluation# For the analytical case, things are simpler and we rewrite the formula as: We can thus use the fact that the backward mode AD provides us with an efficient way to compute vT(2∗CW)v^T (2 * CW)vT(2∗CW) and then perform a dot product of the real part with ururur and the imaginary part with uiuiui before reconstructing the final complex scalar sss.", "prev_chunk_id": "chunk_1244", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1246", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Why not use a complex uuu#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Why not use a complex uuu#", "content": "Why not use a complex uuu# At this point, you might be wondering why we did not select a complex uuu and just performed the reduction 2∗vTCWu′2 * v^T CW u'2∗vTCWu′. To dive into this, in this paragraph, we will use the complex version of uuu noted u′=ur′+iui′u' = ur' + i ui'u′=ur′+iui′. Using such complex u′u'u′, the problem is that when doing the numerical evaluation, we would need to compute: Which would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.", "prev_chunk_id": "chunk_1245", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1247", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Fast gradcheck for functions with complex outputs#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Fast gradcheck for functions with complex outputs#", "content": "Fast gradcheck for functions with complex outputs# Just like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.", "prev_chunk_id": "chunk_1246", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1248", "url": "https://docs.pytorch.org/docs/stable/notes/gradcheck.html", "title": "Gradgradcheck implementation#", "page_title": "Gradcheck mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Gradgradcheck implementation#", "content": "Gradgradcheck implementation# PyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing. This feature is implemented by considering the function F:x,v→vTJfF: x, v \\to v^T J_fF:x,v→vTJf​ and use the gradcheck defined above on this function. Note that vvv in this case is just a random vector with the same type as f(x)f(x)f(x). The fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function FFF.", "prev_chunk_id": "chunk_1247", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1249", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Getting Started on Intel GPU#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Getting Started on Intel GPU#", "content": "Getting Started on Intel GPU# Created On: Jun 14, 2024 | Last Updated On: Jun 18, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1250", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Hardware Prerequisite#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Hardware Prerequisite#", "content": "Hardware Prerequisite# For Intel Data Center GPU For Intel Client GPU Intel GPUs support (Prototype) is ready from PyTorch* 2.5 for Intel® Client GPUs and Intel® Data Center GPU Max Series on both Linux and Windows, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack with consistent user experience to embrace more AI application scenarios.", "prev_chunk_id": "chunk_1249", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1251", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Software Prerequisite#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Software Prerequisite#", "content": "Software Prerequisite# To use PyTorch on Intel GPUs, you need to install the Intel GPUs driver first. For installation guide, visit Intel GPUs Driver Installation. Please skip the Intel® Deep Learning Essentials installation section if you install from binaries. For building from source, please refer to PyTorch Installation Prerequisites for Intel GPUs for both Intel GPU Driver and Intel® Deep Learning Essentials Installation.", "prev_chunk_id": "chunk_1250", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1252", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Binaries#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Binaries#", "content": "Binaries# Now that we have Intel GPU Driver installed, use the following commands to install pytorch, torchvision, torchaudio. For release wheels pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu For nightly wheels pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu", "prev_chunk_id": "chunk_1251", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1253", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "From Source#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "From Source#", "content": "From Source# Now that we have Intel GPU Driver and Intel® Deep Learning Essentials installed. Follow guides to build pytorch, torchvision, torchaudio from source. Build from source for torch refer to PyTorch Installation Build from source. Build from source for torchvision refer to Torchvision Installation Build from source. Build from source for torchaudio refer to Torchaudio Installation Build from source.", "prev_chunk_id": "chunk_1252", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1254", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Check availability for Intel GPU#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Check availability for Intel GPU#", "content": "Check availability for Intel GPU# To check if your Intel GPU is available, you would typically use the following code: import torch print(torch.xpu.is_available()) # torch.xpu is the API for Intel GPU support If the output is False, double check driver installation for Intel GPUs.", "prev_chunk_id": "chunk_1253", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1255", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Minimum Code Change#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Minimum Code Change#", "content": "Minimum Code Change# If you are migrating code from cuda, you would change references from cuda to xpu. For example: # CUDA CODE tensor = torch.tensor([1.0, 2.0]).to(\"cuda\") # CODE for Intel GPU tensor = torch.tensor([1.0, 2.0]).to(\"xpu\") The following points outline the support and limitations for PyTorch with Intel GPU: - Both training and inference workflows are supported. - Both eager mode andtorch.compileis supported. The featuretorch.compileis also supported on Windows from PyTorch* 2.7 with Intel GPU, refer toHow to Use Inductor on Windows with CPU/XPU. - Data types such as FP32, BF16, FP16, and Automatic Mixed Precision (AMP) are all supported.", "prev_chunk_id": "chunk_1254", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1256", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Examples#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Examples#", "content": "Examples# This section contains usage examples for both inference and training workflows.", "prev_chunk_id": "chunk_1255", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1257", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Inference Examples#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Inference Examples#", "content": "Inference Examples# Here is a few inference workflow examples.", "prev_chunk_id": "chunk_1256", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1258", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Inference with FP32#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Inference with FP32#", "content": "Inference with FP32# import torch import torchvision.models as models model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\") model.eval() data = torch.rand(1, 3, 224, 224) model = model.to(\"xpu\") data = data.to(\"xpu\") with torch.no_grad(): model(data) print(\"Execution finished\")", "prev_chunk_id": "chunk_1257", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1259", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Inference with AMP#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Inference with AMP#", "content": "Inference with AMP# import torch import torchvision.models as models model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\") model.eval() data = torch.rand(1, 3, 224, 224) model = model.to(\"xpu\") data = data.to(\"xpu\") with torch.no_grad(): d = torch.rand(1, 3, 224, 224) d = d.to(\"xpu\") # set dtype=torch.bfloat16 for BF16 with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=True): model(data) print(\"Execution finished\")", "prev_chunk_id": "chunk_1258", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1260", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Inference with torch.compile#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Inference with torch.compile#", "content": "Inference with torch.compile# import torch import torchvision.models as models import time model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\") model.eval() data = torch.rand(1, 3, 224, 224) ITERS = 10 model = model.to(\"xpu\") data = data.to(\"xpu\") for i in range(ITERS): start = time.time() with torch.no_grad(): model(data) torch.xpu.synchronize() end = time.time() print(f\"Inference time before torch.compile for iteration {i}: {(end-start)*1000} ms\") model = torch.compile(model) for i in range(ITERS): start = time.time() with torch.no_grad(): model(data) torch.xpu.synchronize() end = time.time() print(f\"Inference time after torch.compile for iteration {i}: {(end-start)*1000} ms\") print(\"Execution finished\")", "prev_chunk_id": "chunk_1259", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1261", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Training Examples#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Training Examples#", "content": "Training Examples# Here is a few training workflow examples.", "prev_chunk_id": "chunk_1260", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1262", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Train with FP32#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Train with FP32#", "content": "Train with FP32# import torch import torchvision LR = 0.001 DOWNLOAD = True DATA = \"datasets/cifar10/\" transform = torchvision.transforms.Compose( [ torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ] ) train_dataset = torchvision.datasets.CIFAR10( root=DATA, train=True, transform=transform, download=DOWNLOAD, ) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128) train_len = len(train_loader) model = torchvision.models.resnet50() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9) model.train() model = model.to(\"xpu\") criterion = criterion.to(\"xpu\") print(f\"Initiating training\") for batch_idx, (data, target) in enumerate(train_loader): data = data.to(\"xpu\") target = target.to(\"xpu\") optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if (batch_idx + 1) % 10 == 0: iteration_loss = loss.item() print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\") torch.save( { \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"checkpoint.pth\", ) print(\"Execution finished\")", "prev_chunk_id": "chunk_1261", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1263", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Train with AMP#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Train with AMP#", "content": "Train with AMP# Note: Training with GradScaler requires hardware support for FP64. FP64 is not natively supported by the Intel® Arc™ A-Series Graphics. If you run your workloads on Intel® Arc™ A-Series Graphics, please disable GradScaler. import torch import torchvision LR = 0.001 DOWNLOAD = True DATA = \"datasets/cifar10/\" use_amp=True transform = torchvision.transforms.Compose( [ torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ] ) train_dataset = torchvision.datasets.CIFAR10( root=DATA, train=True, transform=transform, download=DOWNLOAD, ) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128) train_len = len(train_loader) model = torchvision.models.resnet50() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9) scaler = torch.amp.GradScaler(device=\"xpu\", enabled=use_amp) model.train() model = model.to(\"xpu\") criterion = criterion.to(\"xpu\") print(f\"Initiating training\") for batch_idx, (data, target) in enumerate(train_loader): data = data.to(\"xpu\") target = target.to(\"xpu\") # set dtype=torch.bfloat16 for BF16 with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=use_amp): output = model(data) loss = criterion(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() if (batch_idx + 1) % 10 == 0: iteration_loss = loss.item() print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\") torch.save( { \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"checkpoint.pth\", ) print(\"Execution finished\")", "prev_chunk_id": "chunk_1262", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1264", "url": "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html", "title": "Train with torch.compile#", "page_title": "Getting Started on Intel GPU — PyTorch 2.8 documentation", "breadcrumbs": "Train with torch.compile#", "content": "Train with torch.compile# import torch import torchvision LR = 0.001 DOWNLOAD = True DATA = \"datasets/cifar10/\" transform = torchvision.transforms.Compose( [ torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ] ) train_dataset = torchvision.datasets.CIFAR10( root=DATA, train=True, transform=transform, download=DOWNLOAD, ) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128) train_len = len(train_loader) model = torchvision.models.resnet50() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9) model.train() model = model.to(\"xpu\") criterion = criterion.to(\"xpu\") model = torch.compile(model) print(f\"Initiating training with torch compile\") for batch_idx, (data, target) in enumerate(train_loader): data = data.to(\"xpu\") target = target.to(\"xpu\") optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if (batch_idx + 1) % 10 == 0: iteration_loss = loss.item() print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\") torch.save( { \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"checkpoint.pth\", ) print(\"Execution finished\")", "prev_chunk_id": "chunk_1263", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1265", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP Notes#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP Notes#", "content": "FSDP Notes# Created On: Jan 21, 2024 | Last Updated On: Jun 07, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1266", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP Prefetch Nuances#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP Prefetch Nuances#", "content": "FSDP Prefetch Nuances# For overlapping forward all-gathers with forward compute, there are two possible mechanisms: - Implicit forward prefetching (always enabled) - Explicit forward prefetching (forward_prefetch=True) Implicit forward prefetching refers to relying on issuing the all-gathers from a separate CUDA stream to allow for overlapping an all-gather with forward compute issued before it (from the CPU perspective). For example, if we have layer 0 all-gather -> layer 0 forward compute -> layer 1 all-gather -> …, then layer 1 all-gather can overlap with layer 0 forward compute even though the CPU thread issued it afterwards. (The 1st all-gather will not be able to overlap with anything.) Explicit forward prefetching refers to changing the CPU thread’s issue order: e.g. layer 0 all-gather -> layer 1 all-gather -> layer 0 forward compute -> …. In eager mode, there is no way to know in general which layer is the next layer (e.g. layer 1 in the example) when still executing on layer 0. Therefore, explicit forward prefetching should only be used for models whose execution order is fixed from iteration to iteration (which we sometimes call “static graph”). An example of a model that does not satisfy this constraint is FLAVA). Explicit forward prefetching only saves the time taken to issue a layer’s forward compute kernels at the cost that the next all-gather’s output tensor must be allocated while the current one is still in use. By issuing the next all- gather before the current forward compute kernels, the next all-gather can start sooner on GPU. For most LLM workloads, this is not the case, so there is no motivation for enabling forward_prefetch=True. In contrast, for backward, we must use explicit backward prefetching or else there will be 0 overlap of communication and computation. The reason is because we use a single", "prev_chunk_id": "chunk_1265", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1267", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP Prefetch Nuances#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP Prefetch Nuances#", "content": "NCCL process group for both all-gather and reduce-scatter (partially because in earlier NCCL versions, it was not safe to use multiple concurrently on the same device over the same ranks). A single NCCL process group means a single internal NCCL stream on which reduce-scatters and all-gathers run serially. As such, unless we explicitly reorder the CPU issue order to be next all-gather -> current reduce-scatter, then the current reduce-scatter would block the next all-gather and hence the next backward computation, preventing the current reduce-scatter from overlapping.", "prev_chunk_id": "chunk_1266", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1268", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "Communication payload size#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "Communication payload size#", "content": "Communication payload size# In FSDP the communications are: - all-gather on parameters inforward - all-gather on parameters inbackward - reduce-scatter on gradients inbackward If activation checkpointing (checkpoint()) is used there is no additional communication since the parameters are prefetched anyway during backward. In the FSDP design, the communication payload per rank is determined as follows: Each call to FullyShardedDataParallel creates one communication group consisting of the parameters in module.parameters() except any already assigned to a nested FullyShardedDataParallel instance. For example, for Llama, if you apply FullyShardedDataParallel to every transformer block and also to the root module, then there is one communication group for each transformer block and finally one communication group with the initial embedding and final linear. Each communication group corresponds to a single all-gather call and single reduce-scatter call. In that way, how you apply FullyShardedDataParallel determines the communication size. In general, applying FSDP to each transformer block is a good heuristic for LLMs, and it is hard to do better than that given the current design. Let’s consider an example where we have a Transformer-based model sharded over 8 GPUs, where the sharding happens at the transformer block-level only, and each transformer block contains 1.6B parameters and the parameters are in fp32 (4 bytes each). Which means that once sharded, each transformer block will contain 0.2B parameters on each rank. - Theforwardpass will communicate in chunks of0.2*4=0.8GBin all-gather - Thebackwardpass will communicate 2 times0.8GBeach (1x all-gather and 1x reduce-scatter) In other words there will be 3 communications with a payload of 0.8GB each. If the model was comprised of 10 transformer blocks there would be a total of 30 communications for a total of 30*0.8=24GB. To formalize the payload size per communication per rank is total_transformer_block_params_in_B*dtype_bytes/num_gpus (GBs). Please note that in this example we didn’t include the", "prev_chunk_id": "chunk_1267", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1269", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "Communication payload size#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "Communication payload size#", "content": "additional communications required for the embedding, which should be accounted for as well. And the math would depend on whether the input and output embeddings are tied or not. If they aren’t tied there will be 2x more communications.", "prev_chunk_id": "chunk_1268", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1270", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP buffers sizes#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP buffers sizes#", "content": "FSDP buffers sizes# First, let’s cover the buffers allocated for communications: forward currently requires 2x all-gather buffer size. Here is why: As explained in FSDP Prefetch Nuances in the case of explicit forward prefetching (forward_prefetch=True) case of layer 0 all-gather -> layer 0 forward compute -> layer 1 all-gather there is a need for 2 all-gather-sized buffers, because one buffer is used in the current forward while the other is used to do the prefetching. While the implicit forward prefetching (forward_prefetch=False, default) case of the same sequence in theory should need only 1 buffer, in reality it’s still 2x all-gather-sized buffers. The reason is that in the flat-parameter FSDP design, we do not copy-out of the all-gather buffer. The parameters used for compute are directly viewed into the all-gather buffer (in fact, the main benefit of the “flat parameter” is exactly this reason). In that case, while ‘layer 1 all-gather’ is overlapping with ‘layer 0 forward compute’, the ‘layer 0 forward compute’ is using the parameters viewed into the ‘layer 0 all-gather’ buffer. A natural question then is, when would you want forward_prefetch=False? For static-graph models (like most LLMs), there is a major technical reason. It is more that, practically, we added this option quickly for some CPU-bound internal models and have not tested every code path with it in unit testing, so we are less confident in it. forward_prefetching=False can be slightly easier to reason about since we do not have to check the recorded forward order as a possible ‘failure mode’; a module’s all-gather can always be found under its own record_function label in its profiler trace. backward currently requires at least 2x all-gather buffer size and potentially a bit more. Here is why: The current FSDP design uses recordStream to manage allocations produced in one stream consumed", "prev_chunk_id": "chunk_1269", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1271", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP buffers sizes#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP buffers sizes#", "content": "in another, which can lead to more memory usage than expected. How much more can be “non-deterministic” in that it depends on GPU kernel timing relative to the CPU. The limit_all_gathers=True argument is a mitigation to that - for more details refer to this discussion is FSDP & CUDACachingAllocator. The way existing FSDP works with autograd: - Existing FSDP all-gathers theflat_param, which is the autograd leaf. - It callstorch.splitto get 1D views into theflat_paramcorresponding to its constituent original parameters. - It callstorch.viewon each 1D split to view back to ND. - This means that inbackward, we end up withViewBackward(ND -> 1D) andSplitWithSizesBackward(which is a concat). In particular, each individual gradient is computed as a separate allocation, and an explicit concat happens to construct the reduce-scatter input buffer. This implies actually a 2x buffer size for reduce-scatter at that peak memory point. In summary, for backward, it is about 2x buffer size for reduce-scatter plus any recordStream effects. Second, let’s discuss the additional buffers: Once the sharded parameters are gathered from all ranks, they require an additional buffer of total_transformer_block_params_in_B*dtype_bytes for the full parameters - so continuing the example from earlier if each transformer block is 1.6B parameters and the parameters are in fp32, then it’d be 1.6*4=6.4GB buffer. And there is a need for 2 of those buffers, since there is one currently being used and another being prefetched. To summarize, we have: - 2 times communication buffers oftotal_transformer_block_params_in_B*dtype_bytes/num_gpus - 2 times unsharded transformer block parameters buffer``total_transformer_block_params_in_B*dtype_bytes or if you have been following the example: - 2*1.6*4/8=1.6GB - 2**1.6*4=12.8GB and the total of 14.4GB. Now let’s briefly discuss what happens to the embeddings as we have left those out from the calculations: Given the rule we discussed that you included in the note starting with “the communication buffer size is", "prev_chunk_id": "chunk_1270", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1272", "url": "https://docs.pytorch.org/docs/stable/notes/fsdp.html", "title": "FSDP buffers sizes#", "page_title": "FSDP Notes — PyTorch 2.8 documentation", "breadcrumbs": "FSDP buffers sizes#", "content": "determined as follows”, we can analyze as follows: - Suppose we apply FSDP to the root module (e.g. theTransformerclass). Suppose we further apply FSDP to each transformer block (e.g. theTransformerBlockclass). - Most commonly, the embedding and final linear projection are direct children of the rootTransformerclass. - Following our rule, that means that the embedding and final linear projection are assigned to the rootTransformer’s flat parameter. - We have _another_ special rule, which is that the root does not free its parameters after forward because they will be anyways immediately all-gathered in backward. - Putting this together, this means that the root’s flat parameter including the embedding and final projection are all-gathered to begin forward and kept in GPU memory until the end of backward. - If the embedding and final linear are not weight-tied, then we _could_ further apply FSDP to the embedding and to the final linear. For weight-tied parameters, we require them to be part of the same flat parameter (or else it would get double-counted). That would allow the embedding to be freed after its usage in forward and only all-gathered toward the end of backward. - Hopefully, this gives a better sense – each FSDP module gets assigned parameters in itsmodule.parametersexcept those already assigned to another nested FSDP module, and the FSDP module’sforwarddefines the ‘live’ interval for its parameters. Hence, the nestednn.Modulestructure can affect the all-gather/free schedule and hence the memory/throughput performance.", "prev_chunk_id": "chunk_1271", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1273", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "Frequently Asked Questions#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# Created On: Feb 15, 2018 | Last Updated On: Aug 05, 2021", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1274", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My model reports “cuda runtime error(2): out of memory”#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My model reports “cuda runtime error(2): out of memory”#", "content": "My model reports “cuda runtime error(2): out of memory”# As the error message suggests, you have run out of memory on your GPU. Since we often deal with large amounts of data in PyTorch, small mistakes can rapidly cause your program to use up all of your GPU; fortunately, the fixes in these cases are often simple. Here are a few common things to check: Don’t accumulate history across your training loop. By default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data. Sometimes, it can be non-obvious when differentiable variables can occur. Consider the following training loop (abridged from source): total_loss = 0 for i in range(10000): optimizer.zero_grad() output = model(input) loss = criterion(output) loss.backward() optimizer.step() total_loss += loss Here, total_loss is accumulating history across your training loop, since loss is a differentiable variable with autograd history. You can fix this by writing total_loss += float(loss) instead. Other instances of this problem: 1. Don’t hold onto tensors and variables you don’t need. If you assign a Tensor or Variable to a local, Python will not deallocate until the local goes out of scope. You can free this reference by using del x. Similarly, if you assign a Tensor or Variable to a member variable of an object, it will not deallocate until the object goes out of scope. You will get the best memory usage if you don’t hold onto temporaries you don’t need. The scopes of locals can be larger than you expect. For example: for i in range(5): intermediate = f(input[i]) result += g(intermediate) output = h(result) return output Here, intermediate remains live even", "prev_chunk_id": "chunk_1273", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1275", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My model reports “cuda runtime error(2): out of memory”#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My model reports “cuda runtime error(2): out of memory”#", "content": "while h is executing, because its scope extrudes past the end of the loop. To free it earlier, you should del intermediate when you are done with it. Avoid running RNNs on sequences that are too large. The amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long. The technical term for this phenomenon is backpropagation through time, and there are plenty of references for how to implement truncated BPTT, including in the word language model example; truncation is handled by the repackage function as described in this forum post. Don’t use linear layers that are too large. A linear layer nn.Linear(m, n) uses O(nm)O(nm)O(nm) memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to blow through your memory this way (and remember that you will need at least twice the size of the weights, since you also need to store the gradients.) Consider checkpointing. You can trade-off memory for compute by using checkpoint.", "prev_chunk_id": "chunk_1274", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1276", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My GPU memory isn’t freed properly#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My GPU memory isn’t freed properly#", "content": "My GPU memory isn’t freed properly# PyTorch uses a caching memory allocator to speed up memory allocations. As a result, the values shown in nvidia-smi usually don’t reflect the true memory usage. See Memory management for more details about GPU memory management. If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].", "prev_chunk_id": "chunk_1275", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1277", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My out of memory exception handler can’t allocate memory#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My out of memory exception handler can’t allocate memory#", "content": "My out of memory exception handler can’t allocate memory# You may have some code that tries to recover from out of memory errors. try: run_model(batch_size) except RuntimeError: # Out of memory for _ in range(batch_size): run_model(1) But find that when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the except clause. oom = False try: run_model(batch_size) except RuntimeError: # Out of memory oom = True if oom: for _ in range(batch_size): run_model(1)", "prev_chunk_id": "chunk_1276", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1278", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My data loader workers return identical random numbers#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My data loader workers return identical random numbers#", "content": "My data loader workers return identical random numbers# You are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via fork. See torch.utils.data.DataLoader’s documentation for how to properly set up random seeds in workers with its worker_init_fn option.", "prev_chunk_id": "chunk_1277", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1279", "url": "https://docs.pytorch.org/docs/stable/notes/faq.html", "title": "My recurrent network doesn’t work with data parallelism#", "page_title": "Frequently Asked Questions — PyTorch 2.8 documentation", "breadcrumbs": "My recurrent network doesn’t work with data parallelism#", "content": "My recurrent network doesn’t work with data parallelism# There is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module with DataParallel or data_parallel(). Input to each the forward() on each device will only be part of the entire input. Because the unpack operation torch.nn.utils.rnn.pad_packed_sequence() by default only pads up to the longest input it sees, i.e., the longest on that particular device, size mismatches will happen when results are gathered together. Therefore, you can instead take advantage of the total_length argument of pad_packed_sequence() to make sure that the forward() calls return sequences of same length. For example, you can write: from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence class MyModule(nn.Module): # ... __init__, other methods, etc. # padded_input is of shape [B x T x *] (batch_first mode) and contains # the sequences sorted by lengths # B is the batch size # T is max sequence length def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) # get the max sequence length packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True) packed_output, _ = self.my_lstm(packed_input) output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output m = MyModule().cuda() dp_m = nn.DataParallel(m) Additionally, extra care needs to be taken when batch dimension is dim 1 (i.e., batch_first=False) with data parallelism. In this case, the first argument of pack_padded_sequence padding_input will be of shape [T x B x *] and should be scattered along dim 1, but the second argument input_lengths will be of shape [B] and should be scattered along dim 0. Extra code to manipulate the tensor shapes will be needed.", "prev_chunk_id": "chunk_1278", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1280", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Extending torch.func with autograd.Function#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch.func with autograd.Function#", "content": "Extending torch.func with autograd.Function# Created On: Jan 03, 2023 | Last Updated On: Sep 14, 2023 So you’d like to use torch.autograd.Function with the torch.func transforms like torch.vmap(), torch.func.grad(), etc. There are two main use cases: - you wish to call code that does not contain PyTorch operations and have it work with function transforms. That is, thetorch.autograd.Function’s forward/backward/etc calls into functions from other systems like C++, CUDA, numpy. - you wish to specify custom gradient rules, like JAX’scustom_vjp/custom_jvp PyTorch combines both of these concepts into torch.autograd.Function.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1281", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Basic Usage#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Basic Usage#", "content": "Basic Usage# This guide assumes you are familiar with Extending torch.autograd, which explains how to use torch.autograd.Function. torch.autograd.Function can either have a forward() that accepts a ctx object, or it can have separate forward() (that does not accept ctx) and a setup_context() staticmethod that modifies the ctx object. Only the latter is supported with function transforms: - forward()is the code that performs the operation and it should not accept actxobject. - setup_context(ctx,inputs,output)is the code where you can call methods onctx. Here is where you should save Tensors for backward (by callingctx.save_for_backward(*tensors)), or save non-Tensors (by assigning them to thectxobject). Because setup_context() accepts only inputs and output, the only quantities that can be saved are either objects (such as Tensors) in the inputs or outputs or quantities (like Tensor.shape) derived from them. If you wish to save a non-input intermediate activation from Function.forward() for backward, then you’ll need to return it as an output from forward() so that it gets passed to setup_context(). Depending on the transform, - to support reverse-mode AD (torch.func.grad(),torch.func.vjp()), thetorch.autograd.Functionneeds abackward()staticmethod. - to supporttorch.vmap(), thetorch.autograd.Functionneeds avmap()staticmethod. - to supporttorch.func.jvp(), thetorch.autograd.Functionneeds ajvp()staticmethod. - to support compositions of transforms (liketorch.func.jacrev(),torch.func.jacfwd(),torch.func.hessian()) – you may need multiple of the above. In order for the torch.autograd.Function to be arbitrarily composable with function transforms, we recommend that all other staticmethods other than forward() and setup_context() must be transformable: that is, they must consist of only PyTorch operators or call other torch.autograd.Function (that may call into C++/CUDA/etc). Let’s go over some examples of common use cases.", "prev_chunk_id": "chunk_1280", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1282", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Example 1: autograd.Function calls into another system#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Example 1: autograd.Function calls into another system#", "content": "Example 1: autograd.Function calls into another system# A common case is a torch.autograd.Function with both forward() and backward() calling into another system (like C++, CUDA, numpy, triton). import torch import numpy as np def to_numpy(tensor): return tensor.cpu().numpy() class NumpySort(torch.autograd.Function): # Note that forward does not take ctx @staticmethod def forward(x, dim): device = x.device x = to_numpy(x) ind = np.argsort(x, axis=dim) ind_inv = np.argsort(ind, axis=dim) result = np.take_along_axis(x, ind, axis=dim) # Any intermediates to be saved in backward must be returned as # outputs. return ( # The desired output torch.tensor(result, device=device), # intermediate to save for backward torch.tensor(ind, device=device), # intermediate to save for backward torch.tensor(ind_inv, device=device), ) # setup_context is responsible for calling methods and/or assigning to # the ctx object. Please do not do additional compute (e.g. add # Tensors together) in setup_context. @staticmethod def setup_context(ctx, inputs, output): x, dim = inputs # Note that output is whatever you returned from forward. # If you returned multiple values, then output is a Tuple of multiple values. # If you returned a single Tensor, then output is a Tensor. # If you returned a Tuple with a single Tensor, then output is a # Tuple with a single Tensor. _, ind, ind_inv = output ctx.mark_non_differentiable(ind, ind_inv) # Tensors must be saved via ctx.save_for_backward. Please do not # assign them directly onto the ctx object. ctx.save_for_backward(ind, ind_inv) # Non-tensors may be saved by assigning them as attributes on the ctx object. ctx.dim = dim @staticmethod def backward(ctx, grad_output, _0, _1): # For the autograd.Function to be arbitrarily composable with function # transforms, all staticmethod other than forward and setup_context # must be implemented in a \"transformable\" way; that is, they must # only consist of PyTorch operations or autograd.Function. # # For example, this allows us to do double", "prev_chunk_id": "chunk_1281", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1283", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Example 1: autograd.Function calls into another system#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Example 1: autograd.Function calls into another system#", "content": "backwards and/or compute # second order gradients. # # We've written the backward pass of NumpySort in terms of another # autograd.Function, NumpyTake. ind, ind_inv = ctx.saved_tensors return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None class NumpyTake(torch.autograd.Function): @staticmethod def forward(x, ind, ind_inv, dim): device = x.device x = to_numpy(x) ind = to_numpy(ind) return torch.tensor(np.take_along_axis(x, ind, dim), device=device) @staticmethod def setup_context(ctx, inputs, output): x, ind, ind_inv, dim = inputs ctx.save_for_backward(ind, ind_inv) ctx.dim = dim @staticmethod def backward(ctx, grad_output): ind, ind_inv = ctx.saved_tensors result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim) return result, None, None, None Now, to make it easier to use NumpySort (to hide away the intermediates we returned as outputs, as well as allow default args and kwargs), we create a new function that invokes it: def numpy_sort(x, dim=-1): result, _, _ = NumpySort.apply(x, dim) return result And here’s a sanity check: x = torch.randn(2, 3) grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x) assert torch.allclose(grad_x, torch.ones_like(x))", "prev_chunk_id": "chunk_1282", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1284", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Example 2: autograd.Function specifies custom gradient rules#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Example 2: autograd.Function specifies custom gradient rules#", "content": "Example 2: autograd.Function specifies custom gradient rules# Another common case is an torch.autograd.Function that is implemented with PyTorch operations. PyTorch is able to compute gradients for PyTorch operations automatically, but perhaps we wish to customize how the gradients are computed. Some reasons why we may want a custom backward different from the one PyTorch gives us are: - improving numeric stability - changing the performance characteristics of the backward - changing how edge cases are handled (e.g. nans, inf) - modifying the gradient (e.g. gradient clipping) Here’s an example of an torch.autograd.Function for the function y = x ** 3 where we change the performance characteristics (some computation that would normally happen during the backward pass, computing dx, happens in the forward pass). class MyCube(torch.autograd.Function): @staticmethod def forward(x): result = x ** 3 # In regular PyTorch, if we had just run y = x ** 3, then the backward # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done # that computation here in the forward pass instead. dx = 3 * x ** 2 return result, dx @staticmethod def setup_context(ctx, inputs, output): x, = inputs result, dx = output ctx.save_for_backward(x, dx) @staticmethod def backward(ctx, grad_output, grad_dx): x, dx = ctx.saved_tensors # In order for the autograd.Function to work with higher-order # gradients, we must add the gradient contribution of `dx`. result = grad_output * dx + grad_dx * 6 * x return result Now, to make it easier to use NumpySort (and hide away the intermediates we returned as outputs) we create a new function that invokes it: def my_cube(x): result, _ = MyCube.apply(x) return result Here’s a sanity check computing the second-order gradients: x = torch.randn([]) ggx = torch.func.grad(torch.func.grad(my_cube))(x) assert torch.allclose(ggx, 6 * x)", "prev_chunk_id": "chunk_1283", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1285", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Limitations and gotchas#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Limitations and gotchas#", "content": "Limitations and gotchas# Please do not capture Tensors that are being transformed over, have requires_grad=True, or are dual tensors, into the methods of the torch.autograd.Function. The way to be completely safe is to ensure that the only Tensors being used inside any method of the torch.autograd.Function must be directly passed as inputs (or via the ctx object) rather than come from outside the torch.autograd.Function. torch.autograd.Function does not handle Tensors in pytrees (arbitrary nested Python data structures that may or may not contain Tensors). For those Tensors to be tracked by autograd, they must be passed directly as an argument to torch.autograd.Function. This is in contrast to jax.{custom_vjp, custom_jvp}, which do accept pytrees. Please only use save_for_backward() or save_for_forward() to save Tensors. Please do not assign Tensors or collections of Tensors directly onto the ctx object - these Tensors will not get tracked", "prev_chunk_id": "chunk_1284", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1286", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "torch.vmap() Support#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "torch.vmap() Support#", "content": "torch.vmap() Support# To use an torch.autograd.Function with torch.vmap(), you must either: - provide avmap()staticmethod that tells us the behavior of thetorch.autograd.Functionundertorch.vmap() - ask us to autogenerate it by settinggenerate_vmap_rule=True.", "prev_chunk_id": "chunk_1285", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1287", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Automatically generate a vmap rule#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Automatically generate a vmap rule#", "content": "Automatically generate a vmap rule# If your torch.autograd.Function fulfills the following additional constraints, then we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you want custom behavior under vmap, please manually define a vmap staticmethod (see next section). - Thetorch.autograd.Function’sforward(),backward()(if it exists) andjvp()(if it exists) staticmethods must be transformable viatorch.vmap(). That is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom CUDA kernels). Example: class MyCube(torch.autograd.Function): # Set generate_vmap_rule to True to ask PyTorch to automatically generate # a vmap rule. generate_vmap_rule = True @staticmethod def forward(x): result = x ** 3 dx = 3 * x ** 2 return result, dx @staticmethod def setup_context(ctx, inputs, output): x, = inputs result, dx = output ctx.save_for_backward(x, dx) @staticmethod def backward(ctx, grad_output, grad_dx): x, dx = ctx.saved_tensors result = grad_output * dx + grad_dx * 6 * x return result def my_cube(x): result, dx = MyCube.apply(x) return result x = torch.randn(3) result = torch.vmap(my_cube)(x) assert torch.allclose(result, x ** 3)", "prev_chunk_id": "chunk_1286", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1288", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Defining the vmap staticmethod#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Defining the vmap staticmethod#", "content": "Defining the vmap staticmethod# If your torch.autograd.Function calls into another system (like NumPy, C++, CUDA, triton), then to get it to work with torch.vmap() or transforms that use it, you’ll need to manually define a vmap() staticmethod. Depending on what transforms you want to use and your use case, you may not need to add a vmap() staticmethod to all of your torch.autograd.Function: - For example,torch.func.jacrev()performsvmap()over the backward pass. So if you’re only interested in usingtorch.func.jacrev(), only thebackward()staticmethod needs to be vmappable. We do recommend ensuring all of your torch.autograd.Function have support for torch.vmap() though, especially if you are writing a third-party library and you want your torch.autograd.Function to work with all combinations of torch.func() transforms. Conceptually, the vmap staticmethod is responsible for defining how the forward() should behave under torch.vmap(). That is, it defines how to transform the forward() to run over inputs with an additional dimension (the dimension being vmapped over). This is similar to how torch.vmap() is implemented over PyTorch operations: for each operation, we define a vmap rule (sometimes also referred to as a “batching rule”). Here’s how to define the vmap() staticmethod: - the signature isvmap(info,in_dims:Tuple[Optional[int]],*args), where*argsis the same as the args toforward(). - The vmap staticmethod is responsible for defining how theforward()should behave undertorch.vmap(). That is, given inputs with an additional dimension (specified byin_dims), how do we compute the batched version offorward()? - For each arg inargs,in_dimshas a correspondingOptional[int]. It isNoneif the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over. - infois a collection of additional metadata that may be helpful:info.batch_sizespecifies the size of the dimension being vmapped over, whileinfo.randomnessis therandomnessoption that was passed totorch.vmap(). - The return of the vmap staticmethod is", "prev_chunk_id": "chunk_1287", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1289", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Defining the vmap staticmethod#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Defining the vmap staticmethod#", "content": "a tuple of(output,out_dims). Similar toin_dims,out_dimsshould be of the same structure asoutputand contain oneout_dimper output that specifies if the output has the vmapped dimension and what index it is in. Example: def to_numpy(tensor): return tensor.cpu().numpy() class NumpySort(torch.autograd.Function): @staticmethod def forward(x, dim): device = x.device x = to_numpy(x) ind = np.argsort(x, axis=dim) ind_inv = np.argsort(ind, axis=dim) result = np.take_along_axis(x, ind, axis=dim) return ( torch.tensor(result, device=device), torch.tensor(ind, device=device), torch.tensor(ind_inv, device=device), ) @staticmethod def setup_context(ctx, inputs, output): x, dim = inputs _, ind, ind_inv = output ctx.mark_non_differentiable(ind, ind_inv) ctx.save_for_backward(ind, ind_inv) ctx.dim = dim @staticmethod def backward(ctx, grad_output, _0, _1): ind, ind_inv = ctx.saved_tensors return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None # The signature of the vmap staticmethod is: # vmap(info, in_dims: Tuple[Optional[int]], *args) # where *args is the same as the arguments to `forward`. @staticmethod def vmap(info, in_dims, x, dim): # For every input (x and dim), in_dims stores an Optional[int] # that is: # - None if the input is not being vmapped over or if the input # is not a Tensor # - an integer if the input is being vmapped over that represents # the index of the dimension being vmapped over. x_bdim, _ = in_dims # A \"vmap rule\" is the logic of how to perform the operation given # inputs with one additional dimension. In NumpySort, x has an # additional dimension (x_bdim). The vmap rule is simply # to call NumpySort again but pass it a different `dim`. x = x.movedim(x_bdim, 0) # Handle negative dims correctly dim = dim if dim >= 0 else dim + x.dim() - 1 result = NumpySort.apply(x, dim + 1) # The vmap rule must return a tuple of two things # 1. the output. Should be the same amount of things # as returned by the forward(). # 2. one Optional[int]", "prev_chunk_id": "chunk_1288", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1290", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Defining the vmap staticmethod#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Defining the vmap staticmethod#", "content": "for each output specifying if each output # is being vmapped over, and if so, the index of the # dimension being vmapped over. # # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the # dimension being vmapped over to the front of `x`, that appears at # dimension 0 of all outputs. # The return is (output, out_dims) -- output is a tuple of 3 Tensors # and out_dims is a Tuple of 3 Optional[int] return NumpySort.apply(x, dim + 1), (0, 0, 0) class NumpyTake(torch.autograd.Function): @staticmethod def forward(x, ind, ind_inv, dim): device = x.device x = to_numpy(x) ind = to_numpy(ind) return torch.tensor(np.take_along_axis(x, ind, dim), device=device) @staticmethod def setup_context(ctx, inputs, output): x, ind, ind_inv, dim = inputs ctx.save_for_backward(ind, ind_inv) ctx.dim = dim @staticmethod def backward(ctx, grad_output): ind, ind_inv = ctx.saved_tensors result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim) return result, None, None, None @staticmethod def vmap(info, in_dims, x, ind, ind_inv, dim): x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims # The strategy is: expand {x, ind, ind_inv} to all have the dimension # being vmapped over. # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim). # Handle negative dims by wrapping them to be positive logical_dim = x.dim() if x_bdim is None else x_bdim - 1 dim = dim if dim >= 0 else dim + logical_dim def maybe_expand_bdim_at_front(x, x_bdim): if x_bdim is None: return x.expand(info.batch_size, *x.shape) return x.movedim(x_bdim, 0) # If the Tensor doesn't have the dimension being vmapped over, # expand it out. Otherwise, move it to the front of the Tensor x = maybe_expand_bdim_at_front(x, x_bdim) ind = maybe_expand_bdim_at_front(ind, ind_bdim) ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim) # The return is a tuple (output, out_dims). Since output is a Tensor, # then out_dims is an Optional[int] (instead of being a Tuple). return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0 def numpy_sort(x, dim=-1):", "prev_chunk_id": "chunk_1289", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1291", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "Defining the vmap staticmethod#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "Defining the vmap staticmethod#", "content": "result, _, _ = NumpySort.apply(x, dim) return result x = torch.randn(2, 3) result = torch.vmap(numpy_sort)(x) assert torch.allclose(result, numpy_sort(result, 1))", "prev_chunk_id": "chunk_1290", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1292", "url": "https://docs.pytorch.org/docs/stable/notes/extending.func.html", "title": "torch.func.jvp() Support#", "page_title": "Extending torch.func with autograd.Function — PyTorch 2.8 documentation", "breadcrumbs": "torch.func.jvp() Support#", "content": "torch.func.jvp() Support# To support forward-mode AD, a torch.autograd.Function must have a jvp() staticmethod. Please see Forward mode AD for details.", "prev_chunk_id": "chunk_1291", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1293", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending PyTorch#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending PyTorch#", "content": "Extending PyTorch# Created On: Jan 16, 2017 | Last Updated On: May 07, 2025 In this note we’ll cover ways of extending torch.nn, torch.autograd, torch, and writing custom C++ extensions.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1294", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Adding new operators#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Adding new operators#", "content": "Adding new operators# PyTorch offers a large library of operators that work on Tensors (e.g. torch.add(), torch.sum(), etc). However, you may wish to bring a new custom operation to PyTorch and have it behave like PyTorch’s built-in operators. In order to do so, you must register the custom operation with PyTorch via the Python torch.library or C++ TORCH_LIBRARY APIs. Please see PyTorch Custom Operators Landing Page for more details.", "prev_chunk_id": "chunk_1293", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1295", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch.autograd#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch.autograd#", "content": "Extending torch.autograd# Adding operations to autograd requires implementing a new Function subclass for each operation. Recall that Functions are what autograd uses to encode the operation history and compute gradients. The first part of this doc is focused on backward mode AD as it is the most widely used feature. A section at the end discusses the extensions for forward mode AD.", "prev_chunk_id": "chunk_1294", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1296", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "When to use#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "When to use#", "content": "When to use# In general, implement a custom function if you want to perform computations in your model that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine. In some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a C++ extension, you can wrap them in Function to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together.", "prev_chunk_id": "chunk_1295", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1297", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "When not to use#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "When not to use#", "content": "When not to use# If you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function. If you need to maintain state, i.e., trainable parameters, you should (also) use a custom module. See the section below for more information on extending torch.nn. If you’d like to alter the gradients during the backward pass or perform a side effect, consider registering a tensor or Module hook.", "prev_chunk_id": "chunk_1296", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1298", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "How to use#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "How to use#", "content": "How to use# Take the following steps: 1. Subclass Function and implement the forward(), (optional) setup_context() and backward() methods. 2. Call the proper methods on the ctx argument. 3. Declare whether your function supports double backward. 4. Validate whether your gradients are correct using gradcheck. Step 1: After subclassing Function, you’ll need to define 3 methods: - forward()is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here.Tensorarguments that track history (i.e., withrequires_grad=True) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a singleTensoroutput, or atupleof tensors if there are multiple outputs. Also, please refer to the docs ofFunctionto find descriptions of useful methods that can be called only fromforward(). - setup_context()(optional). One can either write a “combined”forward()that accepts actxobject or (as of PyTorch 2.0) a separateforward()that does not acceptctxand asetup_context()method where thectxmodification happens. Theforward()should have the compute andsetup_context()should only be responsible for thectxmodification (and not have any compute). In general the separateforward()andsetup_context()is closer to how PyTorch native operations work and therefore more composable with various PyTorch subsystems. SeeCombined or separate forward() and setup_context()for more details. - backward()(orvjp()) defines the gradient formula. It will be given as manyTensorarguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient (needs_input_gradis", "prev_chunk_id": "chunk_1297", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1299", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "How to use#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "How to use#", "content": "a tuple of booleans indicating whether each input needs gradient computation), or were non-Tensorobjects, you can returnpython:None. Also, if you have optional arguments toforward()you can return more gradients than there were inputs, as long as they’re allNone. Step 2: It is your responsibility to use the functions in ctx properly in order to ensure that the new Function works properly with the autograd engine. - save_for_backward()should be used to save any tensors needed for the backward pass (as opposed to directly onctx). You cannot usesave_for_backwardfor non-tensors; you should store those directly onctx.Saving tensors viasave_for_backward: 1. Allows the autograd engine to clear them as soon as the backward computation of theautograd.Functioncompletes. (If a tensor is stored directly onctxit will unnecessarily remain alive for the lifetime of the autograd graph – typically until the end of the iteration.) 2. Helps avoid certain reference cycles, (e.g., since the tensor output of theautograd.Functionitself keeps a reference to the ctx). 3. Is important for compatibility with features like activation checkpointing and offloading that rely ontorch.autograd.graph.saved_tensors_hooks.If tensors that are neither inputs nor outputs are saved for backward yourFunctionmay not support double backward (see step 3). - mark_dirty()must be used to mark any input that is modified inplace by the forward function. - mark_non_differentiable()must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients. - set_materialize_grads()can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for which x.defined()", "prev_chunk_id": "chunk_1298", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1300", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "How to use#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "How to use#", "content": "is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True. Step 3: If your Function does not support double backward you should explicitly declare this by decorating backward with the once_differentiable(). With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward. Step 4: It is recommended that you use torch.autograd.gradcheck() to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.", "prev_chunk_id": "chunk_1299", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1301", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Example#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Example#", "content": "Example# Below you can find code for a Linear function, with additional comments: # Inherit from Function class LinearFunction(Function): # Note that forward, setup_context, and backward are @staticmethods @staticmethod def forward(input, weight, bias): output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output @staticmethod # inputs is a Tuple of all of the inputs passed to forward. # output is the output of the forward(). def setup_context(ctx, inputs, output): input, weight, bias = inputs ctx.save_for_backward(input, weight, bias) # This function has only a single output, so it gets only one gradient @staticmethod def backward(ctx, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias Now, to make it easier to use these custom ops, we recommend either aliasing them or wrapping them in a function. Wrapping in a function lets us support default arguments and keyword arguments: # Option 1: alias linear = LinearFunction.apply # Option 2: wrap in a function, to support default args and keyword args. def linear(input, weight, bias=None): return LinearFunction.apply(input, weight, bias) Here, we give an additional example of a function that is parametrized by non-Tensor", "prev_chunk_id": "chunk_1300", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1302", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Example#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Example#", "content": "arguments: class MulConstant(Function): @staticmethod def forward(tensor, constant): return tensor * constant @staticmethod def setup_context(ctx, inputs, output): # ctx is a context object that can be used to stash information # for backward computation tensor, constant = inputs ctx.constant = constant @staticmethod def backward(ctx, grad_output): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None And here, we optimize the above example by calling set_materialize_grads(False): class MulConstant(Function): @staticmethod def forward(tensor, constant): return tensor * constant @staticmethod def setup_context(ctx, inputs, output): tensor, constant = inputs ctx.set_materialize_grads(False) ctx.constant = constant @staticmethod def backward(ctx, grad_output): # Here we must handle None grad_output tensor. In this case we # can skip unnecessary computations and just return None. if grad_output is None: return None, None # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None If you need any “intermediate” Tensors computed in forward() to be saved, either they must be returned as outputs, or combine forward and setup_context() (see Combined or separate forward() and setup_context()). Note that this means if you want gradients to flow through those intermediate values, you need to define the gradient formula for them (see also the double backward tutorial ): class MyCube(torch.autograd.Function): @staticmethod def forward(x): # We wish to save dx for backward. In order to do so, it must # be returned as an output. dx = 3 * x ** 2 result = x ** 3 return result, dx @staticmethod def setup_context(ctx, inputs, output): x, = inputs result, dx = output ctx.save_for_backward(x, dx) @staticmethod def backward(ctx, grad_output, grad_dx): x, dx = ctx.saved_tensors # In order for the autograd.Function to work with higher-order # gradients, we must", "prev_chunk_id": "chunk_1301", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1303", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Example#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Example#", "content": "add the gradient contribution of `dx`, # which is grad_dx * 6 * x. result = grad_output * dx + grad_dx * 6 * x return result # Wrap MyCube in a function so that it is clearer what the output is def my_cube(x): result, dx = MyCube.apply(x) return result You probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences: from torch.autograd import gradcheck # gradcheck takes a tuple of tensors as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True)) test = gradcheck(linear, input, eps=1e-6, atol=1e-4) print(test) See Numerical gradient checking for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the gradgradcheck function from the same package to check higher order derivatives.", "prev_chunk_id": "chunk_1302", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1304", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Combined or separate forward() and setup_context()#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Combined or separate forward() and setup_context()#", "content": "Combined or separate forward() and setup_context()# There are two main ways to define Function. Either: - define aforward()that combines the forward compute logic withsetup_context() - (as of PyTorch 2.0) define a separateforward()andsetup_context() We recommend the second option (separate forward() and setup_context()) because that is closer to how PyTorch native operations are implemented and it composes with torch.func transforms. However, we plan to support both approaches going forward; combining forward() with setup_context(): leads to more flexibility since you are able to save intermediates without returning them as output. Please see the previous section for how to define Function with separate forward() and setup_context(). Here is an example of how to define a Function with combined forward() and setup_context(): class LinearFunction(Function): @staticmethod # ctx is the first argument to forward def forward(ctx, input, weight, bias=None): # The forward pass can use ctx. ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias", "prev_chunk_id": "chunk_1303", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1305", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Forward mode AD#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Forward mode AD#", "content": "Forward mode AD# Overriding the forward mode AD formula has a very similar API with some different subtleties. You can implement the jvp() function. It will be given as many Tensor arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The jvp() will be called just after the forward() method, before the apply() returns. jvp() has a few subtle differences with the backward() function: - You can use thectxto pass any data from theforward()to thejvp()function. If that state will not be needed for thebackward(), you can explicitly free it by doingdelctx.fooat the end of thejvp()function. - The implementation ofjvp()must be backward differentiable or explicitly check that none of the given forward mode gradient hasrequires_gradset. - Thejvp()function must match the view/inplace behavior offorward(). For example, if theith input is modified inplace, then theith gradient must be updated inplace. Similarly, if thejth output is a view of thekth input. Then the returnedjth output gradient must be a view of the givenkth input gradient. - Because the user cannot specify which gradient needs to be computed, thejvp()function should always compute gradients for all the outputs. - The forward mode gradients do respect the flag set byset_materialize_grads()and you can getNoneinput gradients when this is disabled.", "prev_chunk_id": "chunk_1304", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1306", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "torch.func transforms and/or torch.vmap()#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "torch.func transforms and/or torch.vmap()#", "content": "torch.func transforms and/or torch.vmap()# Please see Extending torch.func with autograd.Function for details.", "prev_chunk_id": "chunk_1305", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1307", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch.nn#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch.nn#", "content": "Extending torch.nn# nn exports two kinds of interfaces - modules and their functional versions. You can extend it in both ways, but we recommend using modules for all kinds of layers, that hold any parameters or buffers, and recommend using a functional form parameter-less operations like activation functions, pooling, etc. Adding a functional version of an operation is already fully covered in the section above.", "prev_chunk_id": "chunk_1306", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1308", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Adding a Module#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Adding a Module#", "content": "Adding a Module# Since nn heavily utilizes autograd, adding a new Module requires implementing a Function that performs the operation and can compute the gradient. From now on let’s assume that we want to implement a Linear module and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented: - __init__(optional) - takes in arguments such as kernel sizes, numbers of features, etc. and initializes parameters and buffers. - forward()- instantiates aFunctionand uses it to perform the operation. It’s very similar to a functional wrapper shown above. This is how a Linear module can be implemented: class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): super().__init__() self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Tensor, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters require gradients by default. self.weight = nn.Parameter(torch.empty(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.empty(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights nn.init.uniform_(self.weight, -0.1, 0.1) if self.bias is not None: nn.init.uniform_(self.bias, -0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return LinearFunction.apply(input, self.weight, self.bias) def extra_repr(self): # (Optional)Set the extra information about this module. You can test # it by printing an object of this class. return 'input_features={}, output_features={}, bias={}'.format( self.input_features, self.output_features, self.bias is not None )", "prev_chunk_id": "chunk_1307", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1309", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch Python API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch Python API#", "content": "Extending torch Python API# You can create custom types that emulate Tensor by defining a custom class with methods that match Tensor. But what if you want to be able to pass these types to functions like torch.add() in the top-level torch namespace that accept Tensor operands? If your custom Python type defines a method named __torch_function__, PyTorch will invoke your __torch_function__ implementation when an instance of your custom class is passed to a function in the torch namespace. This makes it possible to define custom implementations for any of the functions in the torch namespace which your __torch_function__ implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for Tensor. This works with “duck” types that are unrelated to Tensor as well as user-defined subclasses of Tensor.", "prev_chunk_id": "chunk_1308", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1310", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch with a Tensor-like type#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch with a Tensor-like type#", "content": "Extending torch with a Tensor-like type# To make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order N and value along the diagonal entries, value: class ScalarTensor(object): def __init__(self, N, value): self._N = N self._value = value def __repr__(self): return \"ScalarTensor(N={}, value={})\".format(self._N, self._value) def tensor(self): return self._value * torch.eye(self._N) This first iteration of the design isn’t very useful. The main functionality of ScalarTensor is to provide a more compact string representation of a scalar tensor than in the base tensor class: >>> d = ScalarTensor(5, 2) >>> d ScalarTensor(N=5, value=2) >>> d.tensor() tensor([[2., 0., 0., 0., 0.], [0., 2., 0., 0., 0.], [0., 0., 2., 0., 0.], [0., 0., 0., 2., 0.], [0., 0., 0., 0., 2.]]) If we try to use this object with the torch API, we will run into issues: >>> import torch >>> torch.mean(d) TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor Adding a __torch_function__ implementation to ScalarTensor makes it possible for the above operation to succeed. Let’s re-do our implementation, this time adding a __torch_function__ implementation: HANDLED_FUNCTIONS = {} class ScalarTensor(object): def __init__(self, N, value): self._N = N self._value = value def __repr__(self): return \"ScalarTensor(N={}, value={})\".format(self._N, self._value) def tensor(self): return self._value * torch.eye(self._N) @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in HANDLED_FUNCTIONS or not all( issubclass(t, (torch.Tensor, ScalarTensor)) for t in types ): return NotImplemented return HANDLED_FUNCTIONS[func](*args, **kwargs) The __torch_function__ method takes four arguments: func, a reference to the torch API function that is being overridden, types, the list of types of Tensor-likes that implement __torch_function__, args, the tuple of arguments passed to the function, and kwargs, the dict of keyword arguments", "prev_chunk_id": "chunk_1309", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1311", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch with a Tensor-like type#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch with a Tensor-like type#", "content": "passed to the function. It uses a global dispatch table named HANDLED_FUNCTIONS to store custom implementations. The keys of this dictionary are functions in the torch namespace and the values are implementations for ScalarTensor. This class definition isn’t quite enough to make torch.mean do the right thing when we pass it a ScalarTensor – we also need to define an implementation for torch.mean for ScalarTensor operands and add the implementation to the HANDLED_FUNCTIONS dispatch table dictionary. One way of doing this is to define a decorator: import functools def implements(torch_function): \"\"\"Register a torch function override for ScalarTensor\"\"\" def decorator(func): functools.update_wrapper(func, torch_function) HANDLED_FUNCTIONS[torch_function] = func return func return decorator which can be applied to the implementation of our override: @implements(torch.mean) def mean(input): return float(input._value) / input._N With this change we can now use torch.mean with ScalarTensor: >>> d = ScalarTensor(5, 2) >>> torch.mean(d) 0.4 Of course torch.mean is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines __torch_function__, for example for torch.add(): def ensure_tensor(data): if isinstance(data, ScalarTensor): return data.tensor() return torch.as_tensor(data) @implements(torch.add) def add(input, other): try: if input._N == other._N: return ScalarTensor(input._N, input._value + other._value) else: raise ValueError(\"Shape mismatch!\") except AttributeError: return torch.add(ensure_tensor(input), ensure_tensor(other)) This version has a fast path for when both operands are ScalarTensor instances and also a slower path which degrades to converting the data to tensors when either operand is not a ScalarTensor. That makes the override function correctly when either operand is a ScalarTensor or a regular Tensor: >>> s = ScalarTensor(2, 2) >>> torch.add(s, s) ScalarTensor(N=2, value=4) >>> t = torch.tensor([[1, 1,], [1, 1]]) >>> torch.add(s, t) tensor([[3., 1.], [1.,", "prev_chunk_id": "chunk_1310", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1312", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch with a Tensor-like type#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch with a Tensor-like type#", "content": "3.]]) Note that our implementation of add does not take alpha or out as keyword arguments like torch.add() does: >>> torch.add(s, s, alpha=2) TypeError: add() got an unexpected keyword argument 'alpha' For speed and flexibility the __torch_function__ dispatch mechanism does not check that the signature of an override function matches the signature of the function being overridden in the torch API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with Tensor, user implementations of torch API functions should take care to exactly emulate the API of the function that is being overridden. Functions in the torch API that do not have explicit overrides will return NotImplemented from __torch_function__. If all operands with __torch_function__ defined on them return NotImplemented, PyTorch will raise a TypeError. This means that most of the time operations that do not have explicit overrides for a type will raise a TypeError when an instance of such a type is passed: >>> torch.mul(s, 3) TypeError: no implementation found for 'torch.mul' on types that implement __torch_function__: [ScalarTensor] In practice this means that if you would like to implement your overrides using a __torch_function__ implementation along these lines, you will need to explicitly implement the full torch API or the entire subset of the API that you care about for your use case. This may be a tall order as the full torch API is quite extensive. Another option is to not return NotImplemented for operations that are not handled but to instead pass a Tensor to the original torch function when no override is available. For example, if we change our implementation of __torch_function__ for ScalarTensor to the one below: @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in HANDLED_FUNCTIONS or not all( issubclass(t,", "prev_chunk_id": "chunk_1311", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1313", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch with a Tensor-like type#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch with a Tensor-like type#", "content": "(torch.Tensor, ScalarTensor)) for t in types ): args = [a.tensor() if hasattr(a, 'tensor') else a for a in args] return func(*args, **kwargs) return HANDLED_FUNCTIONS[func](*args, **kwargs) Then torch.mul() will work correctly, although the return type will always be a Tensor rather than a ScalarTensor, even if both operands are ScalarTensor instances: >>> s = ScalarTensor(2, 2) >>> torch.mul(s, s) tensor([[4., 0.], [0., 4.]]) Also see the MetadataTensor example below for another variation on this pattern but instead always returns a MetadataTensor to propagate metadata through operations in the torch API. The __torch_function__ protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a TypeError. This is especially true for subclasses, where all three of torch.add, torch.Tensor.__add__ and torch.Tensor.add must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. If one requires the implementation of a function from torch.Tensor subclasses, they must use super().__torch_function__ inside their implementation.", "prev_chunk_id": "chunk_1312", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1314", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Subclassing torch.Tensor#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Subclassing torch.Tensor#", "content": "Subclassing torch.Tensor# As of version 1.7.0, methods on torch.Tensor and functions in public torch.* namespaces applied on torch.Tensor subclasses will return subclass instances instead of torch.Tensor instances: >>> class SubTensor(torch.Tensor): ... pass >>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__ 'SubTensor' >>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__ 'SubTensor' If multiple subclasses exist, the lowest one in the hierarchy will be chosen by default. If there is no unique way to determine such a case, then a TypeError is raised: >>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__ 'SubTensor2' >>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__ 'SubTensor2' >>> torch.add(SubTensor([0]), OtherSubTensor([1])) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor] If one wishes to have a global override for all tensor methods, one can use __torch_function__. Here is an example that logs all function/method calls: class LoggingTensor(torch.Tensor): @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion if func is not torch.Tensor.__repr__: logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\") if kwargs is None: kwargs = {} return super().__torch_function__(func, types, args, kwargs) However, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using __torch_function__ and matching with func. One should be careful within __torch_function__ for subclasses to always call super().__torch_function__(func, ...) instead of func directly, as was the case before version 1.7.0. Failing to do this may cause func to recurse back into __torch_function__ and therefore cause infinite recursion.", "prev_chunk_id": "chunk_1313", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1315", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch with a Tensor wrapper type#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch with a Tensor wrapper type#", "content": "Extending torch with a Tensor wrapper type# Another useful case is a type that wraps a Tensor, either as an attribute or via subclassing. Below we implement a special case of this sort of type, a MetadataTensor that attaches a dictionary of metadata to a Tensor that is propagated through torch operations. Since this is a generic sort of wrapping for the full torch API, we do not need to individually implement each override so we can make the __torch_function__ implementation more permissive about what operations are allowed: class MetadataTensor(object): def __init__(self, data, metadata=None, **kwargs): self._t = torch.as_tensor(data, **kwargs) self._metadata = metadata def __repr__(self): return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t) @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata')) args = [getattr(a, '_t', a) for a in args] assert len(metadatas) > 0 ret = func(*args, **kwargs) return MetadataTensor(ret, metadata=metadatas[0]) This simple implementation won’t necessarily work with every function in the torch API but it is good enough to capture most common operations: >>> metadata = {'owner': 'Ministry of Silly Walks'} >>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata) >>> t = torch.tensor([[1, 2], [1, 2]]) >>> torch.add(t, m) Metadata: {'owner': 'Ministry of Silly Walks'} data: tensor([[2, 4], [4, 6]]) >>> torch.mul(t, m) Metadata: {'owner': 'Ministry of Silly Walks'} data: tensor([[1, 4], [3, 8]])", "prev_chunk_id": "chunk_1314", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1316", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Operations on multiple types that define __torch_function__#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Operations on multiple types that define __torch_function__#", "content": "Operations on multiple types that define __torch_function__# It is possible to use the torch API with multiple distinct types that each have a __torch_function__ implementation, but special care must be taken. In such a case the rules are: - The dispatch operation gathers all distinct implementations of__torch_function__for each operand and calls them in order: subclasses before superclasses, and otherwise left to right in the operator expression. - If any value other thanNotImplementedis returned, that value is returned as the result. Implementations can register that they do not implement an operation by returningNotImplemented. - If all of the__torch_function__implementations returnNotImplemented, PyTorch raises aTypeError.", "prev_chunk_id": "chunk_1315", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1317", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Testing Coverage of Overrides for the PyTorch API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Testing Coverage of Overrides for the PyTorch API#", "content": "Testing Coverage of Overrides for the PyTorch API# One troublesome aspect of implementing __torch_function__ is that if some operations do and others do not have overrides, users will at best see an inconsistent experience, or at worst will see errors raised at runtime when they use a function that does not have an override. To ease this process, PyTorch provides a developer-facing API for ensuring full support for __torch_function__ overrides. This API is private and may be subject to changes without warning in the future. First, to get a listing of all overridable functions, use torch.overrides._get_overridable_functions. This returns a dictionary whose keys are namespaces in the PyTorch Python API and whose values are a list of functions in that namespace that can be overridden. For example, let’s print the names of the first 5 functions in torch.nn.functional that can be overridden: >>> from torch.overrides import get_overridable_functions >>> func_dict = get_overridable_functions() >>> nn_funcs = func_dict[torch.nn.functional] >>> print([f.__name__ for f in nn_funcs[:5]) ['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d', 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices'] This listing of functions makes it possible to iterate over all overridable functions, however in practice this is not enough to write tests for all of these functions without laboriously and manually copying the signature of each function for each test. To ease this process, the torch.overrides._get_testing_overrides function returns a dictionary mapping overridable functions in the PyTorch API to dummy lambda functions that have the same signature as the original function but unconditionally return -1. These functions are most useful to use with inspect to analyze the function signature of the original PyTorch function: >>> import inspect >>> from torch.overrides import get_testing_overrides >>> override_dict = get_testing_overrides() >>> dummy_add = override_dict[torch.add] >>> inspect.signature(dummy_add) <Signature (input, other, out=None)> Finally, torch.overrides.get_ignored_functions returns a tuple of functions that explicitly cannot be overridden by __torch_function__. This list can be useful", "prev_chunk_id": "chunk_1316", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1318", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Testing Coverage of Overrides for the PyTorch API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Testing Coverage of Overrides for the PyTorch API#", "content": "to confirm that a function that isn’t present in the dictionary returned by get_overridable_functions cannot be overridden.", "prev_chunk_id": "chunk_1317", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1319", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch native API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch native API#", "content": "Extending torch native API# While __torch_function__ allows one to effectively extend PyTorch’s pure Python components’ behavior, it does not allow one to extend the parts of PyTorch implemented in C++. To that end, a Tensor subclass can also define __torch_dispatch__ which will be able to override the behavior at the C++ level. To effectively use this feature, it is important to know how the native part of PyTorch is implemented. The most important component there is what we call the “dispatcher” (the best description can be found in this blog post even though it is slightly outdated). As hinted by its name, it is responsible for calling the right backend function for a specific call of a function. For example, when calling torch.add(a, b), the dispatcher will inspect both arguments, figure out which “feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally call all the right kernels. A very common thing done by a kernel is to “redispatch”. For example, when running your neural network on GPU with autocast, the first call will be the autocast kernel that will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit. One configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in DispatchKey.h inside the DispatchKey enum. For the", "prev_chunk_id": "chunk_1318", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1320", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch native API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch native API#", "content": "purpose of extending torch, the important subset of the ordering for this discussion is: vmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends The most important key for the purpose of this discussion is Python as every Tensor subclass with the __torch_dispatch__ method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided func again will perform a “redispatch”. Some important implications of this implementation are: - This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc). - If any high level feature implements a given function without redispatching, it will never reach thePythonkey and so the__torch_dispatch__callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead. - When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None). - Our native functions are lazily populated astorch.ops.{namespace}.{func_name}.{overload_name}as callable Python objects to enable easily interacting with them from Python. Thefuncobject given to__torch_dispatch__is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code. In a similar way where __torch_function__ is able to interpose on", "prev_chunk_id": "chunk_1319", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1321", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending torch native API#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending torch native API#", "content": "all of torch’s Python API and Tensor methods, __torch_dispatch__ is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: torch.add(a, 2) and a + 2 will lead to exactly the same aten call. Most of these functions are defined in native_functions.yaml which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen. Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions. It is also possible to add new native functions using torch.library. This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions. You can find many examples of __torch_dispatch__-based subclasses in the subclass zoo repo.", "prev_chunk_id": "chunk_1320", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1322", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "__torch_dispatch__ calling convention#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "__torch_dispatch__ calling convention#", "content": "__torch_dispatch__ calling convention# @classmethod def __torch_dispatch__(cls, func, types, args=(), kwargs=None): pass When a user calls an operator with inputs that have __torch_dispatch__, that call may be forwarded to the __torch_dispatch__. args and kwargs get normalized before the call to __torch_dispatch__, that is: - thekwargsconsist of keyword-only arguments in the operator’s schema. If a kwarg is equal to its default value (in the schema), it will not be passed. - theargsconsists of all other arguments, no matter how they were passed to the operator (positional vs keyword). If an arg is equal to its default value, and it is the right-most positional arg or all the args to the right of it are not passed, it will not be passed.", "prev_chunk_id": "chunk_1321", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1323", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending all torch API with Modes#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending all torch API with Modes#", "content": "Extending all torch API with Modes# Unfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch’s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive. To address this use case, we introduced the concept of “Mode”. These exist for __torch_function__ and __torch_dispatch__ overrides, are created by subclassing respectively torch.overrides.TorchFunctionMode and torch.utils._python_dispatch.TorchDispatchMode, and are used as a context manager. To simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass. This means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first. It is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doing with self:. Here is an example that shows logging modes of each type: import torch from torch.overrides import TorchFunctionMode, resolve_name from torch.utils._python_dispatch import TorchDispatchMode class FunctionLog(TorchFunctionMode): def __torch_function__(self, func, types, args, kwargs=None): print(f\"Function Log: {resolve_name(func)}(*{args}, **{kwargs})\") return func(*args, **(kwargs or {})) class DispatchLog(TorchDispatchMode): def __torch_dispatch__(self, func, types, args, kwargs=None): print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\") return func(*args, **(kwargs or {})) def f(): a = torch.rand(10, requires_grad=True) b = a * 2 b.sum().backward() print(\"TorchFunctionMode logging:\") with FunctionLog(): f() print(\"TorchDispatchMode logging:\") with DispatchLog(): f() Which prints the following, with extra comments: TorchFunctionMode logging: Function Log: torch.rand(*(10,), **{'requires_grad': True}) Function Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624, 0.5970], requires_grad=True), 2), **None) Function Log:", "prev_chunk_id": "chunk_1322", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1324", "url": "https://docs.pytorch.org/docs/stable/notes/extending.html", "title": "Extending all torch API with Modes#", "page_title": "Extending PyTorch — PyTorch 2.8 documentation", "breadcrumbs": "Extending all torch API with Modes#", "content": "torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249, 1.1939], grad_fn=<MulBackward0>),), **None) # Note that at the python level, we only see the call to backward but not what happens in the autograd engine. Function Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None}) TorchDispatchMode logging: # Here the requires_grad flag from autograd is removed while default arguments were populated. Dispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False}) Dispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948, 0.6023], requires_grad=True), 2), **{}) Dispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897, 1.2046], grad_fn=<MulBackward0>),), **{}) # Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient. Dispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format}) # This is the backward of the sum Dispatch Log: aten.expand.default(*(tensor(1.), [10]), **{}) Dispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{}) Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{}) Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})", "prev_chunk_id": "chunk_1323", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1325", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "Distributed Data Parallel#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Data Parallel#", "content": "Distributed Data Parallel# Created On: Jan 15, 2020 | Last Updated On: Jan 25, 2024 torch.nn.parallel.DistributedDataParallel (DDP) transparently performs distributed data parallel training. This page describes how it works and reveals implementation details.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1326", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "Example#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "Example#", "content": "Example# Let us start with a simple torch.nn.parallel.DistributedDataParallel example. This example uses a torch.nn.Linear as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same. import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os from torch.nn.parallel import DistributedDataParallel as DDP def example(rank, world_size): # create default process group dist.init_process_group(\"gloo\", rank=rank, world_size=world_size) # create local model model = nn.Linear(10, 10).to(rank) # construct DDP model ddp_model = DDP(model, device_ids=[rank]) # define loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # forward pass outputs = ddp_model(torch.randn(20, 10).to(rank)) labels = torch.randn(20, 10).to(rank) # backward pass loss_fn(outputs, labels).backward() # update parameters optimizer.step() def main(): world_size = 2 mp.spawn(example, args=(world_size,), nprocs=world_size, join=True) if __name__==\"__main__\": # Environment variables which need to be # set when using c10d's default \"env\" # initialization mode. os.environ[\"MASTER_ADDR\"] = \"localhost\" os.environ[\"MASTER_PORT\"] = \"29500\" main() DDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model wrapper before compiling the model, such that torchdynamo can apply DDPOptimizer (graph-break optimizations) based on DDP bucket sizes. (See TorchDynamo DDPOptimizer for more information.) ddp_model = DDP(model, device_ids=[rank]) ddp_model = torch.compile(ddp_model)", "prev_chunk_id": "chunk_1325", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1327", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "Internal Design#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "Internal Design#", "content": "Internal Design# This section reveals how it works under the hood of torch.nn.parallel.DistributedDataParallel by diving into details of every step in one iteration. - Prerequisite: DDP relies on c10dProcessGroupfor communications. Hence, applications must createProcessGroupinstances before constructing DDP. - Construction: The DDP constructor takes a reference to the local module, and broadcastsstate_dict()from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. Then, each DDP process creates a localReducer, which later will take care of the gradients synchronization during the backward pass. To improve communication efficiency, theReducerorganizes parameter gradients into buckets, and reduces one bucket at a time. Bucket size can be configured by setting thebucket_cap_mbargument in DDP constructor. The mapping from parameter gradients to buckets is determined at the construction time, based on the bucket size limit and parameter sizes. Model parameters are allocated into buckets in (roughly) the reverse order ofModel.parameters()from the given model. The reason for using the reverse order is because DDP expects gradients to become ready during the backward pass in approximately that order. The figure below shows an example. Note that, thegrad0andgrad1are inbucket1, and the other two gradients are inbucket0. Of course, this assumption might not always be true, and when that happens it could hurt DDP backward speed as theReducercannot kick off the communication at the earliest possible time. Besides bucketing, theReduceralso registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready. - Forward Pass: The DDP takes the input and passes it to the local model, and then analyzes the output from the local model iffind_unused_parametersis set toTrue. This mode allows running backward on a subgraph of the model, and DDP finds out which parameters", "prev_chunk_id": "chunk_1326", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1328", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "Internal Design#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "Internal Design#", "content": "are involved in the backward pass by traversing the autograd graph from the model output and marking all unused parameters as ready for reduction. During the backward pass, theReducerwould only wait for unready parameters, but it would still reduce all buckets. Marking a parameter gradient as ready does not help DDP skip buckets as for now, but it will prevent DDP from waiting for absent gradients forever during the backward pass. Note that traversing the autograd graph introduces extra overheads, so applications should only setfind_unused_parameterstoTruewhen necessary. - Backward Pass: Thebackward()function is directly invoked on the lossTensor, which is out of DDP’s control, and DDP uses autograd hooks registered at construction time to trigger gradients synchronizations. When one gradient becomes ready, its corresponding DDP hook on that grad accumulator will fire, and DDP will then mark that parameter gradient as ready for reduction. When gradients in one bucket are all ready, theReducerkicks off an asynchronousallreduceon that bucket to calculate mean of gradients across all processes. When all buckets are ready, theReducerwill block waiting for allallreduceoperations to finish. When this is done, averaged gradients are written to theparam.gradfield of all parameters. So after the backward pass, thegradfield on the same corresponding parameter across different DDP processes should be the same. - Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.", "prev_chunk_id": "chunk_1327", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1329", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "Implementation#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "Implementation#", "content": "Implementation# Below are pointers to the DDP implementation components. The stacked graph shows the structure of the code.", "prev_chunk_id": "chunk_1328", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1330", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "ProcessGroup#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "ProcessGroup#", "content": "ProcessGroup# - ProcessGroup.hpp: contains the abstract API of all process group implementations. Thec10dlibrary provides 3 implementations out of the box, namely,ProcessGroupGloo,ProcessGroupNCCL, andProcessGroupMPI.DistributedDataParallelusesProcessGroup::broadcast()to send model states from the process with rank 0 to others during initialization andProcessGroup::allreduce()to sum gradients. - Store.hpp: assists the rendezvous service for process group instances to find each other.", "prev_chunk_id": "chunk_1329", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1331", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "DistributedDataParallel#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "DistributedDataParallel#", "content": "DistributedDataParallel# - distributed.py: is the Python entry point for DDP. It implements the initialization steps and theforwardfunction for thenn.parallel.DistributedDataParallelmodule which call into C++ libraries. Its_sync_paramfunction performs intra-process parameter synchronization when one DDP process works on multiple devices, and it also broadcasts model buffers from the process with rank 0 to all other processes. The inter-process parameter synchronization happens inReducer.cpp. - comm.h: implements the coalesced broadcast helper function which is invoked to broadcast model states during initialization and synchronize model buffers before the forward pass. - reducer.h: provides the core implementation for gradient synchronization in the backward pass. It has three entry point functions:Reducer: The constructor is called indistributed.pywhich registersReducer::autograd_hook()to gradient accumulators.autograd_hook()function will be invoked by the autograd engine when a gradient becomes ready.prepare_for_backward()is called at the end of DDP forward pass indistributed.py. It traverses the autograd graph to find unused parameters whenfind_unused_parametersis set toTruein DDP constructor.", "prev_chunk_id": "chunk_1330", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1332", "url": "https://docs.pytorch.org/docs/stable/notes/ddp.html", "title": "TorchDynamo DDPOptimizer#", "page_title": "Distributed Data Parallel — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo DDPOptimizer#", "content": "TorchDynamo DDPOptimizer# DDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards. AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph, because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes. TorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break the graph during backwards, and the simplest implementation is to break the forward graphs and then call AotAutograd and compilation on each section. This allows DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications to overlap with compute. See this blog post for a more in-depth explanation and experimental results, or read the docs and code at torch/_dynamo/optimizations/distributed.py To Debug DDPOptimizer, set TORCH_LOGS=’ddp_graphs’ for full graph dumps. For logs without graphs, add any of ‘dynamo’, ‘distributed’, or ‘dist_ddp’ to TORCH_LOGS (for basic info about bucket boundaries). To disable DDPOptimizer, set torch._dynamo.config.optimize_ddp=False. DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.", "prev_chunk_id": "chunk_1331", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1333", "url": "https://docs.pytorch.org/docs/stable/notes/custom_operators.html", "title": "PyTorch Custom Operators Landing Page#", "page_title": "PyTorch Custom Operators Landing Page — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Custom Operators Landing Page#", "content": "PyTorch Custom Operators Landing Page# Created On: May 29, 2024 | Last Updated On: Nov 04, 2024 This page has moved. Redirecting to the new page…", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1334", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "CUDA semantics#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "CUDA semantics#", "content": "CUDA semantics# Created On: Jan 16, 2017 | Last Updated On: Jun 18, 2025 torch.cuda is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a torch.cuda.device context manager. However, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor. Cross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality such as to() and cuda(). Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error. Below you can find a small example showcasing this: cuda = torch.device('cuda') # Default CUDA device cuda0 = torch.device('cuda:0') cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed) x = torch.tensor([1., 2.], device=cuda0) # x.device is device(type='cuda', index=0) y = torch.tensor([1., 2.]).cuda() # y.device is device(type='cuda', index=0) with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.tensor([1., 2.], device=cuda) # transfers a tensor from CPU to GPU 1 b = torch.tensor([1., 2.]).cuda() # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch.tensor([1., 2.]).to(device=cuda) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch.randn(2, device=cuda2) e = torch.randn(2).to(cuda2) f = torch.randn(2).cuda(cuda2) # d.device, e.device, and f.device are all device(type='cuda', index=2)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1335", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "TensorFloat-32 (TF32) on Ampere (and later) devices#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "TensorFloat-32 (TF32) on Ampere (and later) devices#", "content": "TensorFloat-32 (TF32) on Ampere (and later) devices# Starting in PyTorch 1.7, there is a new flag called allow_tf32. This flag defaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later. This flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores, available on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies and batched matrix multiplies) and convolutions. TF32 tensor cores are designed to achieve better performance on matmul and convolutions on torch.float32 tensors by rounding input data to have 10 bits of mantissa, and accumulating results with FP32 precision, maintaining FP32 dynamic range. matmuls and convolutions are controlled separately, and their corresponding flags can be accessed at: # The flag below controls whether to allow TF32 on matmul. This flag defaults to False # in PyTorch 1.12 and later. torch.backends.cuda.matmul.allow_tf32 = True # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True. torch.backends.cudnn.allow_tf32 = True The precision of matmuls can also be set more broadly (limited not just to CUDA) via set_float_32_matmul_precision(). Note that besides matmuls and convolutions themselves, functions and nn modules that internally uses matmuls or convolutions are also affected. These include nn.Linear, nn.Conv*, cdist, tensordot, affine grid and grid sample, adaptive log softmax, GRU and LSTM. To get an idea of the precision and speed, see the example code and benchmark data (on A100) below: a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda') b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda') ab_full = a_full @ b_full mean = ab_full.abs().mean() # 80.7277 a = a_full.float() b = b_full.float() # Do matmul at TF32 mode. torch.backends.cuda.matmul.allow_tf32 = True ab_tf32 = a @ b # takes 0.016s on GA100 error = (ab_tf32 - ab_full).abs().max() # 0.1747 relative_error = error / mean # 0.0022 # Do matmul with", "prev_chunk_id": "chunk_1334", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1336", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "TensorFloat-32 (TF32) on Ampere (and later) devices#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "TensorFloat-32 (TF32) on Ampere (and later) devices#", "content": "TF32 disabled. torch.backends.cuda.matmul.allow_tf32 = False ab_fp32 = a @ b # takes 0.11s on GA100 error = (ab_fp32 - ab_full).abs().max() # 0.0031 relative_error = error / mean # 0.000039 From the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that relative error compared to double precision is approximately 2 orders of magnitude larger. Note that the exact ratio of TF32 to single precision speed depends on the hardware generation, as properties such as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput may vary from generation to generation or model to model. If full FP32 precision is needed, users can disable TF32 by: torch.backends.cuda.matmul.allow_tf32 = False torch.backends.cudnn.allow_tf32 = False To toggle the TF32 flags off in C++, you can do at::globalContext().setAllowTF32CuBLAS(false); at::globalContext().setAllowTF32CuDNN(false); For more information about TF32, see: - TensorFloat-32 - CUDA 11 - Ampere architecture", "prev_chunk_id": "chunk_1335", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1337", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Reduced Precision Reduction in FP16 GEMMs#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Reduced Precision Reduction in FP16 GEMMs#", "content": "Reduced Precision Reduction in FP16 GEMMs# (Distinct from full FP16 accumulation that is intended for hardware that has higher throughput with FP16 accumulation than FP32 accumulation, see Full FP16 accumulation) fp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large k dimension) and GPU architectures at the cost of numerical precision and potential for overflow. Some example benchmark data on V100: [--------------------------- bench_gemm_transformer --------------------------] [ m , k , n ] | allow_fp16_reduc=True | allow_fp16_reduc=False 1 threads: -------------------------------------------------------------------- [4096, 4048, 4096] | 1634.6 | 1639.8 [4096, 4056, 4096] | 1670.8 | 1661.9 [4096, 4080, 4096] | 1664.2 | 1658.3 [4096, 4096, 4096] | 1639.4 | 1651.0 [4096, 4104, 4096] | 1677.4 | 1674.9 [4096, 4128, 4096] | 1655.7 | 1646.0 [4096, 4144, 4096] | 1796.8 | 2519.6 [4096, 5096, 4096] | 2094.6 | 3190.0 [4096, 5104, 4096] | 2144.0 | 2663.5 [4096, 5112, 4096] | 2149.1 | 2766.9 [4096, 5120, 4096] | 2142.8 | 2631.0 [4096, 9728, 4096] | 3875.1 | 5779.8 [4096, 16384, 4096] | 6182.9 | 9656.5 (times in microseconds). If full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False To toggle the reduced precision reduction flags in C++, one can do at::globalContext().setAllowFP16ReductionCuBLAS(false);", "prev_chunk_id": "chunk_1336", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1338", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Reduced Precision Reduction in BF16 GEMMs#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Reduced Precision Reduction in BF16 GEMMs#", "content": "Reduced Precision Reduction in BF16 GEMMs# A similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is set to True by default for BF16, if you observe numerical instability in your workload, you may wish to set it to False. If reduced precision reductions are not desired, users can disable reduced precision reductions in bf16 GEMMs with: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False To toggle the reduced precision reduction flags in C++, one can do at::globalContext().setAllowBF16ReductionCuBLAS(true);", "prev_chunk_id": "chunk_1337", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1339", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Full FP16 Accmumulation in FP16 GEMMs#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Full FP16 Accmumulation in FP16 GEMMs#", "content": "Full FP16 Accmumulation in FP16 GEMMs# Certain GPUs have increased performance when doing _all_ FP16 GEMM accumulation in FP16, at the cost of numerical precision and greater likelihood of overflow. Note that this setting only has an effect on GPUs of compute capability 7.0 (Volta) or newer. This behavior can be enabled via: torch.backends.cuda.matmul.allow_fp16_accumulation = True To toggle the reduced precision reduction flags in C++, one can do at::globalContext().setAllowFP16AccumulationCuBLAS(true);", "prev_chunk_id": "chunk_1338", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1340", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Asynchronous execution#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Asynchronous execution#", "content": "Asynchronous execution# By default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs. In general, the effect of asynchronous computation is invisible to the caller, because (1) each device executes operations in the order they are queued, and (2) PyTorch automatically performs necessary synchronization when copying data between CPU and GPU or between two GPUs. Hence, computation will proceed as if every operation was executed synchronously. You can force synchronous computation by setting environment variable CUDA_LAUNCH_BLOCKING=1. This can be handy when an error occurs on the GPU. (With asynchronous execution, such an error isn’t reported until after the operation is actually executed, so the stack trace does not show where it was requested.) A consequence of the asynchronous computation is that time measurements without synchronizations are not accurate. To get precise measurements, one should either call torch.cuda.synchronize() before measuring, or use torch.cuda.Event to record times as following: start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record() # Run some things here end_event.record() torch.cuda.synchronize() # Wait for the events to be recorded! elapsed_time_ms = start_event.elapsed_time(end_event) As an exception, several functions such as to() and copy_() admit an explicit non_blocking argument, which lets the caller bypass synchronization when it is unnecessary. Another exception is CUDA streams, explained below.", "prev_chunk_id": "chunk_1339", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1341", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "CUDA streams#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "CUDA streams#", "content": "CUDA streams# A CUDA stream is a linear sequence of execution that belongs to a specific device. You normally do not need to create one explicitly: by default, each device uses its own “default” stream. Operations inside each stream are serialized in the order they are created, but operations from different streams can execute concurrently in any relative order, unless explicit synchronization functions (such as synchronize() or wait_stream()) are used. For example, the following code is incorrect: cuda = torch.device('cuda') s = torch.cuda.Stream() # Create a new stream. A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0) with torch.cuda.stream(s): # sum() may start execution before normal_() finishes! B = torch.sum(A) When the “current stream” is the default stream, PyTorch automatically performs necessary synchronization when data is moved around, as explained above. However, when using non-default streams, it is the user’s responsibility to ensure proper synchronization. The fixed version of this example is: cuda = torch.device('cuda') s = torch.cuda.Stream() # Create a new stream. A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0) s.wait_stream(torch.cuda.default_stream(cuda)) # NEW! with torch.cuda.stream(s): B = torch.sum(A) A.record_stream(s) # NEW! There are two new additions. The torch.cuda.Stream.wait_stream() call ensures that the normal_() execution has finished before we start running sum(A) on a side stream. The torch.Tensor.record_stream() (see for more details) ensures that we do not deallocate A before sum(A) has completed. You can also manually wait on the stream at some later point in time with torch.cuda.default_stream(cuda).wait_stream(s) (note that it is pointless to wait immediately, since that will prevent the stream execution from running in parallel with other work on the default stream.) See the documentation for torch.Tensor.record_stream() on more details on when to use one or another. Note that this synchronization is necessary even when there is no read dependency, e.g., as seen in this example: cuda = torch.device('cuda') s = torch.cuda.Stream()", "prev_chunk_id": "chunk_1340", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1342", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "CUDA streams#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "CUDA streams#", "content": "# Create a new stream. A = torch.empty((100, 100), device=cuda) s.wait_stream(torch.cuda.default_stream(cuda)) # STILL REQUIRED! with torch.cuda.stream(s): A.normal_(0.0, 1.0) A.record_stream(s) Despite the computation on s not reading the contents of A and no other uses of A, it is still necessary to synchronize, because A may correspond to memory reallocated by the CUDA caching allocator, with pending operations from the old (deallocated) memory.", "prev_chunk_id": "chunk_1341", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1343", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Stream semantics of backward passes#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Stream semantics of backward passes#", "content": "Stream semantics of backward passes# Each backward CUDA op runs on the same stream that was used for its corresponding forward op. If your forward pass runs independent ops in parallel on different streams, this helps the backward pass exploit that same parallelism. The stream semantics of a backward call with respect to surrounding ops are the same as for any other call. The backward pass inserts internal syncs to ensure this even when backward ops run on multiple streams as described in the previous paragraph. More concretely, when calling autograd.backward, autograd.grad, or tensor.backward, and optionally supplying CUDA tensor(s) as the initial gradient(s) (e.g., autograd.backward(..., grad_tensors=initial_grads), autograd.grad(..., grad_outputs=initial_grads), or tensor.backward(..., gradient=initial_grad)), the acts of - optionally populating initial gradient(s), - invoking the backward pass, and - using the gradients have the same stream-semantics relationship as any group of ops: s = torch.cuda.Stream() # Safe, grads are used in the same stream context as backward() with torch.cuda.stream(s): loss.backward() use grads # Unsafe with torch.cuda.stream(s): loss.backward() use grads # Safe, with synchronization with torch.cuda.stream(s): loss.backward() torch.cuda.current_stream().wait_stream(s) use grads # Safe, populating initial grad and invoking backward are in the same stream context with torch.cuda.stream(s): loss.backward(gradient=torch.ones_like(loss)) # Unsafe, populating initial_grad and invoking backward are in different stream contexts, # without synchronization initial_grad = torch.ones_like(loss) with torch.cuda.stream(s): loss.backward(gradient=initial_grad) # Safe, with synchronization initial_grad = torch.ones_like(loss) s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): initial_grad.record_stream(s) loss.backward(gradient=initial_grad)", "prev_chunk_id": "chunk_1342", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1344", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "BC note: Using grads on the default stream#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "BC note: Using grads on the default stream#", "content": "BC note: Using grads on the default stream# In prior versions of PyTorch (1.9 and earlier), the autograd engine always synced the default stream with all backward ops, so the following pattern: with torch.cuda.stream(s): loss.backward() use grads was safe as long as use grads happened on the default stream. In present PyTorch, that pattern is no longer safe. If backward() and use grads are in different stream contexts, you must sync the streams: with torch.cuda.stream(s): loss.backward() torch.cuda.current_stream().wait_stream(s) use grads even if use grads is on the default stream.", "prev_chunk_id": "chunk_1343", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1345", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Memory management#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Memory management#", "content": "Memory management# PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch. To better understand how CUDA memory is being used over time, Understanding CUDA Memory Usage describes tools for capturing and visualizing traces of memory use. For more advanced users, we offer more comprehensive memory benchmarking via memory_stats(). We also offer the capability to capture a complete snapshot of the memory allocator state via memory_snapshot(), which can help you understand the underlying allocation patterns produced by your code.", "prev_chunk_id": "chunk_1344", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1346", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "content": "Optimizing memory usage with PYTORCH_CUDA_ALLOC_CONF# Use of a caching allocator can interfere with memory checking tools such as cuda-memcheck. To debug memory errors using cuda-memcheck, set PYTORCH_NO_CUDA_MEMORY_CACHING=1 in your environment to disable caching. The behavior of the caching allocator can be controlled via the environment variable PYTORCH_CUDA_ALLOC_CONF. The format is PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>... Available options: - backendallows selecting the underlying allocator implementation. Currently, valid options arenative, which uses PyTorch’s native implementation, andcudaMallocAsync, which usesCUDA’s built-in asynchronous allocator.cudaMallocAsyncrequires CUDA 11.4 or newer. The default isnative.backendapplies to all devices used by the process, and can’t be specified on a per-device basis. - max_split_size_mbprevents the native allocator from splitting blocks larger than this size (in MB). This can reduce fragmentation and may allow some borderline workloads to complete without running out of memory. Performance cost can range from ‘zero’ to ‘substantial’ depending on allocation patterns. Default value is unlimited, i.e. all blocks can be split. Thememory_stats()andmemory_summary()methods are useful for tuning. This option should be used as a last resort for a workload that is aborting due to ‘out of memory’ and showing a large amount of inactive split blocks.max_split_size_mbis only meaningful withbackend:native. Withbackend:cudaMallocAsync,max_split_size_mbis ignored. - roundup_power2_divisionshelps with rounding the requested allocation size to nearest power-2 division and making better use of the blocks. In the native CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works fine for smaller sizes. However, this can be inefficient for large near-by allocations as each will go to different size of blocks and reuse of those blocks are minimized. This might create lots of unused blocks and will waste GPU memory capacity. This option enables the rounding of allocation size to nearest power-2 division. For example, if we need to round-up size of 1200 and if number of divisions is 4, the size", "prev_chunk_id": "chunk_1345", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1347", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "content": "1200 lies between 1024 and 2048 and if we do 4 divisions between them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division. Specify a single value to apply for all allocation sizes or specify an array of key value pairs to set power-2 division individually for each power of two interval. For example to set 1 division for all allocations under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions for allocations between 512MB and 1GB and 8 divisions for any larger allocations, set the knob value to: [256:1,512:2,1024:4,>:8].roundup_power2_divisionsis only meaningful withbackend:native. Withbackend:cudaMallocAsync,roundup_power2_divisionsis ignored. - max_non_split_rounding_mbwill allow non-split blocks for better reuse, eg,a 1024MB cached block can be reused for a 512MB allocation request. In the default case, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block can only be served with between 512-532 MB size block. If we set the value of this option to 1024, it will allow 512-1536 MB size blocks to be used for a 512MB block which increases reuse of larger blocks. This will also help in reducing the stalls in avoiding expensive cudaMalloc calls. - garbage_collection_thresholdhelps actively reclaiming unused GPU memory to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks), which can be unfavorable to latency-critical GPU applications (e.g., servers). Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80% of the total memory allocated to the GPU application). The algorithm prefers to free old & unused blocks first to avoid freeing blocks that are actively being reused. The threshold value should be between greater than 0.0 and less than 1.0. The default value is set", "prev_chunk_id": "chunk_1346", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1348", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "content": "at 1.0.garbage_collection_thresholdis only meaningful withbackend:native. Withbackend:cudaMallocAsync,garbage_collection_thresholdis ignored. - expandable_segments(experimental, default:False) If set toTrue, this setting instructs the allocator to create CUDA allocations that can later be expanded to better handle cases where a job changing allocation sizes frequently, such as having a changing batch size. Normally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations that are the same size as what the user requests. In the future, parts of these allocations can be reused for other requests if they are free. This works well when the program makes many requests of exactly the same size or of sizes that even multiples of that size. Many deep learning models follow this behavior. However, one common exception is when the batch size changes slightly from one iteration to the next, e.g. in batched inference. When the program runs initially with batch sizeN, it will make allocations appropriate for that size. If in the future, it runs at sizeN - 1, the existing allocations will still be big enough. However, if it runs at sizeN + 1, then it will have to make new allocations that are slightly larger. Not all the tensors are the same size. Some might be(N + 1)*Aand others(N + 1)*A*BwhereAandBare some non-batch dimensions in the model. Because the allocator reuses existing allocations when they are big enough, some number of(N + 1)*Aallocations will actually fit in the already existingN*B*Asegments, though not perfectly. As the model runs it will partially fill up all of these segments leaving unusable free slices of memory at the end of these segments. The allocator at some point will need tocudaMalloca new(N + 1)*A*Bsegment. If there is not enough memory, there is now no way to recover the slices of memory that are free at the end of existing segments. With", "prev_chunk_id": "chunk_1347", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1349", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF#", "content": "models 50+ layers deep, this pattern might repeat 50+ times creating many slivers.expandable_segmentsallows the allocator to create a segment initially and then expand its size later when more memory is needed. Instead of making one segment per allocation, it tries to make one segment (per stream) that grows as necessary. Now when theN + 1case runs, the allocations will tile nicely into the one large segment until it fills up. Then more memory is requested and appended to the end of the segment. This process does not create as many slivers of unusable memory, so it is more likely to succeed at finding this memory. - pinned_use_cuda_host_registeroption is a boolean flag that determines whether to use the CUDA API’s cudaHostRegister function for allocating pinned memory instead of the default cudaHostAlloc. When set to True, the memory is allocated using regular malloc and then pages are mapped to the memory before calling cudaHostRegister. This pre-mapping of pages helps reduce the lock time during the execution of cudaHostRegister. - pinned_num_register_threadsoption is only valid when pinned_use_cuda_host_register is set to True. By default, one thread is used to map the pages. This option allows using more threads to parallelize the page mapping operations to reduce the overall allocation time of pinned memory. A good value for this option is 8 based on benchmarking results. - pinned_use_background_threadsoption is a boolean flag to enable background thread for processing events. This avoids any slow path associated with querying/processing of events in the fast allocation path. This feature is disabled by default.", "prev_chunk_id": "chunk_1348", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1350", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Using custom memory allocators for CUDA#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Using custom memory allocators for CUDA#", "content": "Using custom memory allocators for CUDA# It is possible to define allocators as simple functions in C/C++ and compile them as a shared library, the code below shows a basic allocator that just traces all the memory operations. #include <sys/types.h> #include <cuda_runtime_api.h> #include <iostream> // Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC extern \"C\" { void* my_malloc(ssize_t size, int device, cudaStream_t stream) { void *ptr; cudaMalloc(&ptr, size); std::cout<<\"alloc \"<<ptr<<size<<std::endl; return ptr; } void my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) { std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl; cudaFree(ptr); } } This can be used in python through the torch.cuda.memory.CUDAPluggableAllocator. The user is responsible for supplying the path to the .so file and the name of the alloc/free functions that match the signatures specified above. import torch # Load the allocator new_alloc = torch.cuda.memory.CUDAPluggableAllocator( 'alloc.so', 'my_malloc', 'my_free') # Swap the current allocator torch.cuda.memory.change_current_allocator(new_alloc) # This will allocate memory in the device using the new allocator b = torch.zeros(10, device='cuda') import torch # Do an initial memory allocator b = torch.zeros(10, device='cuda') # Load the allocator new_alloc = torch.cuda.memory.CUDAPluggableAllocator( 'alloc.so', 'my_malloc', 'my_free') # This will error since the current allocator was already instantiated torch.cuda.memory.change_current_allocator(new_alloc)", "prev_chunk_id": "chunk_1349", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1351", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Mixing different CUDA system allocators in the same program#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Mixing different CUDA system allocators in the same program#", "content": "Mixing different CUDA system allocators in the same program# Depending on your use case, change_current_allocator() may not be what you want to use, since it swaps the CUDA allocator for the entire program (similar to PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync). For instance, if the swapped allocator doesn’t have caching mechanism, you will lose all the benefits of PyTorch’s CUDACachingAllocator. Instead, you can selectively mark a region of PyTorch code to use a custom allocator using torch.cuda.MemPool. This will let you use multiple CUDA system allocators in the same PyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching). Using torch.cuda.MemPool, you can utilize custom allocators that enable several features, such as: - Allocating output buffers for an all-reduce usingncclMemAllocallocator can enable NVLink Switch Reductions (NVLS). This can reduce contention between overlapping compute and communication kernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads. - For Grace CPU based systems, allocating host outputs buffers for an all-gather usingcuMemCreateand specifyingCU_MEM_LOCATION_TYPE_HOST_NUMAcan enable Extended GPU Memory (EGM) based memory transfers from source GPUs to the destination CPU. This accelerates the all-gather since the transfer happens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface Card (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing. - If you are crafting a model and don’t want to think about the optimal memory placements of a memory intensive module at first (e.g. an embedding table), or perhaps you have a module which is not performance sensitive and doesn’t fit in the GPU, then you could just allocate that module withcudaMallocManagedwith preferred CPU location and get your model working first. The code below shows ncclMemAlloc wrapped in a torch.cuda.memory.CUDAPluggableAllocator. import os import torch import torch.distributed as dist from torch.cuda.memory import CUDAPluggableAllocator from torch.distributed.distributed_c10d import _get_default_group from torch.utils import cpp_extension #", "prev_chunk_id": "chunk_1350", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1352", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Mixing different CUDA system allocators in the same program#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Mixing different CUDA system allocators in the same program#", "content": "create allocator nccl_allocator_source = \"\"\" #include <nccl.h> #include <iostream> extern \"C\" { void* nccl_alloc_plug(size_t size, int device, void* stream) { std::cout << \"Using ncclMemAlloc\" << std::endl; void* ptr; ncclResult_t err = ncclMemAlloc(&ptr, size); return ptr; } void nccl_free_plug(void* ptr, size_t size, int device, void* stream) { std::cout << \"Using ncclMemFree\" << std::endl; ncclResult_t err = ncclMemFree(ptr); } } \"\"\" nccl_allocator_libname = \"nccl_allocator\" nccl_allocator = torch.utils.cpp_extension.load_inline( name=nccl_allocator_libname, cpp_sources=nccl_allocator_source, with_cuda=True, extra_ldflags=[\"-lnccl\"], verbose=True, is_python_module=False, build_directory=\"./\", ) allocator = CUDAPluggableAllocator( f\"./{nccl_allocator_libname}.so\", \"nccl_alloc_plug\", \"nccl_free_plug\" ).allocator() # setup distributed rank = int(os.getenv(\"RANK\")) local_rank = int(os.getenv(\"LOCAL_RANK\")) world_size = int(os.getenv(\"WORLD_SIZE\")) torch.cuda.set_device(local_rank) dist.init_process_group(backend=\"nccl\") device = torch.device(f\"cuda:{local_rank}\") default_pg = _get_default_group() backend = default_pg._get_backend(device) # Note: for convenience, ProcessGroupNCCL backend provides # the ncclMemAlloc allocator as backend.mem_allocator allocator = backend.mem_allocator You can now define a new memory pool by passing this allocator to torch.cuda.MemPool: pool = torch.cuda.MemPool(allocator) The pool can then be used with the torch.cuda.use_mem_pool context manager to allocate tensors into that pool: with torch.cuda.use_mem_pool(pool): # tensor gets allocated with ncclMemAlloc passed in the pool tensor = torch.arange(1024 * 1024 * 2, device=device) print(f\"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}\") # register user buffers using ncclCommRegister (called under the hood) backend.register_mem_pool(pool) # Collective uses Zero Copy NVLS dist.all_reduce(tensor[0:4]) torch.cuda.synchronize() print(tensor[0:4]) Note the usage of register_mem_pool in the above example. This is an extra step for NVLS reductions, where the user buffers need to be registered with NCCL. A user can de-register the buffers with a similar deregister_mem_pool call. To reclaim memory, users will first need to ensure nothing is using the pool. When none of the tensors are holding a reference to the pool, empty_cache() will be called internally on deletion of the pool, hence returning all the memory to the system. del tensor, del pool Users can optionally specify a use_on_oom bool (which is False by default) during MemPool", "prev_chunk_id": "chunk_1351", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1353", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Mixing different CUDA system allocators in the same program#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Mixing different CUDA system allocators in the same program#", "content": "creation. If true, then the CUDACachingAllocator will be able to use memory in this pool as a last resort instead of OOMing. pool = torch.cuda.MemPool(allocator, use_on_oom=True) with torch.cuda.use_mem_pool(pool): a = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\") del a # at the memory limit, this will succeed by using pool's memory in order to avoid the oom b = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\") The following torch.cuda.MemPool.use_count() and torch.cuda.MemPool.snapshot() APIs can be used for debugging purposes: pool = torch.cuda.MemPool(allocator) # pool's use count should be 1 at this point as MemPool object # holds a reference assert pool.use_count() == 1 nelem_1mb = 1024 * 1024 // 4 with torch.cuda.use_mem_pool(pool): out_0 = torch.randn(nelem_1mb, device=\"cuda\") # pool's use count should be 2 at this point as use_mem_pool # holds a reference assert pool.use_count() == 2 # pool's use count should be back to 1 at this point as use_mem_pool # released its reference assert pool.use_count() == 1 with torch.cuda.use_mem_pool(pool): # pool should have 1 segment since we made a small allocation (1 MB) # above and so the CUDACachingAllocator packed it into a 2 MB buffer assert len(pool.snapshot()) == 1 out_1 = torch.randn(nelem_1mb, device=\"cuda\") # pool should still have 1 segment since we made another small allocation # (1 MB) that got packed into the existing 2 MB buffer assert len(pool.snapshot()) == 1 out_2 = torch.randn(nelem_1mb, device=\"cuda\") # pool now should have 2 segments since the CUDACachingAllocator had # to make a new 2 MB buffer to accommodate out_2 assert len(pool.snapshot()) == 2", "prev_chunk_id": "chunk_1352", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1354", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "cuBLAS workspaces#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "cuBLAS workspaces#", "content": "cuBLAS workspaces# For each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated if that handle and stream combination executes a cuBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless torch._C._cuda_clearCublasWorkspaces() is called. The workspace size per allocation can be specified via the environment variable CUBLAS_WORKSPACE_CONFIG with the format :[SIZE]:[COUNT]. As an example, the default workspace size per allocation is CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8 which specifies a total size of 2 * 4096 + 8 * 16 KiB. To force cuBLAS to avoid using workspaces, set CUBLAS_WORKSPACE_CONFIG=:0:0.", "prev_chunk_id": "chunk_1353", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1355", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "cuFFT plan cache#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "cuFFT plan cache#", "content": "cuFFT plan cache# For each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly running FFT methods (e.g., torch.fft.fft()) on CUDA tensors of same geometry with same configuration. Because some cuFFT plans may allocate GPU memory, these caches have a maximum capacity. You may control and query the properties of the cache of current device with the following APIs: - torch.backends.cuda.cufft_plan_cache.max_sizegives the capacity of the cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions). Setting this value directly modifies the capacity. - torch.backends.cuda.cufft_plan_cache.sizegives the number of plans currently residing in the cache. - torch.backends.cuda.cufft_plan_cache.clear()clears the cache. To control and query plan caches of a non-default device, you can index the torch.backends.cuda.cufft_plan_cache object with either a torch.device object or a device index, and access one of the above attributes. E.g., to set the capacity of the cache for device 1, one can write torch.backends.cuda.cufft_plan_cache[1].max_size = 10.", "prev_chunk_id": "chunk_1354", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1356", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Just-in-Time Compilation#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Just-in-Time Compilation#", "content": "Just-in-Time Compilation# PyTorch just-in-time compiles some operations, like torch.special.zeta, when performed on CUDA tensors. This compilation can be time consuming (up to a few seconds depending on your hardware and software) and may occur multiple times for a single operator since many PyTorch operators actually select from a variety of kernels, each of which must be compiled once, depending on their input. This compilation occurs once per process, or just once if a kernel cache is used. By default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except on Windows, where the kernel cache is not yet supported). The caching behavior can be directly controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used as a kernel cache instead of the default location.", "prev_chunk_id": "chunk_1355", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1357", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Device-agnostic code#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Device-agnostic code#", "content": "Device-agnostic code# Due to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code; an example may be creating a new tensor as the initial hidden state of a recurrent neural network. The first step is to determine whether the GPU should be used or not. A common pattern is to use Python’s argparse module to read in user arguments, and have a flag that can be used to disable CUDA, in combination with is_available(). In the following, args.device results in a torch.device object that can be used to move tensors to CPU or CUDA. import argparse import torch parser = argparse.ArgumentParser(description='PyTorch Example') parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA') args = parser.parse_args() args.device = None if not args.disable_cuda and torch.cuda.is_available(): args.device = torch.device('cuda') else: args.device = torch.device('cpu') Now that we have args.device, we can use it to create a Tensor on the desired device. x = torch.empty((8, 42), device=args.device) net = Network().to(device=args.device) This can be used in a number of cases to produce device agnostic code. Below is an example when using a dataloader: cuda0 = torch.device('cuda:0') # CUDA GPU 0 for i, x in enumerate(train_loader): x = x.to(cuda0) When working with multiple GPUs on a system, you can use the CUDA_VISIBLE_DEVICES environment flag to manage which GPUs are available to PyTorch. As mentioned above, to manually control which GPU a tensor is created on, the best practice is to use a torch.cuda.device context manager. print(\"Outside device is 0\") # On device 0 (default in most scenarios) with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside device is still 0\") # On device 0 If you have a tensor and would like to create a new tensor of the same type on the same device, then you can use a torch.Tensor.new_* method (see torch.Tensor). Whilst", "prev_chunk_id": "chunk_1356", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1358", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Device-agnostic code#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Device-agnostic code#", "content": "the previously mentioned torch.* factory functions (Creation Ops) depend on the current GPU context and the attributes arguments you pass in, torch.Tensor.new_* methods preserve the device and other attributes of the tensor. This is the recommended practice when creating modules in which new tensors need to be created internally during the forward pass. cuda = torch.device('cuda') x_cpu = torch.empty(2) x_gpu = torch.empty(2, device=cuda) x_cpu_long = torch.empty(2, dtype=torch.int64) y_cpu = x_cpu.new_full([3, 2], fill_value=0.3) print(y_cpu) tensor([[ 0.3000, 0.3000], [ 0.3000, 0.3000], [ 0.3000, 0.3000]]) y_gpu = x_gpu.new_full([3, 2], fill_value=-5) print(y_gpu) tensor([[-5.0000, -5.0000], [-5.0000, -5.0000], [-5.0000, -5.0000]], device='cuda:0') y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]]) print(y_cpu_long) tensor([[ 1, 2, 3]]) If you want to create a tensor of the same type and size of another tensor, and fill it with either ones or zeros, ones_like() or zeros_like() are provided as convenient helper functions (which also preserve torch.device and torch.dtype of a Tensor). x_cpu = torch.empty(2, 3) x_gpu = torch.empty(2, 3) y_cpu = torch.ones_like(x_cpu) y_gpu = torch.zeros_like(x_gpu)", "prev_chunk_id": "chunk_1357", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1359", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Use pinned memory buffers#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Use pinned memory buffers#", "content": "Use pinned memory buffers# Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region. Also, once you pin a tensor or storage, you can use asynchronous GPU copies. Just pass an additional non_blocking=True argument to a to() or a cuda() call. This can be used to overlap data transfers with computation. You can make the DataLoader return batches placed in pinned memory by passing pin_memory=True to its constructor.", "prev_chunk_id": "chunk_1358", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1360", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel#", "content": "Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel# Most use cases involving batched inputs and multiple GPUs should default to using DistributedDataParallel to utilize more than one GPU. There are significant caveats to using CUDA models with multiprocessing; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior. It is recommended to use DistributedDataParallel, instead of DataParallel to do multi-GPU training, even if there is only a single node. The difference between DistributedDataParallel and DataParallel is: DistributedDataParallel uses multiprocessing where a process is created for each GPU, while DataParallel uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter. If you use DistributedDataParallel, you could use torch.distributed.launch utility to launch your program, see Launch utility.", "prev_chunk_id": "chunk_1359", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1361", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "CUDA Graphs#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "CUDA Graphs#", "content": "CUDA Graphs# A CUDA graph is a record of the work (mostly kernels and their arguments) that a CUDA stream and its dependent streams perform. For general principles and details on the underlying CUDA API, see Getting Started with CUDA Graphs and the Graphs section of the CUDA C Programming Guide. PyTorch supports the construction of CUDA graphs using stream capture, which puts a CUDA stream in capture mode. CUDA work issued to a capturing stream doesn’t actually run on the GPU. Instead, the work is recorded in a graph. After capture, the graph can be launched to run the GPU work as many times as needed. Each replay runs the same kernels with the same arguments. For pointer arguments this means the same memory addresses are used. By filling input memory with new data (e.g., from a new batch) before each replay, you can rerun the same work on new data.", "prev_chunk_id": "chunk_1360", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1362", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Why CUDA Graphs?#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Why CUDA Graphs?#", "content": "Why CUDA Graphs?# Replaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for greatly reduced CPU overhead. A graph’s arguments and kernels are fixed, so a graph replay skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver overheads. Under the hood, a replay submits the entire graph’s work to the GPU with a single call to cudaGraphLaunch. Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead is the main benefit. You should try CUDA graphs if all or part of your network is graph-safe (usually this means static shapes and static control flow, but see the other constraints) and you suspect its runtime is at least somewhat CPU-limited.", "prev_chunk_id": "chunk_1361", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1363", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "PyTorch API#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch API#", "content": "PyTorch API# PyTorch exposes graphs via a raw torch.cuda.CUDAGraph class and two convenience wrappers, torch.cuda.graph and torch.cuda.make_graphed_callables. torch.cuda.graph is a simple, versatile context manager that captures CUDA work in its context. Before capture, warm up the workload to be captured by running a few eager iterations. Warmup must occur on a side stream. Because the graph reads from and writes to the same memory addresses in every replay, you must maintain long-lived references to tensors that hold input and output data during capture. To run the graph on new input data, copy new data to the capture’s input tensor(s), replay the graph, then read the new output from the capture’s output tensor(s). Example: g = torch.cuda.CUDAGraph() # Placeholder input used for capture static_input = torch.empty((5,), device=\"cuda\") # Warmup before capture s = torch.cuda.Stream() s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): for _ in range(3): static_output = static_input * 2 torch.cuda.current_stream().wait_stream(s) # Captures the graph # To allow capture, automatically sets a side stream as the current stream in the context with torch.cuda.graph(g): static_output = static_input * 2 # Fills the graph's input memory with new data to compute on static_input.copy_(torch.full((5,), 3, device=\"cuda\")) g.replay() # static_output holds the results print(static_output) # full of 3 * 2 = 6 # Fills the graph's input memory with more data to compute on static_input.copy_(torch.full((5,), 4, device=\"cuda\")) g.replay() print(static_output) # full of 4 * 2 = 8 See Whole-network capture, Usage with torch.cuda.amp, and Usage with multiple streams for realistic and advanced patterns. make_graphed_callables is more sophisticated. make_graphed_callables accepts Python functions and torch.nn.Modules. For each passed function or Module, it creates separate graphs of the forward-pass and backward-pass work. See Partial-network capture.", "prev_chunk_id": "chunk_1362", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1364", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Constraints#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Constraints#", "content": "Constraints# A set of ops is capturable if it doesn’t violate any of the following constraints. Constraints apply to all work in a torch.cuda.graph context and all work in the forward and backward passes of any callable you pass to torch.cuda.make_graphed_callables(). Violating any of these will likely cause a runtime error: - Capture must occur on a non-default stream. (This is only a concern if you use the rawCUDAGraph.capture_beginandCUDAGraph.capture_endcalls.graphandmake_graphed_callables()set a side stream for you.) - Ops that synchronize the CPU with the GPU (e.g.,.item()calls) are prohibited. - CUDA RNG operations are permitted, and when using multipletorch.Generatorinstances within a graph, they must be registered usingCUDAGraph.register_generator_statebefore graph capture. Avoid usingGenerator.get_stateandGenerator.set_stateduring capture; instead, utilizeGenerator.graphsafe_set_stateandGenerator.graphsafe_get_statefor managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs. Violating any of these will likely cause silent numerical errors or undefined behavior: - Within a process, only one capture may be underway at a time. - No non-captured CUDA work may run in this process (on any thread) while capture is underway. - CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay. - Every replay reads from and writes to the same (virtual) memory addresses. - Dynamic control flow (based on CPU or GPU data) is prohibited. - Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence has the same size and layout in every replay. - Using multiple streams in a capture is allowed, but there arerestrictions.", "prev_chunk_id": "chunk_1363", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1365", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Non-constraints#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Non-constraints#", "content": "Non-constraints# - Once captured, the graph may be replayed on any stream.", "prev_chunk_id": "chunk_1364", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1366", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Whole-network capture#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Whole-network capture#", "content": "Whole-network capture# If your entire network is capturable, you can capture and replay an entire iteration: N, D_in, H, D_out = 640, 4096, 2048, 1024 model = torch.nn.Sequential(torch.nn.Linear(D_in, H), torch.nn.Dropout(p=0.2), torch.nn.Linear(H, D_out), torch.nn.Dropout(p=0.1)).cuda() loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.1) # Placeholders used for capture static_input = torch.randn(N, D_in, device='cuda') static_target = torch.randn(N, D_out, device='cuda') # warmup # Uses static_input and static_target here for convenience, # but in a real setting, because the warmup includes optimizer.step() # you must use a few batches of real data. s = torch.cuda.Stream() s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): for i in range(3): optimizer.zero_grad(set_to_none=True) y_pred = model(static_input) loss = loss_fn(y_pred, static_target) loss.backward() optimizer.step() torch.cuda.current_stream().wait_stream(s) # capture g = torch.cuda.CUDAGraph() # Sets grads to None before capture, so backward() will create # .grad attributes with allocations from the graph's private pool optimizer.zero_grad(set_to_none=True) with torch.cuda.graph(g): static_y_pred = model(static_input) static_loss = loss_fn(static_y_pred, static_target) static_loss.backward() optimizer.step() real_inputs = [torch.rand_like(static_input) for _ in range(10)] real_targets = [torch.rand_like(static_target) for _ in range(10)] for data, target in zip(real_inputs, real_targets): # Fills the graph's input memory with new data to compute on static_input.copy_(data) static_target.copy_(target) # replay() includes forward, backward, and step. # You don't even need to call optimizer.zero_grad() between iterations # because the captured backward refills static .grad tensors in place. g.replay() # Params have been updated. static_y_pred, static_loss, and .grad # attributes hold values from computing on this iteration's data.", "prev_chunk_id": "chunk_1365", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1367", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Partial-network capture#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Partial-network capture#", "content": "Partial-network capture# If some of your network is unsafe to capture (e.g., due to dynamic control flow, dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe part(s) eagerly and use torch.cuda.make_graphed_callables() to graph only the capture-safe part(s). By default, callables returned by make_graphed_callables() are autograd-aware, and can be used in the training loop as direct replacements for the functions or nn.Modules you passed. make_graphed_callables() internally creates CUDAGraph objects, runs warmup iterations, and maintains static inputs and outputs as needed. Therefore (unlike with torch.cuda.graph) you don’t need to handle those manually. In the following example, data-dependent dynamic control flow means the network isn’t capturable end-to-end, but make_graphed_callables() lets us capture and run graph-safe sections as graphs regardless: N, D_in, H, D_out = 640, 4096, 2048, 1024 module1 = torch.nn.Linear(D_in, H).cuda() module2 = torch.nn.Linear(H, D_out).cuda() module3 = torch.nn.Linear(H, D_out).cuda() loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(chain(module1.parameters(), module2.parameters(), module3.parameters()), lr=0.1) # Sample inputs used for capture # requires_grad state of sample inputs must match # requires_grad state of real inputs each callable will see. x = torch.randn(N, D_in, device='cuda') h = torch.randn(N, H, device='cuda', requires_grad=True) module1 = torch.cuda.make_graphed_callables(module1, (x,)) module2 = torch.cuda.make_graphed_callables(module2, (h,)) module3 = torch.cuda.make_graphed_callables(module3, (h,)) real_inputs = [torch.rand_like(x) for _ in range(10)] real_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)] for data, target in zip(real_inputs, real_targets): optimizer.zero_grad(set_to_none=True) tmp = module1(data) # forward ops run as a graph if tmp.sum().item() > 0: tmp = module2(tmp) # forward ops run as a graph else: tmp = module3(tmp) # forward ops run as a graph loss = loss_fn(tmp, target) # module2's or module3's (whichever was chosen) backward ops, # as well as module1's backward ops, run as graphs loss.backward() optimizer.step()", "prev_chunk_id": "chunk_1366", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1368", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Usage with torch.cuda.amp#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Usage with torch.cuda.amp#", "content": "Usage with torch.cuda.amp# For typical optimizers, GradScaler.step syncs the CPU with the GPU, which is prohibited during capture. To avoid errors, either use partial-network capture, or (if forward, loss, and backward are capture-safe) capture forward, loss, and backward but not the optimizer step: # warmup # In a real setting, use a few batches of real data. s = torch.cuda.Stream() s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): for i in range(3): optimizer.zero_grad(set_to_none=True) with torch.cuda.amp.autocast(): y_pred = model(static_input) loss = loss_fn(y_pred, static_target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() torch.cuda.current_stream().wait_stream(s) # capture g = torch.cuda.CUDAGraph() optimizer.zero_grad(set_to_none=True) with torch.cuda.graph(g): with torch.cuda.amp.autocast(): static_y_pred = model(static_input) static_loss = loss_fn(static_y_pred, static_target) scaler.scale(static_loss).backward() # don't capture scaler.step(optimizer) or scaler.update() real_inputs = [torch.rand_like(static_input) for _ in range(10)] real_targets = [torch.rand_like(static_target) for _ in range(10)] for data, target in zip(real_inputs, real_targets): static_input.copy_(data) static_target.copy_(target) g.replay() # Runs scaler.step and scaler.update eagerly scaler.step(optimizer) scaler.update()", "prev_chunk_id": "chunk_1367", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1369", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Usage with multiple streams#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Usage with multiple streams#", "content": "Usage with multiple streams# Capture mode automatically propagates to any streams that sync with a capturing stream. Within capture, you may expose parallelism by issuing calls to different streams, but the overall stream dependency DAG must branch out from the initial capturing stream after capture begins and rejoin the initial stream before capture ends: with torch.cuda.graph(g): # at context manager entrance, torch.cuda.current_stream() # is the initial capturing stream # INCORRECT (does not branch out from or rejoin initial stream) with torch.cuda.stream(s): cuda_work() # CORRECT: # branches out from initial stream s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): cuda_work() # rejoins initial stream before capture ends torch.cuda.current_stream().wait_stream(s)", "prev_chunk_id": "chunk_1368", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1370", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "NCCL < 2.9.6#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "NCCL < 2.9.6#", "content": "NCCL < 2.9.6# NCCL versions earlier than 2.9.6 don’t allow collectives to be captured. You must use partial-network capture, which defers allreduces to happen outside graphed sections of backward. Call make_graphed_callables() on graphable network sections before wrapping the network with DDP.", "prev_chunk_id": "chunk_1369", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1371", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "NCCL >= 2.9.6#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "NCCL >= 2.9.6#", "content": "NCCL >= 2.9.6# NCCL versions 2.9.6 or later allow collectives in the graph. Approaches that capture an entire backward pass are a viable option, but need three setup steps. - Disable DDP’s internal async error handling:os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"]=\"0\"torch.distributed.init_process_group(...) - Before full-backward capture, DDP must be constructed in a side-stream context:withtorch.cuda.stream(s):model=DistributedDataParallel(model) - Your warmup must run at least 11 DDP-enabled eager iterations before capture.", "prev_chunk_id": "chunk_1370", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1372", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Graph memory management#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Graph memory management#", "content": "Graph memory management# A captured graph acts on the same virtual addresses every time it replays. If PyTorch frees the memory, a later replay can hit an illegal memory access. If PyTorch reassigns the memory to new tensors, the replay can corrupt the values seen by those tensors. Therefore, the virtual addresses used by the graph must be reserved for the graph across replays. The PyTorch caching allocator achieves this by detecting when capture is underway and satisfying the capture’s allocations from a graph-private memory pool. The private pool stays alive until its CUDAGraph object and all tensors created during capture go out of scope. Private pools are maintained automatically. By default, the allocator creates a separate private pool for each capture. If you capture multiple graphs, this conservative approach ensures graph replays never corrupt each other’s values, but sometimes needlessly wastes memory.", "prev_chunk_id": "chunk_1371", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1373", "url": "https://docs.pytorch.org/docs/stable/notes/cuda.html", "title": "Sharing memory across captures#", "page_title": "CUDA semantics — PyTorch 2.8 documentation", "breadcrumbs": "Sharing memory across captures#", "content": "Sharing memory across captures# To economize the memory stashed in private pools, torch.cuda.graph and torch.cuda.make_graphed_callables() optionally allow different captures to share the same private pool. It’s safe for a set of graphs to share a private pool if you know they’ll always be replayed in the same order they were captured, and never be replayed concurrently. torch.cuda.graph’s pool argument is a hint to use a particular private pool, and can be used to share memory across graphs as shown: g1 = torch.cuda.CUDAGraph() g2 = torch.cuda.CUDAGraph() # (create static inputs for g1 and g2, run warmups of their workloads...) # Captures g1 with torch.cuda.graph(g1): static_out_1 = g1_workload(static_in_1) # Captures g2, hinting that g2 may share a memory pool with g1 with torch.cuda.graph(g2, pool=g1.pool()): static_out_2 = g2_workload(static_in_2) static_in_1.copy_(real_data_1) static_in_2.copy_(real_data_2) g1.replay() g2.replay() With torch.cuda.make_graphed_callables(), if you want to graph several callables and you know they’ll always run in the same order (and never concurrently) pass them as a tuple in the same order they’ll run in the live workload, and make_graphed_callables() will capture their graphs using a shared private pool. If, in the live workload, your callables will run in an order that occasionally changes, or if they’ll run concurrently, passing them as a tuple to a single invocation of make_graphed_callables() is not allowed. Instead, you must call make_graphed_callables() separately for each one.", "prev_chunk_id": "chunk_1372", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1374", "url": "https://docs.pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html", "title": "CPU threading and TorchScript inference#", "page_title": "CPU threading and TorchScript inference — PyTorch 2.8 documentation", "breadcrumbs": "CPU threading and TorchScript inference#", "content": "CPU threading and TorchScript inference# Created On: Jul 29, 2019 | Last Updated On: Mar 26, 2020 PyTorch allows using multiple CPU threads during TorchScript model inference. The following figure shows different levels of parallelism one would find in a typical application: One or more inference threads execute a model’s forward pass on the given inputs. Each inference thread invokes a JIT interpreter that executes the ops of a model inline, one by one. A model can utilize a fork TorchScript primitive to launch an asynchronous task. Forking several operations at once results in a task that is executed in parallel. The fork operator returns a Future object which can be used to synchronize on later, for example: @torch.jit.script def compute_z(x): return torch.mm(x, self.w_z) @torch.jit.script def forward(x): # launch compute_z asynchronously: fut = torch.jit._fork(compute_z, x) # execute the next operation in parallel to compute_z: y = torch.mm(x, self.w_y) # wait for the result of compute_z: z = torch.jit._wait(fut) return y + z PyTorch uses a single thread pool for the inter-op parallelism, this thread pool is shared by all inference tasks that are forked within the application process. In addition to the inter-op parallelism, PyTorch can also utilize multiple threads within the ops (intra-op parallelism). This can be useful in many cases, including element-wise ops on large tensors, convolutions, GEMMs, embedding lookups and others.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1375", "url": "https://docs.pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html", "title": "Build options#", "page_title": "CPU threading and TorchScript inference — PyTorch 2.8 documentation", "breadcrumbs": "Build options#", "content": "Build options# PyTorch uses an internal ATen library to implement ops. In addition to that, PyTorch can also be built with support of external libraries, such as MKL and MKL-DNN, to speed up computations on CPU. ATen, MKL and MKL-DNN support intra-op parallelism and depend on the following parallelization libraries to implement it: - OpenMP- a standard (and a library, usually shipped with a compiler), widely used in external libraries; - TBB- a newer parallelization library optimized for task-based parallelism and concurrent environments. OpenMP historically has been used by a large number of libraries. It is known for a relative ease of use and support for loop-based parallelism and other primitives. TBB is used to a lesser extent in external libraries, but, at the same time, is optimized for the concurrent environments. PyTorch’s TBB backend guarantees that there’s a separate, single, per-process intra-op thread pool used by all of the ops running in the application. Depending of the use case, one might find one or another parallelization library a better choice in their application. PyTorch allows selecting of the parallelization backend used by ATen and other libraries at the build time with the following build options: It is recommended not to mix OpenMP and TBB within one build. Any of the TBB values above require USE_TBB=1 build setting (default: OFF). A separate setting USE_OPENMP=1 (default: ON) is required for OpenMP parallelism.", "prev_chunk_id": "chunk_1374", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1376", "url": "https://docs.pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html", "title": "Runtime API#", "page_title": "CPU threading and TorchScript inference — PyTorch 2.8 documentation", "breadcrumbs": "Runtime API#", "content": "Runtime API# The following API is used to control thread settings: For the intra-op parallelism settings, at::set_num_threads, torch.set_num_threads always take precedence over environment variables, MKL_NUM_THREADS variable takes precedence over OMP_NUM_THREADS.", "prev_chunk_id": "chunk_1375", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1377", "url": "https://docs.pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html", "title": "Tuning the number of threads#", "page_title": "CPU threading and TorchScript inference — PyTorch 2.8 documentation", "breadcrumbs": "Tuning the number of threads#", "content": "Tuning the number of threads# The following simple script shows how a runtime of matrix multiplication changes with the number of threads: import timeit runtimes = [] threads = [1] + [t for t in range(2, 49, 2)] for t in threads: torch.set_num_threads(t) r = timeit.timeit(setup = \"import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)\", stmt=\"torch.mm(x, y)\", number=100) runtimes.append(r) # ... plotting (threads, runtimes) ... Running the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes: The following considerations should be taken into account when tuning the number of intra- and inter-op threads: - When choosing the number of threads one needs to avoidoversubscription(using too many threads, leads to performance degradation). For example, in an application that uses a large application thread pool or heavily relies on inter-op parallelism, one might find disabling intra-op parallelism as a possible option (i.e. by callingset_num_threads(1)); - In a typical application one might encounter a trade off betweenlatency(time spent on processing an inference request) andthroughput(amount of work done per unit of time). Tuning the number of threads can be a useful tool to adjust this trade off in one way or another. For example, in latency critical applications one might want to increase the number of intra-op threads to process each request as fast as possible. At the same time, parallel implementations of ops may add an extra overhead that increases amount work done per single request and thus reduces the overall throughput.", "prev_chunk_id": "chunk_1376", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1378", "url": "https://docs.pytorch.org/docs/stable/notes/broadcasting.html", "title": "Broadcasting semantics#", "page_title": "Broadcasting semantics — PyTorch 2.8 documentation", "breadcrumbs": "Broadcasting semantics#", "content": "Broadcasting semantics# Created On: Apr 27, 2017 | Last Updated On: Jan 31, 2021 Many PyTorch operations support NumPy’s broadcasting semantics. See https://numpy.org/doc/stable/user/basics.broadcasting.html for details. In short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1379", "url": "https://docs.pytorch.org/docs/stable/notes/broadcasting.html", "title": "General semantics#", "page_title": "Broadcasting semantics — PyTorch 2.8 documentation", "breadcrumbs": "General semantics#", "content": "General semantics# Two tensors are “broadcastable” if the following rules hold: - Each tensor has at least one dimension. - When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist. For Example: >>> x=torch.empty(5,7,3) >>> y=torch.empty(5,7,3) # same shapes are always broadcastable (i.e. the above rules always hold) >>> x=torch.empty((0,)) >>> y=torch.empty(2,2) # x and y are not broadcastable, because x does not have at least 1 dimension # can line up trailing dimensions >>> x=torch.empty(5,3,4,1) >>> y=torch.empty( 3,1,1) # x and y are broadcastable. # 1st trailing dimension: both have size 1 # 2nd trailing dimension: y has size 1 # 3rd trailing dimension: x size == y size # 4th trailing dimension: y dimension doesn't exist # but: >>> x=torch.empty(5,2,4,1) >>> y=torch.empty( 3,1,1) # x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3 If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows: - If the number of dimensions ofxandyare not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length. - Then, for each dimension size, the resulting dimension size is the max of the sizes ofxandyalong that dimension. For Example: # can line up trailing dimensions to make reading easier >>> x=torch.empty(5,1,4,1) >>> y=torch.empty( 3,1,1) >>> (x+y).size() torch.Size([5, 3, 4, 1]) # but not necessary: >>> x=torch.empty(1) >>> y=torch.empty(3,1,7) >>> (x+y).size() torch.Size([3, 1, 7]) >>> x=torch.empty(5,2,4,1) >>> y=torch.empty(3,1,1) >>> (x+y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1", "prev_chunk_id": "chunk_1378", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1380", "url": "https://docs.pytorch.org/docs/stable/notes/broadcasting.html", "title": "In-place semantics#", "page_title": "Broadcasting semantics — PyTorch 2.8 documentation", "breadcrumbs": "In-place semantics#", "content": "In-place semantics# One complication is that in-place operations do not allow the in-place tensor to change shape as a result of the broadcast. For Example: >>> x=torch.empty(5,3,4,1) >>> y=torch.empty(3,1,1) >>> (x.add_(y)).size() torch.Size([5, 3, 4, 1]) # but: >>> x=torch.empty(1,3,1) >>> y=torch.empty(3,1,7) >>> (x.add_(y)).size() RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.", "prev_chunk_id": "chunk_1379", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1381", "url": "https://docs.pytorch.org/docs/stable/notes/broadcasting.html", "title": "Backwards compatibility#", "page_title": "Broadcasting semantics — PyTorch 2.8 documentation", "breadcrumbs": "Backwards compatibility#", "content": "Backwards compatibility# Prior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal. The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements. Note that the introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape, but are broadcastable and have the same number of elements. For Example: >>> torch.add(torch.ones(4,1), torch.randn(4)) would previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]). In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set torch.utils.backcompat.broadcast_warning.enabled to True, which will generate a python warning in such cases. For Example: >>> torch.utils.backcompat.broadcast_warning.enabled=True >>> torch.add(torch.ones(4,1), torch.ones(4)) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.", "prev_chunk_id": "chunk_1380", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1382", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Autograd mechanics#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Autograd mechanics#", "content": "Autograd mechanics# Created On: Jan 16, 2017 | Last Updated On: Jun 16, 2025 This note will present an overview of how autograd works and records the operations. It’s not strictly necessary to understand all this, but we recommend getting familiar with it, as it will help you write more efficient, cleaner programs, and can aid you in debugging.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1383", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "How autograd encodes the history#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "How autograd encodes the history#", "content": "How autograd encodes the history# Autograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule. Internally, autograd represents this graph as a graph of Function objects (really expressions), which can be apply() ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forward pass is completed, we evaluate this graph in the backwards pass to compute the gradients. An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.", "prev_chunk_id": "chunk_1382", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1384", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Saved tensors#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Saved tensors#", "content": "Saved tensors# Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. For example, the function x↦x2x\\mapsto x^2x↦x2 saves the input xxx to compute the gradient. When defining a custom Python Function, you can use save_for_backward() to save tensors during the forward pass and saved_tensors to retrieve them during the backward pass. See Extending PyTorch for more information. For operations that PyTorch defines (e.g. torch.pow()), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain grad_fn by looking for its attributes starting with the prefix _saved. x = torch.randn(5, requires_grad=True) y = x.pow(2) print(x.equal(y.grad_fn._saved_self)) # True print(x is y.grad_fn._saved_self) # True In the previous code, y.grad_fn._saved_self refers to the same Tensor object as x. But that may not always be the case. For instance: x = torch.randn(5, requires_grad=True) y = x.exp() print(y.equal(y.grad_fn._saved_result)) # True print(y is y.grad_fn._saved_result) # False Under the hood, to prevent reference cycles, PyTorch has packed the tensor upon saving and unpacked it into a different tensor for reading. Here, the tensor you get from accessing y.grad_fn._saved_result is a different tensor object than y (but they still share the same storage). Whether a tensor will be packed into a different tensor object depends on whether it is an output of its own grad_fn, which is an implementation detail subject to change and that users should not rely on. You can control how PyTorch does packing / unpacking with Hooks for saved tensors.", "prev_chunk_id": "chunk_1383", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1385", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Gradients for non-differentiable functions#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Gradients for non-differentiable functions#", "content": "Gradients for non-differentiable functions# The gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable. Unfortunately many of the functions we use in practice do not have this property (relu or sqrt at 0, for example). To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order: - If the function is differentiable and thus a gradient exists at the current point, use it. - If the function is convex (at least locally), use the sub-gradient of minimum norm. - If the function is concave (at least locally), use the super-gradient of minimum norm (consider-f(x)and apply the previous point). - If the function is defined, define the gradient at the current point by continuity (note thatinfis possible here, for example forsqrt(0)). If multiple values are possible, pick one arbitrarily. - If the function is not defined (sqrt(-1),log(-1)or most functions when the input isNaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will useNaNas the gradient, but for performance reasons, some functions will use other values (log(-1), for example). - If the function is not a deterministic mapping (i.e. it is not amathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of ano_gradenvironment.", "prev_chunk_id": "chunk_1384", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1386", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Division by Zero in Autograd#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Division by Zero in Autograd#", "content": "Division by Zero in Autograd# When performing division by zero in PyTorch (e.g., x / 0), the forward pass will produce inf values following IEEE-754 floating point arithmetic. While these inf values can be masked out before computing the final loss (e.g., via indexing or masking), the autograd system still tracks and differentiates through the full computation graph, including the division by zero operation. During backpropagation, this can lead to problematic gradient expressions. For example: x = torch.tensor([1., 1.], requires_grad=True) div = torch.tensor([0., 1.]) y = x / div # Results in [inf, 1] mask = div != 0 # [False, True] loss = y[mask].sum() loss.backward() print(x.grad) # [nan, 1], not [0, 1] In this example, even though we only use the masked output (which excludes the division by zero), autograd still computes gradients through the full computation graph, including the division by zero operation. This results in nan gradients for the masked elements, which can cause training instability. To avoid this issue, there are several recommended approaches: - Mask before division: x = torch.tensor([1., 1.], requires_grad=True) div = torch.tensor([0., 1.]) mask = div != 0 safe = torch.zeros_like(x) safe[mask] = x[mask] / div[mask] loss = safe.sum() loss.backward() # Produces safe gradients [0, 1] - Use MaskedTensor (experimental API): from torch.masked import as_masked_tensor x = torch.tensor([1., 1.], requires_grad=True) div = torch.tensor([0., 1.]) y = x / div mask = div != 0 loss = as_masked_tensor(y, mask).sum() loss.backward() # Cleanly handles \"undefined\" vs \"zero\" gradients The key principle is to prevent the division by zero operation from being recorded in the computation graph, rather than masking its results after the fact. This ensures that autograd only computes gradients through valid operations. This behavior is important to keep in mind when working with operations that might produce inf or nan values, as", "prev_chunk_id": "chunk_1385", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1387", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Division by Zero in Autograd#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Division by Zero in Autograd#", "content": "masking the outputs does not prevent the problematic gradients from being computed.", "prev_chunk_id": "chunk_1386", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1388", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Locally disabling gradient computation#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Locally disabling gradient computation#", "content": "Locally disabling gradient computation# There are several mechanisms available from Python to locally disable gradient computation: To disable gradients across entire blocks of code, there are context managers like no-grad mode and inference mode. For more fine-grained exclusion of subgraphs from gradient computation, there is setting the requires_grad field of a tensor. Below, in addition to discussing the mechanisms above, we also describe evaluation mode (nn.Module.eval()), a method that is not used to disable gradient computation but, because of its name, is often mixed up with the three.", "prev_chunk_id": "chunk_1387", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1389", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Setting requires_grad#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Setting requires_grad#", "content": "Setting requires_grad# requires_grad is a flag, defaulting to false unless wrapped in a nn.Parameter, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes: During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass (.backward()), only leaf tensors with requires_grad=True will have gradients accumulated into their .grad fields. It is important to note that even though every tensor has this flag, setting it only makes sense for leaf tensors (tensors that do not have a grad_fn, e.g., a nn.Module’s parameters). Non-leaf tensors (tensors that do have grad_fn) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. From this definition, it is clear that all non-leaf tensors will automatically have require_grad=True. Setting requires_grad should be the main way you control which parts of the model are part of the gradient computation, for example, if you need to freeze parts of your pretrained model during model fine-tuning. To freeze parts of your model, simply apply .requires_grad_(False) to the parameters that you don’t want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won’t have their .grad fields updated in the backward pass because they won’t be part of the backward graph in the first place, as desired. Because this is such a common pattern, requires_grad can also be set at the module level with nn.Module.requires_grad_(). When applied to a module, .requires_grad_() takes effect on all of the module’s parameters (which have requires_grad=True by default).", "prev_chunk_id": "chunk_1388", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1390", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Grad Modes#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Grad Modes#", "content": "Grad Modes# Apart from setting requires_grad there are also three grad modes that can be selected from Python that can affect how computations in PyTorch are processed by autograd internally: default mode (grad mode), no-grad mode, and inference mode, all of which can be togglable via context managers and decorators.", "prev_chunk_id": "chunk_1389", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1391", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Default Mode (Grad Mode)#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Default Mode (Grad Mode)#", "content": "Default Mode (Grad Mode)# The “default mode” is the mode we are implicitly in when no other modes like no-grad and inference mode are enabled. To be contrasted with “no-grad mode” the default mode is also sometimes called “grad mode”. The most important thing to know about the default mode is that it is the only mode in which requires_grad takes effect. requires_grad is always overridden to be False in both the two other modes.", "prev_chunk_id": "chunk_1390", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1392", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "No-grad Mode#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "No-grad Mode#", "content": "No-grad Mode# Computations in no-grad mode behave as if none of the inputs require grad. In other words, computations in no-grad mode are never recorded in the backward graph even if there are inputs that have require_grad=True. Enable no-grad mode when you need to perform operations that should not be recorded by autograd, but you’d still like to use the outputs of these computations in grad mode later. This context manager makes it convenient to disable gradients for a block of code or function without having to temporarily set tensors to have requires_grad=False, and then back to True. For example, no-grad mode might be useful when writing an optimizer: when performing the training update you’d like to update parameters in-place without the update being recorded by autograd. You also intend to use the updated parameters for computations in grad mode in the next forward pass. The implementations in torch.nn.init also rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the initialized parameters in-place.", "prev_chunk_id": "chunk_1391", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1393", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Inference Mode#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Inference Mode#", "content": "Inference Mode# Inference mode is the extreme version of no-grad mode. Just like in no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more. This better runtime comes with a drawback: tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode. Enable inference mode when you are performing computations that do not have interactions with autograd, AND you don’t plan on using the tensors created in inference mode in any computation that is to be recorded by autograd later. It is recommended that you try out inference mode in the parts of your code that do not require autograd tracking (e.g., data processing and model evaluation). If it works out of the box for your use case it’s a free performance win. If you run into errors after enabling inference mode, check that you are not using tensors created in inference mode in computations that are recorded by autograd after exiting inference mode. If you cannot avoid such use in your case, you can always switch back to no-grad mode. For details on inference mode please see Inference Mode. For implementation details of inference mode see RFC-0011-InferenceMode.", "prev_chunk_id": "chunk_1392", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1394", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Evaluation Mode (nn.Module.eval())#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Evaluation Mode (nn.Module.eval())#", "content": "Evaluation Mode (nn.Module.eval())# Evaluation mode is not a mechanism to locally disable gradient computation. It is included here anyway because it is sometimes confused to be such a mechanism. Functionally, module.eval() (or equivalently module.train(False)) are completely orthogonal to no-grad mode and inference mode. How model.eval() affects your model depends entirely on the specific modules used in your model and whether they define any training-mode specific behavior. You are responsible for calling model.eval() and model.train() if your model relies on modules such as torch.nn.Dropout and torch.nn.BatchNorm2d that may behave differently depending on training mode, for example, to avoid updating your BatchNorm running statistics on validation data. It is recommended that you always use model.train() when training and model.eval() when evaluating your model (validation/testing) even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes.", "prev_chunk_id": "chunk_1393", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1395", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "In-place operations with autograd#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "In-place operations with autograd#", "content": "In-place operations with autograd# Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them. There are two main reasons that limit the applicability of in-place operations: - In-place operations can potentially overwrite values required to compute gradients. - Every in-place operation requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to theFunctionrepresenting this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will raise an error if the storage of modified inputs is referenced by any otherTensor.", "prev_chunk_id": "chunk_1394", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1396", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "In-place correctness checks#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "In-place correctness checks#", "content": "In-place correctness checks# Every tensor keeps a version counter, that is incremented every time it is marked dirty in any operation. When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access self.saved_tensors it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.", "prev_chunk_id": "chunk_1395", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1397", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Multithreaded Autograd#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Multithreaded Autograd#", "content": "Multithreaded Autograd# The autograd engine is responsible for running all the backward operations necessary to compute the backward pass. This section will describe all the details that can help you make the best use of it in a multithreaded environment. (This is relevant only for PyTorch 1.6+ as the behavior in previous version was different.) User could train their model with multithreading code (e.g. Hogwild training), and does not block on the concurrent backward computations, example code could be: # Define a train function to be used in different threads def train_fn(): x = torch.ones(5, 5, requires_grad=True) # forward y = (x + 3) * (x + 4) * 0.5 # backward y.sum().backward() # potential optimizer update # User write their own threading code to drive the train_fn threads = [] for _ in range(10): p = threading.Thread(target=train_fn, args=()) p.start() threads.append(p) for p in threads: p.join() Note that some behaviors that user should be aware of:", "prev_chunk_id": "chunk_1396", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1398", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Concurrency on CPU#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Concurrency on CPU#", "content": "Concurrency on CPU# When you run backward() or grad() via python or C++ API in multiple threads on CPU, you are expecting to see extra concurrency instead of serializing all the backward calls in a specific order during execution (behavior before PyTorch 1.6).", "prev_chunk_id": "chunk_1397", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1399", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Non-determinism#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Non-determinism#", "content": "Non-determinism# If you are calling backward() from multiple threads concurrently and have shared inputs (i.e. Hogwild CPU training), then non-determinism should be expected. This can occur because parameters are automatically shared across threads, as such, multiple threads may access and try to accumulate the same .grad attribute during gradient accumulation. This is technically not safe, and it might result in race condition and the result might be invalid to use. Users developing multithreaded models featuring shared parameters should have the threading model in mind and should understand the issues described above. The functional API torch.autograd.grad() may be used to calculate the gradients instead of backward() to avoid non-determinism.", "prev_chunk_id": "chunk_1398", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1400", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Graph retaining#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Graph retaining#", "content": "Graph retaining# If part of the autograd graph is shared between threads, i.e. run first part of forward single thread, then run second part in multiple threads, then the first part of graph is shared. In this case different threads execute grad() or backward() on the same graph might have issue of destroying the graph on the fly of one thread, and the other thread will crash in this case. Autograd will error out to the user similar to what call backward() twice with out retain_graph=True, and let the user know they should use retain_graph=True.", "prev_chunk_id": "chunk_1399", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1401", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Thread Safety on Autograd Node#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Thread Safety on Autograd Node#", "content": "Thread Safety on Autograd Node# Since Autograd allows the caller thread to drive its backward execution for potential parallelism, it’s important that we ensure thread safety on CPU with parallel backward() calls that share part/whole of the GraphTask. Custom Python autograd.Functions are automatically thread safe because of GIL. For built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom autograd::Functions, the Autograd Engine uses thread mutex locking to ensure thread safety on autograd Nodes that might have state write/read.", "prev_chunk_id": "chunk_1400", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1402", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "No thread safety on C++ hooks#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "No thread safety on C++ hooks#", "content": "No thread safety on C++ hooks# Autograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe.", "prev_chunk_id": "chunk_1401", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1403", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Autograd for Complex Numbers#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Autograd for Complex Numbers#", "content": "Autograd for Complex Numbers# The short version: - When you use PyTorch to differentiate any functionf(z)f(z)f(z)with complex domain and/or codomain, the gradients are computed under the assumption that the function is a part of a larger real-valued loss functiong(input)=Lg(input)=Lg(input)=L. The gradient computed is∂L∂z∗\\frac{\\partial L}{\\partial z^*}∂z∗∂L​(note the conjugation of z), the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers work out of the box with complex parameters. - This convention matches TensorFlow’s convention for complex differentiation, but is different from JAX (which computes∂L∂z\\frac{\\partial L}{\\partial z}∂z∂L​). - If you have a real-to-real function which internally uses complex operations, the convention here doesn’t matter: you will always get the same result that you would have gotten if it had been implemented with only real operations. If you are curious about the mathematical details, or want to know how to define complex derivatives in PyTorch, read on.", "prev_chunk_id": "chunk_1402", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1404", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "What are complex derivatives?#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "What are complex derivatives?#", "content": "What are complex derivatives?# The mathematical definition of complex-differentiability takes the limit definition of a derivative and generalizes it to operate on complex numbers. Consider a function f:C→Cf: ℂ → ℂf:C→C, where uuu and vvv are two variable real valued functions and jjj is the imaginary unit. Using the derivative definition, we can write: In order for this limit to exist, not only must uuu and vvv must be real differentiable, but fff must also satisfy the Cauchy-Riemann equations. In other words: the limit computed for real and imaginary steps (hhh) must be equal. This is a more restrictive condition. The complex differentiable functions are commonly known as holomorphic functions. They are well behaved, have all the nice properties that you’ve seen from real differentiable functions, but are practically of no use in the optimization world. For optimization problems, only real valued objective functions are used in the research community since complex numbers are not part of any ordered field and so having complex valued loss does not make much sense. It also turns out that no interesting real-valued objective fulfill the Cauchy-Riemann equations. So the theory with holomorphic function cannot be used for optimization and most people therefore use the Wirtinger calculus.", "prev_chunk_id": "chunk_1403", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1405", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Wirtinger Calculus comes into the picture …#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Wirtinger Calculus comes into the picture …#", "content": "Wirtinger Calculus comes into the picture …# So, we have this great theory of complex differentiability and holomorphic functions, and we can’t use any of it at all, because many of the commonly used functions are not holomorphic. What’s a poor mathematician to do? Well, Wirtinger observed that even if f(z)f(z)f(z) isn’t holomorphic, one could rewrite it as a two variable function f(z,z∗)f(z, z*)f(z,z∗) which is always holomorphic. This is because real and imaginary of the components of zzz can be expressed in terms of zzz and z∗z^*z∗ as: Wirtinger calculus suggests to study f(z,z∗)f(z, z^*)f(z,z∗) instead, which is guaranteed to be holomorphic if fff was real differentiable (another way to think of it is as a change of coordinate system, from f(x,y)f(x, y)f(x,y) to f(z,z∗)f(z, z^*)f(z,z∗).) This function has partial derivatives ∂∂z\\frac{\\partial }{\\partial z}∂z∂​ and ∂∂z∗\\frac{\\partial}{\\partial z^{*}}∂z∗∂​. We can use the chain rule to establish a relationship between these partial derivatives and the partial derivatives w.r.t., the real and imaginary components of zzz. From the above equations, we get: which is the classic definition of Wirtinger calculus that you would find on Wikipedia. There are a lot of beautiful consequences of this change. - For one, the Cauchy-Riemann equations translate into simply saying that∂f∂z∗=0\\frac{\\partial f}{\\partial z^*} = 0∂z∗∂f​=0(that is to say, the functionfffcan be written entirely in terms ofzzz, without making reference toz∗z^*z∗). - Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should take while making variable update is given by∂Loss∂z∗\\frac{\\partial Loss}{\\partial z^*}∂z∗∂Loss​(not∂Loss∂z\\frac{\\partial Loss}{\\partial z}∂z∂Loss​). For more reading, check out: https://arxiv.org/pdf/0906.4835.pdf", "prev_chunk_id": "chunk_1404", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1406", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "How is Wirtinger Calculus useful in optimization?#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "How is Wirtinger Calculus useful in optimization?#", "content": "How is Wirtinger Calculus useful in optimization?# Researchers in audio and other fields, more commonly, use gradient descent to optimize real valued loss functions with complex variables. Typically, these people treat the real and imaginary values as separate channels that can be updated. For a step size α/2\\alpha/2α/2 and loss LLL, we can write the following equations in R2ℝ^2R2: How do these equations translate into complex space CℂC? Something very interesting has happened: Wirtinger calculus tells us that we can simplify the complex variable update formula above to only refer to the conjugate Wirtinger derivative ∂L∂z∗\\frac{\\partial L}{\\partial z^*}∂z∗∂L​, giving us exactly the step we take in optimization. Because the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative when you differentiate a function with a real valued loss.", "prev_chunk_id": "chunk_1405", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1407", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "How does PyTorch compute the conjugate Wirtinger derivative?#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "How does PyTorch compute the conjugate Wirtinger derivative?#", "content": "How does PyTorch compute the conjugate Wirtinger derivative?# Typically, our derivative formulas take in grad_output as an input, representing the incoming Vector-Jacobian product that we’ve already computed, aka, ∂L∂s∗\\frac{\\partial L}{\\partial s^*}∂s∗∂L​, where LLL is the loss of the entire computation (producing a real loss) and sss is the output of our function. The goal here is to compute ∂L∂z∗\\frac{\\partial L}{\\partial z^*}∂z∗∂L​, where zzz is the input of the function. It turns out that in the case of real loss, we can get away with only calculating ∂L∂s∗\\frac{\\partial L}{\\partial s^*}∂s∗∂L​, even though the chain rule implies that we also need to have access to ∂L∂s\\frac{\\partial L}{\\partial s}∂s∂L​. If you want to skip this derivation, look at the last equation in this section and then skip to the next section. Let’s continue working with f:C→Cf: ℂ → ℂf:C→C defined as f(z)=f(x+yj)=u(x,y)+v(x,y)jf(z) = f(x+yj) = u(x, y) + v(x, y)jf(z)=f(x+yj)=u(x,y)+v(x,y)j. As discussed above, autograd’s gradient convention is centered around optimization for real valued loss functions, so let’s assume fff is a part of larger real valued loss function ggg. Using chain rule, we can write: Now using Wirtinger derivative definition, we can write: It should be noted here that since uuu and vvv are real functions, and LLL is real by our assumption that fff is a part of a real valued function, we have: i.e., ∂L∂s\\frac{\\partial L}{\\partial s}∂s∂L​ equals to grad_output∗grad\\_output^*grad_output∗. Solving the above equations for ∂L∂u\\frac{\\partial L}{\\partial u}∂u∂L​ and ∂L∂v\\frac{\\partial L}{\\partial v}∂v∂L​, we get: Substituting (3) in (1), we get: Using (2), we get: This last equation is the important one for writing your own gradients, as it decomposes our derivative formula into a simpler one that is easy to compute by hand.", "prev_chunk_id": "chunk_1406", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1408", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "How can I write my own derivative formula for a complex function?#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "How can I write my own derivative formula for a complex function?#", "content": "How can I write my own derivative formula for a complex function?# The above boxed equation gives us the general formula for all derivatives on complex functions. However, we still need to compute ∂s∂z\\frac{\\partial s}{\\partial z}∂z∂s​ and ∂s∂z∗\\frac{\\partial s}{\\partial z^*}∂z∗∂s​. There are two ways you could do this: Let’s consider the function f(z=x+yj)=c∗z=c∗(x+yj)f(z = x + yj) = c * z = c * (x+yj)f(z=x+yj)=c∗z=c∗(x+yj) as an example, where c∈Rc \\in ℝc∈R. Using the first way to compute the Wirtinger derivatives, we have. Using (4), and grad_output = 1.0 (which is the default grad output value used when backward() is called on a scalar output in PyTorch), we get: Using the second way to compute Wirtinger derivatives, we directly get: And using (4) again, we get ∂L∂z∗=c\\frac{\\partial L}{\\partial z^*} = c∂z∗∂L​=c. As you can see, the second way involves lesser calculations, and comes in more handy for faster calculations.", "prev_chunk_id": "chunk_1407", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1409", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "What about cross-domain functions?#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "What about cross-domain functions?#", "content": "What about cross-domain functions?# Some functions map from complex inputs to real outputs, or vice versa. These functions form a special case of (4), which we can derive using the chain rule:", "prev_chunk_id": "chunk_1408", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1410", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Hooks for saved tensors#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Hooks for saved tensors#", "content": "Hooks for saved tensors# You can control how saved tensors are packed / unpacked by defining a pair of pack_hook / unpack_hook hooks. The pack_hook function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). The unpack_hook function takes as its single argument the output of pack_hook and should return a tensor to be used in the backward pass. The tensor returned by unpack_hook only needs to have the same content as the tensor passed as input to pack_hook. In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking. An example of such pair is: class SelfDeletingTempFile(): def __init__(self): self.name = os.path.join(tmp_dir, str(uuid.uuid4())) def __del__(self): os.remove(self.name) def pack_hook(tensor): temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file def unpack_hook(temp_file): return torch.load(temp_file.name) Notice that the unpack_hook should not delete the temporary file because it might be called multiple times: the temporary file should be alive for as long as the returned SelfDeletingTempFile object is alive. In the above example, we prevent leaking the temporary file by closing it when it is no longer needed (on deletion of the SelfDeletingTempFile object).", "prev_chunk_id": "chunk_1409", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1411", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Registering hooks for a saved tensor#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Registering hooks for a saved tensor#", "content": "Registering hooks for a saved tensor# You can register a pair of hooks on a saved tensor by calling the register_hooks() method on a SavedTensor object. Those objects are exposed as attributes of a grad_fn and start with the _raw_saved_ prefix. x = torch.randn(5, requires_grad=True) y = x.pow(2) y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook) The pack_hook method is called as soon as the pair is registered. The unpack_hook method is called each time the saved tensor needs to be accessed, either by means of y.grad_fn._saved_self or during the backward pass.", "prev_chunk_id": "chunk_1410", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1412", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Registering default hooks for saved tensors#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Registering default hooks for saved tensors#", "content": "Registering default hooks for saved tensors# Alternatively, you can use the context-manager saved_tensors_hooks to register a pair of hooks which will be applied to all saved tensors that are created in that context. Example: # Only save on disk tensors that have size >= 1000 SAVE_ON_DISK_THRESHOLD = 1000 def pack_hook(x): if x.numel() < SAVE_ON_DISK_THRESHOLD: return x.detach() temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file def unpack_hook(tensor_or_sctf): if isinstance(tensor_or_sctf, torch.Tensor): return tensor_or_sctf return torch.load(tensor_or_sctf.name) class Model(nn.Module): def forward(self, x): with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook): # ... compute output output = x return output model = Model() net = nn.DataParallel(model) The hooks defined with this context manager are thread-local. Hence, the following code will not produce the desired effects because the hooks do not go through DataParallel. # Example what NOT to do net = nn.DataParallel(model) with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook): output = net(input) Note that using those hooks disables all the optimization in place to reduce Tensor object creation. For example: with torch.autograd.graph.saved_tensors_hooks(lambda x: x.detach(), lambda x: x): x = torch.randn(5, requires_grad=True) y = x * x Without the hooks, x, y.grad_fn._saved_self and y.grad_fn._saved_other all refer to the same tensor object. With the hooks, PyTorch will pack and unpack x into two new tensor objects that share the same storage with the original x (no copy performed).", "prev_chunk_id": "chunk_1411", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1413", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Backward Hooks execution#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Backward Hooks execution#", "content": "Backward Hooks execution# This section will discuss when different hooks fire or don’t fire. Then it will discuss the order in which they are fired. The hooks that will be covered are: backward hooks registered to Tensor via torch.Tensor.register_hook(), post-accumulate-grad hooks registered to Tensor via torch.Tensor.register_post_accumulate_grad_hook(), post-hooks registered to Node via torch.autograd.graph.Node.register_hook(), and pre-hooks registered to Node via torch.autograd.graph.Node.register_prehook().", "prev_chunk_id": "chunk_1412", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1414", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Whether a particular hook will be fired#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Whether a particular hook will be fired#", "content": "Whether a particular hook will be fired# Hooks registered to a Tensor via torch.Tensor.register_hook() are executed when gradients are being computed for that Tensor. (Note that this does not require the Tensor’s grad_fn to be executed. For example, if the Tensor is passed as part of the inputs argument to torch.autograd.grad(), the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.) Hooks registered to a Tensor via torch.Tensor.register_post_accumulate_grad_hook() are executed after the gradients have been accumulated for that Tensor, meaning the Tensor’s grad field has been set. Whereas hooks registered via torch.Tensor.register_hook() are run as gradients are being computed, hooks registered via torch.Tensor.register_post_accumulate_grad_hook() are only triggered once the Tensor’s grad field is updated by autograd at the end of the backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf Tensors. Registering a hook via torch.Tensor.register_post_accumulate_grad_hook() on a non-leaf Tensor will error, even if you call backward(retain_graph=True). Hooks registered to torch.autograd.graph.Node using torch.autograd.graph.Node.register_hook() or torch.autograd.graph.Node.register_prehook() are only fired if the Node it was registered to is executed. Whether a particular Node is executed may depend on whether the backward pass was called with torch.autograd.grad() or torch.autograd.backward(). Specifically, you should be aware of these differences when you register a hook on a Node corresponding to a Tensor that you are passing to torch.autograd.grad() or torch.autograd.backward() as part of the inputs argument. If you are using torch.autograd.backward(), all of the above mentioned hooks will be executed, whether or not you specified the inputs argument. This is because .backward() executes all Nodes, even if they correspond to a Tensor specified as an input. (Note that the execution of this additional Node corresponding to Tensors passed as inputs is usually unnecessary, but done anyway. This behavior is subject to change; you should not depend on", "prev_chunk_id": "chunk_1413", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1415", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Whether a particular hook will be fired#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Whether a particular hook will be fired#", "content": "it.) On the other hand, if you are using torch.autograd.grad(), the backward hooks registered to Nodes that correspond to the Tensors passed to input may not be executed, because those Nodes will not be executed unless there is another input that depends on the gradient result of this Node.", "prev_chunk_id": "chunk_1414", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1416", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "The order in which the different hooks are fired#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "The order in which the different hooks are fired#", "content": "The order in which the different hooks are fired# The order in which things happen are: - hooks registered to Tensor are executed - pre-hooks registered to Node are executed (if Node is executed). - the.gradfield is updated for Tensors that retain_grad - Node is executed (subject to rules above) - for leaf Tensors that have.gradaccumulated, post-accumulate-grad hooks are executed - post-hooks registered to Node are executed (if Node is executed) If multiple hooks of the same type are registered on the same Tensor or Node they are executed in the order in which they are registered. Hooks that are executed later can observe the modifications to the gradient made by earlier hooks.", "prev_chunk_id": "chunk_1415", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1417", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Special hooks#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Special hooks#", "content": "Special hooks# torch.autograd.graph.register_multi_grad_hook() is implemented using hooks registered to Tensors. Each individual Tensor hook is fired following the Tensor hook ordering defined above and the registered multi-grad hook is called when the last Tensor gradient is computed. torch.nn.modules.module.register_module_full_backward_hook() is implemented using hooks registered to Node. As the forward is computed, hooks are registered to grad_fn corresponding to the inputs and outputs of the module. Because a module may take multiple inputs and return multiple outputs, a dummy custom autograd Function is first applied to the inputs of the module before forward and the outputs of the module before the output of forward is returned to ensure that those Tensors share a single grad_fn, which we can then attach our hooks to.", "prev_chunk_id": "chunk_1416", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1418", "url": "https://docs.pytorch.org/docs/stable/notes/autograd.html", "title": "Behavior of Tensor hooks when Tensor is modified in-place#", "page_title": "Autograd mechanics — PyTorch 2.8 documentation", "breadcrumbs": "Behavior of Tensor hooks when Tensor is modified in-place#", "content": "Behavior of Tensor hooks when Tensor is modified in-place# Usually hooks registered to a Tensor receive the gradient of the outputs with respect to that Tensor, where the value of the Tensor is taken to be its value at the time backward is computed. However, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks registered before in-place modification similarly receive gradients of the outputs with respect to the Tensor, but the value of the Tensor is taken to be its value before in-place modification. If you prefer the behavior in the former case, you should register them to the Tensor after all in-place modifications to it have been made. For example: t = torch.tensor(1., requires_grad=True).sin() t.cos_() t.register_hook(fn) t.backward() Furthermore, it can be helpful to know that under the hood, when hooks are registered to a Tensor, they actually become permanently bound to the grad_fn of that Tensor, so if that Tensor is then modified in-place, even though the Tensor now has a new grad_fn, hooks registered before it was modified in-place will continue to be associated with the old grad_fn, e.g. they will fire when that Tensor’s old grad_fn is reached in the graph by the autograd engine.", "prev_chunk_id": "chunk_1417", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1419", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Automatic Mixed Precision examples#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Automatic Mixed Precision examples#", "content": "Automatic Mixed Precision examples# Created On: Feb 13, 2020 | Last Updated On: Sep 13, 2024 Ordinarily, “automatic mixed precision training” means training with torch.autocast and torch.amp.GradScaler together. Instances of torch.autocast enable autocasting for chosen regions. Autocasting automatically chooses the precision for operations to improve performance while maintaining accuracy. Instances of torch.amp.GradScaler help perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with float16 (by default on CUDA and XPU) gradients by minimizing gradient underflow, as explained here. torch.autocast and torch.amp.GradScaler are modular. In the samples below, each is used as its individual documentation suggests. (Samples here are illustrative. See the Automatic Mixed Precision recipe for a runnable walkthrough.)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1420", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Typical Mixed Precision Training#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Typical Mixed Precision Training#", "content": "Typical Mixed Precision Training# # Creates model and optimizer in default precision model = Net().cuda() optimizer = optim.SGD(model.parameters(), ...) # Creates a GradScaler once at the beginning of training. scaler = GradScaler() for epoch in epochs: for input, target in data: optimizer.zero_grad() # Runs the forward pass with autocasting. with autocast(device_type='cuda', dtype=torch.float16): output = model(input) loss = loss_fn(output, target) # Scales loss. Calls backward() on scaled loss to create scaled gradients. # Backward passes under autocast are not recommended. # Backward ops run in the same dtype autocast chose for corresponding forward ops. scaler.scale(loss).backward() # scaler.step() first unscales the gradients of the optimizer's assigned params. # If these gradients do not contain infs or NaNs, optimizer.step() is then called, # otherwise, optimizer.step() is skipped. scaler.step(optimizer) # Updates the scale for next iteration. scaler.update()", "prev_chunk_id": "chunk_1419", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1421", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Working with Unscaled Gradients#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Working with Unscaled Gradients#", "content": "Working with Unscaled Gradients# All gradients produced by scaler.scale(loss).backward() are scaled. If you wish to modify or inspect the parameters’ .grad attributes between backward() and scaler.step(optimizer), you should unscale them first. For example, gradient clipping manipulates a set of gradients such that their global norm (see torch.nn.utils.clip_grad_norm_()) or maximum magnitude (see torch.nn.utils.clip_grad_value_()) is <=<=<= some user-imposed threshold. If you attempted to clip without unscaling, the gradients’ norm/maximum magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for unscaled gradients) would be invalid. scaler.unscale_(optimizer) unscales gradients held by optimizer’s assigned parameters. If your model or models contain other parameters that were assigned to another optimizer (say optimizer2), you may call scaler.unscale_(optimizer2) separately to unscale those parameters’ gradients as well.", "prev_chunk_id": "chunk_1420", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1422", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Gradient clipping#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Gradient clipping#", "content": "Gradient clipping# Calling scaler.unscale_(optimizer) before clipping enables you to clip unscaled gradients as usual: scaler = GradScaler() for epoch in epochs: for input, target in data: optimizer.zero_grad() with autocast(device_type='cuda', dtype=torch.float16): output = model(input) loss = loss_fn(output, target) scaler.scale(loss).backward() # Unscales the gradients of optimizer's assigned params in-place scaler.unscale_(optimizer) # Since the gradients of optimizer's assigned params are unscaled, clips as usual: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) # optimizer's gradients are already unscaled, so scaler.step does not unscale them, # although it still skips optimizer.step() if the gradients contain infs or NaNs. scaler.step(optimizer) # Updates the scale for next iteration. scaler.update() scaler records that scaler.unscale_(optimizer) was already called for this optimizer this iteration, so scaler.step(optimizer) knows not to redundantly unscale gradients before (internally) calling optimizer.step().", "prev_chunk_id": "chunk_1421", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1423", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Gradient accumulation#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Gradient accumulation#", "content": "Gradient accumulation# Gradient accumulation adds gradients over an effective batch of size batch_per_iter * iters_to_accumulate (* num_procs if distributed). The scale should be calibrated for the effective batch, which means inf/NaN checking, step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity. Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective batch are accumulated. If grads are unscaled (or the scale factor changes) before accumulation is complete, the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor) after which it’s impossible to recover the accumulated unscaled grads step must apply. Therefore, if you want to unscale_ grads (e.g., to allow clipping unscaled grads), call unscale_ just before step, after all (scaled) grads for the upcoming step have been accumulated. Also, only call update at the end of iterations where you called step for a full effective batch: scaler = GradScaler() for epoch in epochs: for i, (input, target) in enumerate(data): with autocast(device_type='cuda', dtype=torch.float16): output = model(input) loss = loss_fn(output, target) loss = loss / iters_to_accumulate # Accumulates scaled gradients. scaler.scale(loss).backward() if (i + 1) % iters_to_accumulate == 0: # may unscale_ here if desired (e.g., to allow clipping unscaled gradients) scaler.step(optimizer) scaler.update() optimizer.zero_grad()", "prev_chunk_id": "chunk_1422", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1424", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Gradient penalty#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Gradient penalty#", "content": "Gradient penalty# A gradient penalty implementation commonly creates gradients using torch.autograd.grad(), combines them to create the penalty value, and adds the penalty value to the loss. Here’s an ordinary example of an L2 penalty without gradient scaling or autocasting: for epoch in epochs: for input, target in data: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) # Creates gradients grad_params = torch.autograd.grad(outputs=loss, inputs=model.parameters(), create_graph=True) # Computes the penalty term and adds it to the loss grad_norm = 0 for grad in grad_params: grad_norm += grad.pow(2).sum() grad_norm = grad_norm.sqrt() loss = loss + grad_norm loss.backward() # clip gradients here, if desired optimizer.step() To implement a gradient penalty with gradient scaling, the outputs Tensor(s) passed to torch.autograd.grad() should be scaled. The resulting gradients will therefore be scaled, and should be unscaled before being combined to create the penalty value. Also, the penalty term computation is part of the forward pass, and therefore should be inside an autocast context. Here’s how that looks for the same L2 penalty: scaler = GradScaler() for epoch in epochs: for input, target in data: optimizer.zero_grad() with autocast(device_type='cuda', dtype=torch.float16): output = model(input) loss = loss_fn(output, target) # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss), inputs=model.parameters(), create_graph=True) # Creates unscaled grad_params before computing the penalty. scaled_grad_params are # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_: inv_scale = 1./scaler.get_scale() grad_params = [p * inv_scale for p in scaled_grad_params] # Computes the penalty term and adds it to the loss with autocast(device_type='cuda', dtype=torch.float16): grad_norm = 0 for grad in grad_params: grad_norm += grad.pow(2).sum() grad_norm = grad_norm.sqrt() loss = loss + grad_norm # Applies scaling to the backward call as usual. # Accumulates leaf gradients that are correctly scaled. scaler.scale(loss).backward() # may unscale_ here if desired (e.g., to allow clipping unscaled", "prev_chunk_id": "chunk_1423", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1425", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Gradient penalty#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Gradient penalty#", "content": "gradients) # step() and update() proceed as usual. scaler.step(optimizer) scaler.update()", "prev_chunk_id": "chunk_1424", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1426", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Working with Multiple Models, Losses, and Optimizers#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Working with Multiple Models, Losses, and Optimizers#", "content": "Working with Multiple Models, Losses, and Optimizers# If your network has multiple losses, you must call scaler.scale on each of them individually. If your network has multiple optimizers, you may call scaler.unscale_ on any of them individually, and you must call scaler.step on each of them individually. However, scaler.update should only be called once, after all optimizers used this iteration have been stepped: scaler = torch.amp.GradScaler() for epoch in epochs: for input, target in data: optimizer0.zero_grad() optimizer1.zero_grad() with autocast(device_type='cuda', dtype=torch.float16): output0 = model0(input) output1 = model1(input) loss0 = loss_fn(2 * output0 + 3 * output1, target) loss1 = loss_fn(3 * output0 - 5 * output1, target) # (retain_graph here is unrelated to amp, it's present because in this # example, both backward() calls share some sections of graph.) scaler.scale(loss0).backward(retain_graph=True) scaler.scale(loss1).backward() # You can choose which optimizers receive explicit unscaling, if you # want to inspect or modify the gradients of the params they own. scaler.unscale_(optimizer0) scaler.step(optimizer0) scaler.step(optimizer1) scaler.update() Each optimizer checks its gradients for infs/NaNs and makes an independent decision whether or not to skip the step. This may result in one optimizer skipping the step while the other one does not. Since step skipping occurs rarely (every several hundred iterations) this should not impede convergence. If you observe poor convergence after adding gradient scaling to a multiple-optimizer model, please report a bug.", "prev_chunk_id": "chunk_1425", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1427", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Working with Multiple GPUs#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Working with Multiple GPUs#", "content": "Working with Multiple GPUs# The issues described here only affect autocast. GradScaler‘s usage is unchanged.", "prev_chunk_id": "chunk_1426", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1428", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "DataParallel in a single process#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "DataParallel in a single process#", "content": "DataParallel in a single process# Even if torch.nn.DataParallel spawns threads to run the forward pass on each device. The autocast state is propagated in each one and the following will work: model = MyModel() dp_model = nn.DataParallel(model) # Sets autocast in the main thread with autocast(device_type='cuda', dtype=torch.float16): # dp_model's internal threads will autocast. output = dp_model(input) # loss_fn also autocast loss = loss_fn(output)", "prev_chunk_id": "chunk_1427", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1429", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "DistributedDataParallel, one GPU per process#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "DistributedDataParallel, one GPU per process#", "content": "DistributedDataParallel, one GPU per process# torch.nn.parallel.DistributedDataParallel’s documentation recommends one GPU per process for best performance. In this case, DistributedDataParallel does not spawn threads internally, so usages of autocast and GradScaler are not affected.", "prev_chunk_id": "chunk_1428", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1430", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "DistributedDataParallel, multiple GPUs per process#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "DistributedDataParallel, multiple GPUs per process#", "content": "DistributedDataParallel, multiple GPUs per process# Here torch.nn.parallel.DistributedDataParallel may spawn a side thread to run the forward pass on each device, like torch.nn.DataParallel. The fix is the same: apply autocast as part of your model’s forward method to ensure it’s enabled in side threads.", "prev_chunk_id": "chunk_1429", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1431", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Autocast and Custom Autograd Functions#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Autocast and Custom Autograd Functions#", "content": "Autocast and Custom Autograd Functions# If your network uses custom autograd functions (subclasses of torch.autograd.Function), changes are required for autocast compatibility if any function - takes multiple floating-point Tensor inputs, - wraps any autocastable op (see theAutocast Op Reference), or - requires a particulardtype(for example, if it wrapsCUDA extensionsthat were only compiled fordtype). In all cases, if you’re importing the function and can’t alter its definition, a safe fallback is to disable autocast and force execution in float32 ( or dtype) at any points of use where errors occur: with autocast(device_type='cuda', dtype=torch.float16): ... with autocast(device_type='cuda', dtype=torch.float16, enabled=False): output = imported_function(input1.float(), input2.float()) If you’re the function’s author (or can alter its definition) a better solution is to use the torch.amp.custom_fwd() and torch.amp.custom_bwd() decorators as shown in the relevant case below.", "prev_chunk_id": "chunk_1430", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1432", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Functions with multiple inputs or autocastable ops#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Functions with multiple inputs or autocastable ops#", "content": "Functions with multiple inputs or autocastable ops# Apply custom_fwd and custom_bwd (with no arguments) to forward and backward respectively. These ensure forward executes with the current autocast state and backward executes with the same autocast state as forward (which can prevent type mismatch errors): class MyMM(torch.autograd.Function): @staticmethod @custom_fwd def forward(ctx, a, b): ctx.save_for_backward(a, b) return a.mm(b) @staticmethod @custom_bwd def backward(ctx, grad): a, b = ctx.saved_tensors return grad.mm(b.t()), a.t().mm(grad) Now MyMM can be invoked anywhere, without disabling autocast or manually casting inputs: mymm = MyMM.apply with autocast(device_type='cuda', dtype=torch.float16): output = mymm(input1, input2)", "prev_chunk_id": "chunk_1431", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1433", "url": "https://docs.pytorch.org/docs/stable/notes/amp_examples.html", "title": "Functions that need a particular dtype#", "page_title": "Automatic Mixed Precision examples — PyTorch 2.8 documentation", "breadcrumbs": "Functions that need a particular dtype#", "content": "Functions that need a particular dtype# Consider a custom function that requires torch.float32 inputs. Apply custom_fwd(device_type='cuda', cast_inputs=torch.float32) to forward and custom_bwd(device_type='cuda') to backward. If forward runs in an autocast-enabled region, the decorators cast floating-point Tensor inputs to float32 on designated device assigned by the argument device_type, CUDA in this example, and locally disable autocast during forward and backward: class MyFloat32Func(torch.autograd.Function): @staticmethod @custom_fwd(device_type='cuda', cast_inputs=torch.float32) def forward(ctx, input): ctx.save_for_backward(input) ... return fwd_output @staticmethod @custom_bwd(device_type='cuda') def backward(ctx, grad): ... Now MyFloat32Func can be invoked anywhere, without manually disabling autocast or casting inputs: func = MyFloat32Func.apply with autocast(device_type='cuda', dtype=torch.float16): # func will run in float32, regardless of the surrounding autocast state output = func(input)", "prev_chunk_id": "chunk_1432", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1434", "url": "https://docs.pytorch.org/docs/stable/torch_environment_variables.html", "title": "Torch Environment Variables#", "page_title": "Torch Environment Variables — PyTorch 2.8 documentation", "breadcrumbs": "Torch Environment Variables#", "content": "Torch Environment Variables# Created On: Feb 15, 2024 | Last Updated On: Jun 10, 2025 PyTorch leverages environment variables for adjusting various settings that influence its runtime behavior. These variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels, specifying the number of threads for parallel processing tasks and many more. Moreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN, which also utilize environment variables to modify their functionality. This interplay of settings allows for a highly customizable development environment that can be optimized for efficiency, debugging, and computational resource management. Please note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive. If you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1435", "url": "https://docs.pytorch.org/docs/stable/logging.html", "title": "torch._logging#", "page_title": "torch._logging — PyTorch 2.8 documentation", "breadcrumbs": "torch._logging#", "content": "torch._logging# Created On: Apr 24, 2023 | Last Updated On: Jun 17, 2025 PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component’s log messages can be completely disabled, while another component’s log messages can be set to maximum verbosity. There are two ways to configure the logging system: through the environment variable TORCH_LOGS or the python API torch._logging.set_logs. The environment variable TORCH_LOGS is a comma-separated list of [+-]<component> pairs, where <component> is a component specified below. The + prefix will decrease the log level of the component, displaying more log messages while the - prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in TORCH_LOGS. In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with + or - will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be off_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the TORCH_LOGS environment variable (see torch._logging.set_logs for the python API):", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1436", "url": "https://docs.pytorch.org/docs/stable/future_mod.html", "title": "torch.__future__#", "page_title": "torch.__future__ — PyTorch 2.8 documentation", "breadcrumbs": "torch.__future__#", "content": "torch.__future__# Created On: Feb 05, 2024 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1437", "url": "https://docs.pytorch.org/docs/stable/config_mod.html", "title": "torch.config#", "page_title": "torch.config — PyTorch 2.8 documentation", "breadcrumbs": "torch.config#", "content": "torch.config# Created On: Apr 09, 2019 | Last Updated On: Jun 13, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1438", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Named Tensors operator coverage#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Named Tensors operator coverage#", "content": "Named Tensors operator coverage# Created On: Oct 08, 2019 | Last Updated On: Jun 08, 2025 Please read Named Tensors first for an introduction to named tensors. This document is a reference for name inference, a process that defines how named tensors: - use names to provide additional automatic runtime correctness checks - propagate names from input tensors to output tensors Below is a list of all operations that are supported with named tensors and their associated name inference rules. If you don’t see an operation listed here, but it would help your use case, please search if an issue has already been filed and if not, file one.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1439", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Keeps input names#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Keeps input names#", "content": "Keeps input names# All pointwise unary functions follow this rule as well as some other unary functions. - Check names: None - Propagate names: input tensor’s names are propagated to the output. >>> x = torch.randn(3, 3, names=('N', 'C')) >>> x.abs().names ('N', 'C')", "prev_chunk_id": "chunk_1438", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1440", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Removes dimensions#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Removes dimensions#", "content": "Removes dimensions# All reduction ops like sum() remove dimensions by reducing over the desired dimensions. Other operations like select() and squeeze() remove dimensions. Wherever one can pass an integer dimension index to an operator, one can also pass a dimension name. Functions that take lists of dimension indices can also take in a list of dimension names. - Check names: Ifdimordimsis passed in as a list of names, check that those names exist inself. - Propagate names: If the dimensions of the input tensor specified bydimordimsare not present in the output tensor, then the corresponding names of those dimensions do not appear inoutput.names. >>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W')) >>> x.squeeze('N').names ('C', 'H', 'W') >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W')) >>> x.sum(['N', 'C']).names ('H', 'W') # Reduction ops with keepdim=True don't actually remove dimensions. >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W')) >>> x.sum(['N', 'C'], keepdim=True).names ('N', 'C', 'H', 'W')", "prev_chunk_id": "chunk_1439", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1441", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Unifies names from inputs#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Unifies names from inputs#", "content": "Unifies names from inputs# All binary arithmetic ops follow this rule. Operations that broadcast still broadcast positionally from the right to preserve compatibility with unnamed tensors. To perform explicit broadcasting by names, use Tensor.align_as(). - Check names: All names must match positionally from the right. i.e., intensor+other,match(tensor.names[i],other.names[i])must be true for alliin(-min(tensor.dim(),other.dim())+1,-1]. - Check names: Furthermore, all named dimensions must be aligned from the right. During matching, if we match a named dimensionAwith an unnamed dimensionNone, thenAmust not appear in the tensor with the unnamed dimension. - Propagate names: unify pairs of names from the right from both tensors to produce output names. For example, # tensor: Tensor[ N, None] # other: Tensor[None, C] >>> tensor = torch.randn(3, 3, names=('N', None)) >>> other = torch.randn(3, 3, names=(None, 'C')) >>> (tensor + other).names ('N', 'C') Check names: - match(tensor.names[-1],other.names[-1])isTrue - match(tensor.names[-2],tensor.names[-2])isTrue - Because we matchedNoneintensorwith'C', check to make sure'C'doesn’t exist intensor(it does not). - Check to make sure'N'doesn’t exists inother(it does not). Finally, the output names are computed with [unify('N', None), unify(None, 'C')] = ['N', 'C'] More examples: # Dimensions don't match from the right: # tensor: Tensor[N, C] # other: Tensor[ N] >>> tensor = torch.randn(3, 3, names=('N', 'C')) >>> other = torch.randn(3, names=('N',)) >>> (tensor + other).names RuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims ['N']: dim 'C' and dim 'N' are at the same position from the right but do not match. # Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]: # tensor: Tensor[N, None] # other: Tensor[ N] >>> tensor = torch.randn(3, 3, names=('N', None)) >>> other = torch.randn(3, names=('N',)) >>> (tensor + other).names RuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and dims ['N', None]: dim 'N' appears in a different position from the right across both lists.", "prev_chunk_id": "chunk_1440", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1442", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Permutes dimensions#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Permutes dimensions#", "content": "Permutes dimensions# Some operations, like Tensor.t(), permute the order of dimensions. Dimension names are attached to individual dimensions so they get permuted as well. If the operator takes in positional index dim, it is also able to take a dimension name as dim. - Check names: Ifdimis passed as a name, check that it exists in the tensor. - Propagate names: Permute dimension names in the same way as the dimensions that are being permuted. >>> x = torch.randn(3, 3, names=('N', 'C')) >>> x.transpose('N', 'C').names ('C', 'N')", "prev_chunk_id": "chunk_1441", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1443", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Contracts away dims#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Contracts away dims#", "content": "Contracts away dims# Matrix multiply functions follow some variant of this. Let’s go through torch.mm() first and then generalize the rule for batch matrix multiplication. For torch.mm(tensor, other): - Check names: None - Propagate names: result names are(tensor.names[-2],other.names[-1]). >>> x = torch.randn(3, 3, names=('N', 'D')) >>> y = torch.randn(3, 3, names=('in', 'out')) >>> x.mm(y).names ('N', 'out') Inherently, a matrix multiplication performs a dot product over two dimensions, collapsing them. When two tensors are matrix-multiplied, the contracted dimensions disappear and do not show up in the output tensor. torch.mv(), torch.dot() work in a similar way: name inference does not check input names and removes the dimensions that are involved in the dot product: >>> x = torch.randn(3, 3, names=('N', 'D')) >>> y = torch.randn(3, names=('something',)) >>> x.mv(y).names ('N',) Now, let’s take a look at torch.matmul(tensor, other). Assume that tensor.dim() >= 2 and other.dim() >= 2. - Check names: Check that the batch dimensions of the inputs are aligned and broadcastable. SeeUnifies names from inputsfor what it means for the inputs to be aligned. - Propagate names: result names are obtained by unifying the batch dimensions and removing the contracted dimensions:unify(tensor.names[:-2],other.names[:-2])+(tensor.names[-2],other.names[-1]). Examples: # Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F']. # 'A', 'B' are batch dimensions. >>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D')) >>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F')) >>> torch.matmul(x, y).names ('A', 'B', 'C', 'F') Finally, there are fused add versions of many matmul functions. i.e., addmm() and addmv(). These are treated as composing name inference for i.e. mm() and name inference for add().", "prev_chunk_id": "chunk_1442", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1444", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "Factory functions#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "Factory functions#", "content": "Factory functions# Factory functions now take a new names argument that associates a name with each dimension. >>> torch.zeros(2, 3, names=('N', 'C')) tensor([[0., 0., 0.], [0., 0., 0.]], names=('N', 'C'))", "prev_chunk_id": "chunk_1443", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1445", "url": "https://docs.pytorch.org/docs/stable/name_inference.html", "title": "out function and in-place variants#", "page_title": "Named Tensors operator coverage — PyTorch 2.8 documentation", "breadcrumbs": "out function and in-place variants#", "content": "out function and in-place variants# A tensor specified as an out= tensor has the following behavior: - If it has no named dimensions, then the names computed from the operation get propagated to it. - If it has any named dimensions, then the names computed from the operation must be exactly equal to the existing names. Otherwise, the operation errors. All in-place methods modify inputs to have names equal to the computed names from name inference. For example: >>> x = torch.randn(3, 3) >>> y = torch.randn(3, 3, names=('N', 'C')) >>> x.names (None, None) >>> x += y >>> x.names ('N', 'C')", "prev_chunk_id": "chunk_1444", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1446", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Named Tensors#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Named Tensors#", "content": "Named Tensors# Created On: Oct 08, 2019 | Last Updated On: Jun 14, 2025 Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support “broadcasting by name” rather than “broadcasting by position”.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1447", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Creating named tensors#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Creating named tensors#", "content": "Creating named tensors# Factory functions now take a new names argument that associates a name with each dimension. >>> torch.zeros(2, 3, names=('N', 'C')) tensor([[0., 0., 0.], [0., 0., 0.]], names=('N', 'C')) Named dimensions, like regular Tensor dimensions, are ordered. tensor.names[i] is the name of dimension i of tensor. The following factory functions support named tensors: - torch.empty() - torch.rand() - torch.randn() - torch.ones() - torch.tensor() - torch.zeros()", "prev_chunk_id": "chunk_1446", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1448", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Named dimensions#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Named dimensions#", "content": "Named dimensions# See names for restrictions on tensor names. Use names to access the dimension names of a tensor and rename() to rename named dimensions. >>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W')) >>> imgs.names ('N', 'C', 'H', 'W') >>> renamed_imgs = imgs.rename(H='height', W='width') >>> renamed_imgs.names ('N', 'C', 'height', 'width) Named tensors can coexist with unnamed tensors; named tensors are instances of torch.Tensor. Unnamed tensors have None-named dimensions. Named tensors do not require all dimensions to be named. >>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W')) >>> imgs.names (None, 'C', 'H', 'W')", "prev_chunk_id": "chunk_1447", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1449", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Name propagation semantics#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Name propagation semantics#", "content": "Name propagation semantics# Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called name inference. More formally, name inference consists of the following two steps: - Check names: an operator may perform automatic checks at runtime that check that certain dimension names must match. - Propagate names: name inference propagates names to output tensors. All operations that support named tensors propagate names. >>> x = torch.randn(3, 3, names=('N', 'C')) >>> x.abs().names ('N', 'C')", "prev_chunk_id": "chunk_1448", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1450", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "match semantics#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "match semantics#", "content": "match semantics# Two names match if they are equal (string equality) or if at least one is None. Nones are essentially a special “wildcard” name. unify(A, B) determines which of the names A and B to propagate to the outputs. It returns the more specific of the two names, if they match. If the names do not match, then it errors.", "prev_chunk_id": "chunk_1449", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1451", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Basic name inference rules#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Basic name inference rules#", "content": "Basic name inference rules# Let’s see how match and unify are used in name inference in the case of adding two one-dim tensors with no broadcasting. x = torch.randn(3, names=('X',)) y = torch.randn(3) z = torch.randn(3, names=('Z',)) Check names: check that the names of the two tensors match. For the following examples: >>> # x + y # match('X', None) is True >>> # x + z # match('X', 'Z') is False >>> # x + x # match('X', 'X') is True >>> x + z Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match. Propagate names: unify the names to select which one to propagate. In the case of x + y, unify('X', None) = 'X' because 'X' is more specific than None. >>> (x + y).names ('X',) >>> (x + x).names ('X',) For a comprehensive list of name inference rules, see Named Tensors operator coverage. Here are two common operations that may be useful to go over: - Binary arithmetic ops:Unifies names from inputs - Matrix multiplication ops:Contracts away dims", "prev_chunk_id": "chunk_1450", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1452", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Explicit alignment by names#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Explicit alignment by names#", "content": "Explicit alignment by names# Use align_as() or align_to() to align tensor dimensions by name to a specified ordering. This is useful for performing “broadcasting by names”. # This function is agnostic to the dimension ordering of `input`, # as long as it has a `C` dimension somewhere. def scale_channels(input, scale): scale = scale.refine_names('C') return input * scale.align_as(input) >>> num_channels = 3 >>> scale = torch.randn(num_channels, names=('C',)) >>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C')) >>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W')) >>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D') >>> scale_channels(imgs, scale) >>> scale_channels(more_imgs, scale) >>> scale_channels(videos, scale)", "prev_chunk_id": "chunk_1451", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1453", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Manipulating dimensions#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Manipulating dimensions#", "content": "Manipulating dimensions# Use align_to() to permute large amounts of dimensions without mentioning all of them as in required by permute(). >>> tensor = torch.randn(2, 2, 2, 2, 2, 2) >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F') # Move the F (dim 5) and E dimension (dim 4) to the front while keeping # the rest in the same order >>> tensor.permute(5, 4, 0, 1, 2, 3) >>> named_tensor.align_to('F', 'E', ...) Use flatten() and unflatten() to flatten and unflatten dimensions, respectively. These methods are more verbose than view() and reshape(), but have more semantic meaning to someone reading the code. >>> imgs = torch.randn(32, 3, 128, 128) >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W') >>> flat_imgs = imgs.view(32, -1) >>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features') >>> named_flat_imgs.names ('N', 'features') >>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)]) >>> unflattened_named_imgs.names ('N', 'C', 'H', 'W')", "prev_chunk_id": "chunk_1452", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1454", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Autograd support#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Autograd support#", "content": "Autograd support# Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us. >>> x = torch.randn(3, names=('D',)) >>> weight = torch.randn(3, names=('D',), requires_grad=True) >>> loss = (x - weight).abs() >>> grad_loss = torch.randn(3) >>> loss.backward(grad_loss) >>> weight.grad # Unnamed for now. Will be named in the future tensor([-1.8107, -0.6357, 0.0783]) >>> weight.grad.zero_() >>> grad_loss = grad_loss.refine_names('C') >>> loss = (x - weight).abs() # Ideally we'd check that the names of loss and grad_loss match but we don't yet. >>> loss.backward(grad_loss) >>> weight.grad tensor([-1.8107, -0.6357, 0.0783])", "prev_chunk_id": "chunk_1453", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1455", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Operators#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Operators#", "content": "Operators# See Named Tensors operator coverage for a full list of the supported torch and tensor operations. We do not yet support the following that is not covered by the link: - indexing, advanced indexing. For torch.nn.functional operators, we support the following: - torch.nn.functional.relu() - torch.nn.functional.softmax() - torch.nn.functional.log_softmax() - torch.nn.functional.tanh() - torch.nn.functional.sigmoid() - torch.nn.functional.dropout()", "prev_chunk_id": "chunk_1454", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1456", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Subsystems#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Subsystems#", "content": "Subsystems# Autograd is supported, see Autograd support. Because gradients are currently unnamed, optimizers may work but are untested. NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs: - NN module parameters are unnamed, so outputs may be partially named. - NN module forward passes have code that don’t support named tensors and will error out appropriately. We also do not support the following subsystems, though some may work out of the box: - distributions - serialization (torch.load(),torch.save()) - multiprocessing - JIT - distributed - ONNX If any of these would help your use case, please search if an issue has already been filed and if not, file one.", "prev_chunk_id": "chunk_1455", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1457", "url": "https://docs.pytorch.org/docs/stable/named_tensor.html", "title": "Named tensor API reference#", "page_title": "Named Tensors — PyTorch 2.8 documentation", "breadcrumbs": "Named tensor API reference#", "content": "Named tensor API reference# In this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see Named Tensors operator coverage.", "prev_chunk_id": "chunk_1456", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1458", "url": "https://docs.pytorch.org/docs/stable/type_info.html", "title": "Type Info#", "page_title": "Type Info — PyTorch 2.8 documentation", "breadcrumbs": "Type Info#", "content": "Type Info# Created On: Jun 06, 2025 | Last Updated On: Jun 06, 2025 The numerical properties of a torch.dtype can be accessed through either the torch.finfo or the torch.iinfo.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1459", "url": "https://docs.pytorch.org/docs/stable/type_info.html", "title": "torch.finfo#", "page_title": "Type Info — PyTorch 2.8 documentation", "breadcrumbs": "torch.finfo#", "content": "torch.finfo# A torch.finfo is an object that represents the numerical properties of a floating point torch.dtype, (i.e. torch.float32, torch.float64, torch.float16, and torch.bfloat16). This is similar to numpy.finfo. A torch.finfo provides the following attributes:", "prev_chunk_id": "chunk_1458", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1460", "url": "https://docs.pytorch.org/docs/stable/type_info.html", "title": "torch.iinfo#", "page_title": "Type Info — PyTorch 2.8 documentation", "breadcrumbs": "torch.iinfo#", "content": "torch.iinfo# A torch.iinfo is an object that represents the numerical properties of a integer torch.dtype (i.e. torch.uint8, torch.int8, torch.int16, torch.int32, and torch.int64). This is similar to numpy.iinfo. A torch.iinfo provides the following attributes:", "prev_chunk_id": "chunk_1459", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1461", "url": "https://docs.pytorch.org/docs/stable/module_tracker.html", "title": "torch.utils.module_tracker#", "page_title": "torch.utils.module_tracker — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.module_tracker#", "content": "torch.utils.module_tracker# Created On: May 04, 2024 | Last Updated On: Jun 11, 2025 This utility can be used to track the current position inside an torch.nn.Module hierarchy. It can be used within other tracking tools to be able to easily associate measured quantities to user-friendly names. This is used in particular in the FlopCounterMode today.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1462", "url": "https://docs.pytorch.org/docs/stable/tensorboard.html", "title": "torch.utils.tensorboard#", "page_title": "torch.utils.tensorboard — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.tensorboard#", "content": "torch.utils.tensorboard# Created On: Apr 25, 2019 | Last Updated On: Mar 10, 2022 Before going further, more details on TensorBoard can be found at https://www.tensorflow.org/tensorboard/ Once you’ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example: import torch import torchvision from torch.utils.tensorboard import SummaryWriter from torchvision import datasets, transforms # Writer will output to ./runs/ directory by default writer = SummaryWriter() transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) model = torchvision.models.resnet50(False) # Have ResNet model take in grayscale rather than RGB model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) images, labels = next(iter(trainloader)) grid = torchvision.utils.make_grid(images) writer.add_image('images', grid, 0) writer.add_graph(model, images) writer.close() This can then be visualized with TensorBoard, which should be installable and runnable with: pip install tensorboard tensorboard --logdir=runs Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, “Loss/train” and “Loss/test” will be grouped together, while “Accuracy/train” and “Accuracy/test” will be grouped separately in the TensorBoard interface. from torch.utils.tensorboard import SummaryWriter import numpy as np writer = SummaryWriter() for n_iter in range(100): writer.add_scalar('Loss/train', np.random.random(), n_iter) writer.add_scalar('Loss/test', np.random.random(), n_iter) writer.add_scalar('Accuracy/train', np.random.random(), n_iter) writer.add_scalar('Accuracy/test', np.random.random(), n_iter) Expected result:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1463", "url": "https://docs.pytorch.org/docs/stable/model_zoo.html", "title": "torch.utils.model_zoo#", "page_title": "torch.utils.model_zoo — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.model_zoo#", "content": "torch.utils.model_zoo# Created On: Jan 09, 2017 | Last Updated On: Jun 11, 2025 Moved to torch.hub.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1464", "url": "https://docs.pytorch.org/docs/stable/mobile_optimizer.html", "title": "torch.utils.mobile_optimizer#", "page_title": "torch.utils.mobile_optimizer — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.mobile_optimizer#", "content": "torch.utils.mobile_optimizer# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025 PyTorch Mobile is no longer actively supported. Redirecting to ExecuTorch documentation.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1465", "url": "https://docs.pytorch.org/docs/stable/dlpack.html", "title": "torch.utils.dlpack#", "page_title": "torch.utils.dlpack — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.dlpack#", "content": "torch.utils.dlpack# Created On: Jul 11, 2018 | Last Updated On: Jun 13, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1466", "url": "https://docs.pytorch.org/docs/stable/jit_utils.html", "title": "JIT Utils - torch.utils.jit#", "page_title": "JIT Utils - torch.utils.jit — PyTorch 2.8 documentation", "breadcrumbs": "JIT Utils - torch.utils.jit#", "content": "JIT Utils - torch.utils.jit# Created On: Jun 03, 2022 | Last Updated On: Jun 03, 2022", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1467", "url": "https://docs.pytorch.org/docs/stable/deterministic.html", "title": "torch.utils.deterministic#", "page_title": "torch.utils.deterministic — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.deterministic#", "content": "torch.utils.deterministic# Created On: Oct 26, 2023 | Last Updated On: Jun 06, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1468", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "torch.utils.data#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.data#", "content": "torch.utils.data# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for - map-style and iterable-style datasets, - customizing data loading order, - automatic batching, - single- and multi-process data loading, - automatic memory pinning. These options are configured by the constructor arguments of a DataLoader, which has signature: DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False) The sections below describe in details the effects and usages of these options.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1469", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Dataset Types#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Dataset Types#", "content": "Dataset Types# The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets: - Map-style datasets, - Iterable-style datasets.", "prev_chunk_id": "chunk_1468", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1470", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Map-style datasets#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Map-style datasets#", "content": "Map-style datasets# A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples. For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk. See Dataset for more details.", "prev_chunk_id": "chunk_1469", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1471", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Iterable-style datasets#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Iterable-style datasets#", "content": "Iterable-style datasets# An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data. For example, such a dataset, when called iter(dataset), could return a stream of data reading from a database, a remote server, or even logs generated in real time. See IterableDataset for more details.", "prev_chunk_id": "chunk_1470", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1472", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Data Loading Order and Sampler#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Data Loading Order and Sampler#", "content": "Data Loading Order and Sampler# For iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time). The rest of this section concerns the case with map-style datasets. torch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD. A sequential or shuffled sampler will be automatically constructed based on the shuffle argument to a DataLoader. Alternatively, users may use the sampler argument to specify a custom Sampler object that at each time yields the next index/key to fetch. A custom Sampler that yields a list of batch indices at a time can be passed as the batch_sampler argument. Automatic batching can also be enabled via batch_size and drop_last arguments. See the next section for more details on this.", "prev_chunk_id": "chunk_1471", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1473", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Loading Batched and Non-Batched Data#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Loading Batched and Non-Batched Data#", "content": "Loading Batched and Non-Batched Data# DataLoader supports automatically collating individual fetched data samples into batches via arguments batch_size, drop_last, batch_sampler, and collate_fn (which has a default function).", "prev_chunk_id": "chunk_1472", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1474", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Automatic batching (default)#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Automatic batching (default)#", "content": "Automatic batching (default)# This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first). When batch_size (default 1) is not None, the data loader yields batched samples instead of individual samples. batch_size and drop_last arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify batch_sampler, which yields a list of keys at a time. After fetching a list of samples using the indices from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches. In this case, loading from a map-style dataset is roughly equivalent with: for indices in batch_sampler: yield collate_fn([dataset[i] for i in indices]) and loading from an iterable-style dataset is roughly equivalent with: dataset_iter = iter(dataset) for indices in batch_sampler: yield collate_fn([next(dataset_iter) for _ in indices]) A custom collate_fn can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about collate_fn.", "prev_chunk_id": "chunk_1473", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1475", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Disable automatic batching#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Disable automatic batching#", "content": "Disable automatic batching# In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where collate_fn is used to collate the samples), but let the data loader directly return each member of the dataset object. When both batch_size and batch_sampler are None (default value for batch_sampler is already None), automatic batching is disabled. Each sample obtained from the dataset is processed with the function passed as the collate_fn argument. When automatic batching is disabled, the default collate_fn simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: for index in sampler: yield collate_fn(dataset[index]) and loading from an iterable-style dataset is roughly equivalent with: for data in iter(dataset): yield collate_fn(data) See this section on more about collate_fn.", "prev_chunk_id": "chunk_1474", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1476", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Working with collate_fn#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Working with collate_fn#", "content": "Working with collate_fn# The use of collate_fn is slightly different when automatic batching is enabled or disabled. When automatic batching is disabled, collate_fn is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default collate_fn simply converts NumPy arrays in PyTorch tensors. When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default collate_fn (default_collate()). For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple (image, class_index), the default collate_fn collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default collate_fn has the following properties: - It always prepends a new dimension as the batch dimension. - It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors. - It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same forlists,tuples,namedtuples, etc. Users may use customized collate_fn to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types. If you run into a situation where the outputs of DataLoader have dimensions or type that is different from your expectation, you may want to check your collate_fn.", "prev_chunk_id": "chunk_1475", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1477", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Single- and Multi-process Data Loading#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Single- and Multi-process Data Loading#", "content": "Single- and Multi-process Data Loading# A DataLoader uses single-process data loading by default. Within a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument num_workers to a positive integer.", "prev_chunk_id": "chunk_1476", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1478", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Single-process data loading (default)#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Single-process data loading (default)#", "content": "Single-process data loading (default)# In this mode, data fetching is done in the same process a DataLoader is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.", "prev_chunk_id": "chunk_1477", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1479", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Multi-process data loading#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Multi-process data loading#", "content": "Multi-process data loading# Setting the argument num_workers as a positive integer will turn on multi-process data loading with the specified number of loader worker processes. In this mode, each time an iterator of a DataLoader is created (e.g., when you call enumerate(dataloader)), num_workers worker processes are created. At this point, the dataset, collate_fn, and worker_init_fn are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including collate_fn) runs in the worker process. torch.utils.data.get_worker_info() returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns None in main process. Users may use this function in dataset code and/or worker_init_fn to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset. For map-style datasets, the main process generates the indices using sampler and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load. For iterable-style datasets, since each worker process gets a replica of the dataset object, naive multi-process loading will often result in duplicated data. Using torch.utils.data.get_worker_info() and/or worker_init_fn, users may configure each replica independently. (See IterableDataset documentations for how to achieve this. ) For similar reasons, in multi-process loading, the drop_last argument drops the last non-full batch of each worker’s iterable-style dataset replica. Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.", "prev_chunk_id": "chunk_1478", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1480", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Platform-specific behaviors#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Platform-specific behaviors#", "content": "Platform-specific behaviors# Since workers rely on Python multiprocessing, worker launch behavior is different on Windows compared to Unix. - On Unix,fork()is the defaultmultiprocessingstart method. Usingfork(), child workers typically can access thedatasetand Python argument functions directly through the cloned address space. - On Windows or MacOS,spawn()is the defaultmultiprocessingstart method. Usingspawn(), another interpreter is launched which runs your main script, followed by the internal worker function that receives thedataset,collate_fnand other arguments throughpickleserialization. This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading: - Wrap most of you main script’s code withinif__name__=='__main__':block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset andDataLoaderinstance creation logic here, as it doesn’t need to be re-executed in workers. - Make sure that any customcollate_fn,worker_init_fnordatasetcode is declared as top level definitions, outside of the__main__check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, notbytecode.)", "prev_chunk_id": "chunk_1479", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1481", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Randomness in multi-process data loading#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Randomness in multi-process data loading#", "content": "Randomness in multi-process data loading# By default, each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified generator. However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See this section in FAQ.). In worker_init_fn, you may access the PyTorch seed set for each worker with either torch.utils.data.get_worker_info().seed or torch.initial_seed(), and use it to seed other libraries before data loading.", "prev_chunk_id": "chunk_1480", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1482", "url": "https://docs.pytorch.org/docs/stable/data.html", "title": "Memory Pinning#", "page_title": "torch.utils.data — PyTorch 2.8 documentation", "breadcrumbs": "Memory Pinning#", "content": "Memory Pinning# Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally. For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs. The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a collate_fn that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a pin_memory() method on your custom type(s). See the example below. Example: class SimpleCustomBatch: def __init__(self, data): transposed_data = list(zip(*data)) self.inp = torch.stack(transposed_data[0], 0) self.tgt = torch.stack(transposed_data[1], 0) # custom memory pinning method on custom type def pin_memory(self): self.inp = self.inp.pin_memory() self.tgt = self.tgt.pin_memory() return self def collate_wrapper(batch): return SimpleCustomBatch(batch) inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5) tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5) dataset = TensorDataset(inps, tgts) loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper, pin_memory=True) for batch_ndx, sample in enumerate(loader): print(sample.inp.is_pinned()) print(sample.tgt.is_pinned())", "prev_chunk_id": "chunk_1481", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1483", "url": "https://docs.pytorch.org/docs/stable/cpp_extension.html", "title": "torch.utils.cpp_extension#", "page_title": "torch.utils.cpp_extension — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.cpp_extension#", "content": "torch.utils.cpp_extension# Created On: Mar 07, 2018 | Last Updated On: Feb 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1484", "url": "https://docs.pytorch.org/docs/stable/checkpoint.html", "title": "torch.utils.checkpoint#", "page_title": "torch.utils.checkpoint — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.checkpoint#", "content": "torch.utils.checkpoint# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1485", "url": "https://docs.pytorch.org/docs/stable/bottleneck.html", "title": "torch.utils.bottleneck#", "page_title": "torch.utils.bottleneck — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils.bottleneck#", "content": "torch.utils.bottleneck# Created On: Mar 23, 2018 | Last Updated On: Sep 28, 2022 torch.utils.bottleneck is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch’s autograd profiler. Run it on the command line with python -m torch.utils.bottleneck /path/to/source/script.py [args] where [args] are any number of arguments to script.py, or run python -m torch.utils.bottleneck -h for more usage instructions. For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile() for more information.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1486", "url": "https://docs.pytorch.org/docs/stable/benchmark_utils.html", "title": "Benchmark Utils - torch.utils.benchmark#", "page_title": "Benchmark Utils - torch.utils.benchmark — PyTorch 2.8 documentation", "breadcrumbs": "Benchmark Utils - torch.utils.benchmark#", "content": "Benchmark Utils - torch.utils.benchmark# Created On: Nov 02, 2020 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1487", "url": "https://docs.pytorch.org/docs/stable/utils.html", "title": "torch.utils#", "page_title": "torch.utils — PyTorch 2.8 documentation", "breadcrumbs": "torch.utils#", "content": "torch.utils# Created On: Jul 21, 2023 | Last Updated On: Jun 06, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1488", "url": "https://docs.pytorch.org/docs/stable/testing.html", "title": "torch.testing#", "page_title": "torch.testing — PyTorch 2.8 documentation", "breadcrumbs": "torch.testing#", "content": "torch.testing# Created On: May 07, 2021 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1489", "url": "https://docs.pytorch.org/docs/stable/storage.html", "title": "torch.Storage#", "page_title": "torch.Storage — PyTorch 2.8 documentation", "breadcrumbs": "torch.Storage#", "content": "torch.Storage# Created On: Dec 30, 2016 | Last Updated On: Apr 14, 2025 In PyTorch, a regular tensor is a multi-dimensional array that is defined by the following components: - Storage: The actual data of the tensor, stored as a contiguous, one-dimensional array of bytes. - dtype: The data type of the elements in the tensor, such as torch.float32 or torch.int64. - shape: A tuple indicating the size of the tensor in each dimension. - Stride: The step size needed to move from one element to the next in each dimension. - Offset: The starting point in the storage from which the tensor data begins. This will usually be 0 for newly created tensors. These components together define the structure and data of a tensor, with the storage holding the actual data and the rest serving as metadata.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1490", "url": "https://docs.pytorch.org/docs/stable/storage.html", "title": "Untyped Storage API#", "page_title": "torch.Storage — PyTorch 2.8 documentation", "breadcrumbs": "Untyped Storage API#", "content": "Untyped Storage API# A torch.UntypedStorage is a contiguous, one-dimensional array of elements. Its length is equal to the number of bytes of the tensor. The storage serves as the underlying data container for tensors. In general, a tensor created in PyTorch using regular constructors such as zeros(), zeros_like() or new_zeros() will produce tensors where there is a one-to-one correspondence between the tensor storage and the tensor itself. However, a storage is allowed to be shared by multiple tensors. For instance, any view of a tensor (obtained through view() or some, but not all, kinds of indexing like integers and slices) will point to the same underlying storage as the original tensor. When serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors continue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage can be faster than deserializing multiple independent tensors. A tensor storage can be accessed through the untyped_storage() method. This will return an object of type torch.UntypedStorage. Fortunately, storages have a unique identifier accessed through the torch.UntypedStorage.data_ptr() method. In regular settings, two tensors with the same data storage will have the same storage data_ptr. However, tensors themselves can point to two separate storages, one for its data attribute and another for its grad attribute. Each will require a data_ptr() of its own. In general, there is no guarantee that a torch.Tensor.data_ptr() and torch.UntypedStorage.data_ptr() match and this should not be assumed to be true. Untyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors with different dtypes or shape can point to the same storage. It also implies that a tensor storage can be changed, as the following example shows: >>> t = torch.ones(3) >>> s0 = t.untyped_storage() >>>", "prev_chunk_id": "chunk_1489", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1491", "url": "https://docs.pytorch.org/docs/stable/storage.html", "title": "Untyped Storage API#", "page_title": "torch.Storage — PyTorch 2.8 documentation", "breadcrumbs": "Untyped Storage API#", "content": "s0 0 0 128 63 0 0 128 63 0 0 128 63 [torch.storage.UntypedStorage(device=cpu) of size 12] >>> s1 = s0.clone() >>> s1.fill_(0) 0 0 0 0 0 0 0 0 0 0 0 0 [torch.storage.UntypedStorage(device=cpu) of size 12] >>> # Fill the tensor with a zeroed storage >>> t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size()) tensor([0., 0., 0.]) Other than data_ptr, untyped storage also have other attributes such as filename (in case the storage points to a file on disk), device or is_cuda for device checks. A storage can also be manipulated in-place or out-of-place with methods like copy_, fill_ or pin_memory. FOr more information, check the API reference below. Keep in mind that modifying storages is a low-level API and comes with risks! Most of these APIs also exist on the tensor level: if present, they should be prioritized over their storage counterparts.", "prev_chunk_id": "chunk_1490", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1492", "url": "https://docs.pytorch.org/docs/stable/storage.html", "title": "Special cases#", "page_title": "torch.Storage — PyTorch 2.8 documentation", "breadcrumbs": "Special cases#", "content": "Special cases# We mentioned that a tensor that has a non-None grad attribute has actually two pieces of data within it. In this case, untyped_storage() will return the storage of the data attribute, whereas the storage of the gradient can be obtained through tensor.grad.untyped_storage(). >>> t = torch.zeros(3, requires_grad=True) >>> t.sum().backward() >>> assert list(t.untyped_storage()) == [0] * 12 # the storage of the tensor is just 0s >>> assert list(t.grad.untyped_storage()) != [0] * 12 # the storage of the gradient isn't Tensor subclasses or tensor-like objects can also display unusual behaviours. In general, we do not expect many use cases to require operating at the Storage level!", "prev_chunk_id": "chunk_1491", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1493", "url": "https://docs.pytorch.org/docs/stable/storage.html", "title": "Legacy Typed Storage#", "page_title": "torch.Storage — PyTorch 2.8 documentation", "breadcrumbs": "Legacy Typed Storage#", "content": "Legacy Typed Storage# torch.Storage is an alias for the storage class that corresponds with the default data type (torch.get_default_dtype()). For example, if the default data type is torch.float, torch.Storage resolves to torch.FloatStorage. The torch.<type>Storage and torch.cuda.<type>Storage classes, like torch.FloatStorage, torch.IntStorage, etc., are not actually ever instantiated. Calling their constructors creates a torch.TypedStorage with the appropriate torch.dtype and torch.device. torch.<type>Storage classes have all of the same class methods that torch.TypedStorage has. A torch.TypedStorage is a contiguous, one-dimensional array of elements of a particular torch.dtype. It can be given any torch.dtype, and the internal data will be interpreted appropriately. torch.TypedStorage contains a torch.UntypedStorage which holds the data as an untyped array of bytes. Every strided torch.Tensor contains a torch.TypedStorage, which stores all of the data that the torch.Tensor views.", "prev_chunk_id": "chunk_1492", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1494", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "torch.sparse#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "torch.sparse#", "content": "torch.sparse# Created On: Apr 26, 2017 | Last Updated On: Jun 18, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1495", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Why and when to use sparsity#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Why and when to use sparsity#", "content": "Why and when to use sparsity# By default, PyTorch stores torch.Tensor elements contiguously in physical memory. This leads to efficient implementations of various array processing algorithms that require fast access to elements. Now, some users might decide to represent data such as graph adjacency matrices, pruned weights or points clouds by Tensors whose elements are mostly zero valued. We recognize these are important applications and aim to provide performance optimizations for these use cases via sparse storage formats. Various sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been developed over the years. While they differ in exact layouts, they all compress data through efficient representation of zero valued elements. We call the uncompressed values specified in contrast to unspecified, compressed elements. By compressing repeat zeros sparse storage formats aim to save memory and computational resources on various CPUs and GPUs. Especially for high degrees of sparsity or highly structured sparsity this can have significant performance implications. As such sparse storage formats can be seen as a performance optimization. Like many other performance optimization sparse storage formats are not always advantageous. When trying sparse formats for your use case you might find your execution time to increase rather than decrease. Please feel encouraged to open a GitHub issue if you analytically expected to see a stark increase in performance but measured a degradation instead. This helps us prioritize the implementation of efficient kernels and wider performance optimizations. We make it easy to try different sparsity layouts, and convert between them, without being opinionated on what’s best for your particular application.", "prev_chunk_id": "chunk_1494", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1496", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Functionality overview#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Functionality overview#", "content": "Functionality overview# We want it to be straightforward to construct a sparse Tensor from a given dense Tensor by providing conversion routines for each layout. In the next example we convert a 2D Tensor with default dense (strided) layout to a 2D Tensor backed by the COO memory layout. Only values and indices of non-zero elements are stored in this case. >>> a = torch.tensor([[0, 2.], [3, 0]]) >>> a.to_sparse() tensor(indices=tensor([[0, 1], [1, 0]]), values=tensor([2., 3.]), size=(2, 2), nnz=2, layout=torch.sparse_coo) PyTorch currently supports COO, CSR, CSC, BSR, and BSC. We also have a prototype implementation to support :ref: semi-structured sparsity<sparse-semi-structured-docs>. Please see the references for more details. Note that we provide slight generalizations of these formats. Batching: Devices such as GPUs require batching for optimal performance and thus we support batch dimensions. We currently offer a very simple version of batching where each component of a sparse format itself is batched. This also requires the same number of specified elements per batch entry. In this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor. >>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]]) >>> t.dim() 3 >>> t.to_sparse_csr() tensor(crow_indices=tensor([[0, 1, 3], [0, 1, 3]]), col_indices=tensor([[0, 0, 1], [0, 0, 1]]), values=tensor([[1., 2., 3.], [4., 5., 6.]]), size=(2, 2, 2), nnz=3, layout=torch.sparse_csr) Dense dimensions: On the other hand, some data such as Graph embeddings might be better viewed as sparse collections of vectors instead of scalars. In this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension from a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is not stored. If however any of the values in the row are non-zero, they are stored entirely. This reduces the number of indices since", "prev_chunk_id": "chunk_1495", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1497", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Functionality overview#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Functionality overview#", "content": "we need one index one per row instead of one per element. But it also increases the amount of storage for the values. Since only rows that are entirely zero can be emitted and the presence of any non-zero valued elements cause the entire row to be stored. >>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]]) >>> t.to_sparse(sparse_dim=2) tensor(indices=tensor([[0, 1], [1, 1]]), values=tensor([[1., 2.], [3., 4.]]), size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)", "prev_chunk_id": "chunk_1496", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1498", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Operator overview#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Operator overview#", "content": "Operator overview# Fundamentally, operations on Tensor with sparse storage formats behave the same as operations on Tensor with strided (or other) storage formats. The particularities of storage, that is the physical layout of the data, influences the performance of an operation but should not influence the semantics. We are actively increasing operator coverage for sparse tensors. Users should not expect support same level of support as for dense Tensors yet. See our operator documentation for a list. >>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]]) >>> b_s = b.to_sparse_csr() >>> b_s.cos() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> RuntimeError: unsupported tensor layout: SparseCsr >>> b_s.sin() tensor(crow_indices=tensor([0, 3, 6]), col_indices=tensor([2, 3, 4, 0, 1, 3]), values=tensor([ 0.8415, 0.9093, 0.1411, -0.7568, -0.9589, -0.2794]), size=(2, 6), nnz=6, layout=torch.sparse_csr) As shown in the example above, we don’t support non-zero preserving unary operators such as cos. The output of a non-zero preserving unary operation will not be able to take advantage of sparse storage formats to the same extent as the input and potentially result in a catastrophic increase in memory. We instead rely on the user to explicitly convert to a dense Tensor first and then run the operation. >>> b_s.to_dense().cos() tensor([[ 1.0000, -0.4161], [-0.9900, 1.0000]]) We are aware that some users want to ignore compressed zeros for operations such as cos instead of preserving the exact semantics of the operation. For this we can point to torch.masked and its MaskedTensor, which is in turn also backed and powered by sparse storage formats and kernels. Also note that, for now, the user doesn’t have a choice of the output layout. For example, adding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some users might prefer for this to", "prev_chunk_id": "chunk_1497", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1499", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Operator overview#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Operator overview#", "content": "stay a sparse layout, because they know the result will still be sufficiently sparse. >>> a + b.to_sparse() tensor([[0., 3.], [3., 0.]]) We acknowledge that access to kernels that can efficiently produce different output layouts can be very useful. A subsequent operation might significantly benefit from receiving a particular layout. We are working on an API to control the result layout and recognize it is an important feature to plan a more optimal path of execution for any given model.", "prev_chunk_id": "chunk_1498", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1500", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse Semi-Structured Tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse Semi-Structured Tensors#", "content": "Sparse Semi-Structured Tensors# Semi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA’s Ampere architecture. It is also referred to as fine-grained structured sparsity or 2:4 structured sparsity. This sparse layout stores n elements out of every 2n elements, with n being determined by the width of the Tensor’s data type (dtype). The most frequently used dtype is float16, where n=2, thus the term “2:4 structured sparsity.” Semi-structured sparsity is explained in greater detail in this NVIDIA blog post. In PyTorch, semi-structured sparsity is implemented via a Tensor subclass. By subclassing, we can override __torch_dispatch__ , allowing us to use faster sparse kernels when performing matrix multiplication. We can also store the tensor in it’s compressed form inside the subclass to reduce memory overhead. In this compressed form, the sparse tensor is stored by retaining only the specified elements and some metadata, which encodes the mask. For 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element. Here, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor. Let (r, c) = tensor.shape and e = bitwidth(tensor.dtype), so e = 16 for torch.float16 and torch.bfloat16 and e = 8 for torch.int8. Using these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation. This gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype. By using this formula, we find that the compression ratio is 56.25% for torch.float16 or torch.bfloat16, and 62.5% for torch.int8.", "prev_chunk_id": "chunk_1499", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1501", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Constructing Sparse Semi-Structured Tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Constructing Sparse Semi-Structured Tensors#", "content": "Constructing Sparse Semi-Structured Tensors# You can transform a dense tensor into a sparse semi-structured tensor by simply using the torch.to_sparse_semi_structured function. Please also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs. The following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor. To construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format. To do this we tile a small 1x4 strip to create a 16x16 dense float16 tensor. Afterwards, we can call to_sparse_semi_structured function to compress it for accelerated inference. >>> from torch.sparse import to_sparse_semi_structured >>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda() tensor([[0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], ..., [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16) >>> A_sparse = to_sparse_semi_structured(A) SparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], ..., [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370]], device='cuda:0', dtype=torch.int16))", "prev_chunk_id": "chunk_1500", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1502", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse Semi-Structured Tensor Operations#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse Semi-Structured Tensor Operations#", "content": "Sparse Semi-Structured Tensor Operations# Currently, the following operations are supported for semi-structured sparse tensors: - torch.addmm(bias, dense, sparse.t()) - torch.mm(dense, sparse) - torch.mm(sparse, dense) - aten.linear.default(dense, sparse, bias) - aten.t.default(sparse) - aten.t.detach(sparse) To use these ops, simply pass the output of to_sparse_semi_structured(tensor) instead of using tensor once your tensor has 0s in a semi-structured sparse format, like this: >>> a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda() >>> b = torch.rand(64, 64).half().cuda() >>> c = torch.mm(a, b) >>> a_sparse = to_sparse_semi_structured(a) >>> torch.allclose(c, torch.mm(a_sparse, b)) True", "prev_chunk_id": "chunk_1501", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1503", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Accelerating nn.Linear with semi-structured sparsity#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Accelerating nn.Linear with semi-structured sparsity#", "content": "Accelerating nn.Linear with semi-structured sparsity# You can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code: >>> input = torch.rand(64, 64).half().cuda() >>> mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool() >>> linear = nn.Linear(64, 64).half().cuda() >>> linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))", "prev_chunk_id": "chunk_1502", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1504", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse COO tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse COO tensors#", "content": "Sparse COO tensors# PyTorch implements the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular, where ndim is the dimensionality of the tensor and nse is the number of specified elements.", "prev_chunk_id": "chunk_1503", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1505", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction#", "content": "Construction# A sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function torch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write: >>> i = [[0, 1, 1], [2, 0, 2]] >>> v = [3, 4, 5] >>> s = torch.sparse_coo_tensor(i, v, (2, 3)) >>> s tensor(indices=tensor([[0, 1, 1], [2, 0, 2]]), values=tensor([3, 4, 5]), size=(2, 3), nnz=3, layout=torch.sparse_coo) >>> s.to_dense() tensor([[0, 0, 3], [4, 0, 5]]) Note that the input i is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor: >>> i = [[0, 2], [1, 0], [1, 2]] >>> v = [3, 4, 5 ] >>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3)) >>> # Or another equivalent formulation to get s >>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3)) >>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense() tensor([[0, 0, 3], [4, 0, 5]]) An empty sparse COO tensor can be constructed by specifying its size only: >>> torch.sparse_coo_tensor(size=(2, 3)) tensor(indices=tensor([], size=(2, 0)), values=tensor([], size=(0,)), size=(2, 3), nnz=0, layout=torch.sparse_coo)", "prev_chunk_id": "chunk_1504", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1506", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse hybrid COO tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse hybrid COO tensors#", "content": "Sparse hybrid COO tensors# PyTorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional tensor so that we have: Suppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write >>> i = [[0, 1, 1], [2, 0, 2]] >>> v = [[3, 4], [5, 6], [7, 8]] >>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2)) >>> s tensor(indices=tensor([[0, 1, 1], [2, 0, 2]]), values=tensor([[3, 4], [5, 6], [7, 8]]), size=(2, 3, 2), nnz=3, layout=torch.sparse_coo) >>> s.to_dense() tensor([[[0, 0], [0, 0], [3, 4]], [[5, 6], [0, 0], [7, 8]]]) In general, if s is a sparse COO tensor and M = s.sparse_dim(), K = s.dense_dim(), then we have the following invariants:", "prev_chunk_id": "chunk_1505", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1507", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Uncoalesced sparse COO tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Uncoalesced sparse COO tensors#", "content": "Uncoalesced sparse COO tensors# PyTorch sparse COO tensor format permits sparse uncoalesced tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, 3 and 4, for the same index 1, that leads to an 1-D uncoalesced tensor: >>> i = [[1, 1]] >>> v = [3, 4] >>> s=torch.sparse_coo_tensor(i, v, (3,)) >>> s tensor(indices=tensor([[1, 1]]), values=tensor( [3, 4]), size=(3,), nnz=2, layout=torch.sparse_coo) while the coalescing process will accumulate the multi-valued elements into a single value using summation: >>> s.coalesce() tensor(indices=tensor([[1]]), values=tensor([7]), size=(3,), nnz=1, layout=torch.sparse_coo) In general, the output of torch.Tensor.coalesce() method is a sparse tensor with the following properties: - the indices of specified tensor elements are unique, - the indices are sorted in lexicographical order, - torch.Tensor.is_coalesced()returnsTrue.", "prev_chunk_id": "chunk_1506", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1508", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Working with sparse COO tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Working with sparse COO tensors#", "content": "Working with sparse COO tensors# Let’s consider the following example: >>> i = [[0, 1, 1], [2, 0, 2]] >>> v = [[3, 4], [5, 6], [7, 8]] >>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2)) As mentioned above, a sparse COO tensor is a torch.Tensor instance and to distinguish it from the Tensor instances that use some other layout, one can use torch.Tensor.is_sparse or torch.Tensor.layout properties: >>> isinstance(s, torch.Tensor) True >>> s.is_sparse True >>> s.layout == torch.sparse_coo True The number of sparse and dense dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim(), respectively. For instance: >>> s.sparse_dim(), s.dense_dim() (2, 1) If s is a sparse COO tensor then its COO format data can be acquired using methods torch.Tensor.indices() and torch.Tensor.values(). Constructing a new sparse COO tensor results a tensor that is not coalesced: >>> s.is_coalesced() False but one can construct a coalesced copy of a sparse COO tensor using the torch.Tensor.coalesce() method: >>> s2 = s.coalesce() >>> s2.indices() tensor([[0, 1, 1], [2, 0, 2]]) When working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on a sparse uncoalesced tensor could be implemented by multiplying all the uncoalesced values with the scalar because c * (a + b) == c * a + c * b holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not hold in general. Slicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions: >>>", "prev_chunk_id": "chunk_1507", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1509", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Working with sparse COO tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Working with sparse COO tensors#", "content": "s[1] tensor(indices=tensor([[0, 2]]), values=tensor([[5, 6], [7, 8]]), size=(3, 2), nnz=2, layout=torch.sparse_coo) >>> s[1, 0, 1] tensor(6) >>> s[1, 0, 1:] tensor([6]) In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, torch.sparse.softmax() computes the softmax with the assumption that the fill value is negative infinity.", "prev_chunk_id": "chunk_1508", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1510", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse Compressed Tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse Compressed Tensors#", "content": "Sparse Compressed Tensors# Sparse Compressed Tensors represents a class of sparse tensors that have a common feature of compressing the indices of a certain dimension using an encoding that enables certain optimizations on linear algebra kernels of sparse compressed tensors. This encoding is based on the Compressed Sparse Row (CSR) format that PyTorch sparse compressed tensors extend with the support of sparse tensor batches, allowing multi-dimensional tensor values, and storing sparse tensor values in dense blocks.", "prev_chunk_id": "chunk_1509", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1511", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse CSR Tensor#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse CSR Tensor#", "content": "Sparse CSR Tensor# The primary advantage of the CSR format over the COO format is better use of storage and much faster computation operations such as sparse matrix-vector multiplication using MKL and MAGMA backends. In the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor consists of three 1-D tensors: crow_indices, col_indices and values: In the general case, the (B + 2 + K)-dimensional sparse CSR tensor consists of two (B + 1)-dimensional index tensors crow_indices and col_indices, and of (1 + K)-dimensional values tensor such that while the shape of the sparse CSR tensor is (*batchsize, nrows, ncols, *densesize) where len(batchsize) == B and len(densesize) == K.", "prev_chunk_id": "chunk_1510", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1512", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction of CSR tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction of CSR tensors#", "content": "Construction of CSR tensors# Sparse CSR tensors can be directly constructed by using the torch.sparse_csr_tensor() function. The user must supply the row and column indices and values tensors separately where the row indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the crow_indices and col_indices if it is not present. >>> crow_indices = torch.tensor([0, 2, 4]) >>> col_indices = torch.tensor([0, 1, 0, 1]) >>> values = torch.tensor([1, 2, 3, 4]) >>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64) >>> csr tensor(crow_indices=tensor([0, 2, 4]), col_indices=tensor([0, 1, 0, 1]), values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, dtype=torch.float64) >>> csr.to_dense() tensor([[1., 2.], [3., 4.]], dtype=torch.float64) The simplest way of constructing a 2-D sparse CSR tensor from a strided or sparse COO tensor is to use torch.Tensor.to_sparse_csr() method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor: >>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64) >>> sp = a.to_sparse_csr() >>> sp tensor(crow_indices=tensor([0, 1, 3, 3]), col_indices=tensor([2, 0, 1]), values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)", "prev_chunk_id": "chunk_1511", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1513", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "CSR Tensor Operations#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "CSR Tensor Operations#", "content": "CSR Tensor Operations# The sparse matrix-vector multiplication can be performed with the tensor.matmul() method. This is currently the only math operation supported on CSR tensors. >>> vec = torch.randn(4, 1, dtype=torch.float64) >>> sp.matmul(vec) tensor([[0.9078], [1.3180], [0.0000]], dtype=torch.float64)", "prev_chunk_id": "chunk_1512", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1514", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse CSC Tensor#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse CSC Tensor#", "content": "Sparse CSC Tensor# The sparse CSC (Compressed Sparse Column) tensor format implements the CSC format for storage of 2 dimensional tensors with an extension to supporting batches of sparse CSC tensors and values being multi-dimensional tensors. Similarly to sparse CSR tensors, a sparse CSC tensor consists of three tensors: ccol_indices, row_indices and values:", "prev_chunk_id": "chunk_1513", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1515", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction of CSC tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction of CSC tensors#", "content": "Construction of CSC tensors# Sparse CSC tensors can be directly constructed by using the torch.sparse_csc_tensor() function. The user must supply the row and column indices and values tensors separately where the column indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the row_indices and ccol_indices tensors if it is not present. >>> ccol_indices = torch.tensor([0, 2, 4]) >>> row_indices = torch.tensor([0, 1, 0, 1]) >>> values = torch.tensor([1, 2, 3, 4]) >>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64) >>> csc tensor(ccol_indices=tensor([0, 2, 4]), row_indices=tensor([0, 1, 0, 1]), values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4, dtype=torch.float64, layout=torch.sparse_csc) >>> csc.to_dense() tensor([[1., 3.], [2., 4.]], dtype=torch.float64) The (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from any two-dimensional tensor using torch.Tensor.to_sparse_csc() method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor: >>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64) >>> sp = a.to_sparse_csc() >>> sp tensor(ccol_indices=tensor([0, 1, 2, 3, 3]), row_indices=tensor([1, 1, 0]), values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64, layout=torch.sparse_csc)", "prev_chunk_id": "chunk_1514", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1516", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse BSR Tensor#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse BSR Tensor#", "content": "Sparse BSR Tensor# The sparse BSR (Block compressed Sparse Row) tensor format implements the BSR format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSR tensors and values being blocks of multi-dimensional tensors. A sparse BSR tensor consists of three tensors: crow_indices, col_indices and values:", "prev_chunk_id": "chunk_1515", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1517", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction of BSR tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction of BSR tensors#", "content": "Construction of BSR tensors# Sparse BSR tensors can be directly constructed by using the torch.sparse_bsr_tensor() function. The user must supply the row and column block indices and values tensors separately where the row block indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the crow_indices and col_indices tensors if it is not present. >>> crow_indices = torch.tensor([0, 2, 4]) >>> col_indices = torch.tensor([0, 1, 0, 1]) >>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]], ... [[3, 4, 5], [9, 10, 11]], ... [[12, 13, 14], [18, 19, 20]], ... [[15, 16, 17], [21, 22, 23]]]) >>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64) >>> bsr tensor(crow_indices=tensor([0, 2, 4]), col_indices=tensor([0, 1, 0, 1]), values=tensor([[[ 0., 1., 2.], [ 6., 7., 8.]], [[ 3., 4., 5.], [ 9., 10., 11.]], [[12., 13., 14.], [18., 19., 20.]], [[15., 16., 17.], [21., 22., 23.]]]), size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr) >>> bsr.to_dense() tensor([[ 0., 1., 2., 3., 4., 5.], [ 6., 7., 8., 9., 10., 11.], [12., 13., 14., 15., 16., 17.], [18., 19., 20., 21., 22., 23.]], dtype=torch.float64) The (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from any two-dimensional tensor using torch.Tensor.to_sparse_bsr() method that also requires the specification of the values block size: >>> dense = torch.tensor([[0, 1, 2, 3, 4, 5], ... [6, 7, 8, 9, 10, 11], ... [12, 13, 14, 15, 16, 17], ... [18, 19, 20, 21, 22, 23]]) >>> bsr = dense.to_sparse_bsr(blocksize=(2, 3)) >>> bsr tensor(crow_indices=tensor([0, 2, 4]), col_indices=tensor([0, 1, 0, 1]), values=tensor([[[ 0, 1, 2], [ 6, 7, 8]], [[ 3, 4, 5], [ 9, 10, 11]], [[12, 13, 14], [18, 19, 20]], [[15, 16, 17], [21, 22, 23]]]), size=(4, 6), nnz=4, layout=torch.sparse_bsr)", "prev_chunk_id": "chunk_1516", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1518", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Sparse BSC Tensor#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Sparse BSC Tensor#", "content": "Sparse BSC Tensor# The sparse BSC (Block compressed Sparse Column) tensor format implements the BSC format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSC tensors and values being blocks of multi-dimensional tensors. A sparse BSC tensor consists of three tensors: ccol_indices, row_indices and values:", "prev_chunk_id": "chunk_1517", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1519", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction of BSC tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction of BSC tensors#", "content": "Construction of BSC tensors# Sparse BSC tensors can be directly constructed by using the torch.sparse_bsc_tensor() function. The user must supply the row and column block indices and values tensors separately where the column block indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the ccol_indices and row_indices tensors if it is not present. >>> ccol_indices = torch.tensor([0, 2, 4]) >>> row_indices = torch.tensor([0, 1, 0, 1]) >>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]], ... [[3, 4, 5], [9, 10, 11]], ... [[12, 13, 14], [18, 19, 20]], ... [[15, 16, 17], [21, 22, 23]]]) >>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64) >>> bsc tensor(ccol_indices=tensor([0, 2, 4]), row_indices=tensor([0, 1, 0, 1]), values=tensor([[[ 0., 1., 2.], [ 6., 7., 8.]], [[ 3., 4., 5.], [ 9., 10., 11.]], [[12., 13., 14.], [18., 19., 20.]], [[15., 16., 17.], [21., 22., 23.]]]), size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsc)", "prev_chunk_id": "chunk_1518", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1520", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Tools for working with sparse compressed tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Tools for working with sparse compressed tensors#", "content": "Tools for working with sparse compressed tensors# All sparse compressed tensors — CSR, CSC, BSR, and BSC tensors — are conceptually very similar in that their indices data is split into two parts: so-called compressed indices that use the CSR encoding, and so-called plain indices that are orthogonal to the compressed indices. This allows various tools on these tensors to share the same implementations that are parameterized by tensor layout.", "prev_chunk_id": "chunk_1519", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1521", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Construction of sparse compressed tensors#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Construction of sparse compressed tensors#", "content": "Construction of sparse compressed tensors# Sparse CSR, CSC, BSR, and CSC tensors can be constructed by using torch.sparse_compressed_tensor() function that have the same interface as the above discussed constructor functions torch.sparse_csr_tensor(), torch.sparse_csc_tensor(), torch.sparse_bsr_tensor(), and torch.sparse_bsc_tensor(), respectively, but with an extra required layout argument. The following example illustrates a method of constructing CSR and CSC tensors using the same input data by specifying the corresponding layout parameter to the torch.sparse_compressed_tensor() function: >>> compressed_indices = torch.tensor([0, 2, 4]) >>> plain_indices = torch.tensor([0, 1, 0, 1]) >>> values = torch.tensor([1, 2, 3, 4]) >>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr) >>> csr tensor(crow_indices=tensor([0, 2, 4]), col_indices=tensor([0, 1, 0, 1]), values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4, layout=torch.sparse_csr) >>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc) >>> csc tensor(ccol_indices=tensor([0, 2, 4]), row_indices=tensor([0, 1, 0, 1]), values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4, layout=torch.sparse_csc) >>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all() tensor(True)", "prev_chunk_id": "chunk_1520", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1522", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Linear Algebra operations#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Linear Algebra operations#", "content": "Linear Algebra operations# The following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here T[layout] denotes a tensor with a given layout. Similarly, M[layout] denotes a matrix (2-D PyTorch tensor), and V[layout] denotes a vector (1-D PyTorch tensor). In addition, f denotes a scalar (float or 0-D PyTorch tensor), * is element-wise multiplication, and @ is matrix multiplication. where “Sparse grad?” column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except torch.smm(), support backward with respect to strided matrix arguments.", "prev_chunk_id": "chunk_1521", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1523", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Tensor methods and sparse#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Tensor methods and sparse#", "content": "Tensor methods and sparse# The following Tensor methods are related to sparse tensors: The following Tensor methods are specific to sparse COO tensors: The following methods are specific to sparse CSR tensors and sparse BSR tensors: The following methods are specific to sparse CSC tensors and sparse BSC tensors: The following Tensor methods support sparse COO tensors: add() add_() addmm() addmm_() any() asin() asin_() arcsin() arcsin_() bmm() clone() deg2rad() deg2rad_() detach() detach_() dim() div() div_() floor_divide() floor_divide_() get_device() index_select() isnan() log1p() log1p_() mm() mul() mul_() mv() narrow_copy() neg() neg_() negative() negative_() numel() rad2deg() rad2deg_() resize_as_() size() pow() sqrt() square() smm() sspaddmm() sub() sub_() t() t_() transpose() transpose_() zero_()", "prev_chunk_id": "chunk_1522", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1524", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Other functions#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Other functions#", "content": "Other functions# The following torch functions support sparse tensors: cat() dstack() empty() empty_like() hstack() index_select() is_complex() is_floating_point() is_nonzero() is_same_size() is_signed() is_tensor() lobpcg() mm() native_norm() pca_lowrank() select() stack() svd_lowrank() unsqueeze() vstack() zeros() zeros_like() To manage checking sparse tensor invariants, see: To use sparse tensors with gradcheck() function, see:", "prev_chunk_id": "chunk_1523", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1525", "url": "https://docs.pytorch.org/docs/stable/sparse.html", "title": "Zero-preserving unary functions#", "page_title": "torch.sparse — PyTorch 2.8 documentation", "breadcrumbs": "Zero-preserving unary functions#", "content": "Zero-preserving unary functions# We aim to support all ‘zero-preserving unary functions’: functions of one argument that map zero to zero. If you find that we are missing a zero-preserving unary function that you need, please feel encouraged to open an issue for a feature request. As always please kindly try the search function first before opening an issue. The following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs. abs() asin() asinh() atan() atanh() ceil() conj_physical() floor() log1p() neg() round() sin() sinh() sign() sgn() signbit() tan() tanh() trunc() expm1() sqrt() angle() isinf() isposinf() isneginf() isnan() erf() erfinv()", "prev_chunk_id": "chunk_1524", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1526", "url": "https://docs.pytorch.org/docs/stable/size.html", "title": "torch.Size#", "page_title": "torch.Size — PyTorch 2.8 documentation", "breadcrumbs": "torch.Size#", "content": "torch.Size# Created On: Apr 19, 2024 | Last Updated On: Jun 18, 2025 torch.Size is the result type of a call to torch.Tensor.size(). It describes the size of all dimensions of the original tensor. As a subclass of tuple, it supports common sequence operations like indexing and length. Example: >>> x = torch.ones(10, 20, 30) >>> s = x.size() >>> s torch.Size([10, 20, 30]) >>> s[1] 20 >>> len(s) 3", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1527", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "torch.nested#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "torch.nested#", "content": "torch.nested# Created On: Mar 02, 2022 | Last Updated On: Jun 14, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1528", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Introduction#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Introduction#", "content": "Introduction# Nested tensors allow for ragged-shaped data to be contained within and operated upon as a single tensor. Such data is stored underneath in an efficient packed representation, while exposing a standard PyTorch tensor interface for applying operations. A common application of nested tensors is for expressing batches of variable-length sequential data present in various domains, such as varying sentence lengths, image sizes, and audio / video clip lengths. Traditionally, such data has been handled by padding sequences to that of the max length within a batch, performing computation on the padded form, and subsequently masking to remove padding. This is inefficient and error-prone, and nested tensors exist to address these problems. The API for calling operations on a nested tensor is no different from that of a regular torch.Tensor, allowing for seamless integration with existing models, with the main difference being construction of the inputs. As this is a prototype feature, the set of operations supported is limited, but growing. We welcome issues, feature requests, and contributions. More information on contributing can be found in this Readme.", "prev_chunk_id": "chunk_1527", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1529", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Construction#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Construction#", "content": "Construction# Construction is straightforward and involves passing a list of tensors to the torch.nested.nested_tensor constructor. A nested tensor with the torch.jagged layout (AKA an “NJT”) supports a single ragged dimension. This constructor will copy the input tensors into a packed, contiguous block of memory according to the layout described in the data_layout_ section below. >>> a, b = torch.arange(3), torch.arange(5) + 3 >>> a tensor([0, 1, 2]) >>> b tensor([3, 4, 5, 6, 7]) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> print([component for component in nt]) [tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])] Each tensor in the list must have the same number of dimensions, but the shapes can otherwise vary along a single dimension. If the dimensionalities of the input components don’t match, the constructor throws an error. >>> a = torch.randn(50, 128) # 2D tensor >>> b = torch.randn(2, 50, 128) # 3D tensor >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) ... RuntimeError: When constructing a nested tensor, all tensors in list must have the same dim During construction, dtype, device, and whether gradients are required can be chosen via the usual keyword arguments. >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device=\"cuda\", requires_grad=True) >>> print([component for component in nt]) [tensor([0., 1., 2.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>)] torch.nested.as_nested_tensor can be used to preserve autograd history from the tensors passed to the constructor. When this constructor is utilized, gradients will flow through the nested tensor back into the original components. Note that this constructor still copies the input components into a packed, contiguous block of memory. >>> a = torch.randn(12, 512, requires_grad=True) >>> b = torch.randn(23, 512, requires_grad=True) >>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt.sum().backward() >>> a.grad tensor([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1.,", "prev_chunk_id": "chunk_1528", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1530", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Construction#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Construction#", "content": "1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]]) >>> b.grad tensor([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]]) The above functions all create contiguous NJTs, where a chunk of memory is allocated to store a packed form of the underlying components (see the data_layout_ section below for more details). It is also possible to create a non-contiguous NJT view over a pre-existing dense tensor with padding, avoiding the memory allocation and copying. torch.nested.narrow() is the tool for accomplishing this. >>> padded = torch.randn(3, 5, 4) >>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64) >>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged) >>> nt.shape torch.Size([3, j1, 4]) >>> nt.is_contiguous() False Note that the nested tensor acts as a view over the original padded dense tensor, referencing the same memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more limited, so if you run into support gaps, it’s always possible to convert to a contiguous NJT using contiguous().", "prev_chunk_id": "chunk_1529", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1531", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Data Layout and Shape#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Data Layout and Shape#", "content": "Data Layout and Shape# For efficiency, nested tensors generally pack their tensor components into a contiguous chunk of memory and maintain additional metadata to specify batch item boundaries. For the torch.jagged layout, the contiguous chunk of memory is stored in the values component, with the offsets component delineating batch item boundaries for the ragged dimension. It’s possible to directly access the underlying NJT components when necessary. >>> a = torch.randn(50, 128) # text 1 >>> b = torch.randn(32, 128) # text 2 >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt.values().shape # note the \"packing\" of the ragged dimension; no padding needed torch.Size([82, 128]) >>> nt.offsets() tensor([ 0, 50, 82]) It can also be useful to construct an NJT from the jagged values and offsets constituents directly; the torch.nested.nested_tensor_from_jagged() constructor serves this purpose. >>> values = torch.randn(82, 128) >>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64) >>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets) An NJT has a well-defined shape with dimensionality 1 greater than that of its components. The underlying structure of the ragged dimension is represented by a symbolic value (j1 in the example below). >>> a = torch.randn(50, 128) >>> b = torch.randn(32, 128) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt.dim() 3 >>> nt.shape torch.Size([2, j1, 128]) NJTs must have the same ragged structure to be compatible with each other. For example, to run a binary operation involving two NJTs, the ragged structures must match (i.e. they must have the same ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact offsets tensor, so both NJTs must have the same offsets tensor to be compatible with each other. >>> a = torch.randn(50, 128) >>> b = torch.randn(32, 128) >>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt2 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt1.offsets()", "prev_chunk_id": "chunk_1530", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1532", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Data Layout and Shape#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Data Layout and Shape#", "content": "is nt2.offsets() False >>> nt3 = nt1 + nt2 RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128) In the above example, even though the conceptual shapes of the two NJTs are the same, they don’t share a reference to the same offsets tensor, so their shapes differ, and they are not compatible. We recognize that this behavior is unintuitive and are working hard to relax this restriction for the beta release of nested tensors. For a workaround, see the Troubleshooting section of this document. In addition to the offsets metadata, NJTs can also compute and cache the minimum and maximum sequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA). There are currently no public APIs for accessing these, but this will change for the beta release.", "prev_chunk_id": "chunk_1531", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1533", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Supported Operations#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Supported Operations#", "content": "Supported Operations# This section contains a list of common operations over nested tensors that you may find useful. It is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While a sizeable subset of these are supported for nested tensors today, full support is a large task. The ideal state for nested tensors is full support of all PyTorch operations that are available for non-nested tensors. To help us accomplish this, please consider: - Requesting particular ops needed for your use casehereto help us prioritize. - Contributing! It’s not too hard to add nested tensor support for a given PyTorch op; see theContributionssection below for details.", "prev_chunk_id": "chunk_1532", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1534", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Viewing nested tensor constituents#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Viewing nested tensor constituents#", "content": "Viewing nested tensor constituents# unbind() allows you to retrieve a view of the nested tensor’s constituents. >>> import torch >>> a = torch.randn(2, 3) >>> b = torch.randn(3, 3) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> nt.unbind() (tensor([[-0.9916, -0.3363, -0.2799], [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104, 1.4841], [ 2.0952, 0.2973, 0.2516], [ 0.9035, 1.3623, 0.2026]])) >>> nt.unbind()[0] is not a True >>> nt.unbind()[0].mul_(3) tensor([[ 3.6858, -3.7030, -4.4525], [-2.3481, 2.0236, 0.1975]]) >>> nt.unbind() (tensor([[-2.9747, -1.0089, -0.8396], [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104, 1.4841], [ 2.0952, 0.2973, 0.2516], [ 0.9035, 1.3623, 0.2026]])) Note that nt.unbind()[0] is not a copy, but rather a slice of the underlying memory, which represents the first entry or constituent of the nested tensor.", "prev_chunk_id": "chunk_1533", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1535", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Conversions to / from padded#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Conversions to / from padded#", "content": "Conversions to / from padded# torch.nested.to_padded_tensor() converts an NJT to a padded dense tensor with the specified padding value. The ragged dimension will be padded out to the size of the maximum sequence length. >>> import torch >>> a = torch.randn(2, 3) >>> b = torch.randn(6, 3) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> padded = torch.nested.to_padded_tensor(nt, padding=4.2) >>> padded tensor([[[ 1.6107, 0.5723, 0.3913], [ 0.0700, -0.4954, 1.8663], [ 4.2000, 4.2000, 4.2000], [ 4.2000, 4.2000, 4.2000], [ 4.2000, 4.2000, 4.2000], [ 4.2000, 4.2000, 4.2000]], [[-0.0479, -0.7610, -0.3484], [ 1.1345, 1.0556, 0.3634], [-1.7122, -0.5921, 0.0540], [-0.5506, 0.7608, 2.0606], [ 1.5658, -1.1934, 0.3041], [ 0.1483, -1.1284, 0.6957]]]) This can be useful as an escape hatch to work around NJT support gaps, but ideally such conversions should be avoided when possible for optimal memory usage and performance, as the more efficient nested tensor layout does not materialize padding. The reverse conversion can be accomplished using torch.nested.narrow(), which applies ragged structure to a given dense tensor to produce an NJT. Note that by default, this operation does not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be useful to explicitly call contiguous() here if a contiguous NJT is desired. >>> padded = torch.randn(3, 5, 4) >>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64) >>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged) >>> nt.shape torch.Size([3, j1, 4]) >>> nt = nt.contiguous() >>> nt.shape torch.Size([3, j2, 4])", "prev_chunk_id": "chunk_1534", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1536", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Shape manipulations#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Shape manipulations#", "content": "Shape manipulations# Nested tensors support a wide array of operations for shape manipulation, including views. >>> a = torch.randn(2, 6) >>> b = torch.randn(4, 6) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> nt.shape torch.Size([2, j1, 6]) >>> nt.unsqueeze(-1).shape torch.Size([2, j1, 6, 1]) >>> nt.unflatten(-1, [2, 3]).shape torch.Size([2, j1, 2, 3]) >>> torch.cat([nt, nt], dim=2).shape torch.Size([2, j1, 12]) >>> torch.stack([nt, nt], dim=2).shape torch.Size([2, j1, 2, 6]) >>> nt.transpose(-1, -2).shape torch.Size([2, 6, j1])", "prev_chunk_id": "chunk_1535", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1537", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Attention mechanisms#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Attention mechanisms#", "content": "Attention mechanisms# As variable-length sequences are common inputs to attention mechanisms, nested tensors support important attention operators Scaled Dot Product Attention (SDPA) and FlexAttention. See here for usage examples of NJT with SDPA and here for usage examples of NJT with FlexAttention.", "prev_chunk_id": "chunk_1536", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1538", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Usage with torch.compile#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Usage with torch.compile#", "content": "Usage with torch.compile# NJTs are designed to be used with torch.compile() for optimal performance, and we always recommend utilizing torch.compile() with NJTs when possible. NJTs work out-of-the-box and graph-break-free both when passed as inputs to a compiled function or module OR when instantiated in-line within the function. >>> import torch >>> a = torch.randn(2, 3) >>> b = torch.randn(4, 3) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> def f(x): return x.sin() + 1 ... >>> compiled_f = torch.compile(f, fullgraph=True) >>> output = compiled_f(nt) >>> output.shape torch.Size([2, j1, 3]) >>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2. ... >>> compiled_g = torch.compile(g, fullgraph=True) >>> output2 = compiled_g(nt.values(), nt.offsets()) >>> output2.shape torch.Size([2, j1, 3]) Note that NJTs support Dynamic Shapes to avoid unnecessary recompiles with changing ragged structure. >>> a = torch.randn(2, 3) >>> b = torch.randn(4, 3) >>> c = torch.randn(5, 3) >>> d = torch.randn(6, 3) >>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged) >>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged) >>> def f(x): return x.sin() + 1 ... >>> compiled_f = torch.compile(f, fullgraph=True) >>> output1 = compiled_f(nt1) >>> output2 = compiled_f(nt2) # NB: No recompile needed even though ragged structure differs If you run into problems or arcane errors when utilizing NJT + torch.compile, please file a PyTorch issue. Full subclass support within torch.compile is a long-term effort and there may be some rough edges at this time.", "prev_chunk_id": "chunk_1537", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1539", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Troubleshooting#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Troubleshooting#", "content": "Troubleshooting# This section contains common errors that you may run into when utilizing nested tensors, alongside the reason for these errors and suggestions for how to address them.", "prev_chunk_id": "chunk_1538", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1540", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Unimplemented ops#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Unimplemented ops#", "content": "Unimplemented ops# This error is becoming rarer as nested tensor op support grows, but it’s still possible to hit it today given that there are a couple thousand ops within PyTorch. NotImplementedError: aten.view_as_real.default The error is straightforward; we haven’t gotten around to adding op support for this particular op yet. If you’d like, you can contribute an implementation yourself OR simply request that we add support for this op in a future PyTorch release.", "prev_chunk_id": "chunk_1539", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1541", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Ragged structure incompatibility#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Ragged structure incompatibility#", "content": "Ragged structure incompatibility# RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128) This error occurs when calling an op that operates over multiple NJTs with incompatible ragged structures. Currently, it is required that input NJTs have the exact same offsets constituent in order to have the same symbolic ragged structure symbol (e.g. j1). As a workaround for this situation, it is possible to construct NJTs from the values and offsets components directly. With both NJTs referencing the same offsets components, they are considered to have the same ragged structure and are thus compatible. >>> a = torch.randn(50, 128) >>> b = torch.randn(32, 128) >>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets()) >>> nt3 = nt1 + nt2 >>> nt3.shape torch.Size([2, j1, 128])", "prev_chunk_id": "chunk_1540", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1542", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Data dependent operation within torch.compile#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Data dependent operation within torch.compile#", "content": "Data dependent operation within torch.compile# torch._dynamo.exc.Unsupported: data dependent operator: aten._local_scalar_dense.default; to enable, set torch._dynamo.config.capture_scalar_outputs = True This error occurs when calling an op that does data-dependent operation within torch.compile; this commonly occurs for ops that need to examine the values of the NJT’s offsets to determine the output shape. For example: >>> a = torch.randn(50, 128) >>> b = torch.randn(32, 128) >>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32) >>> def f(nt): return nt.chunk(2, dim=0)[0] ... >>> compiled_f = torch.compile(f, fullgraph=True) >>> output = compiled_f(nt) In this example, calling chunk() on the batch dimension of the NJT requires examination of the NJT’s offsets data to delineate batch item boundaries within the packed ragged dimension. As a workaround, there are a couple torch.compile flags that can be set: >>> torch._dynamo.config.capture_dynamic_output_shape_ops = True >>> torch._dynamo.config.capture_scalar_outputs = True If, after setting these, you still see data-dependent operator errors, please file an issue with PyTorch. This area of torch.compile() is still in heavy development and certain aspects of NJT support may be incomplete.", "prev_chunk_id": "chunk_1541", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1543", "url": "https://docs.pytorch.org/docs/stable/nested.html", "title": "Contributions#", "page_title": "torch.nested — PyTorch 2.8 documentation", "breadcrumbs": "Contributions#", "content": "Contributions# If you’d like to contribute to nested tensor development, one of the most impactful ways to do so is to add nested tensor support for a currently-unsupported PyTorch op. This process generally consists of a couple simple steps: - Determine the name of the op to add; this should be something likeaten.view_as_real.default. The signature for this op can be found inaten/src/ATen/native/native_functions.yaml. - Register an op implementation intorch/nested/_internal/ops.py, following the pattern established there for other ops. Use the signature fromnative_functions.yamlfor schema validation. The most common way to implement an op is to unwrap the NJT into its constituents, redispatch the op on the underlying values buffer, and propagate the relevant NJT metadata (including offsets) to a new output NJT. If the output of the op is expected to have a different shape from the input, new offsets, etc. metadata must be computed. When an op is applied over the batch or ragged dimension, these tricks can help quickly get a working implementation: - Fornon-batchwiseoperation, anunbind()-based fallback should work. - For operation on the ragged dimension, consider converting to padded dense with a properly-selected padding value that won’t negatively bias the output, running the op, and converting back to NJT. Withintorch.compile, these conversions can be fused to avoid materializing the padded intermediate.", "prev_chunk_id": "chunk_1542", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1544", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "torch.masked#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "torch.masked#", "content": "torch.masked# Created On: Aug 15, 2022 | Last Updated On: Jun 17, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1545", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "Motivation#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "Motivation#", "content": "Motivation# MaskedTensor serves as an extension to torch.Tensor that provides the user with the ability to: - use any masked semantics (e.g. variable length tensors, nan* operators, etc.) - differentiate between 0 and NaN gradients - various sparse applications (see tutorial below) “Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla torch.Tensor class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock sparsity’s potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.", "prev_chunk_id": "chunk_1544", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1546", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "What is a MaskedTensor?#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "What is a MaskedTensor?#", "content": "What is a MaskedTensor?# A MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored. By way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray) and take the max: On top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they’d like during computation. There are already a number of existing tutorials that we’ve written to help users onboard, such as: - Overview – the place to start for new users, discusses how to use MaskedTensors and why they’re useful - Sparsity – MaskedTensor supports sparse COO and CSR data and mask Tensors - Adagrad sparse semantics – a practical example of how MaskedTensor can simplify sparse semantics and implementations - Advanced semantics – discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy’s MaskedArray, and reduction semantics", "prev_chunk_id": "chunk_1545", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1547", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "Unary Operators#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "Unary Operators#", "content": "Unary Operators# Unary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we’ll continue to mask out the data. The available unary operators are: The available inplace unary operators are all of the above except:", "prev_chunk_id": "chunk_1546", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1548", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "Binary Operators#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "Binary Operators#", "content": "Binary Operators# As you may have seen in the tutorial, MaskedTensor also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics. The available binary operators are: The available inplace binary operators are all of the above except:", "prev_chunk_id": "chunk_1547", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1549", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "Reductions#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "Reductions#", "content": "Reductions# The following reductions are available (with autograd support). For more information, the Overview tutorial details some examples of reductions, while the Advanced semantics tutorial has some further in-depth discussions about how we decided on certain reduction semantics.", "prev_chunk_id": "chunk_1548", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1550", "url": "https://docs.pytorch.org/docs/stable/masked.html", "title": "View and select functions#", "page_title": "torch.masked — PyTorch 2.8 documentation", "breadcrumbs": "View and select functions#", "content": "View and select functions# We’ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a MaskedTensor. For a quick example, consider select(): >>> data = torch.arange(12, dtype=torch.float).reshape(3, 4) >>> data tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) >>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]]) >>> mt = masked_tensor(data, mask) >>> data.select(0, 1) tensor([4., 5., 6., 7.]) >>> mask.select(0, 1) tensor([False, True, False, False]) >>> mt.select(0, 1) MaskedTensor( [ --, 5.0000, --, --] ) The following ops are currently supported:", "prev_chunk_id": "chunk_1549", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1551", "url": "https://docs.pytorch.org/docs/stable/random.html", "title": "torch.random#", "page_title": "torch.random — PyTorch 2.8 documentation", "breadcrumbs": "torch.random#", "content": "torch.random# Created On: Aug 07, 2019 | Last Updated On: Jun 18, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1552", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Distributed RPC Framework#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Distributed RPC Framework#", "content": "Distributed RPC Framework# Created On: Nov 14, 2019 | Last Updated On: Jun 18, 2025 The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1553", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Basics#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Basics#", "content": "Basics# The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs. - Remote Procedure Call (RPC)supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs:rpc_sync()(synchronous),rpc_async()(asynchronous), andremote()(asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. Theremote()API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case,rpc_sync()andrpc_async()are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future. - Remote Reference (RRef)serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is", "prev_chunk_id": "chunk_1552", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1554", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Basics#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Basics#", "content": "neither the creator (the caller ofremote()) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases. - Distributed Autogradstitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass. - Distributed Optimizer’s constructor takes aOptimizer()(e.g.,SGD(),Adagrad(), etc.) and a list of parameter RRefs, creates anOptimizer()instance on each distinct RRef owner, and updates parameters accordingly when runningstep(). When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor andstep()API.", "prev_chunk_id": "chunk_1553", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1555", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "RPC#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "RPC#", "content": "RPC# Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use init_rpc() which would initialize the RPC framework, RRef framework and distributed autograd. The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a Tensor as an argument or a return value, the destination worker will try to create a Tensor with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary. The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.", "prev_chunk_id": "chunk_1554", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1556", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Backends#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Backends#", "content": "Backends# The RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the init_rpc() function, by passing a certain value of the BackendType enum. Regardless of what backend is used, the rest of the RPC API won’t change. Each backend also defines its own subclass of the RpcBackendOptions class, an instance of which can also be passed to init_rpc() to configure the backend’s behavior.", "prev_chunk_id": "chunk_1555", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1557", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "TensorPipe Backend#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "TensorPipe Backend#", "content": "TensorPipe Backend# The TensorPipe agent, which is the default, leverages the TensorPipe library, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, …) and can automatically detect their availability and negotiate the best transport to use for each pipe. The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required. Example: import os from torch.distributed import rpc os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' rpc.init_rpc( \"worker1\", rank=0, world_size=2, rpc_backend_options=rpc.TensorPipeRpcBackendOptions( num_worker_threads=8, rpc_timeout=20 # 20 second timeout ) ) # omitting init_rpc invocation on worker2", "prev_chunk_id": "chunk_1556", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1558", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "RRef#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "RRef#", "content": "RRef# An RRef (Remote REFerence) is a reference to a value of some type T (e.g. Tensor) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to nn.Modules that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See Remote Reference Protocol for more details.", "prev_chunk_id": "chunk_1557", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1559", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "RemoteModule#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "RemoteModule#", "content": "RemoteModule# RemoteModule is an easy way to create an nn.Module remotely on a different process. The actual module resides on a remote host, but the local host has a handle to this module and invoke this module similar to a regular nn.Module. The invocation however incurs RPC calls to the remote end and can be performed asynchronously if needed via additional APIs supported by RemoteModule.", "prev_chunk_id": "chunk_1558", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1560", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Distributed Autograd Framework#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Autograd Framework#", "content": "Distributed Autograd Framework# This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see Distributed Autograd Design.", "prev_chunk_id": "chunk_1559", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1561", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Distributed Optimizer#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Optimizer#", "content": "Distributed Optimizer# See the torch.distributed.optim page for documentation on distributed optimizers.", "prev_chunk_id": "chunk_1560", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1562", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Design Notes#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Design Notes#", "content": "Design Notes# The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training. - Distributed Autograd Design The RRef design note covers the design of the RRef (Remote REFerence) protocol used to refer to values on remote workers by the framework. - Remote Reference Protocol", "prev_chunk_id": "chunk_1561", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1563", "url": "https://docs.pytorch.org/docs/stable/rpc.html", "title": "Tutorials#", "page_title": "Distributed RPC Framework — PyTorch 2.8 documentation", "breadcrumbs": "Tutorials#", "content": "Tutorials# The RPC tutorials introduce users to the RPC framework, provide several example applications using torch.distributed.rpc APIs, and demonstrate how to use the profiler to profile RPC-based workloads. - Getting started with Distributed RPC Framework - Implementing a Parameter Server using Distributed RPC Framework - Combining Distributed DataParallel with Distributed RPC Framework(coversRemoteModuleas well) - Profiling RPC-based Workloads - Implementing batch RPC processing - Distributed Pipeline Parallel", "prev_chunk_id": "chunk_1562", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1564", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization#", "content": "Quantization# Created On: Oct 09, 2019 | Last Updated On: Jun 17, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1565", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Introduction to Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Introduction to Quantization#", "content": "Introduction to Quantization# Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. A quantized model executes some or all of the operations on tensors with reduced precision rather than full precision (floating point) values. This allows for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements. Hardware support for INT8 computations is typically 2 to 4 times faster compared to FP32 compute. Quantization is primarily a technique to speed up inference and only the forward pass is supported for quantized operators. PyTorch supports multiple approaches to quantizing a deep learning model. In most cases the model is trained in FP32 and then the model is converted to INT8. In addition, PyTorch also supports quantization aware training, which models quantization errors in both the forward and backward passes using fake-quantization modules. Note that the entire computation is carried out in floating point. At the end of quantization aware training, PyTorch provides conversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and perform operations with them. They can be used to directly construct models that perform all or part of the computation in lower precision. Higher-level APIs are provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss.", "prev_chunk_id": "chunk_1564", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1566", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization API Summary#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization API Summary#", "content": "Quantization API Summary# PyTorch provides three different modes of quantization: Eager Mode Quantization, FX Graph Mode Quantization (maintenance) and PyTorch 2 Export Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is an automated quantization workflow in PyTorch, and currently it’s a prototype feature, it is in maintenance mode since we have PyTorch 2 Export Quantization. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we’ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. PyTorch 2 Export Quantization is the new full graph mode quantization workflow, released as prototype feature in PyTorch 2.1. With PyTorch 2, we are moving to a better solution for full program capture (torch.export) since it can capture a higher percentage (88.8% on 14K models) of models compared to torch.fx.symbolic_trace (72.7% on 14K models), the program capture solution used by FX Graph Mode Quantization. torch.export still has limitations around some python constructs and requires user involvement to support dynamism in the exported model, but overall it is an improvement over the previous program capture solution. PyTorch 2 Export Quantization is", "prev_chunk_id": "chunk_1565", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1567", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization API Summary#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization API Summary#", "content": "built for models captured by torch.export, with flexibility and productivity of both modeling users and backend developers in mind. The main features are (1). Programmable API for configuring how a model is quantized that can scale to many more use cases (2). Simplified UX for modeling users and backend developers since they only need to interact with a single object (Quantizer) for expressing user’s intention about how to quantize a model and what the backend support. (3). Optional reference quantized model representation that can represent quantized computation with integer operations that maps closer to actual quantized computations that happens in hardware. New users of quantization are encouraged to try out PyTorch 2 Export Quantization first, if it does not work well, user can try eager mode quantization. The following table compares the differences between Eager Mode Quantization, FX Graph Mode Quantization and PyTorch 2 Export Quantization: There are three types of quantization supported: - dynamic quantization (weights quantized with activations read/stored in floating point and quantized for compute) - static quantization (weights quantized, activations quantized, calibration required post training) - static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training) Please see our Introduction to Quantization on PyTorch blog post for a more comprehensive overview of the tradeoffs between these quantization types. Operator coverage varies between dynamic and static quantization and is captured in the table below.", "prev_chunk_id": "chunk_1566", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1568", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Eager Mode Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Eager Mode Quantization#", "content": "Eager Mode Quantization# For a general introduction to the quantization flow, including different types of quantization, please take a look at General Quantization Flow.", "prev_chunk_id": "chunk_1567", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1569", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Post Training Dynamic Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Post Training Dynamic Quantization#", "content": "Post Training Dynamic Quantization# This is the simplest to apply form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference. This is used for situations where the model execution time is dominated by loading weights from memory rather than computing the matrix multiplications. This is true for LSTM and Transformer type models with small batch size. Diagram: # original model # all tensors and computations are in floating point previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 / linear_weight_fp32 # dynamically quantized model # linear and LSTM weights are in int8 previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32 / linear_weight_int8 PTDQ API Example: import torch # define a floating point model class M(torch.nn.Module): def __init__(self): super().__init__() self.fc = torch.nn.Linear(4, 4) def forward(self, x): x = self.fc(x) return x # create a model instance model_fp32 = M() # create a quantized model instance model_int8 = torch.ao.quantization.quantize_dynamic( model_fp32, # the original model {torch.nn.Linear}, # a set of layers to dynamically quantize dtype=torch.qint8) # the target dtype for quantized weights # run the model input_fp32 = torch.randn(4, 4, 4, 4) res = model_int8(input_fp32) To learn more about dynamic quantization please see our dynamic quantization tutorial.", "prev_chunk_id": "chunk_1568", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1570", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Post Training Static Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Post Training Static Quantization#", "content": "Post Training Static Quantization# Post Training Static Quantization (PTQ static) quantizes the weights and activations of the model. It fuses activations into preceding layers where possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Static Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case. We may need to modify the model before applying post training static quantization. Please see Model Preparation for Eager Mode Static Quantization. Diagram: # original model # all tensors and computations are in floating point previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 / linear_weight_fp32 # statically quantized model # weights and activations are in int8 previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 / linear_weight_int8 PTSQ API Example: import torch # define a floating point model where some layers could be statically quantized class M(torch.nn.Module): def __init__(self): super().__init__() # QuantStub converts tensors from floating point to quantized self.quant = torch.ao.quantization.QuantStub() self.conv = torch.nn.Conv2d(1, 1, 1) self.relu = torch.nn.ReLU() # DeQuantStub converts tensors from quantized to floating point self.dequant = torch.ao.quantization.DeQuantStub() def forward(self, x): # manually specify where tensors will be converted from floating # point to quantized in the quantized model x = self.quant(x) x = self.conv(x) x = self.relu(x) # manually specify where tensors will be converted from quantized # to floating point in the quantized model x = self.dequant(x) return x # create a model instance model_fp32 = M() # model must be set to eval mode for static quantization logic to work model_fp32.eval() # attach a global qconfig, which contains information about what kind # of observers to attach. Use 'x86' for server inference and 'qnnpack' # for mobile inference. Other quantization configurations such as selecting # symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques", "prev_chunk_id": "chunk_1569", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1571", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Post Training Static Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Post Training Static Quantization#", "content": "# can be specified here. # Note: the old 'fbgemm' is still available but 'x86' is the recommended default # for server inference. # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm') model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86') # Fuse the activations to preceding layers, where applicable. # This needs to be done manually depending on the model architecture. # Common fusions include `conv + relu` and `conv + batchnorm + relu` model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']]) # Prepare the model for static quantization. This inserts observers in # the model that will observe activation tensors during calibration. model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused) # calibrate the prepared model to determine quantization parameters for activations # in a real world setting, the calibration would be done with a representative dataset input_fp32 = torch.randn(4, 1, 4, 4) model_fp32_prepared(input_fp32) # Convert the observed model to a quantized model. This does several things: # quantizes the weights, computes and stores the scale and bias value to be # used with each activation tensor, and replaces key operators with quantized # implementations. model_int8 = torch.ao.quantization.convert(model_fp32_prepared) # run the model, relevant calculations will happen in int8 res = model_int8(input_fp32) To learn more about static quantization, please see the static quantization tutorial.", "prev_chunk_id": "chunk_1570", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1572", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Aware Training for Static Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Aware Training for Static Quantization#", "content": "Quantization Aware Training for Static Quantization# Quantization Aware Training (QAT) models the effects of quantization during training allowing for higher accuracy compared to other quantization methods. We can do QAT for static, dynamic or weight only quantization. During training, all calculations are done in floating point, with fake_quant modules modeling the effects of quantization by clamping and rounding to simulate the effects of INT8. After model conversion, weights and activations are quantized, and activations are fused into the preceding layer where possible. It is commonly used with CNNs and yields a higher accuracy compared to static quantization. We may need to modify the model before applying post training static quantization. Please see Model Preparation for Eager Mode Static Quantization. Diagram: # original model # all tensors and computations are in floating point previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 / linear_weight_fp32 # model with fake_quants for modeling quantization numerics during training previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32 / linear_weight_fp32 -- fq # quantized model # weights and activations are in int8 previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 / linear_weight_int8 QAT API Example: import torch # define a floating point model where some layers could benefit from QAT class M(torch.nn.Module): def __init__(self): super().__init__() # QuantStub converts tensors from floating point to quantized self.quant = torch.ao.quantization.QuantStub() self.conv = torch.nn.Conv2d(1, 1, 1) self.bn = torch.nn.BatchNorm2d(1) self.relu = torch.nn.ReLU() # DeQuantStub converts tensors from quantized to floating point self.dequant = torch.ao.quantization.DeQuantStub() def forward(self, x): x = self.quant(x) x = self.conv(x) x = self.bn(x) x = self.relu(x) x = self.dequant(x) return x # create a model instance model_fp32 = M() # model must be set to eval for fusion to work model_fp32.eval() # attach a global qconfig, which contains information about what kind # of observers to attach. Use 'x86' for", "prev_chunk_id": "chunk_1571", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1573", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Aware Training for Static Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Aware Training for Static Quantization#", "content": "server inference and 'qnnpack' # for mobile inference. Other quantization configurations such as selecting # symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques # can be specified here. # Note: the old 'fbgemm' is still available but 'x86' is the recommended default # for server inference. # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm') model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86') # fuse the activations to preceding layers, where applicable # this needs to be done manually depending on the model architecture model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'bn', 'relu']]) # Prepare the model for QAT. This inserts observers and fake_quants in # the model needs to be set to train for QAT logic to work # the model that will observe weight and activation tensors during calibration. model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train()) # run the training loop (not shown) training_loop(model_fp32_prepared) # Convert the observed model to a quantized model. This does several things: # quantizes the weights, computes and stores the scale and bias value to be # used with each activation tensor, fuses modules where appropriate, # and replaces key operators with quantized implementations. model_fp32_prepared.eval() model_int8 = torch.ao.quantization.convert(model_fp32_prepared) # run the model, relevant calculations will happen in int8 res = model_int8(input_fp32) To learn more about quantization aware training, please see the QAT tutorial.", "prev_chunk_id": "chunk_1572", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1574", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Model Preparation for Eager Mode Static Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Model Preparation for Eager Mode Static Quantization#", "content": "Model Preparation for Eager Mode Static Quantization# It is necessary to currently make some modifications to the model definition prior to Eager mode quantization. This is because currently quantization works on a module by module basis. Specifically, for all quantization techniques, the user needs to: - Convert any operations that require output requantization (and thus have additional parameters) from functionals to module form (for example, usingtorch.nn.ReLUinstead oftorch.nn.functional.relu). - Specify which parts of the model need to be quantized either by assigning.qconfigattributes on submodules or by specifyingqconfig_mapping. For example, settingmodel.conv1.qconfig=Nonemeans that themodel.convlayer will not be quantized, and settingmodel.linear1.qconfig=custom_qconfigmeans that the quantization settings formodel.linear1will be usingcustom_qconfiginstead of the global qconfig. For static quantization techniques which quantize activations, the user needs to do the following in addition: - Specify where activations are quantized and de-quantized. This is done usingQuantStubandDeQuantStubmodules. - UseFloatFunctionalto wrap tensor operations that require special handling for quantization into modules. Examples are operations likeaddandcatwhich require special handling to determine output quantization parameters. - Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using thefuse_modules()API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]", "prev_chunk_id": "chunk_1573", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1575", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "(Prototype - maintenance mode) FX Graph Mode Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "(Prototype - maintenance mode) FX Graph Mode Quantization#", "content": "(Prototype - maintenance mode) FX Graph Mode Quantization# There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_mapping (an argument of the prepare_fx function). FXPTQ API Example: import torch from torch.ao.quantization import ( get_default_qconfig_mapping, get_default_qat_qconfig_mapping, QConfigMapping, ) import torch.ao.quantization.quantize_fx as quantize_fx import copy model_fp = UserModel() # # post training dynamic/weight_only quantization # # we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model model_to_quantize = copy.deepcopy(model_fp) model_to_quantize.eval() qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig) # a tuple of one or more example inputs are needed to trace the model example_inputs = (input_fp32) # prepare model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs) # no calibration needed when we only have dynamic/weight_only quantization # quantize model_quantized = quantize_fx.convert_fx(model_prepared) # # post training static quantization # model_to_quantize = copy.deepcopy(model_fp) qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\") model_to_quantize.eval() # prepare model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs) # calibrate (not shown) # quantize model_quantized = quantize_fx.convert_fx(model_prepared) # # quantization aware training for static quantization # model_to_quantize = copy.deepcopy(model_fp) qconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\") model_to_quantize.train() # prepare model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs) # training loop (not shown) # quantize model_quantized = quantize_fx.convert_fx(model_prepared) # # fusion # model_to_quantize = copy.deepcopy(model_fp) model_fused = quantize_fx.fuse_fx(model_to_quantize) Please follow the tutorials below to learn more about FX Graph Mode Quantization: - User Guide on Using FX Graph Mode Quantization - FX Graph Mode Post Training Static Quantization - FX Graph Mode Post Training Dynamic Quantization", "prev_chunk_id": "chunk_1574", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1576", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "(Prototype) PyTorch 2 Export Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "(Prototype) PyTorch 2 Export Quantization#", "content": "(Prototype) PyTorch 2 Export Quantization# API Example: import torch from torch.ao.quantization.quantize_pt2e import prepare_pt2e from torch.export import export_for_training from torch.ao.quantization.quantizer import ( XNNPACKQuantizer, get_symmetric_quantization_config, ) class M(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(5, 10) def forward(self, x): return self.linear(x) # initialize a floating point model float_model = M().eval() # define calibration function def calibrate(model, data_loader): model.eval() with torch.no_grad(): for image, target in data_loader: model(image) # Step 1. program capture # NOTE: this API will be updated to torch.export API in the future, but the captured # result should mostly stay the same m = export_for_training(m, *example_inputs).module() # we get a model with aten ops # Step 2. quantization # backend developer will write their own Quantizer and expose methods to allow # users to express how they # want the model to be quantized quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config()) # or prepare_qat_pt2e for Quantization Aware Training m = prepare_pt2e(m, quantizer) # run calibration # calibrate(m, sample_inference_data) m = convert_pt2e(m) # Step 3. lowering # lower to target backend Please follow these tutorials to get started on PyTorch 2 Export Quantization: Modeling Users: - PyTorch 2 Export Post Training Quantization - PyTorch 2 Export Post Training Quantization with X86 Backend through Inductor - PyTorch 2 Export Quantization Aware Training Backend Developers (please check out all Modeling Users docs as well): - How to Write a Quantizer for PyTorch 2 Export Quantization", "prev_chunk_id": "chunk_1575", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1577", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Stack#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Stack#", "content": "Quantization Stack# Quantization is the process to convert a floating point model to a quantized model. So at high level the quantization stack can be split into two parts: 1). The building blocks or abstractions for a quantized model 2). The building blocks or abstractions for the quantization flow that converts a floating point model to a quantized model", "prev_chunk_id": "chunk_1576", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1578", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantized Tensor#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Tensor#", "content": "Quantized Tensor# In order to do quantization in PyTorch, we need to be able to represent quantized data in Tensors. A Quantized Tensor allows for storing quantized data (represented as int8/uint8/int32) along with quantization parameters like scale and zero_point. Quantized Tensors allow for many useful operations making quantized arithmetic easy, in addition to allowing for serialization of data in a quantized format. PyTorch supports both per tensor and per channel symmetric and asymmetric quantization. Per tensor means that all the values within the tensor are quantized the same way with the same quantization parameters. Per channel means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are quantized with different quantization parameters. This allows for less error in converting tensors to quantized values since outlier values would only impact the channel it was in, instead of the entire Tensor. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error after quantization, thereby ensuring that operations like padding do not cause additional quantization error. Here are a few key attributes for quantized Tensor: - QScheme (torch.qscheme): a enum that specifies the way we quantize the Tensortorch.per_tensor_affinetorch.per_tensor_symmetrictorch.per_channel_affinetorch.per_channel_symmetric - dtype (torch.dtype): data type of the quantized Tensortorch.quint8torch.qint8torch.qint32torch.float16 - quantization parameters (varies based on QScheme): parameters for the chosen way of quantizationtorch.per_tensor_affine would have quantization parameters ofscale (float)zero_point (int)torch.per_channel_affine would have quantization parameters ofper_channel_scales (list of float)per_channel_zero_points (list of int)axis (int)", "prev_chunk_id": "chunk_1577", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1579", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantize and Dequantize#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantize and Dequantize#", "content": "Quantize and Dequantize# The input and output of a model are floating point Tensors, but activations in the quantized model are quantized, so we need operators to convert between floating point and quantized Tensors. - Quantize (float -> quantized)torch.quantize_per_tensor(x, scale, zero_point, dtype)torch.quantize_per_channel(x, scales, zero_points, axis, dtype)torch.quantize_per_tensor_dynamic(x, dtype, reduce_range)to(torch.float16) - Dequantize (quantized -> float)quantized_tensor.dequantize() - calling dequantize on a torch.float16 Tensor will convert the Tensor back to torch.floattorch.dequantize(x)", "prev_chunk_id": "chunk_1578", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1580", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantized Operators/Modules#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Operators/Modules#", "content": "Quantized Operators/Modules# - Quantized Operator are the operators that takes quantized Tensor as inputs, and outputs a quantized Tensor. - Quantized Modules are PyTorch Modules that performs quantized operations. They are typically defined for weighted operations like linear and conv.", "prev_chunk_id": "chunk_1579", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1581", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantized Engine#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantized Engine#", "content": "Quantized Engine# When a quantized model is executed, the qengine (torch.backends.quantized.engine) specifies which backend is to be used for execution. It is important to ensure that the qengine is compatible with the quantized model in terms of value range of quantized activation and weights.", "prev_chunk_id": "chunk_1580", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1582", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Observer and FakeQuantize#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Observer and FakeQuantize#", "content": "Observer and FakeQuantize# - Observer are PyTorch Modules used to:collect tensor statistics like min value and max value of the Tensor passing through the observerand calculate quantization parameters based on the collected tensor statistics - FakeQuantize are PyTorch Modules used to:simulate quantization (performing quantize/dequantize) for a Tensor in the networkit can calculate quantization parameters based on the collected statistics from observer, or it can learn the quantization parameters as well", "prev_chunk_id": "chunk_1581", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1583", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "QConfig#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "QConfig#", "content": "QConfig# - QConfig is a namedtuple of Observer or FakeQuantize Module class that can are configurable with qscheme, dtype etc. it is used to configure how an operator should be observedQuantization configuration for an operator/moduledifferent types of Observer/FakeQuantizedtypeqschemequant_min/quant_max: can be used to simulate lower precision TensorsCurrently supports configuration for activation and weightWe insert input/weight/output observer based on the qconfig that is configured for a given operator or module", "prev_chunk_id": "chunk_1582", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1584", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "General Quantization Flow#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "General Quantization Flow#", "content": "General Quantization Flow# In general, the flow is the following - prepareinsert Observer/FakeQuantize modules based on user specified qconfig - calibrate/train (depending on post training quantization or quantization aware training)allow Observers to collect statistics or FakeQuantize modules to learn the quantization parameters - convertconvert a calibrated/trained model to a quantized model There are different modes of quantization, they can be classified in two ways: In terms of where we apply the quantization flow, we have: - Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) - Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And in terms of how we quantize the operators, we can have: - Weight Only Quantization (only weight is statically quantized) - Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) - Static Quantization (both weight and activations are statically quantized) We can mix different ways of quantizing operators in the same quantization flow. For example, we can have post training quantization that has both statically and dynamically quantized operators.", "prev_chunk_id": "chunk_1583", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1585", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Mode Support#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Mode Support#", "content": "Quantization Mode Support# Please see our Introduction to Quantization on Pytorch blog post for a more comprehensive overview of the tradeoffs between these quantization types.", "prev_chunk_id": "chunk_1584", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1586", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Flow Support#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Flow Support#", "content": "Quantization Flow Support# PyTorch provides two modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is an automated quantization framework in PyTorch, and currently it’s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we’ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization:", "prev_chunk_id": "chunk_1585", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1587", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Backend/Hardware Support#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Backend/Hardware Support#", "content": "Backend/Hardware Support# Today, PyTorch supports the following backends for running quantized operators efficiently: - x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations), viax86optimized byfbgemmandonednn(see the details atRFC) - ARM CPUs (typically found in mobile/embedded devices), viaqnnpack - (early prototype) support for NVidia GPU viaTensorRTthroughfx2trt(to be open sourced)", "prev_chunk_id": "chunk_1586", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1588", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Note for native CPU backends#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Note for native CPU backends#", "content": "Note for native CPU backends# We expose both x86 and qnnpack with the same native pytorch quantized operators, so we need additional flag to distinguish between them. The corresponding implementation of x86 and qnnpack is chosen automatically based on the PyTorch build mode, though users have the option to override this by setting torch.backends.quantization.engine to x86 or qnnpack. When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for quantized computations match the backend on which the model will be executed. The qconfig controls the type of observers used during the quantization passes. The qengine controls whether x86 or qnnpack specific packing function is used when packing weights for linear and convolution functions and modules. For example: Default settings for x86: # set the qconfig for PTQ # Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs qconfig = torch.ao.quantization.get_default_qconfig('x86') # or, set the qconfig for QAT qconfig = torch.ao.quantization.get_default_qat_qconfig('x86') # set the qengine to control weight packing torch.backends.quantized.engine = 'x86' Default settings for qnnpack: # set the qconfig for PTQ qconfig = torch.ao.quantization.get_default_qconfig('qnnpack') # or, set the qconfig for QAT qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack') # set the qengine to control weight packing torch.backends.quantized.engine = 'qnnpack'", "prev_chunk_id": "chunk_1587", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1589", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Operator Support#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Operator Support#", "content": "Operator Support# Operator coverage varies between dynamic and static quantization and is captured in the table below. Note that for FX Graph Mode Quantization, the corresponding functionals are also supported. Note: this will be updated with some information generated from native backend_config_dict soon.", "prev_chunk_id": "chunk_1588", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1590", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization API Reference#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization API Reference#", "content": "Quantization API Reference# The Quantization API Reference contains documentation of quantization APIs, such as quantization passes, quantized tensor operations, and supported quantized modules and functions.", "prev_chunk_id": "chunk_1589", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1591", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Backend Configuration#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Backend Configuration#", "content": "Quantization Backend Configuration# The Quantization Backend Configuration contains documentation on how to configure the quantization workflows for various backends.", "prev_chunk_id": "chunk_1590", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1592", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Accuracy Debugging#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Accuracy Debugging#", "content": "Quantization Accuracy Debugging# The Quantization Accuracy Debugging contains documentation on how to debug quantization accuracy.", "prev_chunk_id": "chunk_1591", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1593", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Customizations#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Customizations#", "content": "Quantization Customizations# While default implementations of observers to select the scale factor and bias based on observed tensor data are provided, developers can provide their own quantization functions. Quantization can be applied selectively to different parts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv1d(), conv2d(), conv3d() and linear(). Quantization workflows work by adding (e.g. adding observers as .observer submodule) or replacing (e.g. converting nn.Conv2d to nn.quantized.Conv2d) submodules in the model’s module hierarchy. It means that the model stays a regular nn.Module-based instance throughout the process and thus can work with the rest of PyTorch APIs.", "prev_chunk_id": "chunk_1592", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1594", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Custom Module API#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Custom Module API#", "content": "Quantization Custom Module API# Both Eager mode and FX graph mode quantization APIs provide a hook for the user to specify module quantized in a custom way, with user defined logic for observation and quantization. The user needs to specify: - The Python type of the source fp32 module (existing in the model) - The Python type of the observed module (provided by user). This module needs to define afrom_floatfunction which defines how the observed module is created from the original fp32 module. - The Python type of the quantized module (provided by user). This module needs to define afrom_observedfunction which defines how the quantized module is created from the observed module. - A configuration describing (1), (2), (3) above, passed to the quantization APIs. The framework will then do the following: - during thepreparemodule swaps, it will convert every module of type specified in (1) to the type specified in (2), using thefrom_floatfunction of the class in (2). - during theconvertmodule swaps, it will convert every module of type specified in (2) to the type specified in (3), using thefrom_observedfunction of the class in (3). Currently, there is a requirement that ObservedCustomModule will have a single Tensor output, and an observer will be added by the framework (not by the user) on that output. The observer will be stored under the activation_post_process key as an attribute of the custom module instance. Relaxing these restrictions may be done at a future time. Custom API Example: import torch import torch.ao.nn.quantized as nnq from torch.ao.quantization import QConfigMapping import torch.ao.quantization.quantize_fx # original fp32 module to replace class CustomModule(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(3, 3) def forward(self, x): return self.linear(x) # custom observed module, provided by user class ObservedCustomModule(torch.nn.Module): def __init__(self, linear): super().__init__() self.linear = linear def forward(self, x): return self.linear(x) @classmethod def", "prev_chunk_id": "chunk_1593", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1595", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Quantization Custom Module API#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Quantization Custom Module API#", "content": "from_float(cls, float_module): assert hasattr(float_module, 'qconfig') observed = cls(float_module.linear) observed.qconfig = float_module.qconfig return observed # custom quantized module, provided by user class StaticQuantCustomModule(torch.nn.Module): def __init__(self, linear): super().__init__() self.linear = linear def forward(self, x): return self.linear(x) @classmethod def from_observed(cls, observed_module): assert hasattr(observed_module, 'qconfig') assert hasattr(observed_module, 'activation_post_process') observed_module.linear.activation_post_process = \\ observed_module.activation_post_process quantized = cls(nnq.Linear.from_float(observed_module.linear)) return quantized # # example API call (Eager mode quantization) # m = torch.nn.Sequential(CustomModule()).eval() prepare_custom_config_dict = { \"float_to_observed_custom_module_class\": { CustomModule: ObservedCustomModule } } convert_custom_config_dict = { \"observed_to_quantized_custom_module_class\": { ObservedCustomModule: StaticQuantCustomModule } } m.qconfig = torch.ao.quantization.default_qconfig mp = torch.ao.quantization.prepare( m, prepare_custom_config_dict=prepare_custom_config_dict) # calibration (not shown) mq = torch.ao.quantization.convert( mp, convert_custom_config_dict=convert_custom_config_dict) # # example API call (FX graph mode quantization) # m = torch.nn.Sequential(CustomModule()).eval() qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig) prepare_custom_config_dict = { \"float_to_observed_custom_module_class\": { \"static\": { CustomModule: ObservedCustomModule, } } } convert_custom_config_dict = { \"observed_to_quantized_custom_module_class\": { \"static\": { ObservedCustomModule: StaticQuantCustomModule, } } } mp = torch.ao.quantization.quantize_fx.prepare_fx( m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict) # calibration (not shown) mq = torch.ao.quantization.quantize_fx.convert_fx( mp, convert_custom_config=convert_custom_config_dict)", "prev_chunk_id": "chunk_1594", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1596", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Best Practices#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Best Practices#", "content": "Best Practices# 1. If you are using the x86 backend, we need to use 7 bits instead of 8 bits. Make sure you reduce the range for the quant\\_min, quant\\_max, e.g. if dtype is torch.quint8, make sure to set a custom quant_min to be 0 and quant_max to be 127 (255 / 2) if dtype is torch.qint8, make sure to set a custom quant_min to be -64 (-128 / 2) and quant_max to be 63 (127 / 2), we already set this correctly if you call the torch.ao.quantization.get_default_qconfig(backend) or torch.ao.quantization.get_default_qat_qconfig(backend) function to get the default qconfig for x86 or qnnpack backend 2. If onednn backend is selected, 8 bits for activation will be used in the default qconfig mapping torch.ao.quantization.get_default_qconfig_mapping('onednn') and default qconfig torch.ao.quantization.get_default_qconfig('onednn'). It is recommended to be used on CPUs with Vector Neural Network Instruction (VNNI) support. Otherwise, setting reduce_range to True of the activation’s observer to get better accuracy on CPUs without VNNI support.", "prev_chunk_id": "chunk_1595", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1597", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Frequently Asked Questions#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# - How can I do quantized inference on GPU?:We don’t have official GPU support yet, but this is an area of active development, you can find more informationhere - Where can I get ONNX support for my quantized model?If you get errors exporting the model (using APIs undertorch.onnx), you may open an issue in the PyTorch repository. Prefix the issue title with[ONNX]and tag the issue asmodule:onnx.If you encounter issues with ONNX Runtime, open an issue atGitHub - microsoft/onnxruntime. - How can I use quantization with LSTM’s?:LSTM is supported through our custom module api in both eager mode and fx graph mode quantization. Examples can be found at Eager Mode:pytorch/test_quantized_op.py TestQuantizedOps.test_custom_module_lstmFX Graph Mode:pytorch/test_quantize_fx.py TestQuantizeFx.test_static_lstm", "prev_chunk_id": "chunk_1596", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1598", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Passing a non-quantized Tensor into a quantized kernel#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Passing a non-quantized Tensor into a quantized kernel#", "content": "Passing a non-quantized Tensor into a quantized kernel# If you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend... This means that you are trying to pass a non-quantized Tensor to a quantized kernel. A common workaround is to use torch.ao.quantization.QuantStub to quantize the tensor. This needs to be done manually in Eager mode quantization. An e2e example: class M(torch.nn.Module): def __init__(self): super().__init__() self.quant = torch.ao.quantization.QuantStub() self.conv = torch.nn.Conv2d(1, 1, 1) def forward(self, x): # during the convert step, this will be replaced with a # `quantize_per_tensor` call x = self.quant(x) x = self.conv(x) return x", "prev_chunk_id": "chunk_1597", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1599", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Passing a quantized Tensor into a non-quantized kernel#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Passing a quantized Tensor into a non-quantized kernel#", "content": "Passing a quantized Tensor into a non-quantized kernel# If you see an error similar to: RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This means that you are trying to pass a quantized Tensor to a non-quantized kernel. A common workaround is to use torch.ao.quantization.DeQuantStub to dequantize the tensor. This needs to be done manually in Eager mode quantization. An e2e example: class M(torch.nn.Module): def __init__(self): super().__init__() self.quant = torch.ao.quantization.QuantStub() self.conv1 = torch.nn.Conv2d(1, 1, 1) # this module will not be quantized (see `qconfig = None` logic below) self.conv2 = torch.nn.Conv2d(1, 1, 1) self.dequant = torch.ao.quantization.DeQuantStub() def forward(self, x): # during the convert step, this will be replaced with a # `quantize_per_tensor` call x = self.quant(x) x = self.conv1(x) # during the convert step, this will be replaced with a # `dequantize` call x = self.dequant(x) x = self.conv2(x) return x m = M() m.qconfig = some_qconfig # turn off quantization for conv2 m.conv2.qconfig = None", "prev_chunk_id": "chunk_1598", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1600", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Saving and Loading Quantized models#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Saving and Loading Quantized models#", "content": "Saving and Loading Quantized models# When calling torch.load on a quantized model, if you see an error like: AttributeError: 'LinearPackedParams' object has no attribute '_modules' This is because directly saving and loading a quantized model using torch.save and torch.load is not supported. To save/load quantized models, the following ways can be used: - Saving/Loading the quantized model state_dict An example: class M(torch.nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(5, 5) self.relu = nn.ReLU() def forward(self, x): x = self.linear(x) x = self.relu(x) return x m = M().eval() prepare_orig = prepare_fx(m, {'' : default_qconfig}) prepare_orig(torch.rand(5, 5)) quantized_orig = convert_fx(prepare_orig) # Save/load using state_dict b = io.BytesIO() torch.save(quantized_orig.state_dict(), b) m2 = M().eval() prepared = prepare_fx(m2, {'' : default_qconfig}) quantized = convert_fx(prepared) b.seek(0) quantized.load_state_dict(torch.load(b)) - Saving/Loading scripted quantized models usingtorch.jit.saveandtorch.jit.load An example: # Note: using the same model M from previous example m = M().eval() prepare_orig = prepare_fx(m, {'' : default_qconfig}) prepare_orig(torch.rand(5, 5)) quantized_orig = convert_fx(prepare_orig) # save/load using scripted model scripted = torch.jit.script(quantized_orig) b = io.BytesIO() torch.jit.save(scripted, b) b.seek(0) scripted_quantized = torch.jit.load(b)", "prev_chunk_id": "chunk_1599", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1601", "url": "https://docs.pytorch.org/docs/stable/quantization.html", "title": "Symbolic Trace Error when using FX Graph Mode Quantization#", "page_title": "Quantization — PyTorch 2.8 documentation", "breadcrumbs": "Symbolic Trace Error when using FX Graph Mode Quantization#", "content": "Symbolic Trace Error when using FX Graph Mode Quantization# Symbolic traceability is a requirement for (Prototype - maintenance mode) FX Graph Mode Quantization, so if you pass a PyTorch Model that is not symbolically traceable to torch.ao.quantization.prepare_fx or torch.ao.quantization.prepare_qat_fx, we might see an error like the following: torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow Please take a look at Limitations of Symbolic Tracing and use - User Guide on Using FX Graph Mode Quantization to workaround the problem.", "prev_chunk_id": "chunk_1600", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1602", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "DDP Communication Hooks#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "DDP Communication Hooks#", "content": "DDP Communication Hooks# Created On: Jun 06, 2025 | Last Updated On: Jun 06, 2025 DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided, and users can easily apply any of these hooks to optimize communication. Besides, the hook interface can also support user-defined communication strategies for more advanced use cases.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1603", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "How to Use a Communication Hook?#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "How to Use a Communication Hook?#", "content": "How to Use a Communication Hook?# To use a communication hook, the user just needs to let the DDP model register the hook before the training loop as below. torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "prev_chunk_id": "chunk_1602", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1604", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "What Does a Communication Hook Operate On?#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "What Does a Communication Hook Operate On?#", "content": "What Does a Communication Hook Operate On?# A communication hook provides a flexible way to allreduce gradients. Therefore, it mainly operates on the gradients on each replica before allreduce, which are bucketized to increase the overlap between communication and computation. Particularly, torch.distributed.GradBucket represents a bucket of gradient tensors to be allreduced.", "prev_chunk_id": "chunk_1603", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1605", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "Default Communication Hooks#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "Default Communication Hooks#", "content": "Default Communication Hooks# Default communication hooks are simple stateless hooks, so the input state in register_comm_hook is either a process group or None. The input bucket is a torch.distributed.GradBucket object. Additionally, a communication hook wrapper is provided to support fp16_compress_hook() or bf16_compress_hook() as a wrapper, which can be combined with other communication hooks.", "prev_chunk_id": "chunk_1604", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1606", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "PowerSGD Communication Hook#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "PowerSGD Communication Hook#", "content": "PowerSGD Communication Hook# PowerSGD (Vogels et al., NeurIPS 2019) is a gradient compression algorithm, which can provide very high compression rates and accelerate bandwidth-bound distributed training. This algorithm needs to maintain both some hyperparameters and the internal state. Therefore, PowerSGD communication hook is a stateful hook, and the user needs to provide a state object defined as below.", "prev_chunk_id": "chunk_1605", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1607", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "Debugging Communication Hooks#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Communication Hooks#", "content": "Debugging Communication Hooks# As the name implies, debugging communication hooks are only used for debugging and performance optimization purpose.", "prev_chunk_id": "chunk_1606", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1608", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "Checkpointing of Communication Hooks#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "Checkpointing of Communication Hooks#", "content": "Checkpointing of Communication Hooks# A stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts. To make a hook serializable, __setstate__ and __getstate__ should be defined. PowerSGDState has __setstate__ and __getstate__ implemented and can be used as a reference. Here is a simple, end-to-end example of saving and reloading PowerSGD state and hook. import os import sys import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD class SimpleModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(24,24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24,12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def setup(rank, world_size): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '12355' # initialize the process group dist.init_process_group(\"nccl\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def run_demo(demo_fn, world_size): mp.spawn( demo_fn, args=(world_size,), nprocs=world_size, join=True) def demo_serialization(rank, world_size): setup(rank, world_size) CHECKPOINT = tempfile.gettempdir() + \"/checkpoint.pt\" model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) powersgd_hook = powerSGD.powerSGD_hook powersgd_state = powerSGD.PowerSGDState(process_group=None) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) ddp_model.register_comm_hook(powersgd_state, powersgd_hook) state = { 'state_dict': ddp_model.state_dict(), 'comm_hook': powersgd_hook, 'comm_hook_state': powersgd_state} if rank == 0: torch.save(state, CHECKPOINT) dist.barrier() map_location = {'cuda:%d' % 0: 'cuda:%d' % rank} checkpoint = torch.load(CHECKPOINT, map_location=map_location) new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank]) new_ddp_model.load_state_dict(checkpoint['state_dict']) powersgd_hook = checkpoint['comm_hook'] powersgd_state = checkpoint['comm_hook_state'] new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook) if rank == 0: os.remove(CHECKPOINT) cleanup() if __name__ == \"__main__\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\" world_size = n_gpus run_demo(demo_serialization, world_size)", "prev_chunk_id": "chunk_1607", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1609", "url": "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html", "title": "Acknowledgements#", "page_title": "DDP Communication Hooks — PyTorch 2.8 documentation", "breadcrumbs": "Acknowledgements#", "content": "Acknowledgements# Many thanks to PowerSGD paper author Thijs Vogels for the code review on PowerSGD communication hook, as well as the comparison experiments, which show that the performance of PowerSGD communication hook is on par with the implementation in the original paper.", "prev_chunk_id": "chunk_1608", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1610", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Complex Numbers#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Complex Numbers#", "content": "Complex Numbers# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025 Complex numbers are numbers that can be expressed in the form a+bja + bja+bj, where a and b are real numbers, and j is called the imaginary unit, which satisfies the equation j2=−1j^2 = -1j2=−1. Complex numbers frequently occur in mathematics and engineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have handled complex numbers by representing the data in float tensors with shape (...,2)(..., 2)(...,2) where the last dimension contains the real and imaginary values. Tensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on complex tensors (e.g., torch.mv(), torch.matmul()) are likely to be faster and more memory efficient than operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized to use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1611", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Creating Complex Tensors#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Creating Complex Tensors#", "content": "Creating Complex Tensors# We support two complex dtypes: torch.cfloat and torch.cdouble >>> x = torch.randn(2,2, dtype=torch.cfloat) >>> x tensor([[-0.4621-0.0303j, -0.2438-0.5874j], [ 0.7706+0.1421j, 1.2110+0.1918j]]) All factory functions apart from torch.linspace(), torch.logspace(), and torch.arange() are supported for complex tensors.", "prev_chunk_id": "chunk_1610", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1612", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Transition from the old representation#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Transition from the old representation#", "content": "Transition from the old representation# Users who currently worked around the lack of complex tensors with real tensors of shape (...,2)(..., 2)(...,2) can easily to switch using the complex tensors in their code using torch.view_as_complex() and torch.view_as_real(). Note that these functions don’t perform any copy and return a view of the input tensor. >>> x = torch.randn(3, 2) >>> x tensor([[ 0.6125, -0.1681], [-0.3773, 1.3487], [-0.0861, -0.7981]]) >>> y = torch.view_as_complex(x) >>> y tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j]) >>> torch.view_as_real(y) tensor([[ 0.6125, -0.1681], [-0.3773, 1.3487], [-0.0861, -0.7981]])", "prev_chunk_id": "chunk_1611", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1613", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Accessing real and imag#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Accessing real and imag#", "content": "Accessing real and imag# The real and imaginary values of a complex tensor can be accessed using the real and imag. >>> y.real tensor([ 0.6125, -0.3773, -0.0861]) >>> y.imag tensor([-0.1681, 1.3487, -0.7981]) >>> y.real.mul_(2) tensor([ 1.2250, -0.7546, -0.1722]) >>> y tensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j]) >>> y.real.stride() (2,)", "prev_chunk_id": "chunk_1612", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1614", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Angle and abs#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Angle and abs#", "content": "Angle and abs# The angle and absolute values of a complex tensor can be computed using torch.angle() and torch.abs(). >>> x1=torch.tensor([3j, 4+4j]) >>> x1.abs() tensor([3.0000, 5.6569]) >>> x1.angle() tensor([1.5708, 0.7854])", "prev_chunk_id": "chunk_1613", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1615", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Linear Algebra#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Linear Algebra#", "content": "Linear Algebra# Many linear algebra operations, like torch.matmul(), torch.linalg.svd(), torch.linalg.solve() etc., support complex numbers. If you’d like to request an operation we don’t currently support, please search if an issue has already been filed and if not, file one.", "prev_chunk_id": "chunk_1614", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1616", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Serialization#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Serialization#", "content": "Serialization# Complex tensors can be serialized, allowing data to be saved as complex values. >>> torch.save(y, 'complex_tensor.pt') >>> torch.load('complex_tensor.pt') tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])", "prev_chunk_id": "chunk_1615", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1617", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Autograd#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Autograd#", "content": "Autograd# PyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative, the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers can be implemented to work out of the box with complex parameters. For more details, check out the note Autograd for Complex Numbers.", "prev_chunk_id": "chunk_1616", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1618", "url": "https://docs.pytorch.org/docs/stable/complex_numbers.html", "title": "Optimizers#", "page_title": "Complex Numbers — PyTorch 2.8 documentation", "breadcrumbs": "Optimizers#", "content": "Optimizers# Semantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping through the same optimizer on the torch.view_as_real() equivalent of the complex params. More concretely: >>> params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)] >>> real_params = [torch.view_as_real(p) for p in params] >>> complex_optim = torch.optim.AdamW(params) >>> real_optim = torch.optim.AdamW(real_params) real_optim and complex_optim will compute the same updates on the parameters, though there may be slight numerical discrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers and capturable vs default optimizers. For more details, see numbercial accuracy. Specifically, while you can think of our optimizer’s handling of complex tensors as the same as optimizing over their p.real and p.imag pieces separately, the implementation details are not precisely that. Note that the torch.view_as_real() equivalent will convert a complex tensor to a real tensor with shape (...,2)(..., 2)(...,2), whereas splitting a complex tensor into two tensors is 2 tensors of size (...)(...)(...). This distinction has no impact on pointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS). We currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue if you have a use case that requires precisely defining this behavior. We do not fully support the following subsystems: - Quantization - JIT - Sparse Tensors - Distributed If any of these would help your use case, please search if an issue has already been filed and if not, file one.", "prev_chunk_id": "chunk_1617", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1619", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "torch.optim#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "torch.optim#", "content": "torch.optim# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1620", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "How to use an optimizer#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "How to use an optimizer#", "content": "How to use an optimizer# To use torch.optim you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.", "prev_chunk_id": "chunk_1619", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1621", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Constructing it#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Constructing it#", "content": "Constructing it# To construct an Optimizer you have to give it an iterable containing the parameters (all should be Parameter s) or named parameters (tuples of (str, Parameter)) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc. Example: optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr=0.0001) Named parameters example: optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9) optimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)", "prev_chunk_id": "chunk_1620", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1622", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Per-parameter options#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Per-parameter options#", "content": "Per-parameter options# Optimizer s also support specifying per-parameter options. To do this, instead of passing an iterable of Variable s, pass in an iterable of dict s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group. For example, this is very useful when one wants to specify per-layer learning rates: optim.SGD([ {'params': model.base.parameters(), 'lr': 1e-2}, {'params': model.classifier.parameters()} ], lr=1e-3, momentum=0.9) optim.SGD([ {'params': model.base.named_parameters(), 'lr': 1e-2}, {'params': model.classifier.named_parameters()} ], lr=1e-3, momentum=0.9) This means that model.base’s parameters will use a learning rate of 1e-2, whereas model.classifier’s parameters will stick to the default learning rate of 1e-3. Finally a momentum of 0.9 will be used for all parameters. Also consider the following example related to the distinct penalization of parameters. Remember that parameters() returns an iterable that contains all learnable parameters, including biases and other parameters that may prefer distinct penalization. To address this, one can specify individual penalization weights for each parameter group: bias_params = [p for name, p in self.named_parameters() if 'bias' in name] others = [p for name, p in self.named_parameters() if 'bias' not in name] optim.SGD([ {'params': others}, {'params': bias_params, 'weight_decay': 0} ], weight_decay=1e-2, lr=1e-2) In this manner, bias terms are isolated from non-bias terms, and a weight_decay of 0 is set specifically for the bias terms, as to avoid any penalization for this group.", "prev_chunk_id": "chunk_1621", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1623", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Taking an optimization step#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Taking an optimization step#", "content": "Taking an optimization step# All optimizers implement a step() method, that updates the parameters. It can be used in two ways:", "prev_chunk_id": "chunk_1622", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1624", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "optimizer.step()#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "optimizer.step()#", "content": "optimizer.step()# This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward(). Example: for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step()", "prev_chunk_id": "chunk_1623", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1625", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "optimizer.step(closure)#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "optimizer.step(closure)#", "content": "optimizer.step(closure)# Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it. Example: for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure)", "prev_chunk_id": "chunk_1624", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1626", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Algorithms#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Algorithms#", "content": "Algorithms# Many of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user. We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that. In general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. We summarize the stability status for each implementation on the second table below, you are welcome to try them out though! Below is a table showing the available and default implementations of each algorithm: Below table is showing the stability status for fused implementations:", "prev_chunk_id": "chunk_1625", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1627", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "How to adjust learning rate#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "How to adjust learning rate#", "content": "How to adjust learning rate# torch.optim.lr_scheduler.LRScheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way: Example: optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) for epoch in range(20): for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() scheduler.step() Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it. Example: optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler1 = ExponentialLR(optimizer, gamma=0.9) scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) for epoch in range(20): for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() scheduler1.step() scheduler2.step() In many places in the documentation, we will use the following template to refer to schedulers algorithms. >>> scheduler = ... >>> for epoch in range(100): >>> train(...) >>> validate(...) >>> scheduler.step()", "prev_chunk_id": "chunk_1626", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1628", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "How to utilize named parameters to load optimizer state dict#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "How to utilize named parameters to load optimizer state dict#", "content": "How to utilize named parameters to load optimizer state dict# The function load_state_dict() stores the optional param_names content from the loaded state dict if present. However, the process of loading the optimizer state is not affected, as the order of the parameters matters to maintain compatibility (in case of different ordering). To utilize the loaded parameters names from the loaded state dict, a custom register_load_state_dict_pre_hook needs to be implemented according to the desired behavior. This can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to remain unchanged. The following example demonstrates how to implement this customization. Example: class OneLayerModel(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(3, 4) def forward(self, x): return self.fc(x) model = OneLayerModel() optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9) # training.. torch.save(optimizer.state_dict(), PATH) Let’s say that model implements an expert (MoE), and we want to duplicate it and resume training for two experts, both initialized the same way as the fc layer. For the following model2 we create two layers identical to fc and resume training by loading the model weights and optimizer states from model into both fc1 and fc2 of model2 (and adjust them accordingly): class TwoLayerModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(3, 4) self.fc2 = nn.Linear(3, 4) def forward(self, x): return (self.fc1(x) + self.fc2(x)) / 2 model2 = TwoLayerModel() # adapt and load model weights.. optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9) To load the state dict for optimizer2 with the state dict of the previous optimizer such that both fc1 and fc2 will be initialized with a copy of fc optimizer states (to resume training for each layer from fc), we can use the following hook: def adapt_state_dict_ids(optimizer, state_dict): adapted_state_dict = deepcopy(optimizer.state_dict()) # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. for k,", "prev_chunk_id": "chunk_1627", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1629", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "How to utilize named parameters to load optimizer state dict#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "How to utilize named parameters to load optimizer state dict#", "content": "v in state_dict['param_groups'][0].items(): if k not in ['params', 'param_names']: adapted_state_dict['param_groups'][0][k] = v lookup_dict = { 'fc1.weight': 'fc.weight', 'fc1.bias': 'fc.bias', 'fc2.weight': 'fc.weight', 'fc2.bias': 'fc.bias' } clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()} for param_id, param_name in zip( optimizer.state_dict()['param_groups'][0]['params'], optimizer.state_dict()['param_groups'][0]['param_names']): name_in_loaded = lookup_dict[param_name] index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded) id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list] # Copy the state of the corresponding parameter if id_in_loaded in state_dict['state']: adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded]) return adapted_state_dict optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids) optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict This ensures that the adapted state_dict with the correct states for the layers of model2 will be used during model loading. Note that this code is designed specifically for this example (e.g., assuming a single parameter group), and other cases might require different adaptations. The following example shows how to handle missing parameters in a loaded state dict when the model structure changes. The Model_bypass adds a new bypass layer, which is not present in the original Model1. To resume training, a custom adapt_state_dict_missing_param hook is used to adapt the optimizer’s state_dict, ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged (as initialized in this example). This approach enables smooth loading and resuming of the optimizer state despite model changes. The new bypass layer will be trained from scratch: class Model1(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(5, 5) def forward(self, x): return self.fc(x) + x model = Model1() optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9) # training.. torch.save(optimizer.state_dict(), PATH) class Model_bypass(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(5, 5) self.bypass = nn.Linear(5, 5, bias=False) torch.nn.init.eye_(self.bypass.weight) def forward(self, x): return self.fc(x) + self.bypass(x) model2 = Model_bypass() optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9) def adapt_state_dict_missing_param(optimizer, state_dict): adapted_state_dict = deepcopy(optimizer.state_dict()) # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. for k, v in", "prev_chunk_id": "chunk_1628", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1630", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "How to utilize named parameters to load optimizer state dict#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "How to utilize named parameters to load optimizer state dict#", "content": "state_dict['param_groups'][0].items(): if k not in ['params', 'param_names']: adapted_state_dict['param_groups'][0][k] = v lookup_dict = { 'fc.weight': 'fc.weight', 'fc.bias': 'fc.bias', 'bypass.weight': None, } clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()} for param_id, param_name in zip( optimizer.state_dict()['param_groups'][0]['params'], optimizer.state_dict()['param_groups'][0]['param_names']): name_in_loaded = lookup_dict[param_name] if name_in_loaded in state_dict['param_groups'][0]['param_names']: index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded) id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list] # Copy the state of the corresponding parameter if id_in_loaded in state_dict['state']: adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded]) return adapted_state_dict optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids) optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict As a third example, instead of loading a state according to the order of parameters (the default approach), this hook can be used to load according to the parameters’ names: def names_matching(optimizer, state_dict): assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups']) adapted_state_dict = deepcopy(optimizer.state_dict()) for g_ind in range(len(state_dict['param_groups'])): assert len(state_dict['param_groups'][g_ind]['params']) == len( optimizer.state_dict()['param_groups'][g_ind]['params']) for k, v in state_dict['param_groups'][g_ind].items(): if k not in ['params', 'param_names']: adapted_state_dict['param_groups'][g_ind][k] = v for param_id, param_name in zip( optimizer.state_dict()['param_groups'][g_ind]['params'], optimizer.state_dict()['param_groups'][g_ind]['param_names']): index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name) id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list] # Copy the state of the corresponding parameter if id_in_loaded in state_dict['state']: adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded]) return adapted_state_dict", "prev_chunk_id": "chunk_1629", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1631", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Weight Averaging (SWA and EMA)#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Weight Averaging (SWA and EMA)#", "content": "Weight Averaging (SWA and EMA)# torch.optim.swa_utils.AveragedModel implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), torch.optim.swa_utils.SWALR implements the SWA learning rate scheduler and torch.optim.swa_utils.update_bn() is a utility function used to update SWA/EMA batch normalization statistics at the end of training. SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization. EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of Polyak averaging, but using exponential weights instead of equal weights across iterations.", "prev_chunk_id": "chunk_1630", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1632", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Constructing averaged models#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Constructing averaged models#", "content": "Constructing averaged models# The AveragedModel class serves to compute the weights of the SWA or EMA model. You can create an SWA averaged model by running: >>> averaged_model = AveragedModel(model) EMA models are constructed by specifying the multi_avg_fn argument as follows: >>> decay = 0.999 >>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay)) Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to torch.optim.swa_utils.get_ema_multi_avg_fn(), the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues. torch.optim.swa_utils.get_ema_multi_avg_fn() returns a function that applies the following EMA equation to the weights: where alpha is the EMA decay. Here the model model can be an arbitrary torch.nn.Module object. averaged_model will keep track of the running averages of the parameters of the model. To update these averages, you should use the update_parameters() function after the optimizer.step(): >>> averaged_model.update_parameters(model) For SWA and EMA, this call is usually done right after the optimizer step(). In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.", "prev_chunk_id": "chunk_1631", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1633", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Custom averaging strategies#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Custom averaging strategies#", "content": "Custom averaging strategies# By default, torch.optim.swa_utils.AveragedModel computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the avg_fn or multi_avg_fn parameters: - avg_fnallows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter. - multi_avg_fnallows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using thetorch._foreach*functions. This function must update the averaged parameters in-place. In the following example ema_model computes an exponential moving average using the avg_fn parameter: >>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\ >>> 0.9 * averaged_model_parameter + 0.1 * model_parameter >>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg) In the following example ema_model computes an exponential moving average using the more efficient multi_avg_fn parameter: >>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))", "prev_chunk_id": "chunk_1632", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1634", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "SWA learning rate schedules#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "SWA learning rate schedules#", "content": "SWA learning rate schedules# Typically, in SWA the learning rate is set to a high constant value. SWALR is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group: >>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\ >>> anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05) You can also use cosine annealing to a fixed value instead of linear annealing by setting anneal_strategy=\"cos\".", "prev_chunk_id": "chunk_1633", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1635", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Taking care of batch normalization#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Taking care of batch normalization#", "content": "Taking care of batch normalization# update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader loader at the end of training: >>> torch.optim.swa_utils.update_bn(loader, swa_model) update_bn() applies the swa_model to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.", "prev_chunk_id": "chunk_1634", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1636", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Putting it all together: SWA#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Putting it all together: SWA#", "content": "Putting it all together: SWA# In the example below, swa_model is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160: >>> loader, optimizer, model, loss_fn = ... >>> swa_model = torch.optim.swa_utils.AveragedModel(model) >>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300) >>> swa_start = 160 >>> swa_scheduler = SWALR(optimizer, swa_lr=0.05) >>> >>> for epoch in range(300): >>> for input, target in loader: >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() >>> if epoch > swa_start: >>> swa_model.update_parameters(model) >>> swa_scheduler.step() >>> else: >>> scheduler.step() >>> >>> # Update bn statistics for the swa_model at the end >>> torch.optim.swa_utils.update_bn(loader, swa_model) >>> # Use swa_model to make predictions on test data >>> preds = swa_model(test_input)", "prev_chunk_id": "chunk_1635", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1637", "url": "https://docs.pytorch.org/docs/stable/optim.html", "title": "Putting it all together: EMA#", "page_title": "torch.optim — PyTorch 2.8 documentation", "breadcrumbs": "Putting it all together: EMA#", "content": "Putting it all together: EMA# In the example below, ema_model is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately. >>> loader, optimizer, model, loss_fn = ... >>> ema_model = torch.optim.swa_utils.AveragedModel(model, \\ >>> multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999)) >>> >>> for epoch in range(300): >>> for input, target in loader: >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() >>> ema_model.update_parameters(model) >>> >>> # Update bn statistics for the ema_model at the end >>> torch.optim.swa_utils.update_bn(loader, ema_model) >>> # Use ema_model to make predictions on test data >>> preds = ema_model(test_input)", "prev_chunk_id": "chunk_1636", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1638", "url": "https://docs.pytorch.org/docs/stable/onnx.html", "title": "torch.onnx#", "page_title": "torch.onnx — PyTorch 2.8 documentation", "breadcrumbs": "torch.onnx#", "content": "torch.onnx# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1639", "url": "https://docs.pytorch.org/docs/stable/onnx.html", "title": "Overview#", "page_title": "torch.onnx — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# Open Neural Network eXchange (ONNX) is an open standard format for representing machine learning models. The torch.onnx module captures the computation graph from a native PyTorch torch.nn.Module model and converts it into an ONNX graph. The exported model can be consumed by any of the many runtimes that support ONNX, including Microsoft’s ONNX Runtime. There are two flavors of ONNX exporter API that you can use, as listed below. Both can be called through function torch.onnx.export(). Next example shows how to export a simple model. import torch class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.conv1 = torch.nn.Conv2d(1, 128, 5) def forward(self, x): return torch.relu(self.conv1(x)) input_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32) model = MyModel() torch.onnx.export( model, # model to export (input_tensor,), # inputs of the model, \"my_model.onnx\", # filename of the ONNX model input_names=[\"input\"], # Rename inputs for the ONNX model dynamo=True # True or False to select the exporter to use ) Next sections introduce the two versions of the exporter.", "prev_chunk_id": "chunk_1638", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1640", "url": "https://docs.pytorch.org/docs/stable/onnx.html", "title": "TorchDynamo-based ONNX Exporter#", "page_title": "torch.onnx — PyTorch 2.8 documentation", "breadcrumbs": "TorchDynamo-based ONNX Exporter#", "content": "TorchDynamo-based ONNX Exporter# The TorchDynamo-based ONNX exporter is the newest (and Beta) exporter for PyTorch 2.1 and newer TorchDynamo engine is leveraged to hook into Python’s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph. The main advantage of this approach is that the FX graph is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques. Learn more about the TorchDynamo-based ONNX Exporter", "prev_chunk_id": "chunk_1639", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1641", "url": "https://docs.pytorch.org/docs/stable/onnx.html", "title": "TorchScript-based ONNX Exporter#", "page_title": "torch.onnx — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript-based ONNX Exporter#", "content": "TorchScript-based ONNX Exporter# The TorchScript-based ONNX exporter is available since PyTorch 1.2.0 TorchScript is leveraged to trace (through torch.jit.trace()) the model and capture a static computation graph. As a consequence, the resulting graph has a couple limitations: - It does not record any control-flow, like if-statements or loops; - Does not handle nuances betweentrainingandevalmode; - Does not truly handle dynamic inputs As an attempt to support the static tracing limitations, the exporter also supports TorchScript scripting (through torch.jit.script()), which adds support for data-dependent control-flow, for example. However, TorchScript itself is a subset of the Python language, so not all features in Python are supported, such as in-place operations. Learn more about the TorchScript-based ONNX Exporter", "prev_chunk_id": "chunk_1640", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1642", "url": "https://docs.pytorch.org/docs/stable/onnx.html", "title": "Contributing / Developing#", "page_title": "torch.onnx — PyTorch 2.8 documentation", "breadcrumbs": "Contributing / Developing#", "content": "Contributing / Developing# The ONNX exporter is a community project and we welcome contributions. We follow the PyTorch guidelines for contributions, but you might also be interested in reading our development wiki.", "prev_chunk_id": "chunk_1641", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1643", "url": "https://docs.pytorch.org/docs/stable/nn.attention.html", "title": "torch.nn.attention#", "page_title": "torch.nn.attention — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.attention#", "content": "torch.nn.attention# Created On: Jan 24, 2024 | Last Updated On: Oct 29, 2024 This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1644", "url": "https://docs.pytorch.org/docs/stable/nn.init.html", "title": "torch.nn.init#", "page_title": "torch.nn.init — PyTorch 2.8 documentation", "breadcrumbs": "torch.nn.init#", "content": "torch.nn.init# Created On: Jun 11, 2019 | Last Updated On: Jul 07, 2022", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1645", "url": "https://docs.pytorch.org/docs/stable/profiler.html", "title": "torch.profiler#", "page_title": "torch.profiler — PyTorch 2.8 documentation", "breadcrumbs": "torch.profiler#", "content": "torch.profiler# Created On: Dec 18, 2020 | Last Updated On: Jun 13, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1646", "url": "https://docs.pytorch.org/docs/stable/profiler.html", "title": "Overview#", "page_title": "torch.profiler — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler’s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.", "prev_chunk_id": "chunk_1645", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1647", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "torch.package#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "torch.package#", "content": "torch.package# Created On: Jun 10, 2025 | Last Updated On: Jun 10, 2025 torch.package adds support for creating packages containing both artifacts and arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using torch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that will help you learn more about torch.package and how to use it.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1648", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Packaging your first model#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Packaging your first model#", "content": "Packaging your first model# A tutorial that guides you through packaging and unpackaging a simple model is available on Colab. After completing this exercise, you will be familiar with the basic API for creating and using Torch packages.", "prev_chunk_id": "chunk_1647", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1649", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Treat the package like a ZIP archive#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Treat the package like a ZIP archive#", "content": "Treat the package like a ZIP archive# The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should work for exploring the contents. Some common ways to interact with ZIP files: - unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. $ unzip my_package.pt && tree my_package my_package ├── .data │ ├── 94304870911616.storage │ ├── 94304900784016.storage │ ├── extern_modules │ └── version ├── models │ └── model_1.pkl └── torchvision └── models ├── resnet.py └── utils.py ~ cd my_package && cat torchvision/models/resnet.py ... - The Pythonzipfilemodule provides a standard way to read and write ZIP archive contents. from zipfile import ZipFile with ZipFile(\"my_package.pt\") as myzip: file_bytes = myzip.read(\"torchvision/models/resnet.py\") # edit file_bytes in some way myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes) - vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! # add this to your .vimrc to treat `*.pt` files as zip files au BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\")) ~ vi my_package.pt", "prev_chunk_id": "chunk_1648", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1650", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Use the file_structure() API#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Use the file_structure() API#", "content": "Use the file_structure() API# PackageImporter provides a file_structure() method, which will return a printable and queryable Directory object. The Directory object is a simple directory structure that you can use to explore the current contents of a torch.package. The Directory object itself is directly printable and will print out a file tree representation. To filter what is returned, use the glob-style include and exclude filtering arguments. with PackageExporter('my_package.pt') as pe: pe.save_pickle('models', 'model_1.pkl', mod) importer = PackageImporter('my_package.pt') # can limit printed items with include/exclude args print(importer.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\")) print(importer.file_structure()) # will print out all files Output: # filtered with glob pattern: # include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\" ─── my_package.pt ├── models │ └── model_1.pkl └── torchvision └── models └── utils.py # all files ─── my_package.pt ├── .data │ ├── 94304870911616.storage │ ├── 94304900784016.storage │ ├── extern_modules │ └── version ├── models │ └── model_1.pkl └── torchvision └── models ├── resnet.py └── utils.py You can also query Directory objects with the has_file() method. importer_file_structure = importer.file_structure() found: bool = importer_file_structure.has_file(\"package_a/subpackage.py\")", "prev_chunk_id": "chunk_1649", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1651", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "See why a given module was included as a dependency?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "See why a given module was included as a dependency?#", "content": "See why a given module was included as a dependency?# Say there is a given module foo, and you want to know why your PackageExporter is pulling in foo as a dependency. PackageExporter.get_rdeps() will return all modules that directly depend on foo. If you would like to see how a given module src depends on foo, the PackageExporter.all_paths() method will return a DOT-formatted graph showing all the dependency paths between src and foo. If you would just like to see the whole dependency graph of your :class:PackageExporter, you can use PackageExporter.dependency_graph_string().", "prev_chunk_id": "chunk_1650", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1652", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Include arbitrary resources with my package and access them later?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Include arbitrary resources with my package and access them later?#", "content": "Include arbitrary resources with my package and access them later?# PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save Python objects, text, and binary data to a package. with torch.PackageExporter(\"package.pt\") as exporter: # Pickles the object and saves to `my_resources/tensor.pkl` in the archive. exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4)) exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\") exporter.save_binary(\"raw_data\", \"binary\", my_bytes) PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load Python objects, text and binary data from a package. importer = torch.PackageImporter(\"package.pt\") my_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\") text = importer.load_text(\"config_stuff\", \"words.txt\") binary = importer.load_binary(\"raw_data\", \"binary\")", "prev_chunk_id": "chunk_1651", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1653", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Customize how a class is packaged?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Customize how a class is packaged?#", "content": "Customize how a class is packaged?# torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method __reduce_package__ on a class and by defining a corresponding de-packaging function. This is similar to defining __reduce__ for Python’s normal pickling process. Steps: - Define the method__reduce_package__(self,exporter:PackageExporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by thePackageExporterwhen it encounters an instance of the target class. - Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be aPackageImporterinstance, and the rest of the parameters are user defined. # foo.py [Example of customizing how class Foo is packaged] from torch.package import PackageExporter, PackageImporter import time class Foo: def __init__(self, my_string: str): super().__init__() self.my_string = my_string self.time_imported = 0 self.time_exported = 0 def __reduce_package__(self, exporter: PackageExporter): \"\"\" Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when saving an instance of this object. This method should do the work to save this object inside of the ``torch.package`` archive. Returns function w/ arguments to load the object from a ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function. \"\"\" # use this pattern to ensure no naming conflicts with normal dependencies, # anything saved under this module name shouldn't conflict with other # items in the package generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\" exporter.save_text( generated_module_name, \"foo.txt\", self.my_string + \", with exporter modification!\", ) time_exported = time.clock_gettime(1) # returns de-packaging function w/ arguments to invoke with return (unpackage_foo, (generated_module_name, time_exported,)) def unpackage_foo( importer: PackageImporter, generated_module_name: str, time_exported: float ) -> Foo: \"\"\" Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function when depickling", "prev_chunk_id": "chunk_1652", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1654", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Customize how a class is packaged?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Customize how a class is packaged?#", "content": "a Foo object. Performs work of loading and returning a Foo instance from a ``torch.package`` archive. \"\"\" time_imported = time.clock_gettime(1) foo = Foo(importer.load_text(generated_module_name, \"foo.txt\")) foo.time_imported = time_imported foo.time_exported = time_exported return foo # example of saving instances of class Foo import torch from torch.package import PackageImporter, PackageExporter import foo foo_1 = foo.Foo(\"foo_1 initial string\") foo_2 = foo.Foo(\"foo_2 initial string\") with PackageExporter('foo_package.pt') as pe: # save as normal, no extra work necessary pe.save_pickle('foo_collection', 'foo1.pkl', foo_1) pe.save_pickle('foo_collection', 'foo2.pkl', foo_2) pi = PackageImporter('foo_package.pt') print(pi.file_structure()) imported_foo = pi.load_pickle('foo_collection', 'foo1.pkl') print(f\"foo_1 string: '{imported_foo.my_string}'\") print(f\"foo_1 export time: {imported_foo.time_exported}\") print(f\"foo_1 import time: {imported_foo.time_imported}\") # output of running above script ─── foo_package ├── foo-generated │ ├── _0 │ │ └── foo.txt │ └── _1 │ └── foo.txt ├── foo_collection │ ├── foo1.pkl │ └── foo2.pkl └── foo.py foo_1 string: 'foo_1 initial string, with reduction modification!' foo_1 export time: 9857706.650140837 foo_1 import time: 9857706.652698385", "prev_chunk_id": "chunk_1653", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1655", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Test in my source code whether or not it is executing inside a package?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Test in my source code whether or not it is executing inside a package?#", "content": "Test in my source code whether or not it is executing inside a package?# A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not. # In foo/bar.py: if \"__torch_package__\" in dir(): # true if the code is being loaded from a package def is_in_package(): return True UserException = Exception else: def is_in_package(): return False UserException = UnpackageableException Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a torch.package. from foo.bar import is_in_package print(is_in_package()) # False loaded_module = PackageImporter(my_package).import_module(\"foo.bar\") loaded_module.is_in_package() # True Warning: in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to hard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring your code so that it behaves the same way no matter how it was loaded.", "prev_chunk_id": "chunk_1654", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1656", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Patch code into a package?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Patch code into a package?#", "content": "Patch code into a package?# PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing. with PackageExporter(f) as exporter: # Save the my_module.foo available in your current Python environment. exporter.save_module(\"my_module.foo\") # This saves the provided string to my_module/foo.py in the package archive. # It will override the my_module.foo that was previously saved. exporter.save_source_string(\"my_module.foo\", textwrap.dedent( \"\"\"\\ def my_function(): print('hello world') \"\"\" )) # If you want to treat my_module.bar as a package # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py) # pass is_package=True, exporter.save_source_string(\"my_module.bar\", \"def foo(): print('hello')\\n\", is_package=True) importer = PackageImporter(f) importer.import_module(\"my_module.foo\").my_function() # prints 'hello world'", "prev_chunk_id": "chunk_1655", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1657", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Access package contents from packaged code?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Access package contents from packaged code?#", "content": "Access package contents from packaged code?# PackageImporter implements the importlib.resources API for accessing resources from inside a package. with PackageExporter(f) as exporter: # saves text to my_resource/a.txt in the archive exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\") # saves the tensor to my_pickle/obj.pkl exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2)) # see below for module contents exporter.save_module(\"foo\") exporter.save_module(\"bar\") The importlib.resources API allows access to resources from within packaged code. # foo.py: import importlib.resources import my_resource # returns \"hello world!\" def get_my_resource(): return importlib.resources.read_text(my_resource, \"a.txt\") Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies with the Python standard. However, it is also possible to access the parent :class:PackageImporter instance itself from within packaged code. # bar.py: import torch_package_importer # this is the PackageImporter that imported this module. # Prints \"hello world!\", equivalent to importlib.resources.read_text def get_my_resource(): return torch_package_importer.load_text(\"my_resource\", \"a.txt\") # You also do things that the importlib.resources API does not support, like loading # a pickled object from the package. def get_my_pickle(): return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")", "prev_chunk_id": "chunk_1656", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1658", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Distinguish between packaged code and non-packaged code?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Distinguish between packaged code and non-packaged code?#", "content": "Distinguish between packaged code and non-packaged code?# To tell if an object’s code is from a torch.package, use the torch.package.is_from_package() function. Note: if an object is from a package but its definition is from a module marked extern or from stdlib, this check will return False. importer = PackageImporter(f) mod = importer.import_module('foo') obj = importer.load_pickle('model', 'model.pkl') txt = importer.load_text('text', 'my_test.txt') assert is_from_package(mod) assert is_from_package(obj) assert not is_from_package(txt) # str is from stdlib, so this will return False", "prev_chunk_id": "chunk_1657", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1659", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Re-export an imported object?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Re-export an imported object?#", "content": "Re-export an imported object?# To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter aware of the original PackageImporter so that it can find source code for your object’s dependencies. importer = PackageImporter(f) obj = importer.load_pickle(\"model\", \"model.pkl\") # re-export obj in a new package with PackageExporter(f2, importer=(importer, sys_importer)) as exporter: exporter.save_pickle(\"model\", \"model.pkl\", obj)", "prev_chunk_id": "chunk_1658", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1660", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Package a TorchScript module?#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Package a TorchScript module?#", "content": "Package a TorchScript module?# To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object. Saving TorchScript objects that are attributes or submodules is supported as well with no extra work. # save TorchScript just like any other object with PackageExporter(file_name) as e: e.save_pickle(\"res\", \"script_model.pkl\", scripted_model) e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule) # load as normal importer = PackageImporter(file_name) loaded_script = importer.load_pickle(\"res\", \"script_model.pkl\") loaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"", "prev_chunk_id": "chunk_1659", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1661", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "torch.package Format Overview#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "torch.package Format Overview#", "content": "torch.package Format Overview# A torch.package file is a ZIP archive which conventionally uses the .pt extension. Inside the ZIP archive, there are two kinds of files: - Framework files, which are placed in the.data/. - User files, which is everything else. As an example, this is what a fully packaged ResNet model from torchvision looks like: resnet ├── .data # All framework-specific data is stored here. │ │ # It's named to avoid conflicts with user-serialized code. │ ├── 94286146172688.storage # tensor data │ ├── 94286146172784.storage │ ├── extern_modules # text file with names of extern modules (e.g. 'torch') │ ├── version # version metadata │ ├── ... ├── model # the pickled model │ └── model.pkl └── torchvision # all code dependencies are captured as source files └── models ├── resnet.py └── utils.py", "prev_chunk_id": "chunk_1660", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1662", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Framework files#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Framework files#", "content": "Framework files# The .data/ directory is owned by torch.package, and its contents are considered to be a private implementation detail. The torch.package format makes no guarantees about the contents of .data/, but any changes made will be backward compatible (that is, newer version of PyTorch will always be able to load older torch.packages). Currently, the .data/ directory contains the following items: - version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. - extern_modules: a list of modules that are consideredextern.externmodules will be imported using the loading environment’s system importer. - *.storage: serialized tensor data. .data ├── 94286146172688.storage ├── 94286146172784.storage ├── extern_modules ├── version ├── ...", "prev_chunk_id": "chunk_1661", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1663", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "User files#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "User files#", "content": "User files# All other files in the archive were put there by a user. The layout is identical to a Python regular package. For a deeper dive in how Python packaging works, please consult this essay (it’s slightly out of date, so double-check implementation details with the Python reference documentation. <package root> ├── model # the pickled model │ └── model.pkl ├── another_package │ ├── __init__.py │ ├── foo.txt # a resource file , see importlib.resources │ └── ... └── torchvision └── models ├── resnet.py # torchvision.models.resnet └── utils.py # torchvision.models.utils", "prev_chunk_id": "chunk_1662", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1664", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Analyzing an object’s dependencies#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Analyzing an object’s dependencies#", "content": "Analyzing an object’s dependencies# When you issue a save_pickle(obj, ...) call, PackageExporter will pickle the object normally. Then, it uses the pickletools standard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object’s type, like: GLOBAL 'torchvision.models.resnet Resnet` The dependency resolver will gather up all GLOBAL ops and mark them as dependencies of your pickled object. For more information about pickling and the pickle format, please consult the Python docs.", "prev_chunk_id": "chunk_1663", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1665", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Analyzing a module’s dependencies#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Analyzing a module’s dependencies#", "content": "Analyzing a module’s dependencies# When a Python module is identified as a dependency, torch.package walks the module’s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the __import__(...) syntax and does not support importlib.import_module calls. In general, you should not expect dynamic imports to be detected by torch.package.", "prev_chunk_id": "chunk_1664", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1666", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Dependency Management#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Dependency Management#", "content": "Dependency Management# torch.package automatically finds the Python modules that your code and objects depend on. This process is called dependency resolution. For each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: - intern: put this module into the package. - extern: declare this module as an external dependency of the package. - mock: stub out this module. - deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part of torch.package: - Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out. This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a module, so that’s what torch.package uses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern with an action using methods on PackageExporter, e.g. my_exporter.intern(\"torchvision.**\") my_exporter.extern(\"numpy\") If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined, and the first action will be taken.", "prev_chunk_id": "chunk_1665", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1667", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "intern#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "intern#", "content": "intern# If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from torchvision, you will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, PackageImporter will look inside your package for that module. If it can’t find that module, an error will be raised. This ensures that each PackageImporter is isolated from the loading environment—even if you have my_interned_module available in both your package and the loading environment, PackageImporter will only use the version in your package. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if you attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.", "prev_chunk_id": "chunk_1666", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1668", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "extern#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "extern#", "content": "extern# If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this list on package_exporter.extern_modules. On package import, when the packaged code tries to import an extern-ed module, PackageImporter will use the default Python importer to find that module, as if you did importlib.import_module(\"my_externed_module\"). If it can’t find that module, an error will be raised. In this way, you can depend on third-party libraries like numpy and scipy from within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility for your package, try to limit your use of extern.", "prev_chunk_id": "chunk_1667", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1669", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "mock#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "mock#", "content": "mock# If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve objects from it (so that from my_mocked_module import foo will not error), but any use of that object will raise a NotImplementedError. mock should be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents. For example, initialization/configuration code, or code only used for debugging/training. Warning: In general, mock should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code, which may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.", "prev_chunk_id": "chunk_1668", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1670", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Refactoring#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Refactoring#", "content": "Refactoring# The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some guidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try to process them. Qualify your imports. For example, instead of writing import foo and later using foo.bar.baz, prefer to write from foo.bar import baz. This more precisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don’t need all of foo. Split up large files with unrelated functionality into smaller ones. If your utils module contains a hodge-podge of unrelated functionality, any module that depends on utils will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define single-purpose modules that can be packaged independently of one another.", "prev_chunk_id": "chunk_1669", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1671", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Patterns#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Patterns#", "content": "Patterns# Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck glob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a separator string, e.g. foo.bar.baz. A pattern contains one or more segments. Segments can be: - A literal string (e.g.foo), which matches exactly. - A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. - A double wildcard (**). This matches against zero or more complete segments. Examples: - torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. - torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch - torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"]) A module will match against this action if it matches any of the patterns. You can also specify patterns to exclude, e.g. exporter.mock(\"**\", exclude=[\"torchvision.**\"]) A module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except torchvision and its submodules. When a module could potentially match against multiple actions, the first action defined will be taken.", "prev_chunk_id": "chunk_1670", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1672", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Avoid global state in your modules#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Avoid global state in your modules#", "content": "Avoid global state in your modules# Python makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to names this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable global state. Mutable global state is quite useful—it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can cause complications when used with torch.package. Every PackageImporter creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure they are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug errors.", "prev_chunk_id": "chunk_1671", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1673", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Types are not shared between packages and the loading environment#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Types are not shared between packages and the loading environment#", "content": "Types are not shared between packages and the loading environment# Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example: from foo import MyClass my_class_instance = MyClass() with PackageExporter(f) as exporter: exporter.save_module(\"foo\") importer = PackageImporter(f) imported_MyClass = importer.import_module(\"foo\").MyClass assert isinstance(my_class_instance, MyClass) # works assert isinstance(my_class_instance, imported_MyClass) # ERROR! In this example, MyClass and imported_MyClass are not the same type. In this specific example, MyClass and imported_MyClass have exactly the same implementation, so you might think it’s okay to consider them the same class. But consider the situation where imported_MyClass is coming from an older package with an entirely different implementation of MyClass — in that case, it’s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes: print(MyClass.__name__) # prints \"foo.MyClass\" print(imported_MyClass.__name__) # prints <torch_package_0>.foo.MyClass That means you should not expect isinstance checks to work when one of the arguments is from a package and the other is not. If you need this functionality, consider the following options: - Doing duck typing (just using the class instead of explicitly checking that it is of a given type). - Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.", "prev_chunk_id": "chunk_1672", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1674", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "How torch.package keeps packages isolated from each other#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "How torch.package keeps packages isolated from each other#", "content": "How torch.package keeps packages isolated from each other# Each PackageImporter instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import other packaged modules, or modules marked extern. If you use multiple PackageImporter instances to load a single package, you will get multiple independent environments that do not interact. This is achieved by extending Python’s import infrastructure with a custom importer. PackageImporter provides the same core API as the importlib importer; namely, it implements the import_module and __import__ methods. When you invoke PackageImporter.import_module(), PackageImporter will construct and return a new module, much as the system importer does. However, PackageImporter patches the returned module to use self (i.e. that PackageImporter instance) to fulfill future import requests by looking in the package rather than searching the user’s Python environment.", "prev_chunk_id": "chunk_1673", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1675", "url": "https://docs.pytorch.org/docs/stable/package.html", "title": "Mangling#", "page_title": "torch.package — PyTorch 2.8 documentation", "breadcrumbs": "Mangling#", "content": "Mangling# To avoid confusion (“is this foo.bar object the one from my package, or the one from my Python environment?”), PackageImporter mangles the __name__ and __file__ of all imported modules, by adding a mangle prefix to them. For __name__, a name like torchvision.models.resnet18 becomes <torch_package_0>.torchvision.models.resnet18. For __file__, a name like torchvision/models/resnet18.py becomes <torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print statements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult mangling.md in torch/package/.", "prev_chunk_id": "chunk_1674", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1676", "url": "https://docs.pytorch.org/docs/stable/torch.overrides.html", "title": "torch.overrides#", "page_title": "torch.overrides — PyTorch 2.8 documentation", "breadcrumbs": "torch.overrides#", "content": "torch.overrides# Created On: Nov 30, 2020 | Last Updated On: Jun 06, 2025 This module exposes various helper functions for the __torch_function__ protocol. See Extending torch Python API for more details on the __torch_function__ protocol.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1677", "url": "https://docs.pytorch.org/docs/stable/special.html", "title": "torch.special#", "page_title": "torch.special — PyTorch 2.8 documentation", "breadcrumbs": "torch.special#", "content": "torch.special# Created On: Mar 04, 2021 | Last Updated On: Jun 18, 2025 The torch.special module, modeled after SciPy’s special module.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1678", "url": "https://docs.pytorch.org/docs/stable/signal.html", "title": "torch.signal#", "page_title": "torch.signal — PyTorch 2.8 documentation", "breadcrumbs": "torch.signal#", "content": "torch.signal# Created On: Oct 14, 2022 | Last Updated On: Jun 18, 2025 The torch.signal module, modeled after SciPy’s signalmodule.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1679", "url": "https://docs.pytorch.org/docs/stable/monitor.html", "title": "torch.monitor#", "page_title": "torch.monitor — PyTorch 2.8 documentation", "breadcrumbs": "torch.monitor#", "content": "torch.monitor# Created On: Jan 12, 2022 | Last Updated On: Jun 11, 2025 torch.monitor provides an interface for logging events and counters from PyTorch. The stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact. For more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used. Event handlers can be registered to handle the events and pass them to an external event sink.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1680", "url": "https://docs.pytorch.org/docs/stable/linalg.html", "title": "torch.linalg#", "page_title": "torch.linalg — PyTorch 2.8 documentation", "breadcrumbs": "torch.linalg#", "content": "torch.linalg# Created On: Aug 07, 2020 | Last Updated On: Jun 17, 2025 Common linear algebra operations. See Linear algebra (torch.linalg) for some common numerical edge-cases.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1681", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "TorchScript#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript#", "content": "TorchScript# Created On: Sep 07, 2018 | Last Updated On: Jun 07, 2025 TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons. For a gentle introduction to TorchScript, see the Introduction to TorchScript tutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the Loading a PyTorch Model in C++ tutorial.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1682", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Mixing Tracing and Scripting#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Mixing Tracing and Scripting#", "content": "Mixing Tracing and Scripting# In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model. Scripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing. Example (calling a traced function in script): import torch def foo(x, y): return 2 * x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3))) @torch.jit.script def bar(x): return traced_foo(x, x) Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly. Example (calling a script function in a traced function): import torch @torch.jit.script def foo(x, y): if x.max() > y.max(): r = x else: r = y return r def bar(x, y, z): return foo(x, y) + z traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3))) This composition also works for nn.Modules as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module. Example (using a traced module): import torch import torchvision class MyScriptModule(torch.nn.Module): def __init__(self): super().__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) def forward(self, input): return self.resnet(input - self.means) my_script_module = torch.jit.script(MyScriptModule())", "prev_chunk_id": "chunk_1681", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1683", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "TorchScript Language#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Language#", "content": "TorchScript Language# TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full TorchScript Language Reference for details.", "prev_chunk_id": "chunk_1682", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1684", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Built-in Functions and Modules#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Built-in Functions and Modules#", "content": "Built-in Functions and Modules# TorchScript supports the use of most PyTorch functions and many Python built-ins. See TorchScript Builtins for a full reference of supported functions.", "prev_chunk_id": "chunk_1683", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1685", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "PyTorch Functions and Modules#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch Functions and Modules#", "content": "PyTorch Functions and Modules# TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the torch namespace, all functions in torch.nn.functional and most modules from torch.nn are supported in TorchScript. See TorchScript Unsupported PyTorch Constructs for a list of unsupported PyTorch functions and modules.", "prev_chunk_id": "chunk_1684", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1686", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Python Functions and Modules#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Python Functions and Modules#", "content": "Python Functions and Modules# Many of Python’s built-in functions are supported in TorchScript. The math module is also supported (see math Module for details), but no other Python modules (built-in or third party) are supported.", "prev_chunk_id": "chunk_1685", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1687", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Python Language Reference Comparison#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Python Language Reference Comparison#", "content": "Python Language Reference Comparison# For a full listing of supported Python features, see Python Language Reference Coverage.", "prev_chunk_id": "chunk_1686", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1688", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Disable JIT for Debugging#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Disable JIT for Debugging#", "content": "Disable JIT for Debugging# Setting the environment variable PYTORCH_JIT=0 will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like pdb to debug the model code. For example: @torch.jit.script def scripted_fn(x : torch.Tensor): for i in range(12): x = x + x return x def fn(x): x = torch.neg(x) import pdb; pdb.set_trace() return scripted_fn(x) traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),)) traced_fn(torch.rand(3, 4)) Debugging this script with pdb works except for when we invoke the @torch.jit.script function. We can globally disable JIT, so that we can call the @torch.jit.script function as a normal Python function and not compile it. If the above script is called disable_jit_example.py, we can invoke it like so: $ PYTORCH_JIT=0 python disable_jit_example.py and we will be able to step into the @torch.jit.script function as a normal Python function. To disable the TorchScript compiler for a specific function, see @torch.jit.ignore.", "prev_chunk_id": "chunk_1687", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1689", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Inspecting Code#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Inspecting Code#", "content": "Inspecting Code# TorchScript provides a code pretty-printer for all ScriptModule instances. This pretty-printer gives an interpretation of the script method’s code as valid Python syntax. For example: @torch.jit.script def foo(len): # type: (int) -> torch.Tensor rv = torch.zeros(3, 4) for i in range(len): if i < 10: rv = rv - 1.0 else: rv = rv + 1.0 return rv print(foo.code) A ScriptModule with a single forward method will have an attribute code, which you can use to inspect the ScriptModule’s code. If the ScriptModule has more than one method, you will need to access .code on the method itself and not the module. We can inspect the code of a method named foo on a ScriptModule by accessing .foo.code. The example above produces this output: def foo(len: int) -> Tensor: rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) rv0 = rv for i in range(len): if torch.lt(i, 10): rv1 = torch.sub(rv0, 1., 1) else: rv1 = torch.add(rv0, 1., 1) rv0 = rv1 return rv0 This is TorchScript’s compilation of the code for the forward method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.", "prev_chunk_id": "chunk_1688", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1690", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Interpreting Graphs#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Interpreting Graphs#", "content": "Interpreting Graphs# TorchScript also has a representation at a lower level than the code pretty-printer, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example: @torch.jit.script def foo(len): # type: (int) -> torch.Tensor rv = torch.zeros(3, 4) for i in range(len): if i < 10: rv = rv - 1.0 else: rv = rv + 1.0 return rv print(foo.graph) graph follows the same rules described in the Inspecting Code section with regard to forward method lookup. The example script above produces the graph: graph(%len.1 : int): %24 : int = prim::Constant[value=1]() %17 : bool = prim::Constant[value=1]() # test.py:10:5 %12 : bool? = prim::Constant() %10 : Device? = prim::Constant() %6 : int? = prim::Constant() %1 : int = prim::Constant[value=3]() # test.py:9:22 %2 : int = prim::Constant[value=4]() # test.py:9:25 %20 : int = prim::Constant[value=10]() # test.py:11:16 %23 : float = prim::Constant[value=1]() # test.py:12:23 %4 : int[] = prim::ListConstruct(%1, %2) %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10 %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5 block0(%i.1 : int, %rv.14 : Tensor): %21 : bool = aten::lt(%i.1, %20) # test.py:11:12 %rv.13 : Tensor = prim::If(%21) # test.py:11:9 block0(): %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18 -> (%rv.3) block1(): %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18 -> (%rv.6) -> (%17, %rv.13) return (%rv) Take the instruction %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10 for example. - %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. - aten::zerosis the operator (equivalent", "prev_chunk_id": "chunk_1689", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1691", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Interpreting Graphs#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Interpreting Graphs#", "content": "totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. - #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associated blocks, namely the prim::Loop and prim::If operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described by a ScriptModule is correct, in both automated and manual fashion, as described below.", "prev_chunk_id": "chunk_1690", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1692", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Tracing Edge Cases#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Tracing Edge Cases#", "content": "Tracing Edge Cases# There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include: - Tracing of control flow that is dependent on inputs (e.g. tensor shapes) - Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.", "prev_chunk_id": "chunk_1691", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1693", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Automatic Trace Checking#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Automatic Trace Checking#", "content": "Automatic Trace Checking# One way to automatically catch many errors in traces is by using check_inputs on the torch.jit.trace() API. check_inputs takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example: def loop_in_traced_fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs) Gives us the following diagnostic information: ERROR: Graphs differed across invocations! Graph diff: graph(%x : Tensor) { %1 : int = prim::Constant[value=0]() %2 : int = prim::Constant[value=0]() %result.1 : Tensor = aten::select(%x, %1, %2) %4 : int = prim::Constant[value=0]() %5 : int = prim::Constant[value=0]() %6 : Tensor = aten::select(%x, %4, %5) %result.2 : Tensor = aten::mul(%result.1, %6) %8 : int = prim::Constant[value=0]() %9 : int = prim::Constant[value=1]() %10 : Tensor = aten::select(%x, %8, %9) - %result : Tensor = aten::mul(%result.2, %10) + %result.3 : Tensor = aten::mul(%result.2, %10) ? ++ %12 : int = prim::Constant[value=0]() %13 : int = prim::Constant[value=2]() %14 : Tensor = aten::select(%x, %12, %13) + %result : Tensor = aten::mul(%result.3, %14) + %16 : int = prim::Constant[value=0]() + %17 : int = prim::Constant[value=3]() + %18 : Tensor = aten::select(%x, %16, %17) - %15 : Tensor = aten::mul(%result, %14) ? ^ ^ + %19 : Tensor = aten::mul(%result, %18) ? ^ ^ - return (%15); ? ^ + return (%19); ? ^ } This message indicates to us that the computation differed between when we first traced it and when we traced it with the check_inputs. Indeed, the loop within the body of loop_in_traced_fn depends on the shape of the input x, and thus when we try another x with a different shape, the trace differs. In this case, data-dependent control flow", "prev_chunk_id": "chunk_1692", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1694", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Automatic Trace Checking#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Automatic Trace Checking#", "content": "like this can be captured using torch.jit.script() instead: def fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] scripted_fn = torch.jit.script(fn) print(scripted_fn.graph) #print(str(scripted_fn.graph).strip()) for input_tuple in [inputs] + check_inputs: torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple)) Which produces: graph(%x : Tensor) { %5 : bool = prim::Constant[value=1]() %1 : int = prim::Constant[value=0]() %result.1 : Tensor = aten::select(%x, %1, %1) %4 : int = aten::size(%x, %1) %result : Tensor = prim::Loop(%4, %5, %result.1) block0(%i : int, %7 : Tensor) { %10 : Tensor = aten::select(%x, %1, %i) %result.2 : Tensor = aten::mul(%7, %10) -> (%5, %result.2) } return (%result); }", "prev_chunk_id": "chunk_1693", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1695", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Tracer Warnings#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Tracer Warnings#", "content": "Tracer Warnings# The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor: def fill_row_zero(x): x[0] = torch.rand(*x.shape[1:2]) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph) Produces several warnings and a graph which simply returns the input: fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe. x[0] = torch.rand(*x.shape[1:2]) fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%) traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) graph(%0 : Float(3, 4)) { return (%0); } We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with torch.cat: def fill_row_zero(x): x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph)", "prev_chunk_id": "chunk_1694", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1696", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Frequently Asked Questions#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Frequently Asked Questions#", "content": "Frequently Asked Questions# Q: I would like to train a model on GPU and do inference on CPU. What are the best practices? Q: How do I store attributes on a ScriptModule? Q: I would like to trace module’s method but I keep getting this error: RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient", "prev_chunk_id": "chunk_1695", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1697", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Known Issues#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Known Issues#", "content": "Known Issues# If you’re using Sequential with TorchScript, the inputs of some of the Sequential submodules may be falsely inferred to be Tensor, even if they’re annotated otherwise. The canonical solution is to subclass nn.Sequential and redeclare forward with the input typed correctly.", "prev_chunk_id": "chunk_1696", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1698", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Migrating to PyTorch 1.2 Recursive Scripting API#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Migrating to PyTorch 1.2 Recursive Scripting API#", "content": "Migrating to PyTorch 1.2 Recursive Scripting API# This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1. torch.jit.script will now attempt to recursively compile functions, methods, and classes that it encounters. Once you call torch.jit.script, compilation is “opt-out”, rather than “opt-in”. 2. torch.jit.script(nn_module_instance) is now the preferred way to create ScriptModules, instead of inheriting from torch.jit.ScriptModule. These changes combine to provide a simpler, easier-to-use API for converting your nn.Modules into ScriptModules, ready to be optimized and executed in a non-Python environment. The new usage looks like this: import torch import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) my_model = Model() my_scripted_model = torch.jit.script(my_model) - The module’sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. - To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. - To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the - method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. - Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. - Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. - Python 3 type hints can be used in place oftorch.jit.annotate", "prev_chunk_id": "chunk_1697", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1699", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Modules#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Modules#", "content": "Modules# When passed to the torch.jit.script function, a torch.nn.Module's data is copied to a ScriptModule and the TorchScript compiler compiles the module. The module’s forward is compiled by default. Methods called from forward are lazily compiled in the order they are used in forward, as well as any @torch.jit.export methods.", "prev_chunk_id": "chunk_1698", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1700", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Functions#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Functions#", "content": "Functions# Functions don’t change much, they can be decorated with @torch.jit.ignore or torch.jit.unused if needed. # Same behavior as pre-PyTorch 1.2 @torch.jit.script def some_fn(): return 2 # Marks a function as ignored, if nothing # ever calls it then this has no effect @torch.jit.ignore def some_fn2(): return 2 # As with ignore, if nothing calls it then it has no effect. # If it is called in script it is replaced with an exception. @torch.jit.unused def some_fn3(): import pdb; pdb.set_trace() return 4 # Doesn't do anything, this function is already # the main entry point @torch.jit.export def some_fn4(): return 2", "prev_chunk_id": "chunk_1699", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1701", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "TorchScript Classes#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "TorchScript Classes#", "content": "TorchScript Classes# Everything in a user defined TorchScript Class is exported by default, functions can be decorated with @torch.jit.ignore if needed.", "prev_chunk_id": "chunk_1700", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1702", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Attributes#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Attributes#", "content": "Attributes# The TorchScript compiler needs to know the types of module attributes. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with PEP 526-style class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting ScriptModule Old API: from typing import Dict import torch class MyModule(torch.jit.ScriptModule): def __init__(self): super().__init__() self.my_dict = torch.jit.Attribute({}, Dict[str, int]) self.my_int = torch.jit.Attribute(20, int) m = MyModule() New API: from typing import Dict class MyModule(torch.nn.Module): my_dict: Dict[str, int] def __init__(self): super().__init__() # This type cannot be inferred and must be specified self.my_dict = {} # The attribute type here is inferred to be `int` self.my_int = 20 def forward(self): pass m = torch.jit.script(MyModule())", "prev_chunk_id": "chunk_1701", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1703", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Constants#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Constants#", "content": "Constants# The Final type constructor can be used to mark members as constant. If members are not marked constant, they will be copied to the resulting ScriptModule as an attribute. Using Final opens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: class MyModule(torch.jit.ScriptModule): __constants__ = ['my_constant'] def __init__(self): super().__init__() self.my_constant = 2 def forward(self): pass m = MyModule() New API: from typing import Final class MyModule(torch.nn.Module): my_constant: Final[int] def __init__(self): super().__init__() self.my_constant = 2 def forward(self): pass m = torch.jit.script(MyModule())", "prev_chunk_id": "chunk_1702", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1704", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Variables#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Variables#", "content": "Variables# Containers are assumed to have type Tensor and be non-optional (see Default Types for more information). Previously, torch.jit.annotate was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported. import torch from typing import Dict, Optional @torch.jit.script def make_dict(flag: bool): x: Dict[str, int] = {} x['hi'] = 2 b: Optional[int] = None if flag: b = 2 return x, b", "prev_chunk_id": "chunk_1703", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1705", "url": "https://docs.pytorch.org/docs/stable/jit.html", "title": "Fusion Backends#", "page_title": "TorchScript — PyTorch 2.8 documentation", "breadcrumbs": "Fusion Backends#", "content": "Fusion Backends# There are a couple of fusion backends available to optimize TorchScript execution. The default fuser on CPUs is NNC, which can perform fusions for both CPUs and GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators and has demonstrated generated kernels with improved throughput. See the NVFuser documentation for more details on usage and debugging.", "prev_chunk_id": "chunk_1704", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1706", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "torch.hub#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "torch.hub#", "content": "torch.hub# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1707", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Publishing models#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Publishing models#", "content": "Publishing models# Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a GitHub repository by adding a simple hubconf.py file; hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish). def entrypoint_name(*args, **kwargs): # args & kwargs are optional, for models which take positional/keyword arguments. ...", "prev_chunk_id": "chunk_1706", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1708", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "How to implement an entrypoint?#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "How to implement an entrypoint?#", "content": "How to implement an entrypoint?# Here is a code snippet specifies an entrypoint for resnet18 model if we expand the implementation in pytorch/vision/hubconf.py. In most case importing the right function in hubconf.py is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo dependencies = ['torch'] from torchvision.models.resnet import resnet18 as _resnet18 # resnet18 is the name of entrypoint def resnet18(pretrained=False, **kwargs): \"\"\" # This docstring shows up in hub.help() Resnet18 model pretrained (bool): kwargs, load pretrained weights into the model \"\"\" # Call the model, load pretrained weights model = _resnet18(pretrained=pretrained, **kwargs) return model - dependenciesvariable is alistof package names required toloadthe model. Note this might be slightly different from dependencies required for training a model. - argsandkwargsare passed along to the real callable function. - Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here. - Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. - Callables prefixed with underscore are considered as helper functions which won’t show up intorch.hub.list(). - Pretrained weights can either be stored locally in the GitHub repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it’s recommended to attach it to aproject releaseand use the url from the release. In the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. if pretrained: # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth dirname = os.path.dirname(__file__) checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>) state_dict = torch.load(checkpoint) model.load_state_dict(state_dict) # For checkpoint saved elsewhere checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth' model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))", "prev_chunk_id": "chunk_1707", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1709", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Important Notice#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Important Notice#", "content": "Important Notice# - The published models should be at least in a branch/tag. It can’t be a random commit.", "prev_chunk_id": "chunk_1708", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1710", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Loading models from Hub#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Loading models from Hub#", "content": "Loading models from Hub# Pytorch Hub provides convenient APIs to explore all available models in hub through torch.hub.list(), show docstring and examples through torch.hub.help() and load the pre-trained models using torch.hub.load().", "prev_chunk_id": "chunk_1709", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1711", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Running a loaded model:#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Running a loaded model:#", "content": "Running a loaded model:# Note that *args and **kwargs in torch.hub.load() are used to instantiate a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is - dir(model)to see all available methods of the model. - help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.", "prev_chunk_id": "chunk_1710", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1712", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Where are my downloaded models saved?#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Where are my downloaded models saved?#", "content": "Where are my downloaded models saved?# The locations are used in the order of - Callinghub.set_dir(<PATH_TO_HUB_DIR>) - $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. - $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. - ~/.cache/torch/hub", "prev_chunk_id": "chunk_1711", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1713", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Caching logic#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Caching logic#", "content": "Caching logic# By default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by get_dir(). Users can force a reload by calling hub.load(..., force_reload=True). This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.", "prev_chunk_id": "chunk_1712", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1714", "url": "https://docs.pytorch.org/docs/stable/hub.html", "title": "Known limitations:#", "page_title": "torch.hub — PyTorch 2.8 documentation", "breadcrumbs": "Known limitations:#", "content": "Known limitations:# Torch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches sys.modules and sys.path_importer_cache which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a model subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the sys.modules dict; more details can be found in this GitHub issue. A known limitation that is worth mentioning here: users CANNOT load two different branches of the same repo in the same python process. It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.", "prev_chunk_id": "chunk_1713", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1715", "url": "https://docs.pytorch.org/docs/stable/fx.experimental.html", "title": "torch.fx.experimental#", "page_title": "torch.fx.experimental — PyTorch 2.8 documentation", "breadcrumbs": "torch.fx.experimental#", "content": "torch.fx.experimental# Created On: Feb 07, 2024 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1716", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "torch.fx#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "torch.fx#", "content": "torch.fx# Created On: Dec 15, 2020 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1717", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Overview#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action: import torch # Simple module for demonstration class MyModule(torch.nn.Module): def __init__(self) -> None: super().__init__() self.param = torch.nn.Parameter(torch.rand(3, 4)) self.linear = torch.nn.Linear(4, 5) def forward(self, x): return self.linear(x + self.param).clamp(min=0.0, max=1.0) module = MyModule() from torch.fx import symbolic_trace # Symbolic tracing frontend - captures the semantics of the module symbolic_traced: torch.fx.GraphModule = symbolic_trace(module) # High-level intermediate representation (IR) - Graph representation print(symbolic_traced.graph) \"\"\" graph(): %x : [num_users=1] = placeholder[target=x] %param : [num_users=1] = get_attr[target=param] %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {}) %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {}) %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0}) return clamp \"\"\" # Code generation - valid Python code print(symbolic_traced.code) \"\"\" def forward(self, x): param = self.param add = x + param; x = param = None linear = self.linear(add); add = None clamp = linear.clamp(min = 0.0, max = 1.0); linear = None return clamp \"\"\" The symbolic tracer performs “symbolic execution” of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation. The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or Module-to-Module)", "prev_chunk_id": "chunk_1716", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1718", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Overview#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph’s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX! Several example transformations can be found at the examples repository.", "prev_chunk_id": "chunk_1717", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1719", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Writing Transformations#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Writing Transformations#", "content": "Writing Transformations# What is an FX transform? Essentially, it’s a function that looks like this. import torch import torch.fx def transform(m: nn.Module, tracer_class : type = torch.fx.Tracer) -> torch.nn.Module: # Step 1: Acquire a Graph representing the code in `m` # NOTE: torch.fx.symbolic_trace is a wrapper around a call to # fx.Tracer.trace and constructing a GraphModule. We'll # split that out in our transform to allow the caller to # customize tracing behavior. graph : torch.fx.Graph = tracer_class().trace(m) # Step 2: Modify this Graph or create a new one graph = ... # Step 3: Construct a Module to return return torch.fx.GraphModule(m, graph) Your transform will take in a torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module – you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability. Note that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph. Given that you’ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph.", "prev_chunk_id": "chunk_1718", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1720", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "A Quick Primer on Graphs#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "A Quick Primer on Graphs#", "content": "A Quick Primer on Graphs# Full treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is: - What are the inputs to the method? - What are the operations that run inside the method? - What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances. Let’s see what we mean by that with a short example: import torch import torch.fx class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.param = torch.nn.Parameter(torch.rand(3, 4)) self.linear = torch.nn.Linear(4, 5) def forward(self, x): return torch.topk(torch.sum( self.linear(x + self.linear.weight).relu(), dim=-1), 3) m = MyModule() gm = torch.fx.symbolic_trace(m) gm.graph.print_tabular() Here we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph: We can use this information to answer the questions we posed above. - What are the inputs to the method? In FX, method inputs are specified via specialplaceholdernodes. In this case, we have a singleplaceholdernode with atargetofx, meaning we have a single (non-self) argument named x. - What are the operations within the method? Theget_attr,call_function,call_module, andcall_methodnodes represent the operations in the method. A full treatment of the semantics of all of these can be found in theNodedocumentation. - What is the return value of the method? The return value in aGraphis specified by a specialoutputnode. Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph.", "prev_chunk_id": "chunk_1719", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1721", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Direct Graph Manipulation#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Direct Graph Manipulation#", "content": "Direct Graph Manipulation# One approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let’s say we desire to replace torch.add() calls with torch.mul() calls. import torch import torch.fx # Sample module class M(torch.nn.Module): def forward(self, x, y): return torch.add(x, y) def transform(m: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module: graph : fx.Graph = tracer_class().trace(m) # FX represents its Graph as an ordered list of # nodes, so we can iterate through them. for node in graph.nodes: # Checks if we're calling a function (i.e: # torch.add) if node.op == 'call_function': # The target attribute is the function # that call_function calls. if node.target == torch.add: node.target = torch.mul graph.lint() # Does some checks to make sure the # Graph is well-formed. return fx.GraphModule(m, graph) We can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below. # Specifies the insertion point. Any nodes added to the # Graph within this scope will be inserted after `node` with traced.graph.inserting_after(node): # Insert a new `call_function` node calling `torch.relu` new_node = traced.graph.call_function( torch.relu, args=(node,)) # We want all places that used the value of `node` to # now use that value after the `relu` call we've added. # We use the `replace_all_uses_with` API to do this. node.replace_all_uses_with(new_node) For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter.", "prev_chunk_id": "chunk_1720", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1722", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Subgraph Rewriting With replace_pattern()#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Subgraph Rewriting With replace_pattern()#", "content": "Subgraph Rewriting With replace_pattern()# FX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a “find/replace” tool for editing Graph\\s. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.", "prev_chunk_id": "chunk_1721", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1723", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Graph Manipulation Examples#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Graph Manipulation Examples#", "content": "Graph Manipulation Examples# - Replace one op - Conv/Batch Norm fusion - replace_pattern: Basic usage - Quantization - Invert Transformation", "prev_chunk_id": "chunk_1722", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1724", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Proxy/Retracing#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Proxy/Retracing#", "content": "Proxy/Retracing# Another way of manipulating Graph\\s is by reusing the Proxy machinery used in symbolic tracing. For example, let’s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arguments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. # Note that this decomposition rule can be read as regular Python def relu_decomposition(x): return (x > 0) * x decomposition_rules = {} decomposition_rules[F.relu] = relu_decomposition def decompose(model: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module: \"\"\" Decompose `model` into smaller constituent operations. Currently,this only supports decomposing ReLU into its mathematical definition: (x > 0) * x \"\"\" graph : fx.Graph = tracer_class().trace(model) new_graph = fx.Graph() env = {} tracer = torch.fx.proxy.GraphAppendingTracer(new_graph) for node in graph.nodes: if node.op == 'call_function' and node.target in decomposition_rules: # By wrapping the arguments with proxies, # we can dispatch to the appropriate # decomposition rule and implicitly add it # to the Graph by symbolically tracing it. proxy_args = [ fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args] output_proxy = decomposition_rules[node.target](*proxy_args) # Operations on `Proxy` always yield new `Proxy`s, and the # return value of our decomposition rule is no exception. # We need to extract the underlying `Node` from the `Proxy` # to use it in subsequent iterations", "prev_chunk_id": "chunk_1723", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1725", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Proxy/Retracing#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Proxy/Retracing#", "content": "of this transform. new_node = output_proxy.node env[node.name] = new_node else: # Default case: we don't have a decomposition rule for this # node, so just copy the node over into the new graph. new_node = new_graph.node_copy(node, lambda x: env[x.name]) env[node.name] = new_node return fx.GraphModule(model, new_graph) In addition to avoiding explicit graph manipulation, using Proxy\\s also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling Proxy we also passed a tracer pointing to the underlying variable graph. This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to Proxy does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using Proxy especially when the underlying operators can not be safely assumed to be unary. A worked example of using Proxy\\s for Graph manipulation can be found here.", "prev_chunk_id": "chunk_1724", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1726", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "The Interpreter Pattern#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "The Interpreter Pattern#", "content": "The Interpreter Pattern# A useful code organizational pattern in FX is to loop over all the Node\\s in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxy\\s. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like: import torch import torch.fx from torch.fx.node import Node from typing import Dict class ShapeProp: \"\"\" Shape propagation. This class takes a `GraphModule`. Then, its `propagate` method executes the `GraphModule` node-by-node with the given arguments. As each operation executes, the ShapeProp class stores away the shape and element type for the output values of each operation on the `shape` and `dtype` attributes of the operation's `Node`. \"\"\" def __init__(self, mod): self.mod = mod self.graph = mod.graph self.modules = dict(self.mod.named_modules()) def propagate(self, *args): args_iter = iter(args) env : Dict[str, Node] = {} def load_arg(a): return torch.fx.graph.map_arg(a, lambda n: env[n.name]) def fetch_attr(target : str): target_atoms = target.split('.') attr_itr = self.mod for i, atom in enumerate(target_atoms): if not hasattr(attr_itr, atom): raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i])}\") attr_itr = getattr(attr_itr, atom) return attr_itr for node in self.graph.nodes: if node.op == 'placeholder': result = next(args_iter) elif node.op == 'get_attr': result = fetch_attr(node.target) elif node.op == 'call_function': result = node.target(*load_arg(node.args), **load_arg(node.kwargs)) elif node.op == 'call_method': self_obj, *args = load_arg(node.args) kwargs = load_arg(node.kwargs) result = getattr(self_obj, node.target)(*args, **kwargs) elif node.op == 'call_module': result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs)) # This is the only code specific to shape propagation. # you can delete this `if` branch and this becomes # a generic GraphModule interpreter. if isinstance(result, torch.Tensor): node.shape = result.shape node.dtype = result.dtype env[node.name] = result return load_arg(self.graph.result) As you can see, a", "prev_chunk_id": "chunk_1725", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1727", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "The Interpreter Pattern#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "The Interpreter Pattern#", "content": "full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides. In addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods.", "prev_chunk_id": "chunk_1726", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1728", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Examples of the Interpreter Pattern#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Examples of the Interpreter Pattern#", "content": "Examples of the Interpreter Pattern# - ShapePropagation - Performance Profiler", "prev_chunk_id": "chunk_1727", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1729", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Introduction#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Introduction#", "content": "Introduction# Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code. If you’re not familiar with debuggers, please see the auxiliary section Available Debuggers.", "prev_chunk_id": "chunk_1728", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1730", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Common Pitfalls in Transform Authoring#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Common Pitfalls in Transform Authoring#", "content": "Common Pitfalls in Transform Authoring# - Nondeterministicsetiteration order. In Python, thesetdatatype is unordered. Usingsetto contain collections of objects likeNode\\ s, for example, can cause unexpected nondeterminism. An example is iterating over a set ofNodes to insert them into aGraph. Because thesetdata type is unordered, the ordering of the operations in the output program will be nondeterministic and can change across program invocations. The recommended alternative is to use adictdata type, which isinsertion orderedas of Python 3.7 (and as of cPython 3.6). Adictcan be used equivalently to a set by storing values to be deduplicated in the keys of thedict.", "prev_chunk_id": "chunk_1729", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1731", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Checking Correctness of Modules#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Checking Correctness of Modules#", "content": "Checking Correctness of Modules# Because the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let’s use an example: import torch import torch.fx import torchvision.models as models def transform(m : torch.nn.Module) -> torch.nn.Module: gm = torch.fx.symbolic_trace(m) # Imagine we're doing some transforms here # <...> gm.recompile() return gm resnet18 = models.resnet18() transformed_resnet18 = transform(resnet18) input_image = torch.randn(5, 3, 224, 224) assert resnet18(input_image) == transformed_resnet18(input_image) \"\"\" RuntimeError: Boolean value of Tensor with more than one value is ambiguous \"\"\" Here, we’ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold: assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image)) This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.", "prev_chunk_id": "chunk_1730", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1732", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Debugging the Generated Code#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Debugging the Generated Code#", "content": "Debugging the Generated Code# Because FX generates the forward() function on GraphModule\\s, using traditional debugging techniques like print statements or pdb is not as straightforward. Luckily, we have several techniques we can use for debugging the generated code.", "prev_chunk_id": "chunk_1731", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1733", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Use pdb#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Use pdb#", "content": "Use pdb# Invoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked. import torch import torch.fx import torchvision.models as models def my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module: graph = tracer_class().trace(inp) # Transformation logic here # <...> # Return new Module return fx.GraphModule(inp, graph) my_module = models.resnet18() my_module_transformed = my_pass(my_module) input_value = torch.randn(5, 3, 224, 224) # When this line is executed at runtime, we will be dropped into an # interactive `pdb` prompt. We can use the `step` or `s` command to # step into the execution of the next line import pdb; pdb.set_trace() my_module_transformed(input_value)", "prev_chunk_id": "chunk_1732", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1734", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Print the Generated Code#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Print the Generated Code#", "content": "Print the Generated Code# If you’d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there. # Assume that `traced` is a GraphModule that has undergone some # number of transforms # Copy this code for later print(traced) # Print the code generated from symbolic tracing. This outputs: \"\"\" def forward(self, y): x = self.x add_1 = x + y; x = y = None return add_1 \"\"\" # Subclass the original Module class SubclassM(M): def __init__(self): super().__init__() # Paste the generated `forward` function (the one we printed and # copied above) here def forward(self, y): x = self.x add_1 = x + y; x = y = None return add_1 # Create an instance of the original, untraced Module. Then, create an # instance of the Module with the copied `forward` function. We can # now compare the output of both the original and the traced version. pre_trace = M() post_trace = SubclassM()", "prev_chunk_id": "chunk_1733", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1735", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Use the to_folder Function From GraphModule#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Use the to_folder Function From GraphModule#", "content": "Use the to_folder Function From GraphModule# GraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder. m = symbolic_trace(M()) m.to_folder(\"foo\", \"Bar\") from foo import Bar y = Bar() After running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code.", "prev_chunk_id": "chunk_1734", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1736", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Debugging the Transformation#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Debugging the Transformation#", "content": "Debugging the Transformation# Now that we’ve identified that a transformation is creating incorrect code, it’s time to debug the transformation itself. First, we’ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module: # Sample Module class M(torch.nn.Module): def forward(self, x, y): return x + y # Create an instance of `M` m = M() # Symbolically trace an instance of `M` (returns a GraphModule). In # this example, we'll only be discussing how to inspect a # GraphModule, so we aren't showing any sample transforms for the # sake of brevity. traced = symbolic_trace(m) # Print the code produced by tracing the module. print(traced) # The generated `forward` function is: \"\"\" def forward(self, x, y): add = x + y; x = y = None return add \"\"\" # Print the internal Graph. print(traced.graph) # This print-out returns: \"\"\" graph(): %x : [num_users=1] = placeholder[target=x] %y : [num_users=1] = placeholder[target=y] %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {}) return add \"\"\" # Print a tabular representation of the internal Graph. traced.graph.print_tabular() # This gives us: \"\"\" opcode name target args kwargs ------------- ------ ----------------------- ------ -------- placeholder x x () {} placeholder y y () {} call_function add <built-in function add> (x, y) {} output output output (add,) {} \"\"\" Using the utility functions above, we can compare our traced Module before and after we’ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it’s still not clear what’s going wrong, a debugger like pdb can be", "prev_chunk_id": "chunk_1735", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1737", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Debugging the Transformation#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Debugging the Transformation#", "content": "a good next step. Going off of the example above, consider the following code: # Sample user-defined function def transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module: # Get the Graph from our traced Module g = tracer_class().trace(module) \"\"\" Transformations on `g` go here \"\"\" return fx.GraphModule(module, g) # Transform the Graph transformed = transform_graph(traced) # Print the new code after our transforms. Check to see if it was # what we expected print(transformed) Using the above example, let’s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what’s happening during the transform by breaking on transform_graph(traced), then pressing s to “step into” the call to transform_graph(traced). We may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node’s input_nodes and users.)", "prev_chunk_id": "chunk_1736", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1738", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Available Debuggers#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Available Debuggers#", "content": "Available Debuggers# The most common Python debugger is pdb. You can start your program in “debug mode” with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It’s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you’re running your file in debug mode, you can step through the code and examine your program’s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython’s “Python Debugging With Pdb”. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View → Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).", "prev_chunk_id": "chunk_1737", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1739", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Limitations of Symbolic Tracing#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Limitations of Symbolic Tracing#", "content": "Limitations of Symbolic Tracing# FX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some limitations.", "prev_chunk_id": "chunk_1738", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1740", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Dynamic Control Flow#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Dynamic Control Flow#", "content": "Dynamic Control Flow# The main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program. For example, let’s examine the following program: def func_to_trace(x): if x.sum() > 0: return torch.relu(x) else: return torch.neg(x) traced = torch.fx.symbolic_trace(func_to_trace) \"\"\" <...> File \"dyn.py\", line 6, in func_to_trace if x.sum() > 0: File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__ return self.tracer.to_bool(self) File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool raise TraceError('symbolically traced variables cannot be used as inputs to control flow') torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow \"\"\" The condition to the if statement relies on the value of x.sum(), which relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens.", "prev_chunk_id": "chunk_1739", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1741", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Static Control Flow#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Static Control Flow#", "content": "Static Control Flow# On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example: import torch import torch.fx class MyModule(torch.nn.Module): def __init__(self, do_activation : bool = False): super().__init__() self.do_activation = do_activation self.linear = torch.nn.Linear(512, 512) def forward(self, x): x = self.linear(x) # This if-statement is so-called static control flow. # Its condition does not depend on any input values if self.do_activation: x = torch.relu(x) return x without_activation = MyModule(do_activation=False) with_activation = MyModule(do_activation=True) traced_without_activation = torch.fx.symbolic_trace(without_activation) print(traced_without_activation.code) \"\"\" def forward(self, x): linear_1 = self.linear(x); x = None return linear_1 \"\"\" traced_with_activation = torch.fx.symbolic_trace(with_activation) print(traced_with_activation.code) \"\"\" import torch def forward(self, x): linear_1 = self.linear(x); x = None relu_1 = torch.relu(linear_1); linear_1 = None return relu_1 \"\"\" The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by binding concrete values to arguments during symbolic tracing: def f(x, flag): if flag: return x else: return x*2 fx.symbolic_trace(f) # Fails! fx.symbolic_trace(f, concrete_args={'flag': True}) In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather", "prev_chunk_id": "chunk_1740", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1742", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Static Control Flow#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Static Control Flow#", "content": "than tracing through them.", "prev_chunk_id": "chunk_1741", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1743", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Non- torch Functions#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Non- torch Functions#", "content": "Non- torch Functions# FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example: import torch import torch.fx from math import sqrt def normalize(x): \"\"\" Normalize `x` by the size of the batch dimension \"\"\" return x / sqrt(len(x)) # It's valid Python code normalize(torch.rand(3, 4)) traced = torch.fx.symbolic_trace(normalize) \"\"\" <...> File \"sqrt.py\", line 9, in normalize return x / sqrt(len(x)) File \"pytorch/torch/fx/proxy.py\", line 161, in __len__ raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \" RuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope \"\"\" The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API: torch.fx.wrap('len') torch.fx.wrap('sqrt') traced = torch.fx.symbolic_trace(normalize) print(traced.code) \"\"\" import math def forward(self, x): len_1 = len(x) sqrt_1 = math.sqrt(len_1); len_1 = None truediv = x / sqrt_1; x = sqrt_1 = None return truediv \"\"\"", "prev_chunk_id": "chunk_1742", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1744", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Customizing Tracing with the Tracer class#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Customizing Tracing with the Tracer class#", "content": "Customizing Tracing with the Tracer class# The Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so: class MyCustomTracer(torch.fx.Tracer): # Inside here you can override various methods # to customize tracing. See the `Tracer` API # reference pass # Let's use this custom tracer to trace through this module class MyModule(torch.nn.Module): def forward(self, x): return torch.relu(x) + torch.ones(3, 4) mod = MyModule() traced_graph = MyCustomTracer().trace(mod) # trace() returns a Graph. Let's wrap it up in a # GraphModule to make it runnable traced = torch.fx.GraphModule(mod, traced_graph)", "prev_chunk_id": "chunk_1743", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1745", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Leaf Modules#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Leaf Modules#", "content": "Leaf Modules# Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example: class MySpecialSubmodule(torch.nn.Module): def forward(self, x): return torch.neg(x) class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(3, 4) self.submod = MySpecialSubmodule() def forward(self, x): return self.submod(self.linear(x)) traced = torch.fx.symbolic_trace(MyModule()) print(traced.code) # `linear` is preserved as a call, yet `submod` is traced though. # This is because the default set of \"Leaf Modules\" includes all # standard `torch.nn` modules. \"\"\" import torch def forward(self, x): linear_1 = self.linear(x); x = None neg_1 = torch.neg(linear_1); linear_1 = None return neg_1 \"\"\" The set of leaf modules can be customized by overriding Tracer.is_leaf_module().", "prev_chunk_id": "chunk_1744", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1746", "url": "https://docs.pytorch.org/docs/stable/fx.html", "title": "Miscellanea#", "page_title": "torch.fx — PyTorch 2.8 documentation", "breadcrumbs": "Miscellanea#", "content": "Miscellanea# - Tensor constructors (e.g.torch.zeros,torch.ones,torch.rand,torch.randn,torch.sparse_coo_tensor) are currently not traceable.The deterministic constructors (zeros,ones) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case,ones_likeorzeros_likemay be a viable substitute.Nondeterministic constructors (rand,randn) will have a single random value embedded in the trace. This is likely not the intended behavior. One workaround is to wraptorch.randnin atorch.fx.wrapfunction and call that instead.@torch.fx.wrapdeftorch_randn(x,shape):returntorch.randn(shape)deff(x):returnx+torch_randn(x,5)fx.symbolic_trace(f)This behavior may be fixed in a future release. - Type annotationsPython 3-style type annotations (e.g.func(x:torch.Tensor,y:int)->torch.Tensor) are supported and will be preserved by symbolic tracing.Python 2-style comment type annotations#type:(torch.Tensor,int)->torch.Tensorare not currently supported.Annotations on local names within a function are not currently supported. - Gotcha aroundtrainingflag and submodulesWhen using functionals liketorch.nn.functional.dropout, it will be common for the training argument to be passed in asself.training. During FX tracing, this will likely be baked in as a constant value.importtorchimporttorch.fxclassDropoutRepro(torch.nn.Module):defforward(self,x):returntorch.nn.functional.dropout(x,training=self.training)traced=torch.fx.symbolic_trace(DropoutRepro())print(traced.code)\"\"\"def forward(self, x):dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False); x = Nonereturn dropout\"\"\"traced.eval()x=torch.randn(5,3)torch.testing.assert_close(traced(x),x)\"\"\"AssertionError: Tensor-likes are not close!Mismatched elements: 15 / 15 (100.0%)Greatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)Greatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\"\"\"However, when the standardnn.Dropout()submodule is used, the training flag is encapsulated and–because of the preservation of thenn.Moduleobject model–can be changed.classDropoutRepro2(torch.nn.Module):def__init__(self):super().__init__()self.drop=torch.nn.Dropout()defforward(self,x):returnself.drop(x)traced=torch.fx.symbolic_trace(DropoutRepro2())print(traced.code)\"\"\"def forward(self, x):drop = self.drop(x); x = Nonereturn drop\"\"\"traced.eval()x=torch.randn(5,3)torch.testing.assert_close(traced(x),x) - Because of this difference, consider marking modules that interact with thetrainingflag dynamically as leaf modules.", "prev_chunk_id": "chunk_1745", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1747", "url": "https://docs.pytorch.org/docs/stable/futures.html", "title": "torch.futures#", "page_title": "torch.futures — PyTorch 2.8 documentation", "breadcrumbs": "torch.futures#", "content": "torch.futures# Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025 This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1748", "url": "https://docs.pytorch.org/docs/stable/func.html", "title": "torch.func#", "page_title": "torch.func — PyTorch 2.8 documentation", "breadcrumbs": "torch.func#", "content": "torch.func# Created On: Jun 11, 2025 | Last Updated On: Jun 11, 2025 torch.func, previously known as “functorch”, is JAX-like composable function transforms for PyTorch.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1749", "url": "https://docs.pytorch.org/docs/stable/func.html", "title": "What are composable function transforms?#", "page_title": "torch.func — PyTorch 2.8 documentation", "breadcrumbs": "What are composable function transforms?#", "content": "What are composable function transforms?# - A “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity. - torch.funchas auto-differentiation transforms (grad(f)returns a function that computes the gradient off), a vectorization/batching transform (vmap(f)returns a function that computesfover batches of inputs), and others. - These function transforms can compose with each other arbitrarily. For example, composingvmap(grad(f))computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.", "prev_chunk_id": "chunk_1748", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1750", "url": "https://docs.pytorch.org/docs/stable/func.html", "title": "Why composable function transforms?#", "page_title": "torch.func — PyTorch 2.8 documentation", "breadcrumbs": "Why composable function transforms?#", "content": "Why composable function transforms?# There are a number of use cases that are tricky to do in PyTorch today: - computing per-sample-gradients (or other per-sample quantities) - running ensembles of models on a single machine - efficiently batching together tasks in the inner-loop of MAML - efficiently computing Jacobians and Hessians - efficiently computing batched Jacobians and Hessians Composing vmap(), grad(), and vjp() transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the JAX framework.", "prev_chunk_id": "chunk_1749", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1751", "url": "https://docs.pytorch.org/docs/stable/fft.html", "title": "torch.fft#", "page_title": "torch.fft — PyTorch 2.8 documentation", "breadcrumbs": "torch.fft#", "content": "torch.fft# Created On: Aug 06, 2020 | Last Updated On: Jun 13, 2025 Discrete Fourier transforms and related functions.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1752", "url": "https://docs.pytorch.org/docs/stable/torch.compiler.html", "title": "torch.compiler#", "page_title": "torch.compiler — PyTorch 2.8 documentation", "breadcrumbs": "torch.compiler#", "content": "torch.compiler# Created On: Jul 28, 2023 | Last Updated On: Jun 06, 2025 torch.compiler is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is torch.compile. torch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. torch.compile is written in Python and it marks the transition of PyTorch from C++ to Python. torch.compile leverages the following underlying technologies: - TorchDynamo (torch._dynamo)is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through thetorch.compilernamespace. - TorchInductoris the defaulttorch.compiledeep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups throughtorch.compilepossible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key building block. - AOT Autogradcaptures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor. As mentioned above, to run your workflows faster, torch.compile through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as inductor, TorchDynamo has a list of supported backends developed by our partners, which can be see by running torch.compiler.list_backends() each of which with its optional dependencies. Some of the most commonly used backends include: Training & inference backends Inference-only backends", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1753", "url": "https://docs.pytorch.org/docs/stable/distributions.html", "title": "Probability distributions - torch.distributions#", "page_title": "Probability distributions - torch.distributions — PyTorch 2.8 documentation", "breadcrumbs": "Probability distributions - torch.distributions#", "content": "Probability distributions - torch.distributions# Created On: Oct 19, 2017 | Last Updated On: Jun 13, 2025 The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package. It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x)f(x)f(x), the pathwise derivative requires the derivative f′(x)f'(x)f′(x). The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs .", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1754", "url": "https://docs.pytorch.org/docs/stable/distributions.html", "title": "Score function#", "page_title": "Probability distributions - torch.distributions — PyTorch 2.8 documentation", "breadcrumbs": "Score function#", "content": "Score function# When the probability density function is differentiable with respect to its parameters, we only need sample() and log_prob() to implement REINFORCE: where θ\\thetaθ are the parameters, α\\alphaα is the learning rate, rrr is the reward and p(a∣πθ(s))p(a|\\pi^\\theta(s))p(a∣πθ(s)) is the probability of taking action aaa in state sss given policy πθ\\pi^\\thetaπθ. In practice we would sample an action from the output of a network, apply this action in an environment, and then use log_prob to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows: probs = policy_network(state) # Note that this is equivalent to what used to be called multinomial m = Categorical(probs) action = m.sample() next_state, reward = env.step(action) loss = -m.log_prob(action) * reward loss.backward()", "prev_chunk_id": "chunk_1753", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1755", "url": "https://docs.pytorch.org/docs/stable/distributions.html", "title": "Pathwise derivative#", "page_title": "Probability distributions - torch.distributions — PyTorch 2.8 documentation", "breadcrumbs": "Pathwise derivative#", "content": "Pathwise derivative# The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the rsample() method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows: params = policy_network(state) m = Normal(*params) # Any distribution with .has_rsample == True could work based on the application action = m.rsample() next_state, reward = env.step(action) # Assuming that reward is differentiable loss = -reward loss.backward()", "prev_chunk_id": "chunk_1754", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1756", "url": "https://docs.pytorch.org/docs/stable/distributions.html", "title": "Constraint Registry#", "page_title": "Probability distributions - torch.distributions — PyTorch 2.8 documentation", "breadcrumbs": "Constraint Registry#", "content": "Constraint Registry# PyTorch provides two global ConstraintRegistry objects that link Constraint objects to Transform objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity. - biject_to(constraint)looks up a bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is guaranteed to have.bijective=Trueand should implement.log_abs_det_jacobian(). - transform_to(constraint)looks up a not-necessarily bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is not guaranteed to implement.log_abs_det_jacobian(). The transform_to() registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s .arg_constraints dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam: loc = torch.zeros(100, requires_grad=True) unconstrained = torch.zeros(100, requires_grad=True) scale = transform_to(Normal.arg_constraints[\"scale\"])(unconstrained) loss = -Normal(loc, scale).log_prob(data).sum() The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant.: dist = Exponential(rate) unconstrained = torch.zeros(100, requires_grad=True) sample = biject_to(dist.support)(unconstrained) potential_energy = -dist.log_prob(sample).sum() The biject_to and transform_to objects can be extended by user-defined constraints and transforms using their .register() method either as a function on singleton constraints: transform_to.register(my_constraint, my_transform) or as a decorator on parameterized constraints: @transform_to.register(MyConstraintClass) def my_factory(constraint): assert isinstance(constraint, MyConstraintClass) return MyTransform(constraint.param1, constraint.param2) You can create your own registry by creating a new ConstraintRegistry object.", "prev_chunk_id": "chunk_1755", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1757", "url": "https://docs.pytorch.org/docs/stable/distributed.checkpoint.html", "title": "Distributed Checkpoint - torch.distributed.checkpoint#", "page_title": "Distributed Checkpoint - torch.distributed.checkpoint — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Checkpoint - torch.distributed.checkpoint#", "content": "Distributed Checkpoint - torch.distributed.checkpoint# Created On: Nov 16, 2022 | Last Updated On: Jun 16, 2025 Distributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topology and loading into another. DCP is different than torch.save and torch.load in a few significant ways: - It produces multiple files per checkpoint, with at least one per rank. - It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead. The entrypoints to load and save a checkpoint are the following:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1758", "url": "https://docs.pytorch.org/docs/stable/distributed.checkpoint.html", "title": "Additional resources:#", "page_title": "Distributed Checkpoint - torch.distributed.checkpoint — PyTorch 2.8 documentation", "breadcrumbs": "Additional resources:#", "content": "Additional resources:# - Getting Started with Distributed Checkpoint (DCP) - Asynchronous Saving with Distributed Checkpoint (DCP) - TorchTitan Checkpointing Docs - TorchTitan DCP Implementation The following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (torch.distributed.checkpoint.async_save): In addition to the above entrypoints, Stateful objects, as described below, provide additional customization during saving/loading This example shows how to use Pytorch Distributed Checkpoint to save a FSDP model. The following types define the IO interface used during checkpoint: The following types define the planner interface used during checkpoint: We provide a filesystem based storage layer: We also provide other storage layers, including ones to interact with HuggingFace safetensors: .. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageReader :members: .. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageWriter :members: We provide default implementations of LoadPlanner and SavePlanner that can handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor. Due to legacy design decisions, the state dictionaries of FSDP and DDP may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, FSDP offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism). To tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. get_model_state_dict() returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, get_optimizer_state_dict() provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, get_optimizer_state_dict() converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary. Note that results returned by these APIs can be used directly with the torch.distributed.checkpoint.save() and torch.distributed.checkpoint.load() methods", "prev_chunk_id": "chunk_1757", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1759", "url": "https://docs.pytorch.org/docs/stable/distributed.checkpoint.html", "title": "Additional resources:#", "page_title": "Distributed Checkpoint - torch.distributed.checkpoint — PyTorch 2.8 documentation", "breadcrumbs": "Additional resources:#", "content": "without requiring any additional conversions. set_model_state_dict() and set_optimizer_state_dict() are provided to load the model and optimizer state_dict generated by by their respective getter APIs. Note that set_optimizer_state_dict() can only be called before backward() or after step() is called on optimizers. Note that this feature is experimental, and API signatures might change in the future. For users which are used to using and sharing models in the torch.save format, the following methods are provided which provide offline utilities for converting betweeing formats. The following classes can also be utilized for online loading and resharding of models from the torch.save format. The following experimental interfaces are provided for improved observability in production environments:", "prev_chunk_id": "chunk_1758", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1760", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Pipeline Parallelism#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Pipeline Parallelism#", "content": "Pipeline Parallelism# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1761", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Why Pipeline Parallel?#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Why Pipeline Parallel?#", "content": "Why Pipeline Parallel?# Pipeline Parallelism is one of the primitive parallelism for deep learning. It allows the execution of a model to be partitioned such that multiple micro-batches can execute different parts of the model code concurrently. Pipeline parallelism can be an effective technique for: - large-scale training - bandwidth-limited clusters - large model inference The above scenarios share a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP.", "prev_chunk_id": "chunk_1760", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1762", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "What is torch.distributed.pipelining?#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "What is torch.distributed.pipelining?#", "content": "What is torch.distributed.pipelining?# While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered. The pipelining package provides a toolkit that does said things automatically which allows easy implementation of pipeline parallelism on general models. It consists of two parts: a splitting frontend and a distributed runtime. The splitting frontend takes your model code as-is, splits it up into “model partitions”, and captures the data-flow relationship. The distributed runtime executes the pipeline stages on different devices in parallel, handling things like micro-batch splitting, scheduling, communication, and gradient propagation, etc. Overall, the pipelining package provides the following features: - Splitting of model code based on simple specification. - Rich support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules. - First-class support for cross-host pipeline parallelism, as this is where PP is typically used (over slower interconnects). - Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. TheTorchTitanproject demonstrates a “3D parallel” application on the Llama model.", "prev_chunk_id": "chunk_1761", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1763", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Step 1: build PipelineStage#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Step 1: build PipelineStage#", "content": "Step 1: build PipelineStage# Before we can use a PipelineSchedule, we need to create PipelineStage objects that wrap the part of the model running in that stage. The PipelineStage is responsible for allocating communication buffers and creating send/recv ops to communicate with its peers. It manages intermediate buffers e.g. for the outputs of forward that have not been consumed yet, and it provides a utility for running the backwards for the stage model. A PipelineStage needs to know the input and output shapes for the stage model, so that it can correctly allocate communication buffers. The shapes must be static, e.g. at runtime the shapes can not change from step to step. A class PipeliningShapeError will be raised if runtime shapes do not match the expected shapes. When composing with other paralleisms or applying mixed precision, these techniques must be taken into account so the PipelineStage knows the correct shape (and dtype) for the output of the stage module at runtime. Users may construct a PipelineStage instance directly, by passing in an nn.Module representing the portion of the model that should run on the stage. This may require changes to the original model code. See the example in Option 1: splitting a model manually. Alternatively, the splitting frontend can use graph partitioning to split your model into a series of nn.Module automatically. This technique requires the model is traceable with torch.Export. Composability of the resulting nn.Module with other parallelism techniques is experimental, and may require some workarounds. Usage of this frontend may be more appealing if the user cannot easily change the model code. See Option 2: splitting a model automatically for more information.", "prev_chunk_id": "chunk_1762", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1764", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Step 2: use PipelineSchedule for execution#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Step 2: use PipelineSchedule for execution#", "content": "Step 2: use PipelineSchedule for execution# We can now attach the PipelineStage to a pipeline schedule, and run the schedule with input data. Here is a GPipe example: from torch.distributed.pipelining import ScheduleGPipe # Create a schedule schedule = ScheduleGPipe(stage, n_microbatches) # Input data (whole batch) x = torch.randn(batch_size, in_dim, device=device) # Run the pipeline with input `x` # `x` will be divided into microbatches automatically if rank == 0: schedule.step(x) else: output = schedule.step() Note that the above code needs to be launched for each worker, thus we use a launcher service to launch multiple processes: torchrun --nproc_per_node=2 example.py", "prev_chunk_id": "chunk_1763", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1765", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Option 1: splitting a model manually#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Option 1: splitting a model manually#", "content": "Option 1: splitting a model manually# To directly construct a PipelineStage, the user is responsible for providing a single nn.Module instance that owns the relevant nn.Parameters and nn.Buffers, and defines a forward() method that executes the operations relevant for that stage. For example, a condensed version of the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable model. class Transformer(nn.Module): def __init__(self, model_args: ModelArgs): super().__init__() self.tok_embeddings = nn.Embedding(...) # Using a ModuleDict lets us delete layers without affecting names, # ensuring checkpoints will correctly save and load. self.layers = torch.nn.ModuleDict() for layer_id in range(model_args.n_layers): self.layers[str(layer_id)] = TransformerBlock(...) self.output = nn.Linear(...) def forward(self, tokens: torch.Tensor): # Handling layers being 'None' at runtime enables easy pipeline splitting h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens for layer in self.layers.values(): h = layer(h, self.freqs_cis) h = self.norm(h) if self.norm else h output = self.output(h).float() if self.output else h return output A model defined in this manner can be easily configured per stage by first initializing the whole model (using meta-device to avoid OOM errors), deleting undesired layers for that stage, and then creating a PipelineStage that wraps the model. For example: with torch.device(\"meta\"): assert num_stages == 2, \"This is a simple 2-stage example\" # we construct the entire model, then delete the parts we do not need for this stage # in practice, this can be done using a helper function that automatically divides up layers across stages. model = Transformer() if stage_index == 0: # prepare the first stage model del model.layers[\"1\"] model.norm = None model.output = None elif stage_index == 1: # prepare the second stage model model.tok_embeddings = None del model.layers[\"0\"] from torch.distributed.pipelining import PipelineStage stage = PipelineStage( model, stage_index, num_stages, device, ) When composing with other Data or Model parallelism techniques, output_args may also be required,", "prev_chunk_id": "chunk_1764", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1766", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Option 1: splitting a model manually#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Option 1: splitting a model manually#", "content": "if the output shape/dtype of the model chunk will be affected.", "prev_chunk_id": "chunk_1765", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1767", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Option 2: splitting a model automatically#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Option 2: splitting a model automatically#", "content": "Option 2: splitting a model automatically# If you have a full model and do not want to spend time on modifying it into a sequence of “model partitions”, the pipeline API is here to help. Here is a brief example: class Model(torch.nn.Module): def __init__(self) -> None: super().__init__() self.emb = torch.nn.Embedding(10, 3) self.layers = torch.nn.ModuleList( Layer() for _ in range(2) ) self.lm = LMHead() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.emb(x) for layer in self.layers: x = layer(x) x = self.lm(x) return x If we print the model, we can see multiple hierarchies, which makes it hard to split by hand: Model( (emb): Embedding(10, 3) (layers): ModuleList( (0-1): 2 x Layer( (lin): Linear(in_features=3, out_features=3, bias=True) ) ) (lm): LMHead( (proj): Linear(in_features=3, out_features=3, bias=True) ) ) Let us see how the pipeline API works: from torch.distributed.pipelining import pipeline, SplitPoint # An example micro-batch input x = torch.LongTensor([1, 2, 4, 5]) pipe = pipeline( module=mod, mb_args=(x,), split_spec={ \"layers.1\": SplitPoint.BEGINNING, } ) The pipeline API splits your model given a split_spec, where SplitPoint.BEGINNING stands for adding a split point before execution of certain submodule in the forward function, and similarly, SplitPoint.END for split point after such. If we print(pipe), we can see: GraphModule( (submod_0): GraphModule( (emb): InterpreterModule() (layers): Module( (0): InterpreterModule( (lin): InterpreterModule() ) ) ) (submod_1): GraphModule( (layers): Module( (1): InterpreterModule( (lin): InterpreterModule() ) ) (lm): InterpreterModule( (proj): InterpreterModule() ) ) ) def forward(self, x): submod_0 = self.submod_0(x); x = None submod_1 = self.submod_1(submod_0); submod_0 = None return (submod_1,) The “model partitions” are represented by submodules (submod_0, submod_1), each of which is reconstructed with original model operations, weights and hierarchies. In addition, a “root-level” forward function is reconstructed to capture the data flow between those partitions. Such data flow will be replayed by the pipeline runtime later, in a distributed fashion.", "prev_chunk_id": "chunk_1766", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1768", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Option 2: splitting a model automatically#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Option 2: splitting a model automatically#", "content": "The Pipe object provides a method for retrieving the “model partitions”: stage_mod : nn.Module = pipe.get_stage_module(stage_idx) The returned stage_mod is a nn.Module, with which you can create an optimizer, save or load checkpoints, or apply other parallelisms. Pipe also allows you to create a distributed stage runtime on a device given a ProcessGroup: stage = pipe.build_stage(stage_idx, device, group) Alternatively, if you would like to build the stage runtime later after some modification to the stage_mod, you can use a functional version of the build_stage API. For example: from torch.distributed.pipelining import build_stage from torch.nn.parallel import DistributedDataParallel dp_mod = DistributedDataParallel(stage_mod) info = pipe.info() stage = build_stage(dp_mod, stage_idx, info, device, group)", "prev_chunk_id": "chunk_1767", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1769", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Hugging Face Examples#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Hugging Face Examples#", "content": "Hugging Face Examples# In the PiPPy repo where this package was original created, we kept examples based on unmodified Hugging Face models. See the examples/huggingface directory. Examples include: - GPT2 - Llama", "prev_chunk_id": "chunk_1768", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1770", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "How does the pipeline API split a model?#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "How does the pipeline API split a model?#", "content": "How does the pipeline API split a model?# First, the pipeline API turns our model into a directed acyclic graph (DAG) by tracing the model. It traces the model using torch.export – a PyTorch 2 full-graph capturing tool. Then, it groups together the operations and parameters needed by a stage into a reconstructed submodule: submod_0, submod_1, … Different from conventional submodule access methods like Module.children(), the pipeline API does not only cut the module structure of your model, but also the forward function of your model. This is necessary because model structure like Module.children() merely captures information during Module.__init__(), and does not capture any information about Module.forward(). Said differently, Module.children() lacks information about the following aspects key to pipelininig: - Execution order of child modules inforward - Activation flows between child modules - Whether there are any functional operators between child modules (for example,reluoraddoperations will not be captured byModule.children()). The pipeline API, on the contrary, makes sure that the forward behavior is truly preserved. It also captures the activation flow between the partitions, helping the distributed runtime to make correct send/receive calls without human intervention. Another flexibility of the pipeline API is that split points can be at arbitrary levels within your model hierarchy. In the split partitions, the original model hierarchy related to that partition will be reconstructed at no cost to you. At a result, fully-qualified names (FQNs) pointing to a submodule or parameter would be still valid, and services that relies on FQNs (such as FSDP, TP or checkpointing) can still run with your partitioned modules with almost zero code change.", "prev_chunk_id": "chunk_1769", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1771", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Implementing Your Own Schedule#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Implementing Your Own Schedule#", "content": "Implementing Your Own Schedule# You can implement your own pipeline schedule by extending one of the following two class: - PipelineScheduleSingle - PipelineScheduleMulti PipelineScheduleSingle is for schedules that assigns only one stage per rank. PipelineScheduleMulti is for schedules that assigns multiple stages per rank. For example, ScheduleGPipe and Schedule1F1B are subclasses of PipelineScheduleSingle. Whereas, ScheduleInterleaved1F1B, ScheduleLoopedBFS, ScheduleInterleavedZeroBubble, and ScheduleZBVZeroBubble are subclasses of PipelineScheduleMulti.", "prev_chunk_id": "chunk_1770", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1772", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Logging#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Logging#", "content": "Logging# You can turn on additional logging using the TORCH_LOGS environment variable from torch._logging: - TORCH_LOGS=+ppwill displaylogging.DEBUGmessages and all levels above it. - TORCH_LOGS=ppwill displaylogging.INFOmessages and above. - TORCH_LOGS=-ppwill displaylogging.WARNINGmessages and above.", "prev_chunk_id": "chunk_1771", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1773", "url": "https://docs.pytorch.org/docs/stable/distributed.pipelining.html", "title": "Model Split APIs#", "page_title": "Pipeline Parallelism — PyTorch 2.8 documentation", "breadcrumbs": "Model Split APIs#", "content": "Model Split APIs# The following set of APIs transform your model into a pipeline representation.", "prev_chunk_id": "chunk_1772", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1774", "url": "https://docs.pytorch.org/docs/stable/distributed.optim.html", "title": "Distributed Optimizers#", "page_title": "Distributed Optimizers — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Optimizers#", "content": "Distributed Optimizers# Created On: Mar 01, 2021 | Last Updated On: Jun 16, 2025 torch.distributed.optim exposes DistributedOptimizer, which takes a list of remote parameters (RRef) and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer Base class to apply the gradients on each worker.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1775", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.parallel.html", "title": "Tensor Parallelism - torch.distributed.tensor.parallel#", "page_title": "Tensor Parallelism - torch.distributed.tensor.parallel — PyTorch 2.8 documentation", "breadcrumbs": "Tensor Parallelism - torch.distributed.tensor.parallel#", "content": "Tensor Parallelism - torch.distributed.tensor.parallel# Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025 Tensor Parallelism(TP) is built on top of the PyTorch DistributedTensor (DTensor)[https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md] and provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism. The entrypoint to parallelize your nn.Module using Tensor Parallelism is: Tensor Parallelism supports the following parallel styles: To simply configure the nn.Module’s inputs and outputs with DTensor layouts and perform necessary layout redistributions, without distribute the module parameters to DTensors, the following ParallelStyle s can be used in the parallelize_plan when calling parallelize_module: For models like Transformer, we recommend users to use ColwiseParallel and RowwiseParallel together in the parallelize_plan for achieve the desired sharding for the entire model (i.e. Attention and MLP). Parallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager:", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1776", "url": "https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html", "title": "torch.distributed.fsdp.fully_shard#", "page_title": "torch.distributed.fsdp.fully_shard — PyTorch 2.8 documentation", "breadcrumbs": "torch.distributed.fsdp.fully_shard#", "content": "torch.distributed.fsdp.fully_shard# Created On: Dec 04, 2024 | Last Updated On: Jun 16, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1777", "url": "https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html", "title": "PyTorch FSDP2 (fully_shard)#", "page_title": "torch.distributed.fsdp.fully_shard — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch FSDP2 (fully_shard)#", "content": "PyTorch FSDP2 (fully_shard)# PyTorch FSDP2 (RFC) provides a fully sharded data parallelism (FSDP) implementation targeting performant eager-mode while using per-parameter sharding for improved usability - See theGetting Started with FSDP2tutorial for more information. - If you are currently using FSDP1, consider migrating to FSDP2 using ourmigration guide. The user contract for fully_shard(model) is as follows - For model initialization, fully_shard converts model.parameters() from plain torch.Tensor to DTensor in-place. The parameters are moved to the appropriate device according to the device mesh. - Before forward and backward passes, pre-forward/backward hooks are responsible for all-gathering the parameters and converting model.parameters() from DTensor to plain torch.Tensor. - After forward and backward passes, post-forward/backward hooks free the unsharded parameters (no communication needed) and convert model.parameters() from plain torch.Tensor back to DTensor. - For the optimizer, it must be initialized with the DTensor model.parameters(), and the optimizer step should be performed on DTensor parameters. - Callmodel(input)instead ofmodel.forward(input)to trigger pre-forward hooks to all-gather parameters. To make model.forward(input) work, users must either callmodel.unshard()explicitly or useregister_fsdp_forward_method(model,\"forward\")to register the forward method for hooking. - fully_shard groups parameters together for a single all-gather. User should apply fully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard should be applied to each layer before applying it to the root model. When applied to the root model, fully_shard excludes model.parameters() from each layer and groups the remaining parameters (e.g., embeddings, output projection) into a single all-gather group. - type(model)is “unioned” withFSDPModulein-place. For example, if model is originally of type nn.Linear, then fully_shard changestype(model)from nn.Linear toFSDPLinearin-place.FSDPLinearis an instance of both nn.Linear andFSDPModule. It retains all methods of nn.Linear while also exposing FSDP2-specific APIs under FSDPModule, such asreshard()andunshard(). - Fully Qualified Names (FQNs) for parameters remain unchanged. If we callmodel.state_dict(), the FQNs are the same before and after applying fully_shard. This is", "prev_chunk_id": "chunk_1776", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1778", "url": "https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html", "title": "PyTorch FSDP2 (fully_shard)#", "page_title": "torch.distributed.fsdp.fully_shard — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch FSDP2 (fully_shard)#", "content": "because fully_shard does not wrap the module but only registers hooks to the original module. Compared to PyTorch FSDP1 (FullyShardedDataParallel): - FSDP2 usesDTensor-based dim-0 per-parameter sharding for a simpler sharding representation compared to FSDP1’s flat-parameter sharding, while preserving similar throughput performance. More specifically, FSDP2 chunks each parameter on dim-0 across the data parallel workers (usingtorch.chunk(dim=0)), whereas FSDP1 flattens, concatenates, and chunks a group of tensors together, making reasoning about what data is present on each worker and resharding to different parallelisms complex. Per-parameter sharding provides a more intuitive user experience, relaxes constraints around frozen parameters, and allows for communication-free (sharded) state dicts, which otherwise require all-gathers in FSDP1. - FSDP2 implements a different memory management approach to handle the multi-stream usages that avoidstorch.Tensor.record_stream. This ensures deterministic and expected memory usage and does not require blocking the CPU like in FSDP1’slimit_all_gathers=True. - FSDP2 exposes APIs for manual control over prefetching and collective scheduling, allowing power users more customization. See the methods onFSDPModulebelow for details. - FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly support full state dicts. Instead, users can reshard the sharded state dicts containingDTensors to full state dicts themselves usingDTensorAPIs likeDTensor.full_tensor()or by using higher-level APIs likePyTorch Distributed Checkpoint‘s distributed state dict APIs. Also, some other args have been removed; seeherefor details. The frontend API is fully_shard that can be called on a module:", "prev_chunk_id": "chunk_1777", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1779", "url": "https://docs.pytorch.org/docs/stable/fsdp.html", "title": "FullyShardedDataParallel#", "page_title": "FullyShardedDataParallel — PyTorch 2.8 documentation", "breadcrumbs": "FullyShardedDataParallel#", "content": "FullyShardedDataParallel# Created On: Feb 02, 2022 | Last Updated On: Jun 11, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1780", "url": "https://docs.pytorch.org/docs/stable/distributed.elastic.html", "title": "Torch Distributed Elastic#", "page_title": "Torch Distributed Elastic — PyTorch 2.8 documentation", "breadcrumbs": "Torch Distributed Elastic#", "content": "Torch Distributed Elastic# Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025 Makes distributed PyTorch fault-tolerant and elastic.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1781", "url": "https://docs.pytorch.org/docs/stable/distributed.algorithms.join.html", "title": "Generic Join Context Manager#", "page_title": "Generic Join Context Manager — PyTorch 2.8 documentation", "breadcrumbs": "Generic Join Context Manager#", "content": "Generic Join Context Manager# Created On: Jun 06, 2025 | Last Updated On: Jun 06, 2025 The generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: Join, Joinable, and JoinHook. For a tutorial, see Distributed Training with Uneven Inputs Using the Join Context Manager.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1782", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "torch.distributed.tensor#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "torch.distributed.tensor#", "content": "torch.distributed.tensor# Created On: Jun 13, 2025 | Last Updated On: Jun 18, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1783", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "PyTorch DTensor (Distributed Tensor)#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "PyTorch DTensor (Distributed Tensor)#", "content": "PyTorch DTensor (Distributed Tensor)# PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed logic, including sharded storage, operator computation and collective communications across devices/hosts. DTensor could be used to build different parallelism solutions and support sharded state_dict representation when working with multi-dimensional sharding. Please see examples from the PyTorch native parallelism solutions that are built on top of DTensor: - Tensor Parallel - FSDP2 DTensor follows the SPMD (single program, multiple data) programming model to empower users to write distributed program as if it’s a single-device program with the same convergence property. It provides a uniform tensor sharding layout (DTensor Layout) through specifying the DeviceMesh and Placement: - DeviceMeshrepresents the device topology and the communicators of the cluster using an n-dimensional array. - Placementdescribes the sharding layout of the logical tensor on theDeviceMesh. DTensor supports three types of placements:Shard,ReplicateandPartial.", "prev_chunk_id": "chunk_1782", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1784", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "DTensor Class APIs#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "DTensor Class APIs#", "content": "DTensor Class APIs# DTensor is a torch.Tensor subclass. This means once a DTensor is created, it could be used in very similar way to torch.Tensor, including running different types of PyTorch operators as if running them in a single device, allowing proper distributed computation for PyTorch operators. In addition to existing torch.Tensor methods, it also offers a set of additional methods to interact with torch.Tensor, redistribute the DTensor Layout to a new DTensor, get the full tensor content on all devices, etc.", "prev_chunk_id": "chunk_1783", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1785", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "DeviceMesh as the distributed communicator#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "DeviceMesh as the distributed communicator#", "content": "DeviceMesh as the distributed communicator# DeviceMesh was built from DTensor as the abstraction to describe cluster’s device topology and represent multi-dimensional communicators (on top of ProcessGroup). To see the details of how to create/use a DeviceMesh, please refer to the DeviceMesh recipe.", "prev_chunk_id": "chunk_1784", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1786", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "DTensor Placement Types#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "DTensor Placement Types#", "content": "DTensor Placement Types# DTensor supports the following types of Placement on each DeviceMesh dimension:", "prev_chunk_id": "chunk_1785", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1787", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "Create DTensor from a logical torch.Tensor#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "Create DTensor from a logical torch.Tensor#", "content": "Create DTensor from a logical torch.Tensor# The SPMD (single program, multiple data) programming model in torch.distributed launches multiple processes (i.e. via torchrun) to execute the same program, this means that the model inside the program would be initialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly on GPU if enough memory). DTensor offers a distribute_tensor() API that could shard the model weights or Tensors to DTensor s, where it would create a DTensor from the “logical” Tensor on each process. This would empower the created DTensor s to comply with the single device semantic, which is critical for numerical correctness. Along with distribute_tensor(), DTensor also offers a distribute_module() API to allow easier sharding on the nn.Module level", "prev_chunk_id": "chunk_1786", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1788", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "DTensor Factory Functions#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "DTensor Factory Functions#", "content": "DTensor Factory Functions# DTensor also provides dedicated tensor factory functions to allow creating DTensor directly using torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally specifying the DeviceMesh and Placement for the DTensor created:", "prev_chunk_id": "chunk_1787", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1789", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "Logging#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "Logging#", "content": "Logging# When launching the program, you can turn on additional logging using the TORCH_LOGS environment variable from torch._logging : - TORCH_LOGS=+dtensorwill displaylogging.DEBUGmessages and all levels above it. - TORCH_LOGS=dtensorwill displaylogging.INFOmessages and above. - TORCH_LOGS=-dtensorwill displaylogging.WARNINGmessages and above.", "prev_chunk_id": "chunk_1788", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1790", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "Debugging Tools#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "Debugging Tools#", "content": "Debugging Tools# To debug the program that applied DTensor, and understand more details about what collectives happened under the hood, DTensor provides a CommDebugMode: To visualize the sharding of a DTensor that have less than 3 dimensions, DTensor provides visualize_sharding():", "prev_chunk_id": "chunk_1789", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1791", "url": "https://docs.pytorch.org/docs/stable/distributed.tensor.html", "title": "Experimental Features#", "page_title": "torch.distributed.tensor — PyTorch 2.8 documentation", "breadcrumbs": "Experimental Features#", "content": "Experimental Features# DTensor also provides a set of experimental features. These features are either in prototyping stage, or the basic functionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to these features.", "prev_chunk_id": "chunk_1790", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1792", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Distributed communication package - torch.distributed#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Distributed communication package - torch.distributed#", "content": "Distributed communication package - torch.distributed# Created On: Jul 12, 2017 | Last Updated On: Jul 14, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1793", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Backends#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Backends#", "content": "Backends# torch.distributed supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.", "prev_chunk_id": "chunk_1792", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1794", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Backends that come with PyTorch#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Backends that come with PyTorch#", "content": "Backends that come with PyTorch# PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)", "prev_chunk_id": "chunk_1793", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1795", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Which backend to use?#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Which backend to use?#", "content": "Which backend to use?# In the past, we were often asked: “which backend should I use?”. - Rule of thumbUse the NCCL backend for distributedGPUtrainingUse the Gloo backend for distributedCPUtraining. - GPU hosts with InfiniBand interconnectUse NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect. - GPU hosts with Ethernet interconnectUse NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.) - CPU hosts with InfiniBand interconnectIf your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases. - CPU hosts with Ethernet interconnectUse Gloo, unless you have specific reasons to use MPI.", "prev_chunk_id": "chunk_1794", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1796", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Choosing the network interface to use#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Choosing the network interface to use#", "content": "Choosing the network interface to use# By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend): - NCCL_SOCKET_IFNAME, for exampleexportNCCL_SOCKET_IFNAME=eth0 - GLOO_SOCKET_IFNAME, for exampleexportGLOO_SOCKET_IFNAME=eth0 If you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.", "prev_chunk_id": "chunk_1795", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1797", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Other NCCL environment variables#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Other NCCL environment variables#", "content": "Other NCCL environment variables# Debugging - in case of NCCL failure, you can set NCCL_DEBUG=INFO to print an explicit warning message as well as basic NCCL initialization information. You may also use NCCL_DEBUG_SUBSYS to get more details about a specific aspect of NCCL. For example, NCCL_DEBUG_SUBSYS=COLL would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set NCCL_DEBUG_SUBSYS=GRAPH to inspect the detailed detection result and save as reference if further help from NCCL team is needed. Performance tuning - NCCL performs automatic tuning based on its topology detection to save users’ tuning effort. On some socket-based systems, users may still try tuning NCCL_SOCKET_NTHREADS and NCCL_NSOCKS_PERTHREAD to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP. For a full list of NCCL environment variables, please refer to NVIDIA NCCL’s official documentation You can tune NCCL communicators even further using torch.distributed.ProcessGroupNCCL.NCCLConfig and torch.distributed.ProcessGroupNCCL.Options. Learn more about them using help (e.g. help(torch.distributed.ProcessGroupNCCL.NCCLConfig)) in the interpreter.", "prev_chunk_id": "chunk_1796", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1798", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Basics#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Basics#", "content": "Basics# The torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class torch.nn.parallel.DistributedDataParallel() builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process. In the single-machine synchronous case, torch.distributed or the torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other approaches to data-parallelism, including torch.nn.DataParallel(): - Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes. - Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.", "prev_chunk_id": "chunk_1797", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1799", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Initialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Initialization#", "content": "Initialization# The package needs to be initialized using the torch.distributed.init_process_group() or torch.distributed.device_mesh.init_device_mesh() function before calling any other methods. Both block until all processes have joined. Currently three initialization methods are supported:", "prev_chunk_id": "chunk_1798", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1800", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "TCP initialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "TCP initialization#", "content": "TCP initialization# There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired world_size. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks. Note that multicast address is not supported anymore in the latest distributed package. group_name is deprecated as well. import torch.distributed as dist # Use address of one of the machines dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)", "prev_chunk_id": "chunk_1799", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1801", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Shared file-system initialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Shared file-system initialization#", "content": "Shared file-system initialization# Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired world_size. The URL should start with file:// and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next init_process_group() call on the same file path/name. Note that automatic rank assignment is not supported anymore in the latest distributed package and group_name is deprecated as well. import torch.distributed as dist # rank should always be specified dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile', world_size=4, rank=args.rank)", "prev_chunk_id": "chunk_1800", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1802", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Environment variable initialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Environment variable initialization#", "content": "Environment variable initialization# This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are: - MASTER_PORT- required; has to be a free port on machine with rank 0 - MASTER_ADDR- required (except for rank 0); address of rank 0 node - WORLD_SIZE- required; can be set either here, or in a call to init function - RANK- required; can be set either here, or in a call to init function The machine with rank 0 will be used to set up all connections. This is the default method, meaning that init_method does not have to be specified (or can be env://).", "prev_chunk_id": "chunk_1801", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1803", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Improving initialization time#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Improving initialization time#", "content": "Improving initialization time# - TORCH_GLOO_LAZY_INIT- establishes connections on demand rather than using a full mesh which can greatly improve initialization time for non all2all operations.", "prev_chunk_id": "chunk_1802", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1804", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Post-Initialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Post-Initialization#", "content": "Post-Initialization# Once torch.distributed.init_process_group() was run, the following functions can be used. To check whether the process group has already been initialized use torch.distributed.is_initialized().", "prev_chunk_id": "chunk_1803", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1805", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Shutdown#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Shutdown#", "content": "Shutdown# It is important to clean up resources on exit by calling destroy_process_group(). The simplest pattern to follow is to destroy every process group and backend by calling destroy_process_group() with the default value of None for the group argument, at a point in the training script where communications are no longer needed, usually near the end of main(). The call should be made once per trainer-process, not at the outer process-launcher level. if destroy_process_group() is not called by all ranks in a pg within the timeout duration, especially when there are multiple process-groups in the application e.g. for N-D parallelism, hangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort, which must be called collectively, but the order of calling ProcessGroupNCCL’s destructor if called by python’s GC is not deterministic. Calling destroy_process_group() helps by ensuring ncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort during ProcessGroupNCCL’s destructor.", "prev_chunk_id": "chunk_1804", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1806", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Reinitialization#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Reinitialization#", "content": "Reinitialization# destroy_process_group can also be used to destroy individual process groups. One use case could be fault tolerant training, where a process group may be destroyed and then a new one initialized during runtime. In this case, it’s critical to synchronize the trainer processes using some means other than torch.distributed primitives _after_ calling destroy and before subsequently initializing. This behavior is currently unsupported/untested, due to the difficulty of achieving this synchronization, and is considered a known issue. Please file a github issue or RFC if this is a use case that’s blocking you.", "prev_chunk_id": "chunk_1805", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1807", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Groups#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Groups#", "content": "Groups# By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).", "prev_chunk_id": "chunk_1806", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1808", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "DeviceMesh#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "DeviceMesh#", "content": "DeviceMesh# DeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators). It allows user to easily create inter node and intra node process groups without worrying about how to set up the ranks correctly for different sub process groups, and it helps manage those distributed process group easily. init_device_mesh() function can be used to create new DeviceMesh, with a mesh shape describing the device topology.", "prev_chunk_id": "chunk_1807", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1809", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Point-to-point communication#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Point-to-point communication#", "content": "Point-to-point communication# isend() and irecv() return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods: - is_completed()- returns True if the operation has finished - wait()- will block the process until the operation is finished.is_completed()is guaranteed to return True once it returns.", "prev_chunk_id": "chunk_1808", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1810", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Synchronous and asynchronous collective operations#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Synchronous and asynchronous collective operations#", "content": "Synchronous and asynchronous collective operations# Every collective operation function supports the following two kinds of operations, depending on the setting of the async_op flag passed into the collective: Synchronous operation - the default mode, when async_op is set to False. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations. Asynchronous operation - when async_op is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods: - is_completed()- in the case of CPU collectives, returnsTrueif completed. In the case of CUDA operations, returnsTrueif the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization. - wait()- in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU). - get_future()- returnstorch._C.Futureobject. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs,get_future()call might become", "prev_chunk_id": "chunk_1809", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1811", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Synchronous and asynchronous collective operations#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Synchronous and asynchronous collective operations#", "content": "redundant. Example The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams: # Code runs on each rank. dist.init_process_group(\"nccl\", rank=rank, world_size=2) output = torch.tensor([rank]).cuda(rank) s = torch.cuda.Stream() handle = dist.all_reduce(output, async_op=True) # Wait ensures the operation is enqueued, but not necessarily complete. handle.wait() # Using result on non-default stream. with torch.cuda.stream(s): s.wait_stream(torch.cuda.default_stream()) output.add_(100) if rank == 0: # if the explicit call to wait_stream was omitted, the output below will be # non-deterministically 1 or 101, depending on whether the allreduce overwrote # the value after the add completed. print(output)", "prev_chunk_id": "chunk_1810", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1812", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Distributed Key-Value Store#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Distributed Key-Value Store#", "content": "Distributed Key-Value Store# The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in torch.distributed.init_process_group() (by explicitly creating the store as an alternative to specifying init_method.) There are 3 choices for Key-Value Stores: TCPStore, FileStore, and HashStore.", "prev_chunk_id": "chunk_1811", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1813", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Profiling Collective Communication#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Profiling Collective Communication#", "content": "Profiling Collective Communication# Note that you can use torch.profiler (recommended, only available after 1.8.1) or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo, nccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator: import torch import torch.distributed as dist with torch.profiler(): tensor = torch.randn(20, 10) dist.all_reduce(tensor) Please refer to the profiler documentation for a full overview of profiler features.", "prev_chunk_id": "chunk_1812", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1814", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Object collectives#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Object collectives#", "content": "Object collectives# Object collectives are a set of collective-like operations that work on arbitrary Python objects, as long as they can be pickled. There are various collective patterns implemented (e.g. broadcast, all_gather, …) but they each roughly follow this pattern: - convert the input object into a pickle (raw bytes), then shove it into a byte tensor - communicate the size of this byte tensor to peers (first collective operation) - allocate appropriately sized tensor to perform the real collective - communicate the object data (second collective operation) - convert raw data back into Python (unpickle) Object collectives sometimes have surprising performance or memory characteristics that lead to long runtimes or OOMs, and thus they should be used with caution. Here are some common issues. Asymmetric pickle/unpickle time - Pickling objects can be slow, depending on the number, type and size of the objects. When the collective has a fan-in (e.g. gather_object), the receiving rank(s) must unpickle N times more objects than the sending rank(s) had to pickle, which can cause other ranks to time out on their next collective. Inefficient tensor communication - Tensors should be sent via regular collective APIs, not object collective APIs. It is possible to send Tensors via object collective APIs, but they will be serialized and deserialized (including a CPU-sync and device-to-host copy in the case of non-CPU tensors), and in almost every case other than debugging or troubleshooting code, it would be worth the trouble to refactor the code to use non-object collectives instead. Unexpected tensor devices - If you still want to send tensors via object collectives, there is another aspect specific to cuda (and possibly other accelerators) tensors. If you pickle a tensor that is currently on cuda:3, and then unpickle it, you will get another tensor on cuda:3 regardless of", "prev_chunk_id": "chunk_1813", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1815", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Object collectives#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Object collectives#", "content": "which process you are on, or which CUDA device is the ‘default’ device for that process. With regular tensor collective APIs, ‘output tensors’ will always be on the same, local device, which is generally what you’d expect. Unpickling a tensor will implicitly activate a CUDA context if it is the first time a GPU is used by the process, which can waste significant amounts of GPU memory. This issue can be avoided by moving tensors to CPU before passing them as inputs to an object collective.", "prev_chunk_id": "chunk_1814", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1816", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Third-party backends#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Third-party backends#", "content": "Third-party backends# Besides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party backends are decided by their own implementations. The new backend derives from c10d::ProcessGroup and registers the backend name and the instantiating interface through torch.distributed.Backend.register_backend() when imported. When manually importing this backend and invoking torch.distributed.init_process_group() with the corresponding backend name, the torch.distributed package runs on the new backend.", "prev_chunk_id": "chunk_1815", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1817", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Launch utility#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Launch utility#", "content": "Launch utility# The torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training. Module torch.distributed.launch. torch.distributed.launch is a module that spawns up multiple distributed training processes on each of the training nodes. The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (--nproc-per-node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1). How to use this module: - Single-Node multi-process distributed training python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) - Multi-Node multi-process distributed training: (e.g. two nodes) Node 1: (IP: 192.168.1.1, and has a free port: 1234) python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE --nnodes=2 --node-rank=0 --master-addr=\"192.168.1.1\" --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) Node 2: python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE --nnodes=2 --node-rank=1 --master-addr=\"192.168.1.1\" --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments", "prev_chunk_id": "chunk_1816", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1818", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Launch utility#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Launch utility#", "content": "of your training script) - To look up what optional arguments this module offers: python -m torch.distributed.launch --help Important Notices: 1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. 2. In your training program, you must parse the command-line argument: --local-rank=LOCAL_PROCESS_RANK, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by: Parsing the local_rank argument >>> import argparse >>> parser = argparse.ArgumentParser() >>> parser.add_argument(\"--local-rank\", \"--local_rank\", type=int) >>> args = parser.parse_args() Set your device to local rank using either >>> torch.cuda.set_device(args.local_rank) # before your code runs or >>> with torch.cuda.device(args.local_rank): >>> # your code to run >>> ... 3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that init_method=env://. Other init methods (e.g. tcp://) may work, but env:// is the one that is officially supported by this module. >>> torch.distributed.init_process_group(backend='YOUR BACKEND', >>> init_method='env://') 4. In your training program, you can either use regular distributed functions or use torch.nn.parallel.DistributedDataParallel() module. If your training program uses GPUs for training and you would like to use torch.nn.parallel.DistributedDataParallel() module, here is how to configure it. >>> model = torch.nn.parallel.DistributedDataParallel(model, >>> device_ids=[args.local_rank], >>> output_device=args.local_rank) Please ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [args.local_rank], and output_device needs to be args.local_rank in order to use this utility 5. Another way to pass local_rank to the", "prev_chunk_id": "chunk_1817", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1819", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Launch utility#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Launch utility#", "content": "subprocesses via environment variable LOCAL_RANK. This behavior is enabled when you launch the script with --use-env=True. You must adjust the subprocess example above to replace args.local_rank with os.environ['LOCAL_RANK']; the launcher will not pass --local-rank when you specify this flag.", "prev_chunk_id": "chunk_1818", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1820", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Spawn utility#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Spawn utility#", "content": "Spawn utility# The Multiprocessing package - torch.multiprocessing package also provides a spawn function in torch.multiprocessing.spawn(). This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well. For references on how to use it, please refer to PyTorch example - ImageNet implementation Note that this function requires Python 3.4 or higher.", "prev_chunk_id": "chunk_1819", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1821", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Debugging torch.distributed applications#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Debugging torch.distributed applications#", "content": "Debugging torch.distributed applications# Debugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. torch.distributed provides a suite of tools to help debug training applications in a self-serve fashion:", "prev_chunk_id": "chunk_1820", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1822", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Python Breakpoint#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Python Breakpoint#", "content": "Python Breakpoint# It is extremely convenient to use python’s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all. PyTorch offers a customized wrapper around pdb that streamlines the process. torch.distributed.breakpoint makes this process easy. Internally, it customizes pdb’s breakpoint behavior in two ways but otherwise behaves as normal pdb. - Attaches the debugger only on one rank (specified by the user). - Ensures all other ranks stop, by using atorch.distributed.barrier()that will release once the debugged rank issues acontinue - Reroutes stdin from the child process such that it connects to your terminal. To use it, simply issue torch.distributed.breakpoint(rank) on all ranks, using the same value for rank in each case.", "prev_chunk_id": "chunk_1821", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1823", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Monitored Barrier#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Monitored Barrier#", "content": "Monitored Barrier# As of v1.10, torch.distributed.monitored_barrier() exists as an alternative to torch.distributed.barrier() which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into torch.distributed.monitored_barrier() within the provided timeout. torch.distributed.monitored_barrier() implements a host-side barrier using send/recv communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into torch.distributed.monitored_barrier() (in practice this could be due to an application bug or hang in a previous collective): import os from datetime import timedelta import torch import torch.distributed as dist import torch.multiprocessing as mp def worker(rank): dist.init_process_group(\"nccl\", rank=rank, world_size=2) # monitored barrier requires gloo process group to perform host-side sync. group_gloo = dist.new_group(backend=\"gloo\") if rank not in [1]: dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2)) if __name__ == \"__main__\": os.environ[\"MASTER_ADDR\"] = \"localhost\" os.environ[\"MASTER_PORT\"] = \"29501\" mp.spawn(worker, nprocs=2, args=()) The following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further: RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms Original exception: [gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594", "prev_chunk_id": "chunk_1822", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1824", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "TORCH_DISTRIBUTED_DEBUG#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "TORCH_DISTRIBUTED_DEBUG#", "content": "TORCH_DISTRIBUTED_DEBUG# With TORCH_CPP_LOG_LEVEL=INFO, the environment variable TORCH_DISTRIBUTED_DEBUG can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. TORCH_DISTRIBUTED_DEBUG can be set to either OFF (default), INFO, or DETAIL depending on the debugging level required. Please note that the most verbose option, DETAIL may impact the application performance and thus should only be used when debugging issues. Setting TORCH_DISTRIBUTED_DEBUG=INFO will result in additional debug logging when models trained with torch.nn.parallel.DistributedDataParallel() are initialized, and TORCH_DISTRIBUTED_DEBUG=DETAIL will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application: import os import torch import torch.distributed as dist import torch.multiprocessing as mp class TwoLinLayerNet(torch.nn.Module): def __init__(self): super().__init__() self.a = torch.nn.Linear(10, 10, bias=False) self.b = torch.nn.Linear(10, 1, bias=False) def forward(self, x): a = self.a(x) b = self.b(x) return (a, b) def worker(rank): dist.init_process_group(\"nccl\", rank=rank, world_size=2) torch.cuda.set_device(rank) print(\"init model\") model = TwoLinLayerNet().cuda() print(\"init ddp\") ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) inp = torch.randn(10, 10).cuda() print(\"train\") for _ in range(20): output = ddp_model(inp) loss = output[0] + output[1] loss.sum().backward() if __name__ == \"__main__\": os.environ[\"MASTER_ADDR\"] = \"localhost\" os.environ[\"MASTER_PORT\"] = \"29501\" os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\" os.environ[ \"TORCH_DISTRIBUTED_DEBUG\" ] = \"DETAIL\" # set to DETAIL for runtime logging. mp.spawn(worker, nprocs=2, args=()) The following logs are rendered at initialization time: I0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with: broadcast_buffers: 1 bucket_cap_bytes: 26214400 find_unused_parameters: 0 gradient_as_bucket_view: 0 is_multi_device_module: 0 iteration: 0 num_parameter_tensors: 2 output_device: 0 rank: 0 total_parameter_size_bytes: 440 world_size: 2 backend_name: nccl bucket_sizes: 440 cuda_visible_devices: N/A device_ids: 0 dtypes: float master_addr: localhost master_port: 29501 module_name: TwoLinLayerNet nccl_async_error_handling: N/A nccl_blocking_wait: N/A nccl_debug: WARN nccl_ib_timeout: N/A nccl_nthreads: N/A nccl_socket_ifname: N/A torch_distributed_debug: INFO The following logs are rendered during runtime (when TORCH_DISTRIBUTED_DEBUG=DETAIL is set): I0607 16:18:58.085681 544067 logger.cpp:344] [Rank", "prev_chunk_id": "chunk_1823", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1825", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "TORCH_DISTRIBUTED_DEBUG#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "TORCH_DISTRIBUTED_DEBUG#", "content": "1 / 2] Training TwoLinLayerNet unused_parameter_size=0 Avg forward compute time: 40838608 Avg backward compute time: 5983335 Avg backward comm. time: 4326421 Avg backward comm/comp overlap time: 4207652 I0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0 Avg forward compute time: 42850427 Avg backward compute time: 3885553 Avg backward comm. time: 2357981 Avg backward comm/comp overlap time: 2234674 In addition, TORCH_DISTRIBUTED_DEBUG=INFO enhances crash logging in torch.nn.parallel.DistributedDataParallel() due to unused parameters in the model. Currently, find_unused_parameters=True must be passed into torch.nn.parallel.DistributedDataParallel() initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as torch.nn.parallel.DistributedDataParallel() does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, torch.nn.parallel.DistributedDataParallel() will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify loss to be instead computed as loss = output[1], then TwoLinLayerNet.a does not receive a gradient in the backwards pass, and thus results in DDP failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by making sure all `forward` function outputs participate in calculating loss. If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the", "prev_chunk_id": "chunk_1824", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1826", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "TORCH_DISTRIBUTED_DEBUG#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "TORCH_DISTRIBUTED_DEBUG#", "content": "return va lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable). Parameters which did not receive grad for rank 0: a.weight Parameter indices which did not receive grad for rank 0: 0 Setting TORCH_DISTRIBUTED_DEBUG=DETAIL will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP allreduce). This is done by creating a wrapper process group that wraps all process groups returned by torch.distributed.init_process_group() and torch.distributed.new_group() APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a torch.distributed.monitored_barrier(), which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into torch.distributed.all_reduce(): import torch import torch.distributed as dist import torch.multiprocessing as mp def worker(rank): dist.init_process_group(\"nccl\", rank=rank, world_size=2) torch.cuda.set_device(rank) tensor = torch.randn(10 if rank == 0 else 20).cuda() dist.all_reduce(tensor) torch.cuda.synchronize(device=rank) if __name__ == \"__main__\": os.environ[\"MASTER_ADDR\"] = \"localhost\" os.environ[\"MASTER_PORT\"] = \"29501\" os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\" os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\" mp.spawn(worker, nprocs=2, args=()) With the NCCL backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables TORCH_DISTRIBUTED_DEBUG=DETAIL and reruns the application, the following error message reveals the root cause: work = default_pg.allreduce([tensor], opts) RuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input", "prev_chunk_id": "chunk_1825", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1827", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "TORCH_DISTRIBUTED_DEBUG#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "TORCH_DISTRIBUTED_DEBUG#", "content": "shapes into the collective are mismatched across ranks. Got shapes: 10 [ torch.LongTensor{1} ] In addition, TORCH_DISTRIBUTED_DEBUG=DETAIL can be used in conjunction with TORCH_SHOW_CPP_STACKTRACES=1 to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use c10d collective calls backed by process groups created with the torch.distributed.init_process_group() and torch.distributed.new_group() APIs.", "prev_chunk_id": "chunk_1826", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1828", "url": "https://docs.pytorch.org/docs/stable/distributed.html", "title": "Logging#", "page_title": "Distributed communication package - torch.distributed — PyTorch 2.8 documentation", "breadcrumbs": "Logging#", "content": "Logging# In addition to explicit debugging support via torch.distributed.monitored_barrier() and TORCH_DISTRIBUTED_DEBUG, the underlying C++ library of torch.distributed also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of TORCH_CPP_LOG_LEVEL and TORCH_DISTRIBUTED_DEBUG environment variables. Distributed components raise custom Exception types derived from RuntimeError: - torch.distributed.DistError: This is the base type of all distributed exceptions. - torch.distributed.DistBackendError: This exception is thrown when a backend-specific error occurs. For example, if theNCCLbackend is used and the user attempts to use a GPU that is not available to theNCCLlibrary. - torch.distributed.DistNetworkError: This exception is thrown when networking libraries encounter errors (ex: Connection reset by peer) - torch.distributed.DistStoreError: This exception is thrown when the Store encounters an error (ex: TCPStore timeout) If you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank:", "prev_chunk_id": "chunk_1827", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1829", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "torch.export#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "torch.export#", "content": "torch.export# Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1830", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Overview#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "Overview# torch.export.export() takes a torch.nn.Module and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. import torch from torch.export import export class Mod(torch.nn.Module): def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: a = torch.sin(x) b = torch.cos(y) return a + b example_args = (torch.randn(10, 10), torch.randn(10, 10)) exported_program: torch.export.ExportedProgram = export( Mod(), args=example_args ) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[10, 10]\", y: \"f32[10, 10]\"): # code: a = torch.sin(x) sin: \"f32[10, 10]\" = torch.ops.aten.sin.default(x) # code: b = torch.cos(y) cos: \"f32[10, 10]\" = torch.ops.aten.cos.default(y) # code: return a + b add: f32[10, 10] = torch.ops.aten.add.Tensor(sin, cos) return (add,) Graph signature: ExportGraphSignature( input_specs=[ InputSpec( kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None ), InputSpec( kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='y'), target=None, persistent=None ) ], output_specs=[ OutputSpec( kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None ) ] ) Range constraints: {} torch.export produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here. - Soundness: It is guaranteed to be a sound representation of the original program, and maintains the same calling conventions of the original program. - Normalized: There are no Python semantics within the graph. Submodules from the original programs are inlined to form one fully flattened computational graph. - Graph properties: The graph is purely functional, meaning it does not contain operations with side effects such as mutations or aliasing. It does not mutate any intermediate values, parameters, or buffers. - Metadata: The graph contains metadata captured during tracing, such as a stacktrace from user’s code. Under the hood, torch.export leverages the following latest technologies: - TorchDynamo (torch._dynamo)is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved", "prev_chunk_id": "chunk_1829", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1831", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Overview#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Overview#", "content": "graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code. - AOT Autogradprovides a functionalized PyTorch graph and ensures the graph is decomposed/lowered to the ATen operator set. - Torch FX (torch.fx)is the underlying representation of the graph, allowing flexible Python-based transformations.", "prev_chunk_id": "chunk_1830", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1832", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Existing frameworks#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Existing frameworks#", "content": "Existing frameworks# torch.compile() also utilizes the same PT2 stack as torch.export, but is slightly different: - JIT vs. AOT:torch.compile()is a JIT compiler whereas which is not intended to be used to produce compiled artifacts outside of deployment. - Partial vs. Full Graph Capture: Whentorch.compile()runs into an untraceable part of a model, it will “graph break” and fall back to running the program in the eager Python runtime. In comparison,torch.exportaims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Sincetorch.exportproduces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages. - Usability tradeoff: Sincetorch.compile()is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible.torch.exportwill instead require users to provide more information or rewrite their code to make it traceable. Compared to torch.fx.symbolic_trace(), torch.export traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, torch.export keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, torch.export is expected to work on more user programs, and produce lower-level graphs (at the torch.ops.aten operator level). Note that users can still use torch.fx.symbolic_trace() as a preprocessing step before torch.export. Compared to torch.jit.script(), torch.export does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators). Compared to torch.jit.trace(), torch.export is sound: it is able to trace code that performs", "prev_chunk_id": "chunk_1831", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1833", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Existing frameworks#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Existing frameworks#", "content": "integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.", "prev_chunk_id": "chunk_1832", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1834", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "An Example#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "An Example#", "content": "An Example# The main entrypoint is through torch.export.export(), which takes a callable (torch.nn.Module, function, or method) and sample inputs, and captures the computation graph into an torch.export.ExportedProgram. An example: import torch from torch.export import export # Simple module for demonstration class M(torch.nn.Module): def __init__(self) -> None: super().__init__() self.conv = torch.nn.Conv2d( in_channels=3, out_channels=16, kernel_size=3, padding=1 ) self.relu = torch.nn.ReLU() self.maxpool = torch.nn.MaxPool2d(kernel_size=3) def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor: a = self.conv(x) a.add_(constant) return self.maxpool(self.relu(a)) example_args = (torch.randn(1, 3, 256, 256),) example_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)} exported_program: torch.export.ExportedProgram = export( M(), args=example_args, kwargs=example_kwargs ) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_conv_weight: \"f32[16, 3, 3, 3]\", p_conv_bias: \"f32[16]\", x: \"f32[1, 3, 256, 256]\", constant: \"f32[1, 16, 256, 256]\"): # code: a = self.conv(x) conv2d: \"f32[1, 16, 256, 256]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias, [1, 1], [1, 1]) # code: a.add_(constant) add_: \"f32[1, 16, 256, 256]\" = torch.ops.aten.add_.Tensor(conv2d, constant) # code: return self.maxpool(self.relu(a)) relu: \"f32[1, 16, 256, 256]\" = torch.ops.aten.relu.default(add_) max_pool2d: \"f32[1, 16, 85, 85]\" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [3, 3]) return (max_pool2d,) Graph signature: ExportGraphSignature( input_specs=[ InputSpec( kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_conv_weight'), target='conv.weight', persistent=None ), InputSpec( kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_conv_bias'), target='conv.bias', persistent=None ), InputSpec( kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None ), InputSpec( kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='constant'), target=None, persistent=None ) ], output_specs=[ OutputSpec( kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='max_pool2d'), target=None ) ] ) Range constraints: {} Inspecting the ExportedProgram, we can note the following: - Thetorch.fx.Graphcontains the computation graph of the original program, along with records of the original code for easy debugging. - The graph contains onlytorch.ops.atenoperators foundhereand custom operators, and is fully functional, without any inplace operators such astorch.add_. - The parameters (weight and bias to conv) are lifted as inputs to the graph, resulting in noget_attrnodes in the graph, which previously existed in the result oftorch.fx.symbolic_trace(). - Thetorch.export.ExportGraphSignaturemodels the input and output signature, along with specifying", "prev_chunk_id": "chunk_1833", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1835", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "An Example#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "An Example#", "content": "which inputs are parameters. - The resulting shape and dtype of tensors produced by each node in the graph is noted. For example, theconvolutionnode will result in a tensor of dtypetorch.float32and shape (1, 16, 256, 256).", "prev_chunk_id": "chunk_1834", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1836", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Non-Strict Export#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Non-Strict Export#", "content": "Non-Strict Export# In PyTorch 2.3, we introduced a new mode of tracing called non-strict mode. It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag. In non-strict mode, we trace through the program using the Python interpreter. Your code will execute exactly as it would in eager mode; the only difference is that all Tensor objects will be replaced by ProxyTensors, which will record all their operations into a graph. In strict mode, which is currently the default, we first trace through the program using TorchDynamo, a bytecode analysis engine. TorchDynamo does not actually execute your Python code. Instead, it symbolically analyzes it and builds a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported. An example of a case where one might want to use non-strict mode is if you run into a unsupported TorchDynamo feature that might not be easily solved, and you know the python code is not exactly needed for computation. For example: import contextlib import torch class ContextManager(): def __init__(self): self.count = 0 def __enter__(self): self.count += 1 def __exit__(self, exc_type, exc_value, traceback): self.count -= 1 class M(torch.nn.Module): def forward(self, x): with ContextManager(): return x.sin() + x.cos() export(M(), (torch.ones(3, 3),), strict=False) # Non-strict traces successfully export(M(), (torch.ones(3, 3),)) # Strict mode fails with torch._dynamo.exc.Unsupported: ContextManager In this example, the first call using non-strict mode (through the strict=False flag) traces successfully whereas the second call using strict mode (default) results with a failure, where TorchDynamo is unable to support context managers. One option is to rewrite the code (see Limitations of torch.export), but seeing as the context manager does not affect the tensor computations in the model, we can go with", "prev_chunk_id": "chunk_1835", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1837", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Non-Strict Export#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Non-Strict Export#", "content": "the non-strict mode’s result.", "prev_chunk_id": "chunk_1836", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1838", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Export for Training and Inference#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Export for Training and Inference#", "content": "Export for Training and Inference# In PyTorch 2.5, we introduced a new API called export_for_training(). It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag. In this API, we produce the most generic IR that contains all ATen operators (including both functional and non-functional) which can be used to train in eager PyTorch Autograd. This API is intended for eager training use cases such as PT2 Quantization and will soon be the default IR of torch.export.export. To read further about the motivation behind this change, please refer to https://dev-discuss.pytorch.org/t/why-pytorch-does-not-need-a-new-standardized-operator-set/2206 When this API is combined with run_decompositions(), you should be able to get inference IR with any desired decomposition behavior. To show some examples: class ConvBatchnorm(torch.nn.Module): def __init__(self) -> None: super().__init__() self.conv = torch.nn.Conv2d(1, 3, 1, 1) self.bn = torch.nn.BatchNorm2d(3) def forward(self, x): x = self.conv(x) x = self.bn(x) return (x,) mod = ConvBatchnorm() inp = torch.randn(1, 1, 3, 3) ep_for_training = torch.export.export_for_training(mod, (inp,)) print(ep_for_training) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"): conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias) add_: \"i64[]\" = torch.ops.aten.add_.Tensor(b_bn_num_batches_tracked, 1) batch_norm: \"f32[1, 3, 3, 3]\" = torch.ops.aten.batch_norm.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05, True) return (batch_norm,) From the above output, you can see that export_for_training() produces pretty much the same ExportedProgram as export() except for the operators in the graph. You can see that we captured batch_norm in the most general form. This op is non-functional and will be lowered to different ops when running inference. You can also go from this IR to an inference IR via run_decompositions() with arbitrary customizations. # Lower to core aten inference IR, but keep conv2d", "prev_chunk_id": "chunk_1837", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1839", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Export for Training and Inference#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Export for Training and Inference#", "content": "decomp_table = torch.export.default_decompositions() del decomp_table[torch.ops.aten.conv2d.default] ep_for_inference = ep_for_training.run_decompositions(decomp_table) print(ep_for_inference) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"): conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias) add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1) _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05) getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0] getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3] getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4] return (getitem_3, getitem_4, add, getitem) Here you can see that we kept conv2d op in the IR while decomposing the rest. Now the IR is a functional IR containing core aten operators except for conv2d. You can do even more customization by directly registering your chosen decomposition behaviors. You can do even more customizations by directly registering custom decomp behaviour # Lower to core aten inference IR, but customize conv2d decomp_table = torch.export.default_decompositions() def my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1): return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups) decomp_table[torch.ops.aten.conv2d.default] = my_awesome_conv2d_function ep_for_inference = ep_for_training.run_decompositions(decomp_table) print(ep_for_inference) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"): convolution: \"f32[1, 3, 3, 3]\" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1) mul: \"f32[1, 3, 3, 3]\" = torch.ops.aten.mul.Tensor(convolution, 2) add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1) _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(mul, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05) getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0] getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3] getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4]; return (getitem_3, getitem_4, add, getitem)", "prev_chunk_id": "chunk_1838", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1840", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Expressing Dynamism#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Expressing Dynamism#", "content": "Expressing Dynamism# By default torch.export will trace the program assuming all input shapes are static, and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be specified by using the torch.export.Dim() API to create them and by passing them into torch.export.export() through the dynamic_shapes argument. An example: import torch from torch.export import Dim, export class M(torch.nn.Module): def __init__(self): super().__init__() self.branch1 = torch.nn.Sequential( torch.nn.Linear(64, 32), torch.nn.ReLU() ) self.branch2 = torch.nn.Sequential( torch.nn.Linear(128, 64), torch.nn.ReLU() ) self.buffer = torch.ones(32) def forward(self, x1, x2): out1 = self.branch1(x1) out2 = self.branch2(x2) return (out1 + self.buffer, out2) example_args = (torch.randn(32, 64), torch.randn(32, 128)) # Create a dynamic batch size batch = Dim(\"batch\") # Specify that the first dimension of each input is that batch size dynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}} exported_program: torch.export.ExportedProgram = export( M(), args=example_args, dynamic_shapes=dynamic_shapes ) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[s0, 64]\", x2: \"f32[s0, 128]\"): # code: out1 = self.branch1(x1) linear: \"f32[s0, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias) relu: \"f32[s0, 32]\" = torch.ops.aten.relu.default(linear) # code: out2 = self.branch2(x2) linear_1: \"f32[s0, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias) relu_1: \"f32[s0, 64]\" = torch.ops.aten.relu.default(linear_1) # code: return (out1 + self.buffer, out2) add: \"f32[s0, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer) return (add, relu_1) Range constraints: {s0: VR[0, int_oo]} Some additional things to note: - Through thetorch.export.Dim()API and thedynamic_shapesargument, we specified the first dimension of each input to be dynamic. Looking at the inputsx1andx2, they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs.s0is a symbol representing that this dimension can be a range of values. - exported_program.range_constraintsdescribes the", "prev_chunk_id": "chunk_1839", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1841", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Expressing Dynamism#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Expressing Dynamism#", "content": "ranges of each symbol appearing in the graph. In this case, we see thats0has the range [0, int_oo]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. SeeThe 0/1 Specialization Problemfor an in-depth discussion of this topic. We can also specify more expressive relationships between input shapes, such as where a pair of shapes might differ by one, a shape might be double of another, or a shape is even. An example: class M(torch.nn.Module): def forward(self, x, y): return x + y[1:] x, y = torch.randn(5), torch.randn(6) dimx = torch.export.Dim(\"dimx\", min=3, max=6) dimy = dimx + 1 exported_program = torch.export.export( M(), (x, y), dynamic_shapes=({0: dimx}, {0: dimy}), ) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[s0]\", y: \"f32[s0 + 1]\"): # code: return x + y[1:] slice_1: \"f32[s0]\" = torch.ops.aten.slice.Tensor(y, 0, 1, 9223372036854775807) add: \"f32[s0]\" = torch.ops.aten.add.Tensor(x, slice_1) return (add,) Range constraints: {s0: VR[3, 6], s0 + 1: VR[4, 7]} Some things to note: - By specifying{0:dimx}for the first input, we see that the resulting shape of the first input is now dynamic, being[s0]. And now by specifying{0:dimy}for the second input, we see that the resulting shape of the second input is also dynamic. However, because we expresseddimy=dimx+1, instead ofy’s shape containing a new symbol, we see that it is now being represented with the same symbol used inx,s0. We can see that relationship ofdimy=dimx+1is being shown throughs0+1. - Looking at the range constraints, we see thats0has the range [3, 6], which is specified initially, and we can see thats0+1has the solved range of [4, 7].", "prev_chunk_id": "chunk_1840", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1842", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Serialization#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Serialization#", "content": "Serialization# To save the ExportedProgram, users can use the torch.export.save() and torch.export.load() APIs. A convention is to save the ExportedProgram using a .pt2 file extension. An example: import torch import io class MyModule(torch.nn.Module): def forward(self, x): return x + 10 exported_program = torch.export.export(MyModule(), torch.randn(5)) torch.export.save(exported_program, 'exported_program.pt2') saved_exported_program = torch.export.load('exported_program.pt2')", "prev_chunk_id": "chunk_1841", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1843", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Specializations#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Specializations#", "content": "Specializations# A key concept in understanding the behavior of torch.export is the difference between static and dynamic values. A dynamic value is one that can change from run to run. These behave like normal arguments to a Python function—you can pass different values for an argument and expect your function to do the right thing. Tensor data is treated as dynamic. A static value is a value that is fixed at export time and cannot change between executions of the exported program. When the value is encountered during tracing, the exporter will treat it as a constant and hard-code it into the graph. When an operation is performed (e.g. x + y) and all inputs are static, then the output of the operation will be directly hard-coded into the graph, and the operation won’t show up (i.e. it will get constant-folded). When a value has been hard-coded into the graph, we say that the graph has been specialized to that value. The following values are static:", "prev_chunk_id": "chunk_1842", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1844", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Input Tensor Shapes#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Input Tensor Shapes#", "content": "Input Tensor Shapes# By default, torch.export will trace the program specializing on the input tensors’ shapes, unless a dimension is specified as dynamic via the dynamic_shapes argument to torch.export. This means that if there exists shape-dependent control flow, torch.export will specialize on the branch that is being taken with the given sample inputs. For example: import torch from torch.export import export class Mod(torch.nn.Module): def forward(self, x): if x.shape[0] > 5: return x + 1 else: return x - 1 example_inputs = (torch.rand(10, 2),) exported_program = export(Mod(), example_inputs) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[10, 2]\"): # code: return x + 1 add: \"f32[10, 2]\" = torch.ops.aten.add.Tensor(x, 1) return (add,) The conditional of (x.shape[0] > 5) does not appear in the ExportedProgram because the example inputs have the static shape of (10, 2). Since torch.export specializes on the inputs’ static shapes, the else branch (x - 1) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, torch.export.Dim() will need to be used to specify the dimension of the input tensor (x.shape[0]) to be dynamic, and the source code will need to be rewritten. Note that tensors that are part of the module state (e.g. parameters and buffers) always have static shapes.", "prev_chunk_id": "chunk_1843", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1845", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Python Primitives#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Python Primitives#", "content": "Python Primitives# torch.export also specializes on Python primitives, such as int, float, bool, and str. However they do have dynamic variants such as SymInt, SymFloat, and SymBool. For example: import torch from torch.export import export class Mod(torch.nn.Module): def forward(self, x: torch.Tensor, const: int, times: int): for i in range(times): x = x + const return x example_inputs = (torch.rand(2, 2), 1, 3) exported_program = export(Mod(), example_inputs) print(exported_program) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[2, 2]\", const, times): # code: x = x + const add: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(x, 1) add_1: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(add, 1) add_2: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(add_1, 1) return (add_2,) Because integers are specialized, the torch.ops.aten.add.Tensor operations are all computed with the hard-coded constant 1, rather than const. If a user passes a different value for const at runtime, like 2, than the one used during export time, 1, this will result in an error. Additionally, the times iterator used in the for loop is also “inlined” in the graph through the 3 repeated torch.ops.aten.add.Tensor calls, and the input times is never used.", "prev_chunk_id": "chunk_1844", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1846", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Python Containers#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Python Containers#", "content": "Python Containers# Python containers (List, Dict, NamedTuple, etc.) are considered to have static structure.", "prev_chunk_id": "chunk_1845", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1847", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Graph Breaks#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Graph Breaks#", "content": "Graph Breaks# As torch.export is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of torch.compile, an unsupported operation will cause a “graph break” and the unsupported operation will be run with default Python evaluation. In contrast, torch.export will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks. When a graph break is encountered, ExportDB is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable. An option to get past dealing with this graph breaks is by using non-strict export", "prev_chunk_id": "chunk_1846", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1848", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Data/Shape-Dependent Control Flow#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Data/Shape-Dependent Control Flow#", "content": "Data/Shape-Dependent Control Flow# Graph breaks can also be encountered on data-dependent control flow (if x.shape[0] > 2) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators. Currently, we support torch.cond to express if-else like control flow (more coming soon!).", "prev_chunk_id": "chunk_1847", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1849", "url": "https://docs.pytorch.org/docs/stable/export.html", "title": "Missing Fake/Meta/Abstract Kernels for Operators#", "page_title": "torch.export — PyTorch 2.8 documentation", "breadcrumbs": "Missing Fake/Meta/Abstract Kernels for Operators#", "content": "Missing Fake/Meta/Abstract Kernels for Operators# When tracing, a FakeTensor kernel (aka meta kernel, abstract impl) is required for all operators. This is used to reason about the input/output shapes for this operator. Please see torch.library.register_fake() for more details. In the unfortunate case where your model uses an ATen operator that is does not have a FakeTensor kernel implementation yet, please file an issue.", "prev_chunk_id": "chunk_1848", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1850", "url": "https://docs.pytorch.org/docs/stable/backends.html", "title": "torch.backends#", "page_title": "torch.backends — PyTorch 2.8 documentation", "breadcrumbs": "torch.backends#", "content": "torch.backends# Created On: Sep 16, 2020 | Last Updated On: Jun 12, 2025 torch.backends controls the behavior of various backends that PyTorch supports. These backends include: - torch.backends.cpu - torch.backends.cuda - torch.backends.cudnn - torch.backends.cusparselt - torch.backends.mha - torch.backends.mps - torch.backends.mkl - torch.backends.mkldnn - torch.backends.nnpack - torch.backends.openmp - torch.backends.opt_einsum - torch.backends.xeon", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1851", "url": "https://docs.pytorch.org/docs/stable/meta.html", "title": "Meta device#", "page_title": "Meta device — PyTorch 2.8 documentation", "breadcrumbs": "Meta device#", "content": "Meta device# Created On: Jun 17, 2025 | Last Updated On: Jun 17, 2025 The “meta” device is an abstract device which denotes a tensor which records only metadata, but no actual data. Meta tensors have two primary use cases: - Models can be loaded on the meta device, allowing you to load a representation of the model without actually loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data. - Most operations can be performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations liketorch.nonzero()oritem(). In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we typically prefer representing the CUDA behavior faithfully in this situation.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1852", "url": "https://docs.pytorch.org/docs/stable/meta.html", "title": "Idioms for working with meta tensors#", "page_title": "Meta device — PyTorch 2.8 documentation", "breadcrumbs": "Idioms for working with meta tensors#", "content": "Idioms for working with meta tensors# An object can be loaded with torch.load() onto meta device by specifying map_location='meta': >>> torch.save(torch.randn(2), 'foo.pt') >>> torch.load('foo.pt', map_location='meta') tensor(..., device='meta', size=(2,)) If you have some arbitrary code which performs some tensor construction without explicitly specifying a device, you can override it to instead construct on meta device by using the torch.device() context manager: >>> with torch.device('meta'): ... print(torch.randn(30, 30)) ... tensor(..., device='meta', size=(30, 30)) This is especially helpful NN module construction, where you often are not able to explicitly pass in a device for initialization: >>> from torch.nn.modules import Linear >>> with torch.device('meta'): ... print(Linear(20, 30)) ... Linear(in_features=20, out_features=30, bias=True) You cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores no data and we do not know what the correct data values for your new tensor are: >>> torch.ones(5, device='meta').to(\"cpu\") Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NotImplementedError: Cannot copy out of meta tensor; no data! Use a factory function like torch.empty_like() to explicitly specify how you would like the missing data to be filled in. NN modules have a convenience method torch.nn.Module.to_empty() that allows you to move the module to another device, leaving all parameters uninitialized. You are expected to explicitly reinitialize the parameters manually: >>> from torch.nn.modules import Linear >>> with torch.device('meta'): ... m = Linear(20, 30) >>> m.to_empty(device=\"cpu\") Linear(in_features=20, out_features=30, bias=True) torch._subclasses.meta_utils contains undocumented utilities for taking an arbitrary Tensor and constructing an equivalent meta Tensor with high fidelity. These APIs are experimental and may be changed in a BC breaking way at any time.", "prev_chunk_id": "chunk_1851", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1853", "url": "https://docs.pytorch.org/docs/stable/mtia.memory.html", "title": "torch.mtia.memory#", "page_title": "torch.mtia.memory — PyTorch 2.8 documentation", "breadcrumbs": "torch.mtia.memory#", "content": "torch.mtia.memory# Created On: Dec 09, 2024 | Last Updated On: Jun 08, 2025 The MTIA backend is implemented out of the tree, only interfaces are be defined here. This package adds support for device memory management implemented in MTIA.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1854", "url": "https://docs.pytorch.org/docs/stable/mtia.html", "title": "torch.mtia#", "page_title": "torch.mtia — PyTorch 2.8 documentation", "breadcrumbs": "torch.mtia#", "content": "torch.mtia# Created On: Jul 11, 2023 | Last Updated On: Jun 08, 2025 The MTIA backend is implemented out of the tree, only interfaces are be defined here. This package enables an interface for accessing MTIA backend in python", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1855", "url": "https://docs.pytorch.org/docs/stable/xpu.html", "title": "torch.xpu#", "page_title": "torch.xpu — PyTorch 2.8 documentation", "breadcrumbs": "torch.xpu#", "content": "torch.xpu# Created On: Feb 01, 2024 | Last Updated On: Jun 06, 2025 This package introduces support for the XPU backend, specifically tailored for Intel GPU optimization. This package is lazily initialized, so you can always import it, and use is_available() to determine if your system supports XPU.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1856", "url": "https://docs.pytorch.org/docs/stable/mps.html", "title": "torch.mps#", "page_title": "torch.mps — PyTorch 2.8 documentation", "breadcrumbs": "torch.mps#", "content": "torch.mps# Created On: Feb 10, 2023 | Last Updated On: Jun 08, 2025 This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See https://developer.apple.com/documentation/metalperformanceshaders for more details.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1857", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Understanding CUDA Memory Usage#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Understanding CUDA Memory Usage#", "content": "Understanding CUDA Memory Usage# Created On: Aug 23, 2023 | Last Updated On: Jun 10, 2025 To debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory at any point in time, and optionally record the history of allocation events that led up to that snapshot. The generated snapshots can then be drag and dropped onto the interactiver viewer hosted at pytorch.org/memory_viz which can be used to explore the snapshot.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1858", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Generating a Snapshot#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Generating a Snapshot#", "content": "Generating a Snapshot# The common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot: # enable memory history, which will # add tracebacks and event history to snapshots torch.cuda.memory._record_memory_history() run_your_code() torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")", "prev_chunk_id": "chunk_1857", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1859", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Using the visualizer#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Using the visualizer#", "content": "Using the visualizer# Open pytorch.org/memory_viz and drag/drop the pickled snapshot file into the visualizer. The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.", "prev_chunk_id": "chunk_1858", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1860", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Active Memory Timeline#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Active Memory Timeline#", "content": "Active Memory Timeline# The Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations. Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to render fewer allocations and improve performance when there is a lot of data.", "prev_chunk_id": "chunk_1859", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1861", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Allocator State History#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Allocator State History#", "content": "Allocator State History# The Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred, such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why an allocation failed even though reserved memory still exists. The stack trace information also reports the address at which an allocation occurred. The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the “_0”th time this address was allocated. This unique string can be looked up in the Active Memory Timeline and searched in the Active State History to examine the memory state when a tensor was allocated or freed.", "prev_chunk_id": "chunk_1860", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1862", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Identifying Non-PyTorch allocations#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Identifying Non-PyTorch allocations#", "content": "Identifying Non-PyTorch allocations# If you suspect CUDA memory is being allocated outside of PyTorch, you can collect the raw CUDA allocation info using the pynvml package, and compare that to the allocation reported by pytorch. To collect raw memory usage outside pytorch, use device_memory_used() import torch device_idx = ... print(torch.cuda.device_memory_used(device_idx))", "prev_chunk_id": "chunk_1861", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1863", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Buffer behavior#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Buffer behavior#", "content": "Buffer behavior# This will store up to max_entries instances of TraceEntry when enabled. Python trace collection defaults to sys.maxsize, meaning long-running or indefinitely running jobs should set a reasonable limit to avoid excessive memory use. Expect each entry to be several KB. Longer running workflows or those with smaller max_entries values will only store the last accumulated max_entries entries, meaning new entries overwrite older entries. C++ implementation for reference to ring buffer implemenation: if (record_history) { if (alloc_trace->size() < alloc_trace_max_entries_) { alloc_trace->emplace_back(te); } else { (*alloc_trace)[alloc_trace_next++] = te; if (alloc_trace_next == alloc_trace_max_entries_) { alloc_trace_next = 0; } } }", "prev_chunk_id": "chunk_1862", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1864", "url": "https://docs.pytorch.org/docs/stable/torch_cuda_memory.html", "title": "Latency impact#", "page_title": "Understanding CUDA Memory Usage — PyTorch 2.8 documentation", "breadcrumbs": "Latency impact#", "content": "Latency impact# The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues. C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth.", "prev_chunk_id": "chunk_1863", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1865", "url": "https://docs.pytorch.org/docs/stable/cuda.html", "title": "torch.cuda#", "page_title": "torch.cuda — PyTorch 2.8 documentation", "breadcrumbs": "torch.cuda#", "content": "torch.cuda# Created On: Dec 23, 2016 | Last Updated On: Jun 13, 2025 This package adds support for CUDA tensor types. It implements the same function as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1866", "url": "https://docs.pytorch.org/docs/stable/cuda.html", "title": "Graphs (beta)#", "page_title": "torch.cuda — PyTorch 2.8 documentation", "breadcrumbs": "Graphs (beta)#", "content": "Graphs (beta)# This package adds support for device memory management implemented in CUDA.", "prev_chunk_id": "chunk_1865", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1867", "url": "https://docs.pytorch.org/docs/stable/cuda.html", "title": "TunableOp#", "page_title": "torch.cuda — PyTorch 2.8 documentation", "breadcrumbs": "TunableOp#", "content": "TunableOp# Some operations could be implemented using more than one library or more than one technique. For example, a GEMM could be implemented for CUDA or ROCm using either the cublas/cublasLt libraries or hipblas/hipblasLt libraries, respectively. How does one know which implementation is the fastest and should be chosen? That’s what TunableOp provides. Certain operators have been implemented using multiple strategies as Tunable Operators. At runtime, all strategies are profiled and the fastest is selected for all subsequent operations. See the documentation for information on how to use it.", "prev_chunk_id": "chunk_1866", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1868", "url": "https://docs.pytorch.org/docs/stable/cuda.html", "title": "Stream Sanitizer (prototype)#", "page_title": "torch.cuda — PyTorch 2.8 documentation", "breadcrumbs": "Stream Sanitizer (prototype)#", "content": "Stream Sanitizer (prototype)# CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it.", "prev_chunk_id": "chunk_1867", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1869", "url": "https://docs.pytorch.org/docs/stable/cuda.html", "title": "GPUDirect Storage (prototype)#", "page_title": "torch.cuda — PyTorch 2.8 documentation", "breadcrumbs": "GPUDirect Storage (prototype)#", "content": "GPUDirect Storage (prototype)# The APIs in torch.cuda.gds provide thin wrappers around certain cuFile APIs that allow direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the cufile api documentation for more details. These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must ensure that their system is appropriately configured to use GPUDirect Storage per the GPUDirect Storage documentation. See the docs for GdsFile for an example of how to use these.", "prev_chunk_id": "chunk_1868", "next_chunk_id": null, "type": "section"}
]