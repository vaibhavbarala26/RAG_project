[
{"chunk_id": "chunk_0", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1. Linear Models#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1. Linear Models#", "content": "1.1. Linear Models# The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \\(\\hat{y}\\) is the predicted value. Across the module, we designate the vector \\(w = (w_1, ..., w_p)\\) as coef_ and \\(w_0\\) as intercept_. To perform classification with generalized linear models, see Logistic regression.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_1", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.1. Ordinary Least Squares#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.1. Ordinary Least Squares#", "content": "1.1.1. Ordinary Least Squares# LinearRegression fits a linear model with coefficients \\(w = (w_1, ..., w_p)\\) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form: LinearRegression takes in its fit method arguments X, y, sample_weight and stores the coefficients \\(w\\) of the linear model in its coef_ and intercept_ attributes: >>> from sklearn import linear_model >>> reg = linear_model.LinearRegression() >>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2]) LinearRegression() >>> reg.coef_ array([0.5, 0.5]) >>> reg.intercept_ 0.0 The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and some columns of the design matrix \\(X\\) have an approximately linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design. Examples - Ordinary Least Squares and Ridge Regression", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_2", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.1.1. Non-Negative Least Squares#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.1.1. Non-Negative Least Squares#", "content": "1.1.1.1. Non-Negative Least Squares# It is possible to constrain all the coefficients to be non-negative, which may be useful when they represent some physical or naturally non-negative quantities (e.g., frequency counts or prices of goods). LinearRegression accepts a boolean positive parameter: when set to True Non-Negative Least Squares are then applied. Examples - Non-negative least squares", "prev_chunk_id": "chunk_1", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_3", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.1.2. Ordinary Least Squares Complexity#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.1.2. Ordinary Least Squares Complexity#", "content": "1.1.1.2. Ordinary Least Squares Complexity# The least squares solution is computed using the singular value decomposition of \\(X\\). If \\(X\\) is a matrix of shape (n_samples, n_features) this method has a cost of \\(O(n_{\\text{samples}} n_{\\text{features}}^2)\\), assuming that \\(n_{\\text{samples}} \\geq n_{\\text{features}}\\).", "prev_chunk_id": "chunk_2", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_4", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.2.1. Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.2.1. Regression#", "content": "1.1.2.1. Regression# Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares: The complexity parameter \\(\\alpha \\geq 0\\) controls the amount of shrinkage: the larger the value of \\(\\alpha\\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. As with other linear models, Ridge will take in its fit method arrays X, y and will store the coefficients \\(w\\) of the linear model in its coef_ member: >>> from sklearn import linear_model >>> reg = linear_model.Ridge(alpha=.5) >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) Ridge(alpha=0.5) >>> reg.coef_ array([0.34545455, 0.34545455]) >>> reg.intercept_ np.float64(0.13636) Note that the class Ridge allows for the user to specify that the solver be automatically chosen by setting solver=\"auto\". When this option is specified, Ridge will choose between the \"lbfgs\", \"cholesky\", and \"sparse_cg\" solvers. Ridge will begin checking the conditions shown in the following table from top to bottom. If the condition is true, the corresponding solver is chosen. Examples - Ordinary Least Squares and Ridge Regression - Plot Ridge coefficients as a function of the regularization - Common pitfalls in the interpretation of coefficients of linear models", "prev_chunk_id": "chunk_3", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_5", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.2.2. Classification#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.2.2. Classification#", "content": "1.1.2.2. Classification# The Ridge regressor has a classifier variant: RidgeClassifier. This classifier first converts binary targets to {-1, 1} and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressor’s prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value. It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However, in practice, all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the RidgeClassifier allows for a very different choice of the numerical solvers with distinct computational performance profiles. The RidgeClassifier can be significantly faster than e.g. LogisticRegression with a high number of classes because it can compute the projection matrix \\((X^T X)^{-1} X^T\\) only once. This classifier is sometimes referred to as a Least Squares Support Vector Machine with a linear kernel. Examples - Classification of text documents using sparse features", "prev_chunk_id": "chunk_4", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_6", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.2.3. Ridge Complexity#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.2.3. Ridge Complexity#", "content": "1.1.2.3. Ridge Complexity# This method has the same order of complexity as Ordinary Least Squares.", "prev_chunk_id": "chunk_5", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_7", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation#", "content": "1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation# RidgeCV and RidgeClassifierCV implement ridge regression/classification with built-in cross-validation of the alpha parameter. They work in the same way as GridSearchCV except that it defaults to efficient Leave-One-Out cross-validation. When using the default cross-validation, alpha cannot be 0 due to the formulation used to calculate Leave-One-Out error. See [RL2007] for details. Usage example: >>> import numpy as np >>> from sklearn import linear_model >>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13)) >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06])) >>> reg.alpha_ np.float64(0.01) Specifying the value of the cv attribute will trigger the use of cross-validation with GridSearchCV, for example cv=10 for 10-fold cross-validation, rather than Leave-One-Out Cross-Validation.", "prev_chunk_id": "chunk_6", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_8", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3. Lasso#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3. Lasso#", "content": "1.1.3. Lasso# The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason, Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see Compressive sensing: tomography reconstruction with L1 prior (Lasso)). Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is: The lasso estimate thus solves the minimization of the least-squares penalty with \\(\\alpha ||w||_1\\) added, where \\(\\alpha\\) is a constant and \\(||w||_1\\) is the \\(\\ell_1\\)-norm of the coefficient vector. The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See Least Angle Regression for another implementation: >>> from sklearn import linear_model >>> reg = linear_model.Lasso(alpha=0.1) >>> reg.fit([[0, 0], [1, 1]], [0, 1]) Lasso(alpha=0.1) >>> reg.predict([[1, 1]]) array([0.8]) The function lasso_path is useful for lower-level tasks, as it computes the coefficients along the full path of possible values. Examples - L1-based models for Sparse Signals - Compressive sensing: tomography reconstruction with L1 prior (Lasso) - Common pitfalls in the interpretation of coefficients of linear models", "prev_chunk_id": "chunk_7", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_9", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3.1. Setting regularization parameter#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3.1. Setting regularization parameter#", "content": "1.1.3.1. Setting regularization parameter# The alpha parameter controls the degree of sparsity of the estimated coefficients.", "prev_chunk_id": "chunk_8", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_10", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3.1.1. Using cross-validation#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3.1.1. Using cross-validation#", "content": "1.1.3.1.1. Using cross-validation# scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm explained below. For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV.", "prev_chunk_id": "chunk_9", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_11", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3.1.2. Information-criteria based model selection#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3.1.2. Information-criteria based model selection#", "content": "1.1.3.1.2. Information-criteria based model selection# Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. Indeed, these criteria are computed on the in-sample training set. In short, they penalize the over-optimistic scores of the different Lasso models by their flexibility (cf. to “Mathematical details” section below). However, such criteria need a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the correct model is candidates under investigation. They also tend to break when the problem is badly conditioned (e.g. more features than samples). Examples - Lasso model selection: AIC-BIC / cross-validation - Lasso model selection via information criteria", "prev_chunk_id": "chunk_10", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_12", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3.1.3. AIC and BIC criteria#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3.1.3. AIC and BIC criteria#", "content": "1.1.3.1.3. AIC and BIC criteria# The definition of AIC (and thus BIC) might differ in the literature. In this section, we give more information regarding the criterion computed in scikit-learn.", "prev_chunk_id": "chunk_11", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_13", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.3.1.4. Comparison with the regularization parameter of SVM#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.3.1.4. Comparison with the regularization parameter of SVM#", "content": "1.1.3.1.4. Comparison with the regularization parameter of SVM# The equivalence between alpha and the regularization parameter of SVM, C is given by alpha = 1 / C or alpha = 1 / (n_samples * C), depending on the estimator and the exact objective function optimized by the model.", "prev_chunk_id": "chunk_12", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_14", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.4. Multi-task Lasso#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.4. Multi-task Lasso#", "content": "1.1.4. Multi-task Lasso# The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks. The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns. Fitting a time-series model, imposing that any active feature be active at all times. Examples - Joint feature selection with multi-task Lasso", "prev_chunk_id": "chunk_13", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_15", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.5. Elastic-Net#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.5. Elastic-Net#", "content": "1.1.5. Elastic-Net# ElasticNet is a linear regression model trained with both \\(\\ell_1\\) and \\(\\ell_2\\)-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of \\(\\ell_1\\) and \\(\\ell_2\\) using the l1_ratio parameter. Elastic-net is useful when there are multiple features that are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both. A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge’s stability under rotation. The objective function to minimize is in this case The class ElasticNetCV can be used to set the parameters alpha (\\(\\alpha\\)) and l1_ratio (\\(\\rho\\)) by cross-validation. Examples - L1-based models for Sparse Signals - Lasso, Lasso-LARS, and Elastic Net paths - Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples", "prev_chunk_id": "chunk_14", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_16", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.6. Multi-task Elastic-Net#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.6. Multi-task Elastic-Net#", "content": "1.1.6. Multi-task Elastic-Net# The MultiTaskElasticNet is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: Y is a 2D array of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks. Mathematically, it consists of a linear model trained with a mixed \\(\\ell_1\\) \\(\\ell_2\\)-norm and \\(\\ell_2\\)-norm for regularization. The objective function to minimize is: The implementation in the class MultiTaskElasticNet uses coordinate descent as the algorithm to fit the coefficients. The class MultiTaskElasticNetCV can be used to set the parameters alpha (\\(\\alpha\\)) and l1_ratio (\\(\\rho\\)) by cross-validation.", "prev_chunk_id": "chunk_15", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_17", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.7. Least Angle Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.7. Least Angle Regression#", "content": "1.1.7. Least Angle Regression# Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features. The advantages of LARS are: - It is numerically efficient in contexts where the number of features is significantly greater than the number of samples. - It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares. - It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model. - If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable. - It is easily modified to produce solutions for other estimators, like the Lasso. The disadvantages of the LARS method include: - Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article. The LARS model can be used via the estimator Lars, or its low-level implementation lars_path or lars_path_gram.", "prev_chunk_id": "chunk_16", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_18", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.8. LARS Lasso#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.8. LARS Lasso#", "content": "1.1.8. LARS Lasso# LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients. >>> from sklearn import linear_model >>> reg = linear_model.LassoLars(alpha=.1) >>> reg.fit([[0, 0], [1, 1]], [0, 1]) LassoLars(alpha=0.1) >>> reg.coef_ array([0.6, 0. ]) Examples - Lasso, Lasso-LARS, and Elastic Net paths The LARS algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions lars_path or lars_path_gram.", "prev_chunk_id": "chunk_17", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_19", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.9. Orthogonal Matching Pursuit (OMP)#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.9. Orthogonal Matching Pursuit (OMP)#", "content": "1.1.9. Orthogonal Matching Pursuit (OMP)# OrthogonalMatchingPursuit and orthogonal_mp implement the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (i.e. the \\(\\ell_0\\) pseudo-norm). Being a forward feature selection method like Least Angle Regression, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements: Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as: OMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements. Examples - Orthogonal Matching Pursuit", "prev_chunk_id": "chunk_18", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_20", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.10. Bayesian Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.10. Bayesian Regression#", "content": "1.1.10. Bayesian Regression# Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. This can be done by introducing uninformative priors over the hyper parameters of the model. The \\(\\ell_{2}\\) regularization used in Ridge regression and classification is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients \\(w\\) with precision \\(\\lambda^{-1}\\). Instead of setting lambda manually, it is possible to treat it as a random variable to be estimated from the data. To obtain a fully probabilistic model, the output \\(y\\) is assumed to be Gaussian distributed around \\(X w\\): where \\(\\alpha\\) is again treated as a random variable that is to be estimated from the data. The advantages of Bayesian Regression are: - It adapts to the data at hand. - It can be used to include regularization parameters in the estimation procedure. The disadvantages of Bayesian regression include: - Inference of the model can be time consuming.", "prev_chunk_id": "chunk_19", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_21", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.10.1. Bayesian Ridge Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.10.1. Bayesian Ridge Regression#", "content": "1.1.10.1. Bayesian Ridge Regression# BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the coefficient \\(w\\) is given by a spherical Gaussian: The priors over \\(\\alpha\\) and \\(\\lambda\\) are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters \\(w\\), \\(\\alpha\\) and \\(\\lambda\\) are estimated jointly during the fit of the model, the regularization parameters \\(\\alpha\\) and \\(\\lambda\\) being estimated by maximizing the log marginal likelihood. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \\(\\alpha\\) and \\(\\lambda\\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters alpha_init and lambda_init. There are four more hyperparameters, \\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\lambda_1\\) and \\(\\lambda_2\\) of the gamma prior distributions over \\(\\alpha\\) and \\(\\lambda\\). These are usually chosen to be non-informative. By default \\(\\alpha_1 = \\alpha_2 = \\lambda_1 = \\lambda_2 = 10^{-6}\\). Bayesian Ridge Regression is used for regression: >>> from sklearn import linear_model >>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]] >>> Y = [0., 1., 2., 3.] >>> reg = linear_model.BayesianRidge() >>> reg.fit(X, Y) BayesianRidge() After being fitted, the model can then be used to predict new values: >>> reg.predict([[1, 0.]]) array([0.50000013]) The coefficients \\(w\\) of the model can be accessed: >>> reg.coef_ array([0.49999993, 0.49999993]) Due to the Bayesian framework, the weights found are slightly different from the ones found by Ordinary Least Squares. However, Bayesian Ridge Regression is more robust to ill-posed problems. Examples - Curve Fitting with Bayesian Ridge Regression", "prev_chunk_id": "chunk_20", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_22", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.10.2. Automatic Relevance Determination - ARD#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.10.2. Automatic Relevance Determination - ARD#", "content": "1.1.10.2. Automatic Relevance Determination - ARD# The Automatic Relevance Determination (as being implemented in ARDRegression) is a kind of linear model which is very similar to the Bayesian Ridge Regression, but that leads to sparser coefficients \\(w\\) [1] [2]. ARDRegression poses a different prior over \\(w\\): it drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. This means each coefficient \\(w_{i}\\) can itself be drawn from a Gaussian distribution, centered on zero and with a precision \\(\\lambda_{i}\\): with \\(A\\) being a positive definite diagonal matrix and \\(\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}\\). In contrast to the Bayesian Ridge Regression, each coordinate of \\(w_{i}\\) has its own standard deviation \\(\\frac{1}{\\lambda_i}\\). The prior over all \\(\\lambda_i\\) is chosen to be the same gamma distribution given by the hyperparameters \\(\\lambda_1\\) and \\(\\lambda_2\\). ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine [3] [4]. See Comparing Linear Bayesian Regressors for a worked-out comparison between ARD and Bayesian Ridge Regression. See L1-based models for Sparse Signals for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data. References", "prev_chunk_id": "chunk_21", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_23", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.11. Logistic regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.11. Logistic regression#", "content": "1.1.11. Logistic regression# The logistic regression is implemented in LogisticRegression. Despite its name, it is implemented as a linear model for classification rather than regression in terms of the scikit-learn/ML nomenclature. The logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \\(\\ell_1\\), \\(\\ell_2\\) or Elastic-Net regularization. Examples - L1 Penalty and Sparsity in Logistic Regression - Regularization path of L1- Logistic Regression - Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression - Multiclass sparse logistic regression on 20newgroups - MNIST classification using multinomial logistic + L1 - Plot classification probability", "prev_chunk_id": "chunk_22", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_24", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.11.1. Binary Case#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.11.1. Binary Case#", "content": "1.1.11.1. Binary Case# For notational ease, we assume that the target \\(y_i\\) takes values in the set \\(\\{0, 1\\}\\) for data point \\(i\\). Once fitted, the predict_proba method of LogisticRegression predicts the probability of the positive class \\(P(y_i=1|X_i)\\) as As an optimization problem, binary class logistic regression with regularization term \\(r(w)\\) minimizes the following cost function: where \\({s_i}\\) corresponds to the weights assigned by the user to a specific training sample (the vector \\(s\\) is formed by element-wise multiplication of the class weights and sample weights), and the sum \\(S = \\sum_{i=1}^n s_i\\). We currently provide four choices for the regularization term \\(r(w)\\) via the penalty argument: For ElasticNet, \\(\\rho\\) (which corresponds to the l1_ratio parameter) controls the strength of \\(\\ell_1\\) regularization vs. \\(\\ell_2\\) regularization. Elastic-Net is equivalent to \\(\\ell_1\\) when \\(\\rho = 1\\) and equivalent to \\(\\ell_2\\) when \\(\\rho=0\\). Note that the scale of the class weights and the sample weights will influence the optimization problem. For instance, multiplying the sample weights by a constant \\(b>0\\) is equivalent to multiplying the (inverse) regularization strength C by \\(b\\).", "prev_chunk_id": "chunk_23", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_25", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.11.2. Multinomial Case#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.11.2. Multinomial Case#", "content": "1.1.11.2. Multinomial Case# The binary case can be extended to \\(K\\) classes leading to the multinomial logistic regression, see also log-linear model.", "prev_chunk_id": "chunk_24", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_26", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.11.3. Solvers#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.11.3. Solvers#", "content": "1.1.11.3. Solvers# The solvers implemented in the class LogisticRegression are “lbfgs”, “liblinear”, “newton-cg”, “newton-cholesky”, “sag” and “saga”: The following table summarizes the penalties and multinomial multiclass supported by each solver: The “lbfgs” solver is used by default for its robustness. For n_samples >> n_features, “newton-cholesky” is a good choice and can reach high precision (tiny tol values). For large datasets the “saga” solver is usually faster (than “lbfgs”), in particular for low precision (high tol). For large dataset, you may also consider using SGDClassifier with loss=\"log_loss\", which might be even faster but requires more tuning.", "prev_chunk_id": "chunk_25", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_27", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.11.3.1. Differences between solvers#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.11.3.1. Differences between solvers#", "content": "1.1.11.3.1. Differences between solvers# There might be a difference in the scores obtained between LogisticRegression with solver=liblinear or LinearSVC and the external liblinear library directly, when fit_intercept=False and the fit coef_ (or) the data to be predicted are zeroes. This is because for the sample(s) with decision_function zero, LogisticRegression and LinearSVC predict the negative class, while liblinear predicts the positive class. Note that a model with fit_intercept=False and having many samples with decision_function zero, is likely to be an underfit, bad model and you are advised to set fit_intercept=True and increase the intercept_scaling. LogisticRegressionCV implements Logistic Regression with built-in cross-validation support, to find the optimal C and l1_ratio parameters according to the scoring attribute. The “newton-cg”, “sag”, “saga” and “lbfgs” solvers are found to be faster for high-dimensional dense data, due to warm-starting (see Glossary).", "prev_chunk_id": "chunk_26", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_28", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.12. Generalized Linear Models#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.12. Generalized Linear Models#", "content": "1.1.12. Generalized Linear Models# Generalized Linear Models (GLM) extend linear models in two ways [10]. First, the predicted values \\(\\hat{y}\\) are linked to a linear combination of the input variables \\(X\\) via an inverse link function \\(h\\) as Secondly, the squared loss function is replaced by the unit deviance \\(d\\) of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM) [11]). The minimization problem becomes: where \\(\\alpha\\) is the L2 regularization penalty. When sample weights are provided, the average becomes a weighted average. The following table lists some specific EDMs and their unit deviance : The Probability Density Functions (PDF) of these distributions are illustrated in the following figure, The Bernoulli distribution is a discrete probability distribution modelling a Bernoulli trial - an event that has only two mutually exclusive outcomes. The Categorical distribution is a generalization of the Bernoulli distribution for a categorical random variable. While a random variable in a Bernoulli distribution has two possible outcomes, a Categorical random variable can take on one of K possible categories, with the probability of each category specified separately. The choice of the distribution depends on the problem at hand: - If the target values\\(y\\)are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson distribution with a log-link. - If the target values are positive valued and skewed, you might try a Gamma distribution with a log-link. - If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian distribution (or even higher variance powers of the Tweedie family). - If the target values\\(y\\)are probabilities, you can use the Bernoulli distribution. The Bernoulli distribution with a logit link can be used for binary classification. The Categorical distribution with a softmax link can be", "prev_chunk_id": "chunk_27", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_29", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.12. Generalized Linear Models#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.12. Generalized Linear Models#", "content": "used for multiclass classification. References", "prev_chunk_id": "chunk_28", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_30", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.12.1. Usage#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.12.1. Usage#", "content": "1.1.12.1. Usage# TweedieRegressor implements a generalized linear model for the Tweedie distribution, that allows to model any of the above mentioned distributions using the appropriate power parameter. In particular: - power=0: Normal distribution. Specific estimators such asRidge,ElasticNetare generally more appropriate in this case. - power=1: Poisson distribution.PoissonRegressoris exposed for convenience. However, it is strictly equivalent toTweedieRegressor(power=1,link='log'). - power=2: Gamma distribution.GammaRegressoris exposed for convenience. However, it is strictly equivalent toTweedieRegressor(power=2,link='log'). - power=3: Inverse Gaussian distribution. The link function is determined by the link parameter. Usage example: >>> from sklearn.linear_model import TweedieRegressor >>> reg = TweedieRegressor(power=1, alpha=0.5, link='log') >>> reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2]) TweedieRegressor(alpha=0.5, link='log', power=1) >>> reg.coef_ array([0.2463, 0.4337]) >>> reg.intercept_ np.float64(-0.7638) Examples - Poisson regression and non-normal loss - Tweedie regression on insurance claims", "prev_chunk_id": "chunk_29", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_31", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.13. Stochastic Gradient Descent - SGD#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.13. Stochastic Gradient Descent - SGD#", "content": "1.1.13. Stochastic Gradient Descent - SGD# Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The partial_fit method allows online/out-of-core learning. The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier fits a logistic regression model, while with loss=\"hinge\" it fits a linear support vector machine (SVM). You can refer to the dedicated Stochastic Gradient Descent documentation section for more details.", "prev_chunk_id": "chunk_30", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_32", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.14. Perceptron#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.14. Perceptron#", "content": "1.1.14. Perceptron# The Perceptron is another simple classification algorithm suitable for large scale learning. By default: - It does not require a learning rate. - It is not regularized (penalized). - It updates its model only on mistakes. The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser. In fact, the Perceptron is a wrapper around the SGDClassifier class using a perceptron loss and a constant learning rate. Refer to mathematical section of the SGD procedure for more details.", "prev_chunk_id": "chunk_31", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_33", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.15. Passive Aggressive Algorithms#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.15. Passive Aggressive Algorithms#", "content": "1.1.15. Passive Aggressive Algorithms# The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C. For classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).", "prev_chunk_id": "chunk_32", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_34", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.16. Robustness regression: outliers and modeling errors#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.16. Robustness regression: outliers and modeling errors#", "content": "1.1.16. Robustness regression: outliers and modeling errors# Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model.", "prev_chunk_id": "chunk_33", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_35", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.16.1. Different scenario and useful concepts#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.16.1. Different scenario and useful concepts#", "content": "1.1.16.1. Different scenario and useful concepts# There are different things to keep in mind when dealing with data corrupted by outliers: - Outliers in X or in y?Outliers in the y directionOutliers in the X direction - Fraction of outliers versus amplitude of errorThe number of outlying points matters, but also how much they are outliers.Small outliersLarge outliers An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data. Note that in general, robust fitting in high-dimensional setting (large n_features) is very hard. The robust models here will probably not work in these settings.", "prev_chunk_id": "chunk_34", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_36", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.16.2. RANSAC: RANdom SAmple Consensus#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.16.2. RANSAC: RANdom SAmple Consensus#", "content": "1.1.16.2. RANSAC: RANdom SAmple Consensus# RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set. RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations (see max_trials parameter). It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision. The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers. Examples - Robust linear model estimation using RANSAC - Robust linear estimator fitting", "prev_chunk_id": "chunk_35", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_37", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.16.3. Theil-Sen estimator: generalized-median-based estimator#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.16.3. Theil-Sen estimator: generalized-median-based estimator#", "content": "1.1.16.3. Theil-Sen estimator: generalized-median-based estimator# The TheilSenRegressor estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension. Examples - Theil-Sen Regression - Robust linear estimator fitting", "prev_chunk_id": "chunk_36", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_38", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.16.4. Huber Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.16.4. Huber Regression#", "content": "1.1.16.4. Huber Regression# The HuberRegressor is different from Ridge because it applies a linear loss to samples that are defined as outliers by the epsilon parameter. A sample is classified as an inlier if the absolute error of that sample is less than the threshold epsilon. It differs from TheilSenRegressor and RANSACRegressor because it does not ignore the effect of the outliers but gives a lesser weight to them. Examples - HuberRegressor vs Ridge on dataset with strong outliers The HuberRegressor differs from using SGDRegressor with loss set to huber in the following ways. - HuberRegressoris scaling invariant. Onceepsilonis set, scalingXandydown or up by different values would produce the same robustness to outliers as before. as compared toSGDRegressorwhereepsilonhas to be set again whenXandyare scaled. - HuberRegressorshould be more efficient to use on data with small number of samples whileSGDRegressorneeds a number of passes on the training data to produce the same robustness. Note that this estimator is different from the R implementation of Robust Regression because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.", "prev_chunk_id": "chunk_37", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_39", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.17. Quantile Regression#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.17. Quantile Regression#", "content": "1.1.17. Quantile Regression# Quantile regression estimates the median or other quantiles of \\(y\\) conditional on \\(X\\), while ordinary least squares (OLS) estimates the conditional mean. Quantile regression may be useful if one is interested in predicting an interval instead of point prediction. Sometimes, prediction intervals are calculated based on the assumption that prediction error is distributed normally with zero mean and constant variance. Quantile regression provides sensible prediction intervals even for errors with non-constant (but predictable) variance or non-normal distribution. Based on minimizing the pinball loss, conditional quantiles can also be estimated by models other than linear models. For example, GradientBoostingRegressor can predict conditional quantiles if its parameter loss is set to \"quantile\" and parameter alpha is set to the quantile that should be predicted. See the example in Prediction Intervals for Gradient Boosting Regression. Most implementations of quantile regression are based on linear programming problem. The current implementation is based on scipy.optimize.linprog. Examples - Quantile regression", "prev_chunk_id": "chunk_38", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_40", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.18. Polynomial regression: extending linear models with basis functions#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.18. Polynomial regression: extending linear models with basis functions#", "content": "1.1.18. Polynomial regression: extending linear models with basis functions# One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data. Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees: This figure is created using the PolynomialFeatures transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows: >>> from sklearn.preprocessing import PolynomialFeatures >>> import numpy as np >>> X = np.arange(6).reshape(3, 2) >>> X array([[0, 1], [2, 3], [4, 5]]) >>> poly = PolynomialFeatures(degree=2) >>> poly.fit_transform(X) array([[ 1., 0., 1., 0., 0., 1.], [ 1., 2., 3., 4., 6., 9.], [ 1., 4., 5., 16., 20., 25.]]) The features of X have been transformed from \\([x_1, x_2]\\) to \\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\\), and can now be used within any linear model. This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows: >>> from sklearn.preprocessing import PolynomialFeatures >>> from sklearn.linear_model import LinearRegression >>> from sklearn.pipeline import Pipeline >>> import numpy as np >>> model = Pipeline([('poly', PolynomialFeatures(degree=3)), ... ('linear', LinearRegression(fit_intercept=False))]) >>> # fit to an order-3 polynomial data >>> x = np.arange(5) >>> y = 3 - 2 * x + x ** 2 - x ** 3 >>> model = model.fit(x[:, np.newaxis], y) >>> model.named_steps['linear'].coef_ array([ 3., -2., 1., -1.]) The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients. In some cases it’s not necessary to include higher powers of any single feature, but only the so-called", "prev_chunk_id": "chunk_39", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_41", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "title": "1.1.18. Polynomial regression: extending linear models with basis functions#", "page_title": "1.1. Linear Models — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.1.18. Polynomial regression: extending linear models with basis functions#", "content": "interaction features that multiply together at most \\(d\\) distinct features. These can be gotten from PolynomialFeatures with the setting interaction_only=True. For example, when dealing with boolean features, \\(x_i^n = x_i\\) for all \\(n\\) and is therefore useless; but \\(x_i x_j\\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier: >>> from sklearn.linear_model import Perceptron >>> from sklearn.preprocessing import PolynomialFeatures >>> import numpy as np >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) >>> y = X[:, 0] ^ X[:, 1] >>> y array([0, 1, 1, 0]) >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int) >>> X array([[1, 0, 0, 0], [1, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 1]]) >>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None, ... shuffle=False).fit(X, y) And the classifier “predictions” are perfect: >>> clf.predict(X) array([0, 1, 1, 0]) >>> clf.score(X, y) 1.0", "prev_chunk_id": "chunk_40", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_42", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2. Linear and Quadratic Discriminant Analysis#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2. Linear and Quadratic Discriminant Analysis#", "content": "1.2. Linear and Quadratic Discriminant Analysis# Linear Discriminant Analysis (LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively. These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune. The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible. Examples - Linear and Quadratic Discriminant Analysis with covariance ellipsoid: Comparison of LDA and QDA on synthetic data.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_43", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.1. Dimensionality reduction using Linear Discriminant Analysis#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.1. Dimensionality reduction using Linear Discriminant Analysis#", "content": "1.2.1. Dimensionality reduction using Linear Discriminant Analysis# LinearDiscriminantAnalysis can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is in general a rather strong dimensionality reduction, and only makes sense in a multiclass setting. This is implemented in the transform method. The desired dimensionality can be set using the n_components parameter. This parameter has no influence on the fit and predict methods. Examples - Comparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and PCA for dimensionality reduction of the Iris dataset", "prev_chunk_id": "chunk_42", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_44", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.2. Mathematical formulation of the LDA and QDA classifiers#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.2. Mathematical formulation of the LDA and QDA classifiers#", "content": "1.2.2. Mathematical formulation of the LDA and QDA classifiers# Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \\(P(X|y=k)\\) for each class \\(k\\). Predictions can then be obtained by using Bayes’ rule, for each training sample \\(x \\in \\mathcal{R}^d\\): and we select the class \\(k\\) which maximizes this posterior probability. More specifically, for linear and quadratic discriminant analysis, \\(P(x|y)\\) is modeled as a multivariate Gaussian distribution with density: where \\(d\\) is the number of features.", "prev_chunk_id": "chunk_43", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_45", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.2.1. QDA#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.2.1. QDA#", "content": "1.2.2.1. QDA# According to the model above, the log of the posterior is: where the constant term \\(Cst\\) corresponds to the denominator \\(P(x)\\), in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior.", "prev_chunk_id": "chunk_44", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_46", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.2.2. LDA#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.2.2. LDA#", "content": "1.2.2.2. LDA# LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \\(\\Sigma_k = \\Sigma\\) for all \\(k\\). This reduces the log posterior to: The term \\((x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)\\) corresponds to the Mahalanobis Distance between the sample \\(x\\) and the mean \\(\\mu_k\\). The Mahalanobis distance tells how close \\(x\\) is from \\(\\mu_k\\), while also accounting for the variance of each feature. We can thus interpret LDA as assigning \\(x\\) to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. The log-posterior of LDA can also be written [3] as: where \\(\\omega_k = \\Sigma^{-1} \\mu_k\\) and \\(\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)\\). These quantities correspond to the coef_ and intercept_ attributes, respectively. From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices \\(\\Sigma_k\\) of the Gaussians, leading to quadratic decision surfaces. See [1] for more details.", "prev_chunk_id": "chunk_45", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_47", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.3. Mathematical formulation of LDA dimensionality reduction#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.3. Mathematical formulation of LDA dimensionality reduction#", "content": "1.2.3. Mathematical formulation of LDA dimensionality reduction# First note that the K means \\(\\mu_k\\) are vectors in \\(\\mathcal{R}^d\\), and they lie in an affine subspace \\(H\\) of dimension at most \\(K - 1\\) (2 points lie on a line, 3 points lie on a plane, etc.). As mentioned above, we can interpret LDA as assigning \\(x\\) to the class whose mean \\(\\mu_k\\) is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first sphering the data so that the covariance matrix is the identity, and then assigning \\(x\\) to the closest mean in terms of Euclidean distance (still accounting for the class priors). Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into \\(H\\), and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if \\(x\\) is closest to \\(\\mu_k\\) in the original space, it will also be the case in \\(H\\). This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \\(K-1\\) dimensional space. We can reduce the dimension even more, to a chosen \\(L\\), by projecting onto the linear subspace \\(H_L\\) which maximizes the variance of the \\(\\mu^*_k\\) after projection (in effect, we are doing a form of PCA for the transformed class means \\(\\mu^*_k\\)). This \\(L\\) corresponds to the n_components parameter used in the transform method. See [1] for more details.", "prev_chunk_id": "chunk_46", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_48", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.4. Shrinkage and Covariance Estimator#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.4. Shrinkage and Covariance Estimator#", "content": "1.2.4. Shrinkage and Covariance Estimator# Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the shrinkage parameter of the LinearDiscriminantAnalysis class to 'auto'. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf [2]. Note that currently shrinkage only works when setting the solver parameter to 'lsqr' or 'eigen'. The shrinkage parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix. The shrunk Ledoit and Wolf estimator of covariance may not always be the best choice. For example if the distribution of the data is normally distributed, the Oracle Approximating Shrinkage estimator sklearn.covariance.OAS yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula used with shrinkage=\"auto\". In LDA, the data are assumed to be gaussian conditionally to the class. If these assumptions hold, using LDA with the OAS estimator of covariance will yield a better classification accuracy than if Ledoit and Wolf or the empirical covariance estimator is used. The covariance estimator can be chosen using the covariance_estimator parameter of the discriminant_analysis.LinearDiscriminantAnalysis class. A covariance estimator should have a fit method and", "prev_chunk_id": "chunk_47", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_49", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.4. Shrinkage and Covariance Estimator#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.4. Shrinkage and Covariance Estimator#", "content": "a covariance_ attribute like all covariance estimators in the sklearn.covariance module. Examples - Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification: Comparison of LDA classifiers with Empirical, Ledoit Wolf and OAS covariance estimator.", "prev_chunk_id": "chunk_48", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_50", "url": "https://scikit-learn.org/stable/modules/lda_qda.html", "title": "1.2.5. Estimation algorithms#", "page_title": "1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.2.5. Estimation algorithms#", "content": "1.2.5. Estimation algorithms# Using LDA and QDA requires computing the log-posterior which depends on the class priors \\(P(y=k)\\), the class means \\(\\mu_k\\), and the covariance matrices. The ‘svd’ solver is the default solver used for LinearDiscriminantAnalysis, and it is the only available solver for QuadraticDiscriminantAnalysis. It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the ‘svd’ solver may be preferable in situations where the number of features is large. The ‘svd’ solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix \\(\\Sigma_k\\) is, by definition, equal to \\(\\frac{1}{n - 1} X_k^tX_k = \\frac{1}{n - 1} V S^2 V^t\\) where \\(V\\) comes from the SVD of the (centered) matrix: \\(X_k = U S V^t\\). It turns out that we can compute the log-posterior above without having to explicitly compute \\(\\Sigma\\): computing \\(S\\) and \\(V\\) via the SVD of \\(X\\) is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix \\(X\\) and the SVD of the class-wise mean vectors. The 'lsqr' solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix \\(\\Sigma\\), and supports shrinkage and custom covariance estimators. This solver computes the coefficients \\(\\omega_k = \\Sigma^{-1}\\mu_k\\) by solving for \\(\\Sigma \\omega = \\mu_k\\), thus avoiding the explicit computation of the inverse \\(\\Sigma^{-1}\\). The 'eigen' solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the 'eigen' solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features. References", "prev_chunk_id": "chunk_49", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_51", "url": "https://scikit-learn.org/stable/modules/kernel_ridge.html", "title": "1.3. Kernel ridge regression#", "page_title": "1.3. Kernel ridge regression — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.3. Kernel ridge regression#", "content": "1.3. Kernel ridge regression# Kernel ridge regression (KRR) [M2012] combines Ridge regression and classification (linear least squares with \\(L_2\\)-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space. The form of the model learned by KernelRidge is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \\(\\epsilon\\)-insensitive loss, both combined with \\(L_2\\) regularization. In contrast to SVR, fitting KernelRidge can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \\(\\epsilon > 0\\), at prediction-time. The following figure compares KernelRidge and SVR on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of KernelRidge and SVR is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting KernelRidge is approximately seven times faster than fitting SVR (both with grid-search). However, prediction of 100,000 target values is more than three times faster with SVR since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors. The next figure compares the time for fitting and prediction of KernelRidge and SVR for different sizes of the training set. Fitting KernelRidge is faster than SVR for medium-sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KernelRidge for all sizes of the training set because of the", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_52", "url": "https://scikit-learn.org/stable/modules/kernel_ridge.html", "title": "1.3. Kernel ridge regression#", "page_title": "1.3. Kernel ridge regression — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.3. Kernel ridge regression#", "content": "learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \\(\\epsilon\\) and \\(C\\) of the SVR; \\(\\epsilon = 0\\) would correspond to a dense model. Examples - Comparison of kernel ridge regression and SVR References", "prev_chunk_id": "chunk_51", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_53", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4. Support Vector Machines#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4. Support Vector Machines#", "content": "1.4. Support Vector Machines# Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: - Effective in high dimensional spaces. - Still effective in cases where number of dimensions is greater than the number of samples. - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. - Versatile: differentKernel functionscan be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels. The disadvantages of support vector machines include: - If the number of features is much greater than the number of samples, avoid over-fitting in choosingKernel functionsand regularization term is crucial. - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (seeScores and probabilities, below). The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_54", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.1. Classification#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.1. Classification#", "content": "1.4.1. Classification# SVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. It also lacks some of the attributes of SVC and NuSVC, like support_. LinearSVC uses squared_hinge loss and due to its implementation in liblinear it also regularizes the intercept, if considered. This effect can however be reduced by carefully fine tuning its intercept_scaling parameter, which allows the intercept term to have a different regularization behavior compared to the other features. The classification results and score can therefore differ from the other two classifiers. As other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of shape (n_samples, n_features) holding the training samples, and an array y of class labels (strings or integers), of shape (n_samples): >>> from sklearn import svm >>> X = [[0, 0], [1, 1]] >>> y = [0, 1] >>> clf = svm.SVC() >>> clf.fit(X, y) SVC() After being fitted, the model can then be used to predict new values: >>> clf.predict([[2., 2.]]) array([1]) SVMs decision function (detailed in the Mathematical formulation) depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes support_vectors_, support_ and n_support_: >>> # get support vectors >>> clf.support_vectors_ array([[0., 0.], [1., 1.]]) >>> # get indices of support vectors >>> clf.support_ array([0, 1]...) >>> # get number of support vectors for each class >>> clf.n_support_ array([1, 1]...) Examples - SVM: Maximum margin separating hyperplane - SVM-Anova: SVM with univariate feature selection - Plot classification probability", "prev_chunk_id": "chunk_53", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_55", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.1.1. Multi-class classification#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.1.1. Multi-class classification#", "content": "1.4.1.1. Multi-class classification# SVC and NuSVC implement the “one-versus-one” approach for multi-class classification. In total, n_classes * (n_classes - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to monotonically transform the results of the “one-versus-one” classifiers to a “one-vs-rest” decision function of shape (n_samples, n_classes), which is the default setting of the parameter (default=’ovr’). >>> X = [[0], [1], [2], [3]] >>> Y = [0, 1, 2, 3] >>> clf = svm.SVC(decision_function_shape='ovo') >>> clf.fit(X, Y) SVC(decision_function_shape='ovo') >>> dec = clf.decision_function([[1]]) >>> dec.shape[1] # 6 classes: 4*3/2 = 6 6 >>> clf.decision_function_shape = \"ovr\" >>> dec = clf.decision_function([[1]]) >>> dec.shape[1] # 4 classes 4 On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_classes models. >>> lin_clf = svm.LinearSVC() >>> lin_clf.fit(X, Y) LinearSVC() >>> dec = lin_clf.decision_function([[1]]) >>> dec.shape[1] 4 See Mathematical formulation for a complete description of the decision function. Examples - Plot different SVM classifiers in the iris dataset", "prev_chunk_id": "chunk_54", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_56", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.1.2. Scores and probabilities#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.1.2. Scores and probabilities#", "content": "1.4.1.2. Scores and probabilities# The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities are calibrated using Platt scaling [9]: logistic regression on the SVM’s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per [10]. The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores: - the “argmax” of the scores may not be the argmax of the probabilities - in binary classification, a sample may be labeled bypredictas belonging to the positive class even if the output ofpredict_probais less than 0.5; and similarly, it could be labeled as negative even if the output ofpredict_probais more than 0.5. Platt’s method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba. Please note that when decision_function_shape='ovr' and n_classes > 2, unlike decision_function, the predict method does not try to break ties by default. You can set break_ties=True for the output of predict to be the same as np.argmax(clf.decision_function(...), axis=1), otherwise the first class among the tied classes will always be returned; but have in mind that it comes with a computational cost. See SVM Tie Breaking Example for an example on tie breaking.", "prev_chunk_id": "chunk_55", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_57", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.1.3. Unbalanced problems#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.1.3. Unbalanced problems#", "content": "1.4.1.3. Unbalanced problems# In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters class_weight and sample_weight can be used. SVC (but not NuSVC) implements the parameter class_weight in the fit method. It’s a dictionary of the form {class_label : value}, where value is a floating point number > 0 that sets the parameter C of class class_label to C * value. The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction. SVC, NuSVC, SVR, NuSVR, LinearSVC, LinearSVR and OneClassSVM implement also weights for individual samples in the fit method through the sample_weight parameter. Similar to class_weight, this sets the parameter C for the i-th example to C * sample_weight[i], which will encourage the classifier to get these samples right. The figure below illustrates the effect of sample weighting on the decision boundary. The size of the circles is proportional to the sample weights: Examples - SVM: Separating hyperplane for unbalanced classes - SVM: Weighted samples", "prev_chunk_id": "chunk_56", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_58", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.2. Regression#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.2. Regression#", "content": "1.4.2. Regression# The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression. The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target. There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers the linear kernel, while NuSVR implements a slightly different formulation than SVR and LinearSVR. Due to its implementation in liblinear LinearSVR also regularizes the intercept, if considered. This effect can however be reduced by carefully fine tuning its intercept_scaling parameter, which allows the intercept term to have a different regularization behavior compared to the other features. The classification results and score can therefore differ from the other two classifiers. See Implementation details for further details. As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values: >>> from sklearn import svm >>> X = [[0, 0], [2, 2]] >>> y = [0.5, 2.5] >>> regr = svm.SVR() >>> regr.fit(X, y) SVR() >>> regr.predict([[1, 1]]) array([1.5]) Examples - Support Vector Regression (SVR) using linear and non-linear kernels", "prev_chunk_id": "chunk_57", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_59", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.3. Density estimation, novelty detection#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.3. Density estimation, novelty detection#", "content": "1.4.3. Density estimation, novelty detection# The class OneClassSVM implements a One-Class SVM which is used in outlier detection. See Novelty and Outlier Detection for the description and usage of OneClassSVM.", "prev_chunk_id": "chunk_58", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_60", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.4. Complexity#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.4. Complexity#", "content": "1.4.4. Complexity# Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm-based implementation scales between \\(O(n_{features} \\times n_{samples}^2)\\) and \\(O(n_{features} \\times n_{samples}^3)\\) depending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse \\(n_{features}\\) should be replaced by the average number of non-zero features in a sample vector. For the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features.", "prev_chunk_id": "chunk_59", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_61", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.5. Tips on Practical Use#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.5. Tips on Practical Use#", "content": "1.4.5. Tips on Practical Use# - Avoiding data copy: ForSVC,SVR,NuSVCandNuSVR, if the data passed to certain methods is not C-ordered contiguous and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting itsflagsattribute.ForLinearSVC(andLogisticRegression) any input passed as a numpy array will be copied and converted to theliblinearinternal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use theSGDClassifierclass instead. The objective function can be configured to be almost the same as theLinearSVCmodel. - Kernel cache size: ForSVC,SVR,NuSVCandNuSVR, the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to setcache_sizeto a higher value than the default of 200(MB), such as 500(MB) or 1000(MB). - Setting C:Cis1by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization.LinearSVCandLinearSVRare less sensitive toCwhen it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, largerCvalues will take more time to train, sometimes up to 10 times longer, as shown in[11]. - Support Vector Machine algorithms are not scale invariant, soit is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that thesamescaling must be applied to the test vector to obtain meaningful results. This can be done easily by using aPipeline:>>>fromsklearn.pipelineimportmake_pipeline>>>fromsklearn.preprocessingimportStandardScaler>>>fromsklearn.svmimportSVC>>>clf=make_pipeline(StandardScaler(),SVC())See sectionPreprocessing datafor more details on scaling and normalization. - Regarding theshrinkingparameter, quoting[12]:We found that if the number of iterations is", "prev_chunk_id": "chunk_60", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_62", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.5. Tips on Practical Use#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.5. Tips on Practical Use#", "content": "large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster - ParameternuinNuSVC/OneClassSVM/NuSVRapproximates the fraction of training errors and support vectors. - InSVC, if the data is unbalanced (e.g. many positive and few negative), setclass_weight='balanced'and/or try different penalty parametersC. - Randomness of the underlying implementations: The underlying implementations ofSVCandNuSVCuse a random number generator only to shuffle the data for probability estimation (whenprobabilityis set toTrue). This randomness can be controlled with therandom_stateparameter. Ifprobabilityis set toFalsethese estimators are not random andrandom_statehas no effect on the results. The underlyingOneClassSVMimplementation is similar to the ones ofSVCandNuSVC. As no probability estimation is provided forOneClassSVM, it is not random.The underlyingLinearSVCimplementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e. whendualis set toTrue). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smallertolparameter. This randomness can also be controlled with therandom_stateparameter. Whendualis set toFalsethe underlying implementation ofLinearSVCis not random andrandom_statehas no effect on the results. - Using L1 penalization as provided byLinearSVC(penalty='l1',dual=False)yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. IncreasingCyields a more complex model (more features are selected). TheCvalue that yields a “null” model (all weights equal to zero) can be calculated usingl1_min_c.", "prev_chunk_id": "chunk_61", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_63", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.6. Kernel functions#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.6. Kernel functions#", "content": "1.4.6. Kernel functions# The kernel function can be any of the following: - linear:\\(\\langle x, x'\\rangle\\). - polynomial:\\((\\gamma \\langle x, x'\\rangle + r)^d\\), where\\(d\\)is specified by parameterdegree,\\(r\\)bycoef0. - rbf:\\(\\exp(-\\gamma \\|x-x'\\|^2)\\), where\\(\\gamma\\)is specified by parametergamma, must be greater than 0. - sigmoid\\(\\tanh(\\gamma \\langle x,x'\\rangle + r)\\), where\\(r\\)is specified bycoef0. Different kernels are specified by the kernel parameter: >>> linear_svc = svm.SVC(kernel='linear') >>> linear_svc.kernel 'linear' >>> rbf_svc = svm.SVC(kernel='rbf') >>> rbf_svc.kernel 'rbf' See also Kernel Approximation for a solution to use RBF kernels that is much faster and more scalable.", "prev_chunk_id": "chunk_62", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_64", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.6.1. Parameters of the RBF Kernel#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.6.1. Parameters of the RBF Kernel#", "content": "1.4.6.1. Parameters of the RBF Kernel# When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. Proper choice of C and gamma is critical to the SVM’s performance. One is advised to use GridSearchCV with C and gamma spaced exponentially far apart to choose good values. Examples - RBF SVM parameters - Scaling the regularization parameter for SVCs", "prev_chunk_id": "chunk_63", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_65", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.6.2. Custom Kernels#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.6.2. Custom Kernels#", "content": "1.4.6.2. Custom Kernels# You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix. Classifiers with custom kernels behave the same way as any other classifiers, except that: - Fieldsupport_vectors_is now empty, only indices of support vectors are stored insupport_ - A reference (and not a copy) of the first argument in thefit()method is stored for future reference. If that array changes between the use offit()andpredict()you will have unexpected results. Examples - SVM with custom kernel", "prev_chunk_id": "chunk_64", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_66", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.7. Mathematical formulation#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.7. Mathematical formulation#", "content": "1.4.7. Mathematical formulation# A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”: In general, when the problem isn’t linearly separable, the support vectors are the samples within the margin boundaries. We recommend [13] and [14] as good references for the theory and practicalities of SVMs.", "prev_chunk_id": "chunk_65", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_67", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.7.1. SVC#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.7.1. SVC#", "content": "1.4.7.1. SVC# Given training vectors \\(x_i \\in \\mathbb{R}^p\\), i=1,…, n, in two classes, and a vector \\(y \\in \\{1, -1\\}^n\\), our goal is to find \\(w \\in \\mathbb{R}^p\\) and \\(b \\in \\mathbb{R}\\) such that the prediction given by \\(\\text{sign} (w^T\\phi(x) + b)\\) is correct for most samples. SVC solves the following primal problem: Intuitively, we’re trying to maximize the margin (by minimizing \\(||w||^2 = w^Tw\\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \\(y_i (w^T \\phi (x_i) + b)\\) would be \\(\\geq 1\\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \\(\\zeta_i\\) from their correct margin boundary. The penalty term C controls the strength of this penalty, and as a result, acts as an inverse regularization parameter (see note below). The dual problem to the primal is where \\(e\\) is the vector of all ones, and \\(Q\\) is an \\(n\\) by \\(n\\) positive semidefinite matrix, \\(Q_{ij} \\equiv y_i y_j K(x_i, x_j)\\), where \\(K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)\\) is the kernel. The terms \\(\\alpha_i\\) are called the dual coefficients, and they are upper-bounded by \\(C\\). This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \\(\\phi\\): see kernel trick. Once the optimization problem is solved, the output of decision_function for a given sample \\(x\\) becomes: and the predicted class corresponds to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients \\(\\alpha_i\\) are zero for the other samples. These parameters can be accessed through the attributes dual_coef_ which holds the product \\(y_i \\alpha_i\\), support_vectors_", "prev_chunk_id": "chunk_66", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_68", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.7.1. SVC#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.7.1. SVC#", "content": "which holds the support vectors, and intercept_ which holds the independent term \\(b\\).", "prev_chunk_id": "chunk_67", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_69", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.7.2. SVR#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.7.2. SVR#", "content": "1.4.7.2. SVR# Given training vectors \\(x_i \\in \\mathbb{R}^p\\), i=1,…, n, and a vector \\(y \\in \\mathbb{R}^n\\) \\(\\varepsilon\\)-SVR solves the following primal problem: Here, we are penalizing samples whose prediction is at least \\(\\varepsilon\\) away from their true target. These samples penalize the objective by \\(\\zeta_i\\) or \\(\\zeta_i^*\\), depending on whether their predictions lie above or below the \\(\\varepsilon\\) tube. The dual problem is where \\(e\\) is the vector of all ones, \\(Q\\) is an \\(n\\) by \\(n\\) positive semidefinite matrix, \\(Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)\\) is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \\(\\phi\\). The prediction is: These parameters can be accessed through the attributes dual_coef_ which holds the difference \\(\\alpha_i - \\alpha_i^*\\), support_vectors_ which holds the support vectors, and intercept_ which holds the independent term \\(b\\)", "prev_chunk_id": "chunk_68", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_70", "url": "https://scikit-learn.org/stable/modules/svm.html", "title": "1.4.8. Implementation details#", "page_title": "1.4. Support Vector Machines — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.4.8. Implementation details#", "content": "1.4.8. Implementation details# Internally, we use libsvm [12] and liblinear [11] to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers. References", "prev_chunk_id": "chunk_69", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_71", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5. Stochastic Gradient Descent#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5. Stochastic Gradient Descent#", "content": "1.5. Stochastic Gradient Descent# Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning. SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than \\(10^5\\) training examples and more than \\(10^5\\) features. Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. Often, an instance of SGDClassifier or SGDRegressor will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using SGDClassifier(loss='log_loss') results in logistic regression, i.e. a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression. Similarly, SGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means. The advantages of Stochastic Gradient Descent are: - Efficiency. - Ease of implementation (lots of opportunities for code tuning). The disadvantages of Stochastic Gradient Descent include: - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations. - SGD is sensitive to feature scaling.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_72", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.1. Classification#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.1. Classification#", "content": "1.5.1. Classification# The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the hinge loss, equivalent to a linear SVM. As other classifiers, SGD has to be fitted with two arrays: an array X of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples: >>> from sklearn.linear_model import SGDClassifier >>> X = [[0., 0.], [1., 1.]] >>> y = [0, 1] >>> clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5) >>> clf.fit(X, y) SGDClassifier(max_iter=5) After being fitted, the model can then be used to predict new values: >>> clf.predict([[2., 2.]]) array([1]) SGD fits a linear model to the training data. The coef_ attribute holds the model parameters: >>> clf.coef_ array([[9.9, 9.9]]) The intercept_ attribute holds the intercept (aka offset or bias): >>> clf.intercept_ array([-9.9]) Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter fit_intercept. The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by SGDClassifier.decision_function: >>> clf.decision_function([[2., 2.]]) array([29.6]) The concrete loss function can be set via the loss parameter. SGDClassifier supports the following loss functions: - loss=\"hinge\": (soft-margin) linear Support Vector Machine, - loss=\"modified_huber\": smoothed hinge loss, - loss=\"log_loss\": logistic regression, - and all regression losses below. In this case the target is encoded as\\(-1\\)or\\(1\\), and the problem is treated as a regression problem. The predicted class then corresponds to the sign of the predicted target. Please refer to the mathematical section below for formulas. The first two loss functions are lazy, they only update the model parameters if an example violates the margin", "prev_chunk_id": "chunk_71", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_73", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.1. Classification#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.1. Classification#", "content": "constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficients), even when \\(L_2\\) penalty is used. Using loss=\"log_loss\" or loss=\"modified_huber\" enables the predict_proba method, which gives a vector of probability estimates \\(P(y|x)\\) per sample \\(x\\): >>> clf = SGDClassifier(loss=\"log_loss\", max_iter=5).fit(X, y) >>> clf.predict_proba([[1., 1.]]) array([[0.00, 0.99]]) The concrete penalty can be set via the penalty parameter. SGD supports the following penalties: - penalty=\"l2\":\\(L_2\\)norm penalty oncoef_. - penalty=\"l1\":\\(L_1\\)norm penalty oncoef_. - penalty=\"elasticnet\": Convex combination of\\(L_2\\)and\\(L_1\\);(1-l1_ratio)*L2+l1_ratio*L1. The default setting is penalty=\"l2\". The \\(L_1\\) penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net [11] solves some deficiencies of the \\(L_1\\) penalty in the presence of highly correlated attributes. The parameter l1_ratio controls the convex combination of \\(L_1\\) and \\(L_2\\) penalty. SGDClassifier supports multi-class classification by combining multiple binary classifiers in a “one versus all” (OVA) scheme. For each of the \\(K\\) classes, a binary classifier is learned that discriminates between that and all other \\(K-1\\) classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers. In the case of multi-class classification coef_ is a two-dimensional array of shape (n_classes, n_features) and intercept_ is a one-dimensional array of shape (n_classes,). The \\(i\\)-th row of coef_ holds the weight vector of the OVA classifier for the \\(i\\)-th class; classes are indexed in ascending order (see attribute classes_). Note that, in principle, since they allow to create a probability model, loss=\"log_loss\" and loss=\"modified_huber\" are more suitable for one-vs-all classification. SGDClassifier supports both weighted classes and weighted instances", "prev_chunk_id": "chunk_72", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_74", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.1. Classification#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.1. Classification#", "content": "via the fit parameters class_weight and sample_weight. See the examples below and the docstring of SGDClassifier.fit for further information. SGDClassifier supports averaged SGD (ASGD) [10]. Averaging can be enabled by setting average=True. ASGD performs the same updates as the regular SGD (see Mathematical formulation), but instead of using the last value of the coefficients as the coef_ attribute (i.e. the values of the last update), coef_ is set instead to the average value of the coefficients across all updates. The same is done for the intercept_ attribute. When using ASGD the learning rate can be larger and even constant, leading on some datasets to a speed up in training time. For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in LogisticRegression. Examples - SGD: Maximum margin separating hyperplane - Plot multi-class SGD on the iris dataset - SGD: Weighted samples - SVM: Separating hyperplane for unbalanced classes(See the Note in the example)", "prev_chunk_id": "chunk_73", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_75", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.2. Regression#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.2. Regression#", "content": "1.5.2. Regression# The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend Ridge, Lasso, or ElasticNet. The concrete loss function can be set via the loss parameter. SGDRegressor supports the following loss functions: - loss=\"squared_error\": Ordinary least squares, - loss=\"huber\": Huber loss for robust regression, - loss=\"epsilon_insensitive\": linear Support Vector Regression. Please refer to the mathematical section below for formulas. The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter epsilon. This parameter depends on the scale of the target variables. The penalty parameter determines the regularization to be used (see description above in the classification section). SGDRegressor also supports averaged SGD [10] (here again, see description above in the classification section). For regression with a squared loss and a \\(L_2\\) penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in Ridge. Examples - Prediction Latency", "prev_chunk_id": "chunk_74", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_76", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.3. Online One-Class SVM#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.3. Online One-Class SVM#", "content": "1.5.3. Online One-Class SVM# The class sklearn.linear_model.SGDOneClassSVM implements an online linear version of the One-Class SVM using a stochastic gradient descent. Combined with kernel approximation techniques, sklearn.linear_model.SGDOneClassSVM can be used to approximate the solution of a kernelized One-Class SVM, implemented in sklearn.svm.OneClassSVM, with a linear complexity in the number of samples. Note that the complexity of a kernelized One-Class SVM is at best quadratic in the number of samples. sklearn.linear_model.SGDOneClassSVM is thus well suited for datasets with a large number of training samples (over 10,000) for which the SGD variant can be several orders of magnitude faster. As SGDClassifier and SGDRegressor, SGDOneClassSVM supports averaged SGD. Averaging can be enabled by setting average=True. Examples - One-Class SVM versus One-Class SVM using Stochastic Gradient Descent", "prev_chunk_id": "chunk_75", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_77", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.4. Stochastic Gradient Descent for sparse data#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.4. Stochastic Gradient Descent for sparse data#", "content": "1.5.4. Stochastic Gradient Descent for sparse data# There is built-in support for sparse data given in any matrix in a format supported by scipy.sparse. For maximum efficiency, however, use the CSR matrix format as defined in scipy.sparse.csr_matrix. Examples - Classification of text documents using sparse features", "prev_chunk_id": "chunk_76", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_78", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.5. Complexity#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.5. Complexity#", "content": "1.5.5. Complexity# The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If \\(X\\) is a matrix of size \\(n \\times p\\) (with \\(n\\) samples and \\(p\\) features), training has a cost of \\(O(k n \\bar p)\\), where \\(k\\) is the number of iterations (epochs) and \\(\\bar p\\) is the average number of non-zero attributes per sample. Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases.", "prev_chunk_id": "chunk_77", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_79", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.6. Stopping criterion#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.6. Stopping criterion#", "content": "1.5.6. Stopping criterion# The classes SGDClassifier and SGDRegressor provide two criteria to stop the algorithm when a given level of convergence is reached: - Withearly_stopping=True, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using thescoremethod) computed on the validation set. The size of the validation set can be changed with the parametervalidation_fraction. - Withearly_stopping=False, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data. In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve n_iter_no_change times in a row. The improvement is evaluated with absolute tolerance tol, and the algorithm stops in any case after a maximum number of iterations max_iter. See Early stopping of Stochastic Gradient Descent for an example of the effects of early stopping.", "prev_chunk_id": "chunk_78", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_80", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.7. Tips on Practical Use#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.7. Tips on Practical Use#", "content": "1.5.7. Tips on Practical Use# - Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector\\(X\\)to\\([0,1]\\)or\\([-1,1]\\), or standardize it to have mean\\(0\\)and variance\\(1\\). Note that thesamescaling must be applied to the test vector to obtain meaningful results. This can be easily done usingStandardScaler:fromsklearn.preprocessingimportStandardScalerscaler=StandardScaler()scaler.fit(X_train)# Don't cheat - fit only on training dataX_train=scaler.transform(X_train)X_test=scaler.transform(X_test)# apply same transformation to test data# Or better yet: use a pipeline!fromsklearn.pipelineimportmake_pipelineest=make_pipeline(StandardScaler(),SGDClassifier())est.fit(X_train)est.predict(X_test)If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed. - Finding a reasonable regularization term\\(\\alpha\\)is best done using automatic hyper-parameter search, e.g.GridSearchCVorRandomizedSearchCV, usually in the range10.0**-np.arange(1,7). - Empirically, we found that SGD converges after observing approximately\\(10^6\\)training samples. Thus, a reasonable first guess for the number of iterations ismax_iter=np.ceil(10**6/n), wherenis the size of the training set. - If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constantcsuch that the average\\(L_2\\)norm of the training data equals one. - We found that Averaged SGD works best with a larger number of features and a highereta0. References - “Efficient BackProp”Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.", "prev_chunk_id": "chunk_79", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_81", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.8. Mathematical formulation#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.8. Mathematical formulation#", "content": "1.5.8. Mathematical formulation# We describe here the mathematical details of the SGD procedure. A good overview with convergence rates can be found in [12]. Given a set of training examples \\(\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}\\) where \\(x_i \\in \\mathbf{R}^m\\) and \\(y_i \\in \\mathbf{R}\\) (\\(y_i \\in \\{-1, 1\\}\\) for classification), our goal is to learn a linear scoring function \\(f(x) = w^T x + b\\) with model parameters \\(w \\in \\mathbf{R}^m\\) and intercept \\(b \\in \\mathbf{R}\\). In order to make predictions for binary classification, we simply look at the sign of \\(f(x)\\). To find the model parameters, we minimize the regularized training error given by where \\(L\\) is a loss function that measures model (mis)fit and \\(R\\) is a regularization term (aka penalty) that penalizes model complexity; \\(\\alpha > 0\\) is a non-negative hyperparameter that controls the regularization strength. All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below. Popular choices for the regularization term \\(R\\) (the penalty parameter) include: - \\(L_2\\)norm:\\(R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2\\), - \\(L_1\\)norm:\\(R(w) := \\sum_{j=1}^{m} |w_j|\\), which leads to sparse solutions. - Elastic Net:\\(R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 + (1-\\rho) \\sum_{j=1}^{m} |w_j|\\), a convex combination of\\(L_2\\)and\\(L_1\\), where\\(\\rho\\)is given by1-l1_ratio. The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\\(m=2\\)) when \\(R(w) = 1\\).", "prev_chunk_id": "chunk_80", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_82", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.8.1. SGD#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.8.1. SGD#", "content": "1.5.8.1. SGD# Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of \\(E(w,b)\\) by considering a single training example at a time. The class SGDClassifier implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by where \\(\\eta\\) is the learning rate which controls the step-size in the parameter space. The intercept \\(b\\) is updated similarly but without regularization (and with additional decay for sparse matrices, as detailed in Implementation details). The learning rate \\(\\eta\\) can be either constant or gradually decaying. For classification, the default learning rate schedule (learning_rate='optimal') is given by where \\(t\\) is the time step (there are a total of n_samples * n_iter time steps), \\(t_0\\) is determined based on a heuristic proposed by Léon Bottou such that the expected initial updates are comparable with the expected size of the weights (this assumes that the norm of the training samples is approximately 1). The exact definition can be found in _init_t in BaseSGD. For regression the default learning rate schedule is inverse scaling (learning_rate='invscaling'), given by where \\(\\eta_0\\) and \\(power\\_t\\) are hyperparameters chosen by the user via eta0 and power_t, respectively. For a constant learning rate use learning_rate='constant' and use eta0 to specify the learning rate. For an adaptively decreasing learning rate, use learning_rate='adaptive' and use eta0 to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6. The model parameters can be accessed through the coef_ and intercept_ attributes: coef_ holds the weights \\(w\\) and intercept_ holds \\(b\\). When using Averaged", "prev_chunk_id": "chunk_81", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_83", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.8.1. SGD#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.8.1. SGD#", "content": "SGD (with the average parameter), coef_ is set to the average weight across all updates: coef_ \\(= \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}\\), where \\(T\\) is the total number of updates, found in the t_ attribute.", "prev_chunk_id": "chunk_82", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_84", "url": "https://scikit-learn.org/stable/modules/sgd.html", "title": "1.5.9. Implementation details#", "page_title": "1.5. Stochastic Gradient Descent — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.5.9. Implementation details#", "content": "1.5.9. Implementation details# The implementation of SGD is influenced by the Stochastic Gradient SVM of [7]. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of \\(L_2\\) regularization. In the case of sparse input X, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from [8]. For multi-class classification, a “one versus all” approach is used. We use the truncated gradient algorithm proposed in [9] for \\(L_1\\) regularization (and the Elastic Net). The code is written in Cython. References", "prev_chunk_id": "chunk_83", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_85", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6. Nearest Neighbors#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6. Nearest Neighbors#", "content": "1.6. Nearest Neighbors# sklearn.neighbors provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels. The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree). Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular. The classes in sklearn.neighbors can handle either NumPy arrays or scipy.sparse matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches. There are many learning routines which rely on nearest neighbors at their core. One example is kernel density estimation, discussed in the density estimation section.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_86", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.1. Unsupervised Nearest Neighbors#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.1. Unsupervised Nearest Neighbors#", "content": "1.6.1. Unsupervised Nearest Neighbors# NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise. The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see Nearest Neighbor Algorithms.", "prev_chunk_id": "chunk_85", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_87", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.1.1. Finding the Nearest Neighbors#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.1.1. Finding the Nearest Neighbors#", "content": "1.6.1.1. Finding the Nearest Neighbors# For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within sklearn.neighbors can be used: >>> from sklearn.neighbors import NearestNeighbors >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X) >>> distances, indices = nbrs.kneighbors(X) >>> indices array([[0, 1], [1, 0], [2, 1], [3, 4], [4, 3], [5, 4]]...) >>> distances array([[0. , 1. ], [0. , 1. ], [0. , 1.41421356], [0. , 1. ], [0. , 1. ], [0. , 1.41421356]]) Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero. It is also possible to efficiently produce a sparse graph showing the connections between neighboring points: >>> nbrs.kneighbors_graph(X).toarray() array([[1., 1., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [0., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 0.], [0., 0., 0., 1., 1., 0.], [0., 0., 0., 0., 1., 1.]]) The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see Isomap, LocallyLinearEmbedding, and SpectralClustering.", "prev_chunk_id": "chunk_86", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_88", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.1.2. KDTree and BallTree Classes#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.1.2. KDTree and BallTree Classes#", "content": "1.6.1.2. KDTree and BallTree Classes# Alternatively, one can use the KDTree or BallTree classes directly to find nearest neighbors. This is the functionality wrapped by the NearestNeighbors class used above. The Ball Tree and KD Tree have the same interface; we’ll show an example of using the KD Tree here: >>> from sklearn.neighbors import KDTree >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> kdt = KDTree(X, leaf_size=30, metric='euclidean') >>> kdt.query(X, k=2, return_distance=False) array([[0, 1], [1, 0], [2, 1], [3, 4], [4, 3], [5, 4]]...) Refer to the KDTree and BallTree class documentation for more information on the options available for nearest neighbors searches, including specification of query strategies, distance metrics, etc. For a list of valid metrics use KDTree.valid_metrics and BallTree.valid_metrics: >>> from sklearn.neighbors import KDTree, BallTree >>> KDTree.valid_metrics ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity'] >>> BallTree.valid_metrics ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc']", "prev_chunk_id": "chunk_87", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_89", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.2. Nearest Neighbors Classification#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.2. Nearest Neighbors Classification#", "content": "1.6.2. Nearest Neighbors Classification# Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. scikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier implements learning based on the \\(k\\) nearest neighbors of each query point, where \\(k\\) is an integer value specified by the user. RadiusNeighborsClassifier implements learning based on the number of neighbors within a fixed radius \\(r\\) of each training point, where \\(r\\) is a floating-point value specified by the user. The \\(k\\)-neighbors classification in KNeighborsClassifier is the most commonly used technique. The optimal choice of the value \\(k\\) is highly data-dependent: in general a larger \\(k\\) suppresses the effects of noise, but makes the classification boundaries less distinct. In cases where the data is not uniformly sampled, radius-based neighbors classification in RadiusNeighborsClassifier can be a better choice. The user specifies a fixed radius \\(r\\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”. The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance' assigns weights proportional to the inverse of the distance from the", "prev_chunk_id": "chunk_88", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_90", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.2. Nearest Neighbors Classification#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.2. Nearest Neighbors Classification#", "content": "query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights. Examples - Nearest Neighbors Classification: an example of classification using nearest neighbors.", "prev_chunk_id": "chunk_89", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_91", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.3. Nearest Neighbors Regression#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.3. Nearest Neighbors Regression#", "content": "1.6.3. Nearest Neighbors Regression# Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors. scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the \\(k\\) nearest neighbors of each query point, where \\(k\\) is an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius \\(r\\) of the query point, where \\(r\\) is a floating-point value specified by the user. The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns equal weights to all points. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights. The use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces. Examples - Nearest Neighbors regression: an example of regression using nearest neighbors. - Face completion with a multi-output estimators: an example of multi-output regression using nearest neighbors.", "prev_chunk_id": "chunk_90", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_92", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.4.1. Brute Force#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.4.1. Brute Force#", "content": "1.6.4.1. Brute Force# Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \\(N\\) samples in \\(D\\) dimensions, this approach scales as \\(O[D N^2]\\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \\(N\\) grows, the brute-force approach quickly becomes infeasible. In the classes within sklearn.neighbors, brute-force neighbors searches are specified using the keyword algorithm = 'brute', and are computed using the routines available in sklearn.metrics.pairwise.", "prev_chunk_id": "chunk_91", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_93", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.4.2. K-D Tree#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.4.2. K-D Tree#", "content": "1.6.4.2. K-D Tree# To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \\(A\\) is very distant from point \\(B\\), and point \\(B\\) is very close to point \\(C\\), then we know that points \\(A\\) and \\(C\\) are very distant, without having to explicitly calculate their distance. In this way, the computational cost of a nearest neighbors search can be reduced to \\(O[D N \\log(N)]\\) or better. This is a significant improvement over brute-force for large \\(N\\). An early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-dimensional tree), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \\(D\\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \\(O[\\log(N)]\\) distance computations. Though the KD tree approach is very fast for low-dimensional (\\(D < 20\\)) neighbors searches, it becomes inefficient as \\(D\\) grows very large: this is one manifestation of the so-called “curse of dimensionality”. In scikit-learn, KD tree neighbors searches are specified using the keyword algorithm = 'kd_tree', and are computed using the class KDTree.", "prev_chunk_id": "chunk_92", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_94", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.4.3. Ball Tree#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.4.3. Ball Tree#", "content": "1.6.4.3. Ball Tree# To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions. A ball tree recursively divides the data into nodes defined by a centroid \\(C\\) and radius \\(r\\), such that each point in the node lies within the hyper-sphere defined by \\(r\\) and \\(C\\). The number of candidate points for a neighbor search is reduced through use of the triangle inequality: With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword algorithm = 'ball_tree', and are computed using the class BallTree. Alternatively, the user can work with the BallTree class directly.", "prev_chunk_id": "chunk_93", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_95", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.5. Nearest Centroid Classifier#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.5. Nearest Centroid Classifier#", "content": "1.6.5. Nearest Centroid Classifier# The NearestCentroid classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the KMeans algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (QuadraticDiscriminantAnalysis) for more complex methods that do not make this assumption. Usage of the default NearestCentroid is simple: >>> from sklearn.neighbors import NearestCentroid >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> clf = NearestCentroid() >>> clf.fit(X, y) NearestCentroid() >>> print(clf.predict([[-0.8, -1]])) [1]", "prev_chunk_id": "chunk_94", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_96", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.5.1. Nearest Shrunken Centroid#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.5.1. Nearest Shrunken Centroid#", "content": "1.6.5.1. Nearest Shrunken Centroid# The NearestCentroid classifier has a shrink_threshold parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by shrink_threshold. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features. In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82. Examples - Nearest Centroid Classification: an example of classification using nearest centroid with different shrink thresholds.", "prev_chunk_id": "chunk_95", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_97", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.6. Nearest Neighbors Transformer#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.6. Nearest Neighbors Transformer#", "content": "1.6.6. Nearest Neighbors Transformer# Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as KNeighborsClassifier and KNeighborsRegressor, but also some clustering methods such as DBSCAN and SpectralClustering, and some manifold embeddings such as TSNE and Isomap. All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors sparse graph, as given by kneighbors_graph and radius_neighbors_graph. With mode mode='connectivity', these functions return a binary adjacency sparse graph as required, for instance, in SpectralClustering. Whereas with mode='distance', they return a distance sparse graph as required, for instance, in DBSCAN. To include these functions in a scikit-learn pipeline, one can also use the corresponding classes KNeighborsTransformer and RadiusNeighborsTransformer. The benefits of this sparse graph API are multiple. First, the precomputed graph can be reused multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline: >>> import tempfile >>> from sklearn.manifold import Isomap >>> from sklearn.neighbors import KNeighborsTransformer >>> from sklearn.pipeline import make_pipeline >>> from sklearn.datasets import make_regression >>> cache_path = tempfile.gettempdir() # we use a temporary folder here >>> X, _ = make_regression(n_samples=50, n_features=25, random_state=0) >>> estimator = make_pipeline( ... KNeighborsTransformer(mode='distance'), ... Isomap(n_components=3, metric='precomputed'), ... memory=cache_path) >>> X_embedded = estimator.fit_transform(X) >>> X_embedded.shape (50, 3) Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter n_jobs, which might not be available in all estimators. Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors sparse graph needs to be formatted as in radius_neighbors_graph output: - a CSR matrix (although COO, CSC or LIL will be accepted). - only", "prev_chunk_id": "chunk_96", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_98", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.6. Nearest Neighbors Transformer#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.6. Nearest Neighbors Transformer#", "content": "explicitly store nearest neighborhoods of each sample with respect to the training data. This should include those at 0 distance from a query point, including the matrix diagonal when computing the nearest neighborhoods between the training data and itself. - each row’sdatashould store the distance in increasing order (optional. Unsorted data will be stable-sorted, adding a computational overhead). - all values in data should be non-negative. - there should be no duplicateindicesin any row (seescipy/scipy#5807). - if the algorithm being passed the precomputed matrix uses k nearest neighbors (as opposed to radius neighborhood), at least k neighbors must be stored in each row (or k+1, as explained in the following note). Examples - Approximate nearest neighbors in TSNE: an example of pipeliningKNeighborsTransformerandTSNE. Also proposes two custom nearest neighbors estimators based on external packages. - Caching nearest neighbors: an example of pipeliningKNeighborsTransformerandKNeighborsClassifierto enable caching of the neighbors graph during a hyper-parameter grid-search.", "prev_chunk_id": "chunk_97", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_99", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7. Neighborhood Components Analysis#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7. Neighborhood Components Analysis#", "content": "1.6.7. Neighborhood Components Analysis# Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification. In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the mathematical formulation for more details.", "prev_chunk_id": "chunk_98", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_100", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.1. Classification#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.1. Classification#", "content": "1.6.7.1. Classification# Combined with a nearest neighbors classifier (KNeighborsClassifier), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user. NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries. To use this model for classification, one needs to combine a NeighborhoodComponentsAnalysis instance that learns the optimal transformation with a KNeighborsClassifier instance that performs the classification in the projected space. Here is an example using the two classes: >>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis, ... KNeighborsClassifier) >>> from sklearn.datasets import load_iris >>> from sklearn.model_selection import train_test_split >>> from sklearn.pipeline import Pipeline >>> X, y = load_iris(return_X_y=True) >>> X_train, X_test, y_train, y_test = train_test_split(X, y, ... stratify=y, test_size=0.7, random_state=42) >>> nca = NeighborhoodComponentsAnalysis(random_state=42) >>> knn = KNeighborsClassifier(n_neighbors=3) >>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)]) >>> nca_pipe.fit(X_train, y_train) Pipeline(...) >>> print(nca_pipe.score(X_test, y_test)) 0.96190476... The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.", "prev_chunk_id": "chunk_99", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_101", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.2. Dimensionality reduction#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.2. Dimensionality reduction#", "content": "1.6.7.2. Dimensionality reduction# NCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter n_components. For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis (PCA), Linear Discriminant Analysis (LinearDiscriminantAnalysis) and Neighborhood Component Analysis (NeighborhoodComponentsAnalysis) on the Digits dataset, a dataset with size \\(n_{samples} = 1797\\) and \\(n_{features} = 64\\). The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes. Examples - Comparing Nearest Neighbors with and without Neighborhood Components Analysis - Dimensionality Reduction with Neighborhood Components Analysis - Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…", "prev_chunk_id": "chunk_100", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_102", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.3. Mathematical formulation#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.3. Mathematical formulation#", "content": "1.6.7.3. Mathematical formulation# The goal of NCA is to learn an optimal linear transformation matrix of size (n_components, n_features), which maximises the sum over all samples \\(i\\) of the probability \\(p_i\\) that \\(i\\) is correctly classified, i.e.: with \\(N\\) = n_samples and \\(p_i\\) the probability of sample \\(i\\) being correctly classified according to a stochastic nearest neighbors rule in the learned embedded space: where \\(C_i\\) is the set of points in the same class as sample \\(i\\), and \\(p_{i j}\\) is the softmax over Euclidean distances in the embedded space:", "prev_chunk_id": "chunk_101", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_103", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.4. Implementation#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.4. Implementation#", "content": "1.6.7.4. Implementation# This implementation follows what is explained in the original paper [1]. For the optimisation method, it currently uses scipy’s L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning. See the examples below and the docstring of NeighborhoodComponentsAnalysis.fit for further information.", "prev_chunk_id": "chunk_102", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_104", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.5.1. Training#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.5.1. Training#", "content": "1.6.7.5.1. Training# NCA stores a matrix of pairwise distances, taking n_samples ** 2 memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument max_iter. For each iteration, time complexity is O(n_components x n_samples x min(n_samples, n_features)).", "prev_chunk_id": "chunk_103", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_105", "url": "https://scikit-learn.org/stable/modules/neighbors.html", "title": "1.6.7.5.2. Transform#", "page_title": "1.6. Nearest Neighbors — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.6.7.5.2. Transform#", "content": "1.6.7.5.2. Transform# Here the transform operation returns \\(LX^T\\), therefore its time complexity equals n_components * n_features * n_samples_test. There is no added space complexity in the operation. References - Wikipedia entry on Neighborhood Components Analysis", "prev_chunk_id": "chunk_104", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_106", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7. Gaussian Processes#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7. Gaussian Processes#", "content": "1.7. Gaussian Processes# Gaussian Processes (GP) are a nonparametric supervised learning method used to solve regression and probabilistic classification problems. The advantages of Gaussian processes are: - The prediction interpolates the observations (at least for regular kernels). - The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest. - Versatile: differentkernelscan be specified. Common kernels are provided, but it is also possible to specify custom kernels. The disadvantages of Gaussian processes include: - Our implementation is not sparse, i.e., they use the whole samples/features information to perform the prediction. - They lose efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_107", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.1. Gaussian Process Regression (GPR)#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.1. Gaussian Process Regression (GPR)#", "content": "1.7.1. Gaussian Process Regression (GPR)# The GaussianProcessRegressor implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. GP will combine this prior and the likelihood function based on training samples. It allows to give a probabilistic approach to prediction by giving the mean and standard deviation as output when predicting. The prior mean is assumed to be constant and zero (for normalize_y=False) or the training data’s mean (for normalize_y=True). The prior’s covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized when fitting the GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, None can be passed as optimizer. The noise level in the targets can be specified by passing it via the parameter alpha, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric instabilities during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below). The figure below shows the effect of noisy target handled by setting the parameter alpha. The implementation is based on Algorithm 2.1 of [RW2006]. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor: - allows prediction without", "prev_chunk_id": "chunk_106", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_108", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.1. Gaussian Process Regression (GPR)#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.1. Gaussian Process Regression (GPR)#", "content": "prior fitting (based on the GP prior) - provides an additional methodsample_y(X), which evaluates samples drawn from the GPR (prior or posterior) at given inputs - exposes a methodlog_marginal_likelihood(theta), which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo. Examples - Gaussian Processes regression: basic introductory example - Ability of Gaussian process regression (GPR) to estimate data noise-level - Comparison of kernel ridge and Gaussian process regression - Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)", "prev_chunk_id": "chunk_107", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_109", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.2. Gaussian Process Classification (GPC)#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.2. Gaussian Process Classification (GPC)#", "content": "1.7.2. Gaussian Process Classification (GPC)# The GaussianProcessClassifier implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \\(f\\), which is then squashed through a link function \\(\\pi\\) to obtain the probabilistic classification. The latent function \\(f\\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \\(f\\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case. In contrast to the regression setting, the posterior of the latent function \\(f\\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006]. The GP prior mean is assumed to be zero. The prior’s covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, None can be passed as optimizer. In some scenarios, information about the latent function \\(f\\) is desired (i.e. the mean \\(\\bar{f_*}\\) and", "prev_chunk_id": "chunk_108", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_110", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.2. Gaussian Process Classification (GPC)#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.2. Gaussian Process Classification (GPC)#", "content": "the variance \\(\\text{Var}[f_*]\\) described in Eqs. (3.21) and (3.24) of [RW2006]). The GaussianProcessClassifier provides access to these quantities via the latent_mean_and_variance method. GaussianProcessClassifier supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In “one_vs_one”, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on multi-class classification for more details. In the case of Gaussian process classification, “one_vs_one” might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that “one_vs_one” does not support predicting probability estimates but only plain predictions. Moreover, note that GaussianProcessClassifier does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.", "prev_chunk_id": "chunk_109", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_111", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.3.1. Probabilistic predictions with GPC#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.3.1. Probabilistic predictions with GPC#", "content": "1.7.3.1. Probabilistic predictions with GPC# This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML). While the hyperparameters chosen by optimizing LML have a considerably larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad). This undesirable effect is caused by the Laplace approximation used internally by GPC. The second figure shows the log-marginal-likelihood for different choices of the kernel’s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.", "prev_chunk_id": "chunk_110", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_112", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.3.2. Illustration of GPC on the XOR dataset#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.3.2. Illustration of GPC on the XOR dataset#", "content": "1.7.3.2. Illustration of GPC on the XOR dataset# This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as RBF often obtain better results.", "prev_chunk_id": "chunk_111", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_113", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.3.3. Gaussian process classification (GPC) on iris dataset#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.3.3. Gaussian process classification (GPC) on iris dataset#", "content": "1.7.3.3. Gaussian process classification (GPC) on iris dataset# This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.", "prev_chunk_id": "chunk_112", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_114", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4. Kernels for Gaussian Processes#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4. Kernels for Gaussian Processes#", "content": "1.7.4. Kernels for Gaussian Processes# Kernels (also called “covariance functions” in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the “similarity” of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \\(k(x_i, x_j)= k(d(x_i, x_j))\\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006]. This example shows how to define a custom kernel over discrete data. For guidance on how to best combine different kernels, we refer to [Duv2014].", "prev_chunk_id": "chunk_113", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_115", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.1. Basic kernels#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.1. Basic kernels#", "content": "1.7.4.1. Basic kernels# The ConstantKernel kernel can be used as part of a Product kernel where it scales the magnitude of the other factor (kernel) or as part of a Sum kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \\(constant\\_value\\). It is defined as: The main use-case of the WhiteKernel kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \\(noise\\_level\\) corresponds to estimating the noise-level. It is defined as:", "prev_chunk_id": "chunk_114", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_116", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.2. Kernel operators#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.2. Kernel operators#", "content": "1.7.4.2. Kernel operators# Kernel operators take one or two base kernels and combine them into a new kernel. The Sum kernel takes two kernels \\(k_1\\) and \\(k_2\\) and combines them via \\(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\\). The Product kernel takes two kernels \\(k_1\\) and \\(k_2\\) and combines them via \\(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\\). The Exponentiation kernel takes one base kernel and a scalar parameter \\(p\\) and combines them via \\(k_{exp}(X, Y) = k(X, Y)^p\\). Note that magic methods __add__, __mul___ and __pow__ are overridden on the Kernel objects, so one can use e.g. RBF() + RBF() as a shortcut for Sum(RBF(), RBF()).", "prev_chunk_id": "chunk_115", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_117", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.3. Radial basis function (RBF) kernel#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.3. Radial basis function (RBF) kernel#", "content": "1.7.4.3. Radial basis function (RBF) kernel# The RBF kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized by a length-scale parameter \\(l>0\\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \\(x\\) (anisotropic variant of the kernel). The kernel is given by: where \\(d(\\cdot, \\cdot)\\) is the Euclidean distance. This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:", "prev_chunk_id": "chunk_116", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_118", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.4. Matérn kernel#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.4. Matérn kernel#", "content": "1.7.4.4. Matérn kernel# The Matern kernel is a stationary kernel and a generalization of the RBF kernel. It has an additional parameter \\(\\nu\\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \\(l>0\\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \\(x\\) (anisotropic variant of the kernel). The prior and posterior of a GP resulting from a Matérn kernel are shown in the following figure: See [RW2006], pp84 for further details regarding the different variants of the Matérn kernel.", "prev_chunk_id": "chunk_117", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_119", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.5. Rational quadratic kernel#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.5. Rational quadratic kernel#", "content": "1.7.4.5. Rational quadratic kernel# The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \\(l>0\\) and a scale mixture parameter \\(\\alpha>0\\) Only the isotropic variant where \\(l\\) is a scalar is supported at the moment. The kernel is given by: The prior and posterior of a GP resulting from a RationalQuadratic kernel are shown in the following figure:", "prev_chunk_id": "chunk_118", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_120", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.6. Exp-Sine-Squared kernel#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.6. Exp-Sine-Squared kernel#", "content": "1.7.4.6. Exp-Sine-Squared kernel# The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \\(l>0\\) and a periodicity parameter \\(p>0\\). Only the isotropic variant where \\(l\\) is a scalar is supported at the moment. The kernel is given by: The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:", "prev_chunk_id": "chunk_119", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_121", "url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "title": "1.7.4.7. Dot-Product kernel#", "page_title": "1.7. Gaussian Processes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.7.4.7. Dot-Product kernel#", "content": "1.7.4.7. Dot-Product kernel# The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \\(N(0, 1)\\) priors on the coefficients of \\(x_d (d = 1, . . . , D)\\) and a prior of \\(N(0, \\sigma_0^2)\\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \\(\\sigma_0^2\\). For \\(\\sigma_0^2 = 0\\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by The DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:", "prev_chunk_id": "chunk_120", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_122", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8. Cross decomposition#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8. Cross decomposition#", "content": "1.8. Cross decomposition# The cross decomposition module contains supervised estimators for dimensionality reduction and regression, belonging to the “Partial Least Squares” family. Cross decomposition algorithms find the fundamental relations between two matrices (X and Y). They are latent variable approaches to modeling the covariance structures in these two spaces. They will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. In other words, PLS projects both X and Y into a lower-dimensional subspace such that the covariance between transformed(X) and transformed(Y) is maximal. PLS draws similarities with Principal Component Regression (PCR), where the samples are first projected into a lower-dimensional subspace, and the targets y are predicted using transformed(X). One issue with PCR is that the dimensionality reduction is unsupervised, and may lose some important variables: PCR would keep the features with the most variance, but it’s possible that features with small variances are relevant for predicting the target. In a way, PLS allows for the same kind of dimensionality reduction, but by taking into account the targets y. An illustration of this fact is given in the following example: * Principal Component Regression vs Partial Least Squares Regression. Apart from CCA, the PLS estimators are particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among the features. By contrast, standard linear regression would fail in these cases unless it is regularized. Classes included in this module are PLSRegression, PLSCanonical, CCA and PLSSVD", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_123", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8.1. PLSCanonical#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8.1. PLSCanonical#", "content": "1.8.1. PLSCanonical# We here describe the algorithm used in PLSCanonical. The other estimators use variants of this algorithm, and are detailed below. We recommend section [1] for more details and comparisons between these algorithms. In [1], PLSCanonical corresponds to “PLSW2A”. Given two centered matrices \\(X \\in \\mathbb{R}^{n \\times d}\\) and \\(Y \\in \\mathbb{R}^{n \\times t}\\), and a number of components \\(K\\), PLSCanonical proceeds as follows: Set \\(X_1\\) to \\(X\\) and \\(Y_1\\) to \\(Y\\). Then, for each \\(k \\in [1, K]\\): - a) compute\\(u_k \\in \\mathbb{R}^d\\)and\\(v_k \\in \\mathbb{R}^t\\), the first left and right singular vectors of the cross-covariance matrix\\(C = X_k^T Y_k\\).\\(u_k\\)and\\(v_k\\)are called theweights. By definition,\\(u_k\\)and\\(v_k\\)are chosen so that they maximize the covariance between the projected\\(X_k\\)and the projected target, that is\\(\\text{Cov}(X_k u_k, Y_k v_k)\\). - b) Project\\(X_k\\)and\\(Y_k\\)on the singular vectors to obtainscores:\\(\\xi_k = X_k u_k\\)and\\(\\omega_k = Y_k v_k\\) - c) Regress\\(X_k\\)on\\(\\xi_k\\), i.e. find a vector\\(\\gamma_k \\in \\mathbb{R}^d\\)such that the rank-1 matrix\\(\\xi_k \\gamma_k^T\\)is as close as possible to\\(X_k\\). Do the same on\\(Y_k\\)with\\(\\omega_k\\)to obtain\\(\\delta_k\\). The vectors\\(\\gamma_k\\)and\\(\\delta_k\\)are called theloadings. - d)deflate\\(X_k\\)and\\(Y_k\\), i.e. subtract the rank-1 approximations:\\(X_{k+1} = X_k - \\xi_k \\gamma_k^T\\), and\\(Y_{k + 1} = Y_k - \\omega_k \\delta_k^T\\). At the end, we have approximated \\(X\\) as a sum of rank-1 matrices: \\(X = \\Xi \\Gamma^T\\) where \\(\\Xi \\in \\mathbb{R}^{n \\times K}\\) contains the scores in its columns, and \\(\\Gamma^T \\in \\mathbb{R}^{K \\times d}\\) contains the loadings in its rows. Similarly for \\(Y\\), we have \\(Y = \\Omega \\Delta^T\\). Note that the scores matrices \\(\\Xi\\) and \\(\\Omega\\) correspond to the projections of the training data \\(X\\) and \\(Y\\), respectively. Step a) may be performed in two ways: either by computing the whole SVD of \\(C\\) and only retaining the singular vectors with the biggest singular values, or by directly computing the singular vectors using the power method (cf section 11.3 in [1]), which corresponds", "prev_chunk_id": "chunk_122", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_124", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8.1. PLSCanonical#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8.1. PLSCanonical#", "content": "to the 'nipals' option of the algorithm parameter.", "prev_chunk_id": "chunk_123", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_125", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8.2. PLSSVD#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8.2. PLSSVD#", "content": "1.8.2. PLSSVD# PLSSVD is a simplified version of PLSCanonical described earlier: instead of iteratively deflating the matrices \\(X_k\\) and \\(Y_k\\), PLSSVD computes the SVD of \\(C = X^TY\\) only once, and stores the n_components singular vectors corresponding to the biggest singular values in the matrices U and V, corresponding to the x_weights_ and y_weights_ attributes. Here, the transformed data is simply transformed(X) = XU and transformed(Y) = YV. If n_components == 1, PLSSVD and PLSCanonical are strictly equivalent.", "prev_chunk_id": "chunk_124", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_126", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8.3. PLSRegression#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8.3. PLSRegression#", "content": "1.8.3. PLSRegression# The PLSRegression estimator is similar to PLSCanonical with algorithm='nipals', with 2 significant differences: - at step a) in the power method to compute\\(u_k\\)and\\(v_k\\),\\(v_k\\)is never normalized. - at step c), the targets\\(Y_k\\)are approximated using the projection of\\(X_k\\)(i.e.\\(\\xi_k\\)) instead of the projection of\\(Y_k\\)(i.e.\\(\\omega_k\\)). In other words, the loadings computation is different. As a result, the deflation in step d) will also be affected. These two modifications affect the output of predict and transform, which are not the same as for PLSCanonical. Also, while the number of components is limited by min(n_samples, n_features, n_targets) in PLSCanonical, here the limit is the rank of \\(X^TX\\), i.e. min(n_samples, n_features). PLSRegression is also known as PLS1 (single targets) and PLS2 (multiple targets). Much like Lasso, PLSRegression is a form of regularized linear regression where the number of components controls the strength of the regularization.", "prev_chunk_id": "chunk_125", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_127", "url": "https://scikit-learn.org/stable/modules/cross_decomposition.html", "title": "1.8.4. Canonical Correlation Analysis#", "page_title": "1.8. Cross decomposition — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.8.4. Canonical Correlation Analysis#", "content": "1.8.4. Canonical Correlation Analysis# Canonical Correlation Analysis was developed prior and independently to PLS. But it turns out that CCA is a special case of PLS, and corresponds to PLS in “Mode B” in the literature. CCA differs from PLSCanonical in the way the weights \\(u_k\\) and \\(v_k\\) are computed in the power method of step a). Details can be found in section 10 of [1]. Since CCA involves the inversion of \\(X_k^TX_k\\) and \\(Y_k^TY_k\\), this estimator can be unstable if the number of features or targets is greater than the number of samples. References Examples - Compare cross decomposition methods - Principal Component Regression vs Partial Least Squares Regression", "prev_chunk_id": "chunk_126", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_128", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9. Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9. Naive Bayes#", "content": "1.9. Naive Bayes# Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Bayes’ theorem states the following relationship, given class variable \\(y\\) and dependent feature vector \\(x_1\\) through \\(x_n\\), : Using the naive conditional independence assumption that for all \\(i\\), this relationship is simplified to Since \\(P(x_1, \\dots, x_n)\\) is constant given the input, we can use the following classification rule: and we can use Maximum A Posteriori (MAP) estimation to estimate \\(P(y)\\) and \\(P(x_i \\mid y)\\); the former is then the relative frequency of class \\(y\\) in the training set. The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \\(P(x_i \\mid y)\\). In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.) Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality. On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_129", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.1. Gaussian Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.1. Gaussian Naive Bayes#", "content": "1.9.1. Gaussian Naive Bayes# GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian: The parameters \\(\\sigma_y\\) and \\(\\mu_y\\) are estimated using maximum likelihood. >>> from sklearn.datasets import load_iris >>> from sklearn.model_selection import train_test_split >>> from sklearn.naive_bayes import GaussianNB >>> X, y = load_iris(return_X_y=True) >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) >>> gnb = GaussianNB() >>> y_pred = gnb.fit(X_train, y_train).predict(X_test) >>> print(\"Number of mislabeled points out of a total %d points : %d\" ... % (X_test.shape[0], (y_test != y_pred).sum())) Number of mislabeled points out of a total 75 points : 4", "prev_chunk_id": "chunk_128", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_130", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.2. Multinomial Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.2. Multinomial Naive Bayes#", "content": "1.9.2. Multinomial Naive Bayes# MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \\(\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})\\) for each class \\(y\\), where \\(n\\) is the number of features (in text classification, the size of the vocabulary) and \\(\\theta_{yi}\\) is the probability \\(P(x_i \\mid y)\\) of feature \\(i\\) appearing in a sample belonging to class \\(y\\). The parameters \\(\\theta_y\\) are estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting: where \\(N_{yi} = \\sum_{x \\in T} x_i\\) is the number of times feature \\(i\\) appears in all samples of class \\(y\\) in the training set \\(T\\), and \\(N_{y} = \\sum_{i=1}^{n} N_{yi}\\) is the total count of all features for class \\(y\\). The smoothing priors \\(\\alpha \\ge 0\\) account for features not present in the learning samples and prevent zero probabilities in further computations. Setting \\(\\alpha = 1\\) is called Laplace smoothing, while \\(\\alpha < 1\\) is called Lidstone smoothing.", "prev_chunk_id": "chunk_129", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_131", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.3. Complement Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.3. Complement Naive Bayes#", "content": "1.9.3. Complement Naive Bayes# ComplementNB implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the model’s weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks.", "prev_chunk_id": "chunk_130", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_132", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.4. Bernoulli Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.4. Bernoulli Naive Bayes#", "content": "1.9.4. Bernoulli Naive Bayes# BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter). The decision rule for Bernoulli naive Bayes is based on which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature \\(i\\) that is an indicator for class \\(y\\), where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.", "prev_chunk_id": "chunk_131", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_133", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.5. Categorical Naive Bayes#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.5. Categorical Naive Bayes#", "content": "1.9.5. Categorical Naive Bayes# CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index \\(i\\), has its own categorical distribution. For each feature \\(i\\) in the training set \\(X\\), CategoricalNB estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \\(J = \\{ 1, \\dots, m \\}\\), with \\(m\\) as the number of samples. CategoricalNB assumes that the sample matrix \\(X\\) is encoded (for instance with the help of OrdinalEncoder) such that all categories for each feature \\(i\\) are represented with numbers \\(0, ..., n_i - 1\\) where \\(n_i\\) is the number of available categories of feature \\(i\\).", "prev_chunk_id": "chunk_132", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_134", "url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "title": "1.9.6. Out-of-core naive Bayes model fitting#", "page_title": "1.9. Naive Bayes — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.9.6. Out-of-core naive Bayes model fitting#", "content": "1.9.6. Out-of-core naive Bayes model fitting# Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting. Contrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels. For an overview of available strategies in scikit-learn, see also the out-of-core learning documentation.", "prev_chunk_id": "chunk_133", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_135", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10. Decision Trees#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10. Decision Trees#", "content": "1.10. Decision Trees# Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation. For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model. Some advantages of decision trees are: - Simple to understand and to interpret. Trees can be visualized. - Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations supportmissing values. - The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree. - Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. Seealgorithmsfor more information. - Able to handle multi-output problems. - Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret. - Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model. - Performs well even if its assumptions are somewhat violated by the true model from which the data were generated. The disadvantages of decision trees include: - Decision-tree learners", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_136", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10. Decision Trees#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10. Decision Trees#", "content": "can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem. - Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble. - Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation. - The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement. - There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. - Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.", "prev_chunk_id": "chunk_135", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_137", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.1. Classification#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.1. Classification#", "content": "1.10.1. Classification# DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset. As with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of shape (n_samples, n_features) holding the training samples, and an array Y of integer values, shape (n_samples,), holding the class labels for the training samples: >>> from sklearn import tree >>> X = [[0, 0], [1, 1]] >>> Y = [0, 1] >>> clf = tree.DecisionTreeClassifier() >>> clf = clf.fit(X, Y) After being fitted, the model can then be used to predict the class of samples: >>> clf.predict([[2., 2.]]) array([1]) In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes. As an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf: >>> clf.predict_proba([[2., 2.]]) array([[0., 1.]]) DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, …, K-1]) classification. Using the Iris dataset, we can construct a tree as follows: >>> from sklearn.datasets import load_iris >>> from sklearn import tree >>> iris = load_iris() >>> X, y = iris.data, iris.target >>> clf = tree.DecisionTreeClassifier() >>> clf = clf.fit(X, y) Once trained, you can plot the tree with the plot_tree function: >>> tree.plot_tree(clf) [...] Examples - Plot the decision surface of decision trees trained on the iris dataset - Understanding the decision tree structure", "prev_chunk_id": "chunk_136", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_138", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.2. Regression#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.2. Regression#", "content": "1.10.2. Regression# Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class. As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values: >>> from sklearn import tree >>> X = [[0, 0], [2, 2]] >>> y = [0.5, 2.5] >>> clf = tree.DecisionTreeRegressor() >>> clf = clf.fit(X, y) >>> clf.predict([[1, 1]]) array([0.5]) Examples - Decision Tree Regression", "prev_chunk_id": "chunk_137", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_139", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.3. Multi-output problems#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.3. Multi-output problems#", "content": "1.10.3. Multi-output problems# A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape (n_samples, n_outputs). When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased. With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes: - Store n output values in leaves, instead of 1; - Use splitting criteria that compute the average reduction across all n outputs. This module offers support for multi-output problems by implementing this strategy in both DecisionTreeClassifier and DecisionTreeRegressor. If a decision tree is fit on an output array Y of shape (n_samples, n_outputs) then the resulting estimator will: - Output n_output values uponpredict; - Output a list of n_output arrays of class probabilities uponpredict_proba. The use of multi-output trees for regression is demonstrated in Decision Tree Regression. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X. The use of multi-output trees for classification is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of", "prev_chunk_id": "chunk_138", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_140", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.3. Multi-output problems#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.3. Multi-output problems#", "content": "the lower half of those faces. Examples - Face completion with a multi-output estimators References - M. Dumont et al,Fast multi-class image annotation with random subwindows and multiple output randomized trees, International Conference on Computer Vision Theory and Applications 2009", "prev_chunk_id": "chunk_139", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_141", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.4. Complexity#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.4. Complexity#", "content": "1.10.4. Complexity# In general, the run time cost to construct a balanced binary tree is \\(O(n_{samples}n_{features}\\log(n_{samples}))\\) and query time \\(O(\\log(n_{samples}))\\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \\(O(n_{features})\\) to find the feature that offers the largest reduction in the impurity criterion, e.g. log loss (which is equivalent to an information gain). This has a cost of \\(O(n_{features}n_{samples}\\log(n_{samples}))\\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \\(O(n_{features}n_{samples}^{2}\\log(n_{samples}))\\).", "prev_chunk_id": "chunk_140", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_142", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.5. Tips on practical use#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.5. Tips on practical use#", "content": "1.10.5. Tips on practical use# - Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit. - Consider performing dimensionality reduction (PCA,ICA, orFeature selection) beforehand to give your tree a better chance of finding features that are discriminative. - Understanding the decision tree structurewill help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data. - Visualize your tree as you are training by using theexportfunction. Usemax_depth=3as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth. - Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Usemax_depthto control the size of the tree to prevent overfitting. - Usemin_samples_splitormin_samples_leafto ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Trymin_samples_leaf=5as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. Whilemin_samples_splitcan create arbitrarily small leaves,min_samples_leafguarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes,min_samples_leaf=1is often the best choice.Note thatmin_samples_splitconsiders samples directly and independent ofsample_weight, if provided (e.g. a node with m weighted samples is still treated as having exactly m samples). Considermin_weight_fraction_leaformin_impurity_decreaseif accounting for sample weights is required at splits. - Balance your dataset before training to prevent the tree from being biased", "prev_chunk_id": "chunk_141", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_143", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.5. Tips on practical use#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.5. Tips on practical use#", "content": "toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such asmin_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, likemin_samples_leaf. - If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such asmin_weight_fraction_leaf, which ensures that leaf nodes contain at least a fraction of the overall sum of the sample weights. - All decision trees usenp.float32arrays internally. If training data is not in this format, a copy of the dataset will be made. - If the input matrix X is very sparse, it is recommended to convert to sparsecsc_matrixbefore calling fit and sparsecsr_matrixbefore calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.", "prev_chunk_id": "chunk_142", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_144", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART#", "content": "1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART# What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn? scikit-learn uses an optimized version of the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now.", "prev_chunk_id": "chunk_143", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_145", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.7. Mathematical formulation#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.7. Mathematical formulation#", "content": "1.10.7. Mathematical formulation# Given training vectors \\(x_i \\in R^n\\), i=1,…, l and a label vector \\(y \\in R^l\\), a decision tree recursively partitions the feature space such that the samples with the same labels or similar target values are grouped together. Let the data at node \\(m\\) be represented by \\(Q_m\\) with \\(n_m\\) samples. For each candidate split \\(\\theta = (j, t_m)\\) consisting of a feature \\(j\\) and threshold \\(t_m\\), partition the data into \\(Q_m^{left}(\\theta)\\) and \\(Q_m^{right}(\\theta)\\) subsets The quality of a candidate split of node \\(m\\) is then computed using an impurity function or loss function \\(H()\\), the choice of which depends on the task being solved (classification or regression) Select the parameters that minimises the impurity Recurse for subsets \\(Q_m^{left}(\\theta^*)\\) and \\(Q_m^{right}(\\theta^*)\\) until the maximum allowable depth is reached, \\(n_m < \\min_{samples}\\) or \\(n_m = 1\\).", "prev_chunk_id": "chunk_144", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_146", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.7.1. Classification criteria#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.7.1. Classification criteria#", "content": "1.10.7.1. Classification criteria# If a target is a classification outcome taking on values 0,1,…,K-1, for node \\(m\\), let be the proportion of class k observations in node \\(m\\). If \\(m\\) is a terminal node, predict_proba for this region is set to \\(p_{mk}\\). Common measures of impurity are the following. Gini: Log Loss or Entropy:", "prev_chunk_id": "chunk_145", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_147", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.7.2. Regression criteria#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.7.2. Regression criteria#", "content": "1.10.7.2. Regression criteria# If the target is a continuous value, then for node \\(m\\), common criteria to minimize as for determining locations for future splits are Mean Squared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute Error (MAE or L1 error). MSE and Poisson deviance both set the predicted value of terminal nodes to the learned mean value \\(\\bar{y}_m\\) of the node whereas the MAE sets the predicted value of terminal nodes to the median \\(median(y)_m\\). Mean Squared Error: Mean Poisson deviance: Setting criterion=\"poisson\" might be a good choice if your target is a count or a frequency (count per some unit). In any case, \\(y >= 0\\) is a necessary condition to use this criterion. Note that it fits much slower than the MSE criterion. For performance reasons the actual implementation minimizes the half mean poisson deviance, i.e. the mean poisson deviance divided by 2. Mean Absolute Error: Note that it fits much slower than the MSE criterion.", "prev_chunk_id": "chunk_146", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_148", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.8. Missing Values Support#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.8. Missing Values Support#", "content": "1.10.8. Missing Values Support# DecisionTreeClassifier, DecisionTreeRegressor have built-in support for missing values using splitter='best', where the splits are determined in a greedy fashion. ExtraTreeClassifier, and ExtraTreeRegressor have built-in support for missing values for splitter='random', where the splits are determined randomly. For more details on how the splitter differs on non-missing values, see the Forest section. The criterion supported when there are missing values are 'gini', 'entropy', or 'log_loss', for classification or 'squared_error', 'friedman_mse', or 'poisson' for regression. First we will describe how DecisionTreeClassifier, DecisionTreeRegressor handle missing-values in the data. For each potential threshold on the non-missing data, the splitter will evaluate the split with all the missing values going to the left node or the right node. Decisions are made as follows: - By default when predicting, the samples with missing values are classified with the class used in the split found during training:>>>fromsklearn.treeimportDecisionTreeClassifier>>>importnumpyasnp>>>X=np.array([0,1,6,np.nan]).reshape(-1,1)>>>y=[0,0,1,1]>>>tree=DecisionTreeClassifier(random_state=0).fit(X,y)>>>tree.predict(X)array([0, 0, 1, 1]) - If the criterion evaluation is the same for both nodes, then the tie for missing value at predict time is broken by going to the right node. The splitter also checks the split where all the missing values go to one child and non-missing values go to the other:>>>fromsklearn.treeimportDecisionTreeClassifier>>>importnumpyasnp>>>X=np.array([np.nan,-1,np.nan,1]).reshape(-1,1)>>>y=[0,0,1,1]>>>tree=DecisionTreeClassifier(random_state=0).fit(X,y)>>>X_test=np.array([np.nan]).reshape(-1,1)>>>tree.predict(X_test)array([1]) - If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples:>>>fromsklearn.treeimportDecisionTreeClassifier>>>importnumpyasnp>>>X=np.array([0,1,2,3]).reshape(-1,1)>>>y=[0,1,1,1]>>>tree=DecisionTreeClassifier(random_state=0).fit(X,y)>>>X_test=np.array([np.nan]).reshape(-1,1)>>>tree.predict(X_test)array([1]) ExtraTreeClassifier, and ExtraTreeRegressor handle missing values in a slightly different way. When splitting a node, a random threshold will be chosen to split the non-missing values on. Then the non-missing values will be sent to the left and right child based on the randomly selected threshold, while the missing values will also be randomly sent to the left or right child. This is repeated for every feature considered at each split. The best split among these", "prev_chunk_id": "chunk_147", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_149", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.8. Missing Values Support#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.8. Missing Values Support#", "content": "is chosen. During prediction, the treatment of missing-values is the same as that of the decision tree: - By default when predicting, the samples with missing values are classified with the class used in the split found during training. - If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples.", "prev_chunk_id": "chunk_148", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_150", "url": "https://scikit-learn.org/stable/modules/tree.html", "title": "1.10.9. Minimal Cost-Complexity Pruning#", "page_title": "1.10. Decision Trees — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.10.9. Minimal Cost-Complexity Pruning#", "content": "1.10.9. Minimal Cost-Complexity Pruning# Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized by \\(\\alpha\\ge0\\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \\(R_\\alpha(T)\\) of a given tree \\(T\\): where \\(|\\widetilde{T}|\\) is the number of terminal nodes in \\(T\\) and \\(R(T)\\) is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for \\(R(T)\\). As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of \\(T\\) that minimizes \\(R_\\alpha(T)\\). The cost complexity measure of a single node is \\(R_\\alpha(t)=R(t)+\\alpha\\). The branch, \\(T_t\\), is defined to be a tree where node \\(t\\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \\(R(T_t)<R(t)\\). However, the cost complexity measure of a node, \\(t\\), and its branch, \\(T_t\\), can be equal depending on \\(\\alpha\\). We define the effective \\(\\alpha\\) of a node to be the value where they are equal, \\(R_\\alpha(T_t)=R_\\alpha(t)\\) or \\(\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}\\). A non-terminal node with the smallest value of \\(\\alpha_{eff}\\) is the weakest link and will be pruned. This process stops when the pruned tree’s minimal \\(\\alpha_{eff}\\) is greater than the ccp_alpha parameter. Examples - Post pruning decision trees with cost complexity pruning References - https://en.wikipedia.org/wiki/Decision_tree_learning - https://en.wikipedia.org/wiki/Predictive_analytics - J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993. - T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.", "prev_chunk_id": "chunk_149", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_151", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking#", "content": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking# Ensemble methods combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. Two very famous examples of ensemble methods are gradient-boosted trees and random forests. More generally, ensemble models can be applied to any base learner beyond trees, in averaging methods such as Bagging methods, model stacking, or Voting, or in boosting, as AdaBoost.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_152", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1. Gradient-boosted trees#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1. Gradient-boosted trees#", "content": "1.11.1. Gradient-boosted trees# Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions, see the seminal work of [Friedman2001]. GBDT is an excellent model for both regression and classification, in particular for tabular data.", "prev_chunk_id": "chunk_151", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_153", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1. Histogram-Based Gradient Boosting#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1. Histogram-Based Gradient Boosting#", "content": "1.11.1.1. Histogram-Based Gradient Boosting# Scikit-learn 0.21 introduced two new implementations of gradient boosted trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor, inspired by LightGBM (See [LightGBM]). These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples. They also have built-in support for missing values, which avoids the need for an imputer. These fast estimators first bin the input samples X into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from GradientBoostingClassifier and GradientBoostingRegressor are not yet supported, for instance some loss functions. Examples - Partial Dependence and Individual Conditional Expectation Plots - Comparing Random Forests and Histogram Gradient Boosting models", "prev_chunk_id": "chunk_152", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_154", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.1. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.1. Usage#", "content": "1.11.1.1.1. Usage# Most of the parameters are unchanged from GradientBoostingClassifier and GradientBoostingRegressor. One exception is the max_iter parameter that replaces n_estimators, and controls the number of iterations of the boosting process: >>> from sklearn.ensemble import HistGradientBoostingClassifier >>> from sklearn.datasets import make_hastie_10_2 >>> X, y = make_hastie_10_2(random_state=0) >>> X_train, X_test = X[:2000], X[2000:] >>> y_train, y_test = y[:2000], y[2000:] >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train) >>> clf.score(X_test, y_test) 0.8965 Available losses for regression are: - ‘squared_error’, which is the default loss; - ‘absolute_error’, which is less sensitive to outliers than the squared error; - ‘gamma’, which is well suited to model strictly positive outcomes; - ‘poisson’, which is well suited to model counts and frequencies; - ‘quantile’, which allows for estimating a conditional quantile that can later be used to obtain prediction intervals. For classification, ‘log_loss’ is the only option. For binary classification it uses the binary log loss, also known as binomial deviance or binary cross-entropy. For n_classes >= 3, it uses the multi-class log loss function, with multinomial deviance and categorical cross-entropy as alternative names. The appropriate loss version is selected based on y passed to fit. The size of the trees can be controlled through the max_leaf_nodes, max_depth, and min_samples_leaf parameters. The number of bins used to bin the data is controlled with the max_bins parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible (255), which is the default. The l2_regularization parameter acts as a regularizer for the loss function, and corresponds to \\(\\lambda\\) in the following expression (see equation (2) in [XGBoost]): Note that early-stopping is enabled by default if the number of samples is larger than 10,000. The early-stopping behaviour is controlled via the early_stopping, scoring, validation_fraction, n_iter_no_change, and tol parameters. It is possible to", "prev_chunk_id": "chunk_153", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_155", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.1. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.1. Usage#", "content": "early-stop using an arbitrary scorer, or just the training or validation loss. Note that for technical reasons, using a callable as a scorer is significantly slower than using the loss. By default, early-stopping is performed if there are at least 10,000 samples in the training set, using the validation loss.", "prev_chunk_id": "chunk_154", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_156", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.2. Missing values support#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.2. Missing values support#", "content": "1.11.1.1.2. Missing values support# HistGradientBoostingClassifier and HistGradientBoostingRegressor have built-in support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently: >>> from sklearn.ensemble import HistGradientBoostingClassifier >>> import numpy as np >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1) >>> y = [0, 0, 1, 1] >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y) >>> gbdt.predict(X) array([0, 0, 1, 1]) When the missingness pattern is predictive, the splits can be performed on whether the feature value is missing or not: >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1) >>> y = [0, 1, 0, 0, 1] >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1, ... max_depth=2, ... learning_rate=1, ... max_iter=1).fit(X, y) >>> gbdt.predict(X) array([0, 1, 0, 0, 1]) If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples. Examples - Features in Histogram Gradient Boosting Trees", "prev_chunk_id": "chunk_155", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_157", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.3. Sample weight support#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.3. Sample weight support#", "content": "1.11.1.1.3. Sample weight support# HistGradientBoostingClassifier and HistGradientBoostingRegressor support sample weights during fit. The following toy example demonstrates that samples with a sample weight of zero are ignored: >>> X = [[1, 0], ... [1, 0], ... [1, 0], ... [0, 1]] >>> y = [0, 0, 1, 0] >>> # ignore the first 2 training samples by setting their weight to 0 >>> sample_weight = [0, 0, 1, 1] >>> gb = HistGradientBoostingClassifier(min_samples_leaf=1) >>> gb.fit(X, y, sample_weight=sample_weight) HistGradientBoostingClassifier(...) >>> gb.predict([[1, 0]]) array([1]) >>> gb.predict_proba([[1, 0]])[0, 1] np.float64(0.999) As you can see, the [1, 0] is comfortably classified as 1 since the first two samples are ignored due to their sample weights. Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account.", "prev_chunk_id": "chunk_156", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_158", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.4. Categorical Features Support#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.4. Categorical Features Support#", "content": "1.11.1.1.4. Categorical Features Support# HistGradientBoostingClassifier and HistGradientBoostingRegressor have native support for categorical features: they can consider splits on non-ordered, categorical data. For datasets with categorical features, using the native categorical support is often better than relying on one-hot encoding (OneHotEncoder), because one-hot encoding requires more tree depth to achieve equivalent splits. It is also usually better to rely on the native categorical support rather than to treat categorical features as continuous (ordinal), which happens for ordinal-encoded categorical data, since categories are nominal quantities where order does not matter. To enable categorical support, a boolean mask can be passed to the categorical_features parameter, indicating which feature is categorical. In the following, the first feature will be treated as categorical and the second feature as numerical: >>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False]) Equivalently, one can pass a list of integers indicating the indices of the categorical features: >>> gbdt = HistGradientBoostingClassifier(categorical_features=[0]) When the input is a DataFrame, it is also possible to pass a list of column names: >>> gbdt = HistGradientBoostingClassifier(categorical_features=[\"site\", \"manufacturer\"]) Finally, when the input is a DataFrame we can use categorical_features=\"from_dtype\" in which case all columns with a categorical dtype will be treated as categorical features. The cardinality of each categorical feature must be less than the max_bins parameter. For an example using histogram-based gradient boosting on categorical features, see Categorical Feature Support in Gradient Boosting. If there are missing values during training, the missing values will be treated as a proper category. If there are no missing values during training, then at prediction time, missing values are mapped to the child node that has the most samples (just like for continuous features). When predicting, categories that were not seen during fit time will be treated as missing values. Examples - Categorical Feature Support in Gradient Boosting", "prev_chunk_id": "chunk_157", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_159", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.5. Monotonic Constraints#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.5. Monotonic Constraints#", "content": "1.11.1.1.5. Monotonic Constraints# Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model. For a predictor \\(F\\) with two features: - amonotonic increase constraintis a constraint of the form:\\[x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)\\] - amonotonic decrease constraintis a constraint of the form:\\[x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)\\] You can specify a monotonic constraint on each feature using the monotonic_cst parameter. For each feature, a value of 0 indicates no constraint, while 1 and -1 indicate a monotonic increase and monotonic decrease constraint, respectively: >>> from sklearn.ensemble import HistGradientBoostingRegressor ... # monotonic increase, monotonic decrease, and no constraint on the 3 features >>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0]) In a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed to have a positive (negative) effect on the probability of samples to belong to the positive class. Nevertheless, monotonic constraints only marginally constrain feature effects on the output. For instance, monotonic increase and decrease constraints cannot be used to enforce the following modelling constraint: Also, monotonic constraints are not supported for multiclass classification. For a practical implementation of monotonic constraints with the histogram-based gradient boosting, including how they can improve generalization when domain knowledge is available, see Monotonic Constraints. Examples - Features in Histogram Gradient Boosting Trees", "prev_chunk_id": "chunk_158", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_160", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.6. Interaction constraints#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.6. Interaction constraints#", "content": "1.11.1.1.6. Interaction constraints# A priori, the histogram gradient boosted trees are allowed to use any feature to split a node into child nodes. This creates so called interactions between features, i.e. usage of different features as split along a branch. Sometimes, one wants to restrict the possible interactions, see [Mayer2022]. This can be done by the parameter interaction_cst, where one can specify the indices of features that are allowed to interact. For instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}] forbids all interactions. The constraints [{0, 1}, {1, 2}] specify two groups of possibly interacting features. Features 0 and 1 may interact with each other, as well as features 1 and 2. But note that features 0 and 2 are forbidden to interact. The following depicts a tree and the possible splits of the tree: 1 <- Both constraint groups could be applied from now on / \\ 1 2 <- Left split still fulfills both constraint groups. / \\ / \\ Right split at feature 2 has only group {1, 2} from now on. LightGBM uses the same logic for overlapping groups. Note that features not listed in interaction_cst are automatically assigned an interaction group for themselves. With again 3 features, this means that [{0}] is equivalent to [{0}, {1, 2}]. Examples - Partial Dependence and Individual Conditional Expectation Plots References", "prev_chunk_id": "chunk_159", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_161", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.7. Low-level parallelism#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.7. Low-level parallelism#", "content": "1.11.1.1.7. Low-level parallelism# HistGradientBoostingClassifier and HistGradientBoostingRegressor use OpenMP for parallelization through Cython. For more details on how to control the number of threads, please refer to our Parallelism notes. The following parts are parallelized: - mapping samples from real values to integer-valued bins (finding the bin thresholds is however sequential) - building histograms is parallelized over features - finding the best split point at a node is parallelized over features - during fit, mapping samples into the left and right children is parallelized over samples - gradient and hessians computations are parallelized over samples - predicting is parallelized over samples", "prev_chunk_id": "chunk_160", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_162", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.1.8. Why it’s faster#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.1.8. Why it’s faster#", "content": "1.11.1.1.8. Why it’s faster# The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs GradientBoostingClassifier and GradientBoostingRegressor) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \\(\\mathcal{O}(n_\\text{features} \\times n \\log(n))\\) where \\(n\\) is the number of samples at the node. HistGradientBoostingClassifier and HistGradientBoostingRegressor, in contrast, do not require sorting the feature values and instead use a data-structure called a histogram, where the samples are implicitly ordered. Building a histogram has a \\(\\mathcal{O}(n)\\) complexity, so the node splitting procedure has a \\(\\mathcal{O}(n_\\text{features} \\times n)\\) complexity, much smaller than the previous one. In addition, instead of considering \\(n\\) split points, we consider only max_bins split points, which might be much smaller. In order to build histograms, the input data X needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in GradientBoostingClassifier and GradientBoostingRegressor). Finally, many parts of the implementation of HistGradientBoostingClassifier and HistGradientBoostingRegressor are parallelized. References", "prev_chunk_id": "chunk_161", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_163", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor#", "content": "1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor# The usage and the parameters of GradientBoostingClassifier and GradientBoostingRegressor are described below. The 2 most important parameters of these estimators are n_estimators and learning_rate. Examples - Gradient Boosting regression - Gradient Boosting Out-of-Bag estimates", "prev_chunk_id": "chunk_162", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_164", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.1. Fitting additional weak-learners#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.1. Fitting additional weak-learners#", "content": "1.11.1.2.1. Fitting additional weak-learners# Both GradientBoostingRegressor and GradientBoostingClassifier support warm_start=True which allows you to add more estimators to an already fitted model. >>> import numpy as np >>> from sklearn.metrics import mean_squared_error >>> from sklearn.datasets import make_friedman1 >>> from sklearn.ensemble import GradientBoostingRegressor >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0) >>> X_train, X_test = X[:200], X[200:] >>> y_train, y_test = y[:200], y[200:] >>> est = GradientBoostingRegressor( ... n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, ... loss='squared_error' ... ) >>> est = est.fit(X_train, y_train) # fit with 100 trees >>> mean_squared_error(y_test, est.predict(X_test)) 5.00 >>> _ = est.set_params(n_estimators=200, warm_start=True) # set warm_start and increase num of trees >>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est >>> mean_squared_error(y_test, est.predict(X_test)) 3.84", "prev_chunk_id": "chunk_163", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_165", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.2. Controlling the tree size#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.2. Controlling the tree size#", "content": "1.11.1.2.2. Controlling the tree size# The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth h can capture interactions of order h . There are two ways in which the size of the individual regression trees can be controlled. If you specify max_depth=h then complete binary trees of depth h will be grown. Such trees will have (at most) 2**h leaf nodes and 2**h - 1 split nodes. Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter max_leaf_nodes. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with max_leaf_nodes=k has k - 1 split nodes and thus can model interactions of up to order max_leaf_nodes - 1 . We found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error. The parameter max_leaf_nodes corresponds to the variable J in the chapter on gradient boosting in [Friedman2001] and is related to the parameter interaction.depth in R’s gbm package where max_leaf_nodes == interaction.depth + 1 .", "prev_chunk_id": "chunk_164", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_166", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.3. Mathematical formulation#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.3. Mathematical formulation#", "content": "1.11.1.2.3. Mathematical formulation# We first present GBRT for regression, and then detail the classification case.", "prev_chunk_id": "chunk_165", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_167", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.4. Loss Functions#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.4. Loss Functions#", "content": "1.11.1.2.4. Loss Functions# The following loss functions are supported and can be specified using the parameter loss:", "prev_chunk_id": "chunk_166", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_168", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.5. Shrinkage via learning rate#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.5. Shrinkage via learning rate#", "content": "1.11.1.2.5. Shrinkage via learning rate# [Friedman2001] proposed a simple regularization strategy that scales the contribution of each weak learner by a constant factor \\(\\nu\\): The parameter \\(\\nu\\) is also called the learning rate because it scales the step length of the gradient descent procedure; it can be set via the learning_rate parameter. The parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learners to fit. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error. [HTF] recommend to set the learning rate to a small constant (e.g. learning_rate <= 0.1) and choose n_estimators large enough that early stopping applies, see Early stopping in Gradient Boosting for a more detailed discussion of the interaction between learning_rate and n_estimators see [R2007].", "prev_chunk_id": "chunk_167", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_169", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.6. Subsampling#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.6. Subsampling#", "content": "1.11.1.2.6. Subsampling# [Friedman2002] proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction subsample of the available training data. The subsample is drawn without replacement. A typical value of subsample is 0.5. The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in RandomForestClassifier. The number of subsampled features can be controlled via the max_features parameter. Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute oob_improvement_. oob_improvement_[i] holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming. Examples - Gradient Boosting regularization - Gradient Boosting Out-of-Bag estimates - OOB Errors for Random Forests", "prev_chunk_id": "chunk_168", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_170", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.1.2.7. Interpretation with feature importance#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.1.2.7. Interpretation with feature importance#", "content": "1.11.1.2.7. Interpretation with feature importance# Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models. Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contribute in predicting the target response? Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see Feature importance evaluation for more details). The feature importance scores of a fit gradient boosting model can be accessed via the feature_importances_ property: >>> from sklearn.datasets import make_hastie_10_2 >>> from sklearn.ensemble import GradientBoostingClassifier >>> X, y = make_hastie_10_2(random_state=0) >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, ... max_depth=1, random_state=0).fit(X, y) >>> clf.feature_importances_ array([0.107, 0.105, 0.113, 0.0987, 0.0947, 0.107, 0.0916, 0.0972, 0.0958, 0.0906]) Note that this computation of feature importance is based on entropy, and it is distinct from sklearn.inspection.permutation_importance which is based on permutation of the features. Examples - Gradient Boosting regression References", "prev_chunk_id": "chunk_169", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_171", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2. Random forests and other randomized tree ensembles#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2. Random forests and other randomized tree ensembles#", "content": "1.11.2. Random forests and other randomized tree ensembles# The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques [B1998] specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers. As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of shape (n_samples, n_features) holding the training samples, and an array Y of shape (n_samples,) holding the target values (class labels) for the training samples: >>> from sklearn.ensemble import RandomForestClassifier >>> X = [[0, 0], [1, 1]] >>> Y = [0, 1] >>> clf = RandomForestClassifier(n_estimators=10) >>> clf = clf.fit(X, Y) Like decision trees, forests of trees also extend to multi-output problems (if Y is an array of shape (n_samples, n_outputs)).", "prev_chunk_id": "chunk_170", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_172", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.1. Random Forests#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.1. Random Forests#", "content": "1.11.2.1. Random Forests# In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found through an exhaustive search of the feature values of either all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details.) The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model. In contrast to the original publication [B2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class. A competitive alternative to random forests are Histogram-Based Gradient Boosting (HGBT) models: - Building trees: Random forests typically rely on deep trees (that overfit individually) which uses much computational resources, as they require several splittings and evaluations of candidate splits. Boosting models build shallow trees (that underfit individually) which are faster to fit and predict. - Sequential boosting: In HGBT, the decision trees are built sequentially, where each tree is trained to correct the errors made by the previous ones. This allows them to iteratively improve the model’s performance using relatively few trees. In contrast, random forests use a majority vote to predict the outcome, which can require a", "prev_chunk_id": "chunk_171", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_173", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.1. Random Forests#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.1. Random Forests#", "content": "larger number of trees to achieve the same level of accuracy. - Efficient binning: HGBT uses an efficient binning algorithm that can handle large datasets with a high number of features. The binning algorithm can pre-process the data to speed up the subsequent tree construction (seeWhy it’s faster). In contrast, the scikit-learn implementation of random forests does not use binning and relies on exact splitting, which can be computationally expensive. Overall, the computational cost of HGBT versus RF depends on the specific characteristics of the dataset and the modeling task. It’s a good idea to try both models and compare their performance and computational efficiency on your specific problem to determine which model is the best fit. Examples - Comparing Random Forests and Histogram Gradient Boosting models", "prev_chunk_id": "chunk_172", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_174", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.2. Extremely Randomized Trees#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.2. Extremely Randomized Trees#", "content": "1.11.2.2. Extremely Randomized Trees# In extremely randomized trees (see ExtraTreesClassifier and ExtraTreesRegressor classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias: >>> from sklearn.model_selection import cross_val_score >>> from sklearn.datasets import make_blobs >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.ensemble import ExtraTreesClassifier >>> from sklearn.tree import DecisionTreeClassifier >>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100, ... random_state=0) >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, ... random_state=0) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores.mean() np.float64(0.98) >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None, ... min_samples_split=2, random_state=0) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores.mean() np.float64(0.999) >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, ... min_samples_split=2, random_state=0) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores.mean() > 0.999 np.True_", "prev_chunk_id": "chunk_173", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_175", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.3. Parameters#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.3. Parameters#", "content": "1.11.2.3. Parameters# The main parameters to adjust when using these methods is n_estimators and max_features. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are max_features=1.0 or equivalently max_features=None (always considering all features instead of a random subset) for regression problems, and max_features=\"sqrt\" (using a random subset of size sqrt(n_features)) for classification tasks (where n_features is the number of features in the data). The default value of max_features=1.0 is equivalent to bagged trees and more randomness can be achieved by setting smaller values (e.g. 0.3 is a typical default in the literature). Good results are often achieved when setting max_depth=None in combination with min_samples_split=2 (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using bootstrap sampling the generalization error can be estimated on the left out or out-of-bag samples. This can be enabled by setting oob_score=True.", "prev_chunk_id": "chunk_174", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_176", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.4. Parallelization#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.4. Parallelization#", "content": "1.11.2.4. Parallelization# Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the n_jobs parameter. If n_jobs=k then computations are partitioned into k jobs, and run on k cores of the machine. If n_jobs=-1 then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using k jobs will unfortunately not be k times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets). Examples - Plot the decision surfaces of ensembles of trees on the iris dataset - Face completion with a multi-output estimators References - P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.", "prev_chunk_id": "chunk_175", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_177", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.5. Feature importance evaluation#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.5. Feature importance evaluation#", "content": "1.11.2.5. Feature importance evaluation# The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature. By averaging the estimates of predictive ability over several randomized trees one can reduce the variance of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to [L2014] for more information on MDI and feature importance evaluation with Random Forests. In practice those estimates are stored as an attribute named feature_importances_ on the fitted model. This is an array with shape (n_features,) whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function. Examples - Feature importances with a forest of trees References", "prev_chunk_id": "chunk_176", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_178", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.6. Totally Random Trees Embedding#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.6. Totally Random Trees Embedding#", "content": "1.11.2.6. Totally Random Trees Embedding# RandomTreesEmbedding implements an unsupervised transformation of the data. Using a forest of completely random trees, RandomTreesEmbedding encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most n_estimators * 2 ** max_depth, the maximum number of leaves in the forest. As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation. Examples - Hashing feature transformation using Totally Random Trees - Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…compares non-linear dimensionality reduction techniques on handwritten digits. - Feature transformations with ensembles of treescompares supervised and unsupervised tree based feature transformations.", "prev_chunk_id": "chunk_177", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_179", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.2.7. Fitting additional trees#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.2.7. Fitting additional trees#", "content": "1.11.2.7. Fitting additional trees# RandomForest, Extra-Trees and RandomTreesEmbedding estimators all support warm_start=True which allows you to add more trees to an already fitted model. >>> from sklearn.datasets import make_classification >>> from sklearn.ensemble import RandomForestClassifier >>> X, y = make_classification(n_samples=100, random_state=1) >>> clf = RandomForestClassifier(n_estimators=10) >>> clf = clf.fit(X, y) # fit with 10 trees >>> len(clf.estimators_) 10 >>> # set warm_start and increase num of estimators >>> _ = clf.set_params(n_estimators=20, warm_start=True) >>> _ = clf.fit(X, y) # fit additional 10 trees >>> len(clf.estimators_) 20 When random_state is also set, the internal random state is also preserved between fit calls. This means that training a model once with n estimators is the same as building the model iteratively via multiple fit calls, where the final number of estimators is equal to n. >>> clf = RandomForestClassifier(n_estimators=20) # set `n_estimators` to 10 + 10 >>> _ = clf.fit(X, y) # fit `estimators_` will be the same as `clf` above Note that this differs from the usual behavior of random_state in that it does not result in the same result across different calls.", "prev_chunk_id": "chunk_178", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_180", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.3. Bagging meta-estimator#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.3. Bagging meta-estimator#", "content": "1.11.3. Bagging meta-estimator# In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees). Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set: - When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting[B1999]. - When samples are drawn with replacement, then the method is known as Bagging[B1996]. - When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces[H1998]. - Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches[LG2012]. In scikit-learn, bagging methods are offered as a unified BaggingClassifier meta-estimator (resp. BaggingRegressor), taking as input a user-specified estimator along with parameters specifying the strategy to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms of samples and features), while bootstrap and bootstrap_features control whether samples and features are", "prev_chunk_id": "chunk_179", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_181", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.3. Bagging meta-estimator#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.3. Bagging meta-estimator#", "content": "drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. As an example, the snippet below illustrates how to instantiate a bagging ensemble of KNeighborsClassifier estimators, each built on random subsets of 50% of the samples and 50% of the features. >>> from sklearn.ensemble import BaggingClassifier >>> from sklearn.neighbors import KNeighborsClassifier >>> bagging = BaggingClassifier(KNeighborsClassifier(), ... max_samples=0.5, max_features=0.5) Examples - Single estimator versus bagging: bias-variance decomposition References", "prev_chunk_id": "chunk_180", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_182", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.4. Voting Classifier#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.4. Voting Classifier#", "content": "1.11.4. Voting Classifier# The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing models in order to balance out their individual weaknesses.", "prev_chunk_id": "chunk_181", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_183", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.4.1. Majority Class Labels (Majority/Hard Voting)#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.4.1. Majority Class Labels (Majority/Hard Voting)#", "content": "1.11.4.1. Majority Class Labels (Majority/Hard Voting)# In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. E.g., if the prediction for a given sample is - classifier 1 -> class 1 - classifier 2 -> class 1 - classifier 3 -> class 2 the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario - classifier 1 -> class 2 - classifier 2 -> class 1 the class label 1 will be assigned to the sample.", "prev_chunk_id": "chunk_182", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_184", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.4.2. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.4.2. Usage#", "content": "1.11.4.2. Usage# The following example shows how to fit the majority rule classifier: >>> from sklearn import datasets >>> from sklearn.model_selection import cross_val_score >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.ensemble import VotingClassifier >>> iris = datasets.load_iris() >>> X, y = iris.data[:, 1:3], iris.target >>> clf1 = LogisticRegression(random_state=1) >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1) >>> clf3 = GaussianNB() >>> eclf = VotingClassifier( ... estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], ... voting='hard') >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']): ... scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5) ... print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) Accuracy: 0.95 (+/- 0.04) [Logistic Regression] Accuracy: 0.94 (+/- 0.04) [Random Forest] Accuracy: 0.91 (+/- 0.04) [naive Bayes] Accuracy: 0.95 (+/- 0.04) [Ensemble]", "prev_chunk_id": "chunk_183", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_185", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.4.3. Weighted Average Probabilities (Soft Voting)#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.4.3. Weighted Average Probabilities (Soft Voting)#", "content": "1.11.4.3. Weighted Average Probabilities (Soft Voting)# In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. To illustrate this with a simple example, let’s assume we have 3 classifiers and a 3-class classification problem where we assign equal weights to all classifiers: w1=1, w2=1, w3=1. The weighted average probabilities for a sample would then be calculated as follows: Here, the predicted class label is 2, since it has the highest average predicted probability. See the example on Visualizing the probabilistic predictions of a VotingClassifier for a demonstration of how the predicted class label can be obtained from the weighted average of predicted probabilities. The following figure illustrates how the decision regions may change when a soft VotingClassifier is trained with weights on three linear models:", "prev_chunk_id": "chunk_184", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_186", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.4.4. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.4.4. Usage#", "content": "1.11.4.4. Usage# In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support predict_proba method): >>> eclf = VotingClassifier( ... estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], ... voting='soft' ... ) Optionally, weights can be provided for the individual classifiers: >>> eclf = VotingClassifier( ... estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], ... voting='soft', weights=[2,5,1] ... )", "prev_chunk_id": "chunk_185", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_187", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.5. Voting Regressor#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.5. Voting Regressor#", "content": "1.11.5. Voting Regressor# The idea behind the VotingRegressor is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.", "prev_chunk_id": "chunk_186", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_188", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.5.1. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.5.1. Usage#", "content": "1.11.5.1. Usage# The following example shows how to fit the VotingRegressor: >>> from sklearn.datasets import load_diabetes >>> from sklearn.ensemble import GradientBoostingRegressor >>> from sklearn.ensemble import RandomForestRegressor >>> from sklearn.linear_model import LinearRegression >>> from sklearn.ensemble import VotingRegressor >>> # Loading some example data >>> X, y = load_diabetes(return_X_y=True) >>> # Training classifiers >>> reg1 = GradientBoostingRegressor(random_state=1) >>> reg2 = RandomForestRegressor(random_state=1) >>> reg3 = LinearRegression() >>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)]) >>> ereg = ereg.fit(X, y) Examples - Plot individual and voting regression predictions", "prev_chunk_id": "chunk_187", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_189", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.6. Stacked generalization#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.6. Stacked generalization#", "content": "1.11.6. Stacked generalization# Stacked generalization is a method for combining estimators to reduce their biases [W1992] [HTF]. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation. The StackingClassifier and StackingRegressor provide such strategies which can be applied to classification and regression problems. The estimators parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators: >>> from sklearn.linear_model import RidgeCV, LassoCV >>> from sklearn.neighbors import KNeighborsRegressor >>> estimators = [('ridge', RidgeCV()), ... ('lasso', LassoCV(random_state=42)), ... ('knr', KNeighborsRegressor(n_neighbors=20, ... metric='euclidean'))] The final_estimator will use the predictions of the estimators as input. It needs to be a classifier or a regressor when using StackingClassifier or StackingRegressor, respectively: >>> from sklearn.ensemble import GradientBoostingRegressor >>> from sklearn.ensemble import StackingRegressor >>> final_estimator = GradientBoostingRegressor( ... n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, ... random_state=42) >>> reg = StackingRegressor( ... estimators=estimators, ... final_estimator=final_estimator) To train the estimators and final_estimator, the fit method needs to be called on the training data: >>> from sklearn.datasets import load_diabetes >>> X, y = load_diabetes(return_X_y=True) >>> from sklearn.model_selection import train_test_split >>> X_train, X_test, y_train, y_test = train_test_split(X, y, ... random_state=42) >>> reg.fit(X_train, y_train) StackingRegressor(...) During training, the estimators are fitted on the whole training data X_train. They will be used when calling predict or predict_proba. To generalize and avoid over-fitting, the final_estimator is trained on out-samples using sklearn.model_selection.cross_val_predict internally. For StackingClassifier, note that the output of the estimators is controlled by the parameter stack_method and it is called by each estimator. This parameter is either a string, being estimator method names, or 'auto' which will automatically identify an available method depending on the availability, tested", "prev_chunk_id": "chunk_188", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_190", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.6. Stacked generalization#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.6. Stacked generalization#", "content": "in the order of preference: predict_proba, decision_function and predict. A StackingRegressor and StackingClassifier can be used as any other regressor or classifier, exposing a predict, predict_proba, or decision_function method, e.g.: >>> y_pred = reg.predict(X_test) >>> from sklearn.metrics import r2_score >>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred))) R2 score: 0.53 Note that it is also possible to get the output of the stacked estimators using the transform method: >>> reg.transform(X_test[:5]) array([[142, 138, 146], [179, 182, 151], [139, 132, 158], [286, 292, 225], [126, 124, 164]]) In practice, a stacking predictor predicts as good as the best predictor of the base layer and even sometimes outperforms it by combining the different strengths of these predictors. However, training a stacking predictor is computationally expensive. Examples - Combine predictors using stacking References", "prev_chunk_id": "chunk_189", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_191", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.7. AdaBoost#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.7. AdaBoost#", "content": "1.11.7. AdaBoost# The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995]. The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consists of applying weights \\(w_1\\), \\(w_2\\), …, \\(w_N\\) to each of the training samples. Initially, those weights are all set to \\(w_i = 1/N\\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [HTF]. AdaBoost can be used both for classification and regression problems: - For multi-class classification,AdaBoostClassifierimplements AdaBoost.SAMME[ZZRH2009]. - For regression,AdaBoostRegressorimplements AdaBoost.R2[D1997].", "prev_chunk_id": "chunk_190", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_192", "url": "https://scikit-learn.org/stable/modules/ensemble.html", "title": "1.11.7.1. Usage#", "page_title": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.11.7.1. Usage#", "content": "1.11.7.1. Usage# The following example shows how to fit an AdaBoost classifier with 100 weak learners: >>> from sklearn.model_selection import cross_val_score >>> from sklearn.datasets import load_iris >>> from sklearn.ensemble import AdaBoostClassifier >>> X, y = load_iris(return_X_y=True) >>> clf = AdaBoostClassifier(n_estimators=100) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores.mean() np.float64(0.95) The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split). Examples - Multi-class AdaBoosted Decision Treesshows the performance of AdaBoost on a multi-class problem. - Two-class AdaBoostshows the decision boundary and decision function values for a non-linearly separable two-class problem using AdaBoost-SAMME. - Decision Tree Regression with AdaBoostdemonstrates regression with the AdaBoost.R2 algorithm. References", "prev_chunk_id": "chunk_191", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_193", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12. Multiclass and multioutput algorithms#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12. Multiclass and multioutput algorithms#", "content": "1.12. Multiclass and multioutput algorithms# This section of the user guide covers functionality related to multi-learning problems, including multiclass, multilabel, and multioutput classification and regression. The modules in this section implement meta-estimators, which require a base estimator to be provided in their constructor. Meta-estimators extend the functionality of the base estimator to support multi-learning problems, which is accomplished by transforming the multi-learning problem into a set of simpler problems, then fitting one estimator per problem. This section covers two modules: sklearn.multiclass and sklearn.multioutput. The chart below demonstrates the problem types that each module is responsible for, and the corresponding meta-estimators that each module provides. The table below provides a quick reference on the differences between problem types. More detailed explanations can be found in subsequent sections of this guide. Below is a summary of scikit-learn estimators that have multi-learning support built-in, grouped by strategy. You don’t need the meta-estimators provided by this section if you’re using one of these estimators. However, meta-estimators can provide additional strategies beyond what is built-in: - Inherently multiclass:naive_bayes.BernoulliNBtree.DecisionTreeClassifiertree.ExtraTreeClassifierensemble.ExtraTreesClassifiernaive_bayes.GaussianNBneighbors.KNeighborsClassifiersemi_supervised.LabelPropagationsemi_supervised.LabelSpreadingdiscriminant_analysis.LinearDiscriminantAnalysissvm.LinearSVC(setting multi_class=”crammer_singer”)linear_model.LogisticRegression(with most solvers)linear_model.LogisticRegressionCV(with most solvers)neural_network.MLPClassifierneighbors.NearestCentroiddiscriminant_analysis.QuadraticDiscriminantAnalysisneighbors.RadiusNeighborsClassifierensemble.RandomForestClassifierlinear_model.RidgeClassifierlinear_model.RidgeClassifierCV - Multiclass as One-Vs-One:svm.NuSVCsvm.SVC.gaussian_process.GaussianProcessClassifier(setting multi_class = “one_vs_one”) - Multiclass as One-Vs-The-Rest:ensemble.GradientBoostingClassifiergaussian_process.GaussianProcessClassifier(setting multi_class = “one_vs_rest”)svm.LinearSVC(setting multi_class=”ovr”)linear_model.LogisticRegression(most solvers)linear_model.LogisticRegressionCV(most solvers)linear_model.SGDClassifierlinear_model.Perceptronlinear_model.PassiveAggressiveClassifier - Support multilabel:tree.DecisionTreeClassifiertree.ExtraTreeClassifierensemble.ExtraTreesClassifierneighbors.KNeighborsClassifierneural_network.MLPClassifierneighbors.RadiusNeighborsClassifierensemble.RandomForestClassifierlinear_model.RidgeClassifierlinear_model.RidgeClassifierCV - Support multiclass-multioutput:tree.DecisionTreeClassifiertree.ExtraTreeClassifierensemble.ExtraTreesClassifierneighbors.KNeighborsClassifierneighbors.RadiusNeighborsClassifierensemble.RandomForestClassifier", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_194", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1. Multiclass classification#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1. Multiclass classification#", "content": "1.12.1. Multiclass classification# Multiclass classification is a classification task with more than two classes. Each sample can only be labeled as one class. For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labeled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple. While all scikit-learn classifiers are capable of multiclass classification, the meta-estimators offered by sklearn.multiclass permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).", "prev_chunk_id": "chunk_193", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_195", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.1. Target format#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.1. Target format#", "content": "1.12.1.1. Target format# Valid multiclass representations for type_of_target (y) are: - 1d or column vector containing more than two discrete values. An example of a vectoryfor 4 samples:>>>importnumpyasnp>>>y=np.array(['apple','pear','apple','orange'])>>>print(y)['apple' 'pear' 'apple' 'orange'] - Dense or sparsebinarymatrix of shape(n_samples,n_classes)with a single sample per row, where each column represents one class. An example of both a dense and sparsebinarymatrixyfor 4 samples, where the columns, in order, are apple, orange, and pear:>>>importnumpyasnp>>>fromsklearn.preprocessingimportLabelBinarizer>>>y=np.array(['apple','pear','apple','orange'])>>>y_dense=LabelBinarizer().fit_transform(y)>>>print(y_dense)[[1 0 0][0 0 1][1 0 0][0 1 0]]>>>fromscipyimportsparse>>>y_sparse=sparse.csr_matrix(y_dense)>>>print(y_sparse)<Compressed Sparse Row sparse matrix of dtype 'int64'with 4 stored elements and shape (4, 3)>Coords Values(0, 0) 1(1, 2) 1(2, 0) 1(3, 1) 1 For more information about LabelBinarizer, refer to Transforming the prediction target (y).", "prev_chunk_id": "chunk_194", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_196", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.2. OneVsRestClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.2. OneVsRestClassifier#", "content": "1.12.1.2. OneVsRestClassifier# The one-vs-rest strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice. Below is an example of multiclass learning using OvR: >>> from sklearn import datasets >>> from sklearn.multiclass import OneVsRestClassifier >>> from sklearn.svm import LinearSVC >>> X, y = datasets.load_iris(return_X_y=True) >>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) OneVsRestClassifier also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell [i, j] indicates the presence of label j in", "prev_chunk_id": "chunk_195", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_197", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.2. OneVsRestClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.2. OneVsRestClassifier#", "content": "sample i. Examples - Multilabel classification - Plot classification probability - Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression", "prev_chunk_id": "chunk_196", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_198", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.3. OneVsOneClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.3. OneVsOneClassifier#", "content": "1.12.1.3. OneVsOneClassifier# OneVsOneClassifier constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. Since it requires to fit n_classes * (n_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don’t scale well with n_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times. The decision function is the result of a monotonic transformation of the one-versus-one classification. Below is an example of multiclass learning using OvO: >>> from sklearn import datasets >>> from sklearn.multiclass import OneVsOneClassifier >>> from sklearn.svm import LinearSVC >>> X, y = datasets.load_iris(return_X_y=True) >>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,", "prev_chunk_id": "chunk_197", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_199", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.3. OneVsOneClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.3. OneVsOneClassifier#", "content": "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) References - “Pattern Recognition and Machine Learning. Springer”, Christopher M. Bishop, page 183, (First Edition)", "prev_chunk_id": "chunk_198", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_200", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.4. OutputCodeClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.4. OutputCodeClassifier#", "content": "1.12.1.4. OutputCodeClassifier# Error-Correcting Output Code-based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in [3] although more elaborate methods may be added in the future. At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. In OutputCodeClassifier, the code_size attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since log2(n_classes) is much smaller than n_classes. A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name “error-correcting”. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging. Below is an example of multiclass learning using Output-Codes:", "prev_chunk_id": "chunk_199", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_201", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.1.4. OutputCodeClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.1.4. OutputCodeClassifier#", "content": ">>> from sklearn import datasets >>> from sklearn.multiclass import OutputCodeClassifier >>> from sklearn.svm import LinearSVC >>> X, y = datasets.load_iris(return_X_y=True) >>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0) >>> clf.fit(X, y).predict(X) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) References - “Solving multiclass learning problems via error-correcting output codes”, Dietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995. - “The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.", "prev_chunk_id": "chunk_200", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_202", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.2. Multilabel classification#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.2. Multilabel classification#", "content": "1.12.2. Multilabel classification# Multilabel classification (closely related to multioutput classification) is a classification task labeling each sample with m labels from n_classes possible classes, where m can be 0 to n_classes inclusive. This can be thought of as predicting properties of a sample that are not mutually exclusive. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running n_classes binary classification tasks, for example with MultiOutputClassifier. This approach treats each label independently whereas multilabel classifiers may treat the multiple classes simultaneously, accounting for correlated behavior among them. For example, prediction of the topics relevant to a text document or video. The document or video may be about one of ‘religion’, ‘politics’, ‘finance’ or ‘education’, several of the topic classes or all of the topic classes.", "prev_chunk_id": "chunk_201", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_203", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.2.1. Target format#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.2.1. Target format#", "content": "1.12.2.1. Target format# A valid representation of multilabel y is an either dense or sparse binary matrix of shape (n_samples, n_classes). Each column represents a class. The 1’s in each row denote the positive classes a sample has been labeled with. An example of a dense matrix y for 3 samples: >>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]]) >>> print(y) [[1 0 0 1] [0 0 1 1] [0 0 0 0]] Dense binary matrices can also be created using MultiLabelBinarizer. For more information, refer to Transforming the prediction target (y). An example of the same y in sparse matrix form: >>> y_sparse = sparse.csr_matrix(y) >>> print(y_sparse) <Compressed Sparse Row sparse matrix of dtype 'int64' with 4 stored elements and shape (3, 4)> Coords Values (0, 0) 1 (0, 3) 1 (1, 2) 1 (1, 3) 1", "prev_chunk_id": "chunk_202", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_204", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.2.2. MultiOutputClassifier#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.2.2. MultiOutputClassifier#", "content": "1.12.2.2. MultiOutputClassifier# Multilabel classification support can be added to any classifier with MultiOutputClassifier. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3…,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3…,yn). You can find a usage example for MultiOutputClassifier as part of the section on Multiclass-multioutput classification since it is a generalization of multilabel classification to multiclass outputs instead of binary outputs.", "prev_chunk_id": "chunk_203", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_205", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.2.3. ClassifierChain#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.2.3. ClassifierChain#", "content": "1.12.2.3. ClassifierChain# Classifier chains (see ClassifierChain) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets. For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number. When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features. Clearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together. References - Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, “Classifier Chains for Multi-label Classification”, 2009.", "prev_chunk_id": "chunk_204", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_206", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.3. Multiclass-multioutput classification#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.3. Multiclass-multioutput classification#", "content": "1.12.3. Multiclass-multioutput classification# Multiclass-multioutput classification (also known as multitask classification) is a classification task which labels each sample with a set of non-binary properties. Both the number of properties and the number of classes per property is greater than 2. A single estimator thus handles several joint classification tasks. This is both a generalization of the multilabel classification task, which only considers binary attributes, as well as a generalization of the multiclass classification task, where only one property is considered. For example, classification of the properties “type of fruit” and “colour” for a set of images of fruit. The property “type of fruit” has the possible classes: “apple”, “pear” and “orange”. The property “colour” has the possible classes: “green”, “red”, “yellow” and “orange”. Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property. Note that all classifiers handling multiclass-multioutput (also known as multitask classification) tasks, support the multilabel classification task as a special case. Multitask classification is similar to the multioutput classification task with different model formulations. For more information, see the relevant estimator documentation. Below is an example of multiclass-multioutput classification: >>> from sklearn.datasets import make_classification >>> from sklearn.multioutput import MultiOutputClassifier >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.utils import shuffle >>> import numpy as np >>> X, y1 = make_classification(n_samples=10, n_features=100, ... n_informative=30, n_classes=3, ... random_state=1) >>> y2 = shuffle(y1, random_state=1) >>> y3 = shuffle(y1, random_state=2) >>> Y = np.vstack((y1, y2, y3)).T >>> n_samples, n_features = X.shape # 10,100 >>> n_outputs = Y.shape[1] # 3 >>> n_classes = 3 >>> forest = RandomForestClassifier(random_state=1) >>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2) >>> multi_target_forest.fit(X, Y).predict(X) array([[2, 2, 0], [1, 2, 1], [2, 1, 0], [0, 0, 2], [0, 2, 1], [0, 0, 2], [1,", "prev_chunk_id": "chunk_205", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_207", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.3. Multiclass-multioutput classification#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.3. Multiclass-multioutput classification#", "content": "1, 0], [1, 1, 1], [0, 0, 2], [2, 0, 0]])", "prev_chunk_id": "chunk_206", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_208", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.3.1. Target format#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.3.1. Target format#", "content": "1.12.3.1. Target format# A valid representation of multioutput y is a dense matrix of shape (n_samples, n_classes) of class labels. A column wise concatenation of 1d multiclass variables. An example of y for 3 samples: >>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']]) >>> print(y) [['apple' 'green'] ['orange' 'orange'] ['pear' 'green']]", "prev_chunk_id": "chunk_207", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_209", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.4. Multioutput regression#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.4. Multioutput regression#", "content": "1.12.4. Multioutput regression# Multioutput regression predicts multiple numerical properties for each sample. Each property is a numerical variable and the number of properties to be predicted for each sample is greater than or equal to 2. Some estimators that support multioutput regression are faster than just running n_output estimators. For example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and direction would be output for each sample. The following regressors natively support multioutput regression: - cross_decomposition.CCA - tree.DecisionTreeRegressor - dummy.DummyRegressor - linear_model.ElasticNet - tree.ExtraTreeRegressor - ensemble.ExtraTreesRegressor - gaussian_process.GaussianProcessRegressor - neighbors.KNeighborsRegressor - kernel_ridge.KernelRidge - linear_model.Lars - linear_model.Lasso - linear_model.LassoLars - linear_model.LinearRegression - multioutput.MultiOutputRegressor - linear_model.MultiTaskElasticNet - linear_model.MultiTaskElasticNetCV - linear_model.MultiTaskLasso - linear_model.MultiTaskLassoCV - linear_model.OrthogonalMatchingPursuit - cross_decomposition.PLSCanonical - cross_decomposition.PLSRegression - linear_model.RANSACRegressor - neighbors.RadiusNeighborsRegressor - ensemble.RandomForestRegressor - multioutput.RegressorChain - linear_model.Ridge - linear_model.RidgeCV - compose.TransformedTargetRegressor", "prev_chunk_id": "chunk_208", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_210", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.4.1. Target format#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.4.1. Target format#", "content": "1.12.4.1. Target format# A valid representation of multioutput y is a dense matrix of shape (n_samples, n_output) of floats. A column wise concatenation of continuous variables. An example of y for 3 samples: >>> y = np.array([[31.4, 94], [40.5, 109], [25.0, 30]]) >>> print(y) [[ 31.4 94. ] [ 40.5 109. ] [ 25. 30. ]]", "prev_chunk_id": "chunk_209", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_211", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.4.2. MultiOutputRegressor#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.4.2. MultiOutputRegressor#", "content": "1.12.4.2. MultiOutputRegressor# Multioutput regression support can be added to any regressor with MultiOutputRegressor. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As MultiOutputRegressor fits one regressor per target it can not take advantage of correlations between targets. Below is an example of multioutput regression: >>> from sklearn.datasets import make_regression >>> from sklearn.multioutput import MultiOutputRegressor >>> from sklearn.ensemble import GradientBoostingRegressor >>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1) >>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X) array([[-154.75474165, -147.03498585, -50.03812219], [ 7.12165031, 5.12914884, -81.46081961], [-187.8948621 , -100.44373091, 13.88978285], [-141.62745778, 95.02891072, -191.48204257], [ 97.03260883, 165.34867495, 139.52003279], [ 123.92529176, 21.25719016, -7.84253 ], [-122.25193977, -85.16443186, -107.12274212], [ -30.170388 , -94.80956739, 12.16979946], [ 140.72667194, 176.50941682, -17.50447799], [ 149.37967282, -81.15699552, -5.72850319]])", "prev_chunk_id": "chunk_210", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_212", "url": "https://scikit-learn.org/stable/modules/multiclass.html", "title": "1.12.4.3. RegressorChain#", "page_title": "1.12. Multiclass and multioutput algorithms — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.12.4.3. RegressorChain#", "content": "1.12.4.3. RegressorChain# Regressor chains (see RegressorChain) is analogous to ClassifierChain as a way of combining a number of regressions into a single multi-target model that is capable of exploiting correlations among targets.", "prev_chunk_id": "chunk_211", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_213", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13. Feature selection#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13. Feature selection#", "content": "1.13. Feature selection# The classes in the sklearn.feature_selection module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_214", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.1. Removing features with low variance#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.1. Removing features with low variance#", "content": "1.13.1. Removing features with low variance# VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples. As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by so we can select using the threshold .8 * (1 - .8): >>> from sklearn.feature_selection import VarianceThreshold >>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]] >>> sel = VarianceThreshold(threshold=(.8 * (1 - .8))) >>> sel.fit_transform(X) array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) As expected, VarianceThreshold has removed the first column, which has a probability \\(p = 5/6 > .8\\) of containing a zero.", "prev_chunk_id": "chunk_213", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_215", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.2. Univariate feature selection#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.2. Univariate feature selection#", "content": "1.13.2. Univariate feature selection# Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method: - SelectKBestremoves all but the\\(k\\)highest scoring features - SelectPercentileremoves all but a user-specified highest scoring percentage of features - using common univariate statistical tests for each feature: false positive rateSelectFpr, false discovery rateSelectFdr, or family wise errorSelectFwe. - GenericUnivariateSelectallows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. For instance, we can use a F-test to retrieve the two best features for a dataset as follows: >>> from sklearn.datasets import load_iris >>> from sklearn.feature_selection import SelectKBest >>> from sklearn.feature_selection import f_classif >>> X, y = load_iris(return_X_y=True) >>> X.shape (150, 4) >>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y) >>> X_new.shape (150, 2) These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile): - For regression:r_regression,f_regression,mutual_info_regression - For classification:chi2,f_classif,mutual_info_classif The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. Note that the \\(\\chi^2\\)-test should only be applied to non-negative features, such as frequencies. Examples - Univariate Feature Selection - Comparison of F-test and mutual information", "prev_chunk_id": "chunk_214", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_216", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.3. Recursive feature elimination#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.3. Recursive feature elimination#", "content": "1.13.3. Recursive feature elimination# Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. Then, the least important features are pruned from the current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. RFECV performs RFE in a cross-validation loop to find the optimal number of features. In more details, the number of features selected is tuned automatically by fitting an RFE selector on the different cross-validation splits (provided by the cv parameter). The performance of the RFE selector is evaluated using scorer for different numbers of selected features and aggregated together. Finally, the scores are averaged across folds and the number of features selected is set to the number of features that maximize the cross-validation score. Examples - Recursive feature elimination: A recursive feature elimination example showing the relevance of pixels in a digit classification task. - Recursive feature elimination with cross-validation: A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.", "prev_chunk_id": "chunk_215", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_217", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.4. Feature selection using SelectFromModel#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.4. Feature selection using SelectFromModel#", "content": "1.13.4. Feature selection using SelectFromModel# SelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as coef_, feature_importances_) or via an importance_getter callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values is below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the max_features parameter to set a limit on the number of features to select. For examples on how it is to be used refer to the sections below. Examples - Model-based and sequential feature selection", "prev_chunk_id": "chunk_216", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_218", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.4.1. L1-based feature selection#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.4.1. L1-based feature selection#", "content": "1.13.4.1. L1-based feature selection# Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the Lasso for regression, and of LogisticRegression and LinearSVC for classification: >>> from sklearn.svm import LinearSVC >>> from sklearn.datasets import load_iris >>> from sklearn.feature_selection import SelectFromModel >>> X, y = load_iris(return_X_y=True) >>> X.shape (150, 4) >>> lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y) >>> model = SelectFromModel(lsvc, prefit=True) >>> X_new = model.transform(X) >>> X_new.shape (150, 3) With SVMs and logistic regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected. Examples - Lasso on dense and sparse data.", "prev_chunk_id": "chunk_217", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_219", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.4.2. Tree-based feature selection#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.4.2. Tree-based feature selection#", "content": "1.13.4.2. Tree-based feature selection# Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the SelectFromModel meta-transformer): >>> from sklearn.ensemble import ExtraTreesClassifier >>> from sklearn.datasets import load_iris >>> from sklearn.feature_selection import SelectFromModel >>> X, y = load_iris(return_X_y=True) >>> X.shape (150, 4) >>> clf = ExtraTreesClassifier(n_estimators=50) >>> clf = clf.fit(X, y) >>> clf.feature_importances_ array([ 0.04, 0.05, 0.4, 0.4]) >>> model = SelectFromModel(clf, prefit=True) >>> X_new = model.transform(X) >>> X_new.shape (150, 2) Examples - Feature importances with a forest of trees: example on synthetic data showing the recovery of the actually meaningful features. - Permutation Importance vs Random Forest Feature Importance (MDI): example discussing the caveats of using impurity-based feature importances as a proxy for feature relevance.", "prev_chunk_id": "chunk_218", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_220", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.5. Sequential Feature Selection#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.5. Sequential Feature Selection#", "content": "1.13.5. Sequential Feature Selection# Sequential Feature Selection [sfs] (SFS) is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward: Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero features and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter. Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no features and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used. Examples - Model-based and sequential feature selection", "prev_chunk_id": "chunk_219", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_221", "url": "https://scikit-learn.org/stable/modules/feature_selection.html", "title": "1.13.6. Feature selection as part of a pipeline#", "page_title": "1.13. Feature selection — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.13.6. Feature selection as part of a pipeline#", "content": "1.13.6. Feature selection as part of a pipeline# Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a Pipeline: clf = Pipeline([ ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))), ('classification', RandomForestClassifier()) ]) clf.fit(X, y) In this snippet we make use of a LinearSVC coupled with SelectFromModel to evaluate feature importances and select the most relevant features. Then, a RandomForestClassifier is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the Pipeline examples for more details.", "prev_chunk_id": "chunk_220", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_222", "url": "https://scikit-learn.org/stable/modules/semi_supervised.html", "title": "1.14. Semi-supervised learning#", "page_title": "1.14. Semi-supervised learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.14. Semi-supervised learning#", "content": "1.14. Semi-supervised learning# Semi-supervised learning is a situation in which in your training data some of the samples are not labeled. The semi-supervised estimators in sklearn.semi_supervised are able to make use of this additional unlabeled data to better capture the shape of the underlying data distribution and generalize better to new samples. These algorithms can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_223", "url": "https://scikit-learn.org/stable/modules/semi_supervised.html", "title": "1.14.1. Self Training#", "page_title": "1.14. Semi-supervised learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.14.1. Self Training#", "content": "1.14.1. Self Training# This self-training implementation is based on Yarowsky’s [1] algorithm. Using this algorithm, a given supervised classifier can function as a semi-supervised classifier, allowing it to learn from unlabeled data. SelfTrainingClassifier can be called with any classifier that implements predict_proba, passed as the parameter estimator. In each iteration, the estimator predicts labels for the unlabeled samples and adds a subset of these labels to the labeled dataset. The choice of this subset is determined by the selection criterion. This selection can be done using a threshold on the prediction probabilities, or by choosing the k_best samples according to the prediction probabilities. The labels used for the final fit as well as the iteration in which each sample was labeled are available as attributes. The optional max_iter parameter specifies how many times the loop is executed at most. The max_iter parameter may be set to None, causing the algorithm to iterate until all samples have labels or no new samples are selected in that iteration. Examples - Effect of varying threshold for self-training - Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset References", "prev_chunk_id": "chunk_222", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_224", "url": "https://scikit-learn.org/stable/modules/semi_supervised.html", "title": "1.14.2. Label Propagation#", "page_title": "1.14. Semi-supervised learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.14.2. Label Propagation#", "content": "1.14.2. Label Propagation# Label propagation denotes a few variations of semi-supervised graph inference algorithms. scikit-learn provides two label propagation models: LabelPropagation and LabelSpreading. Both work by constructing a similarity graph over all items in the input dataset. LabelPropagation and LabelSpreading differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The LabelPropagation algorithm performs hard clamping of input labels, which means \\(\\alpha=0\\). This clamping factor can be relaxed, to say \\(\\alpha=0.2\\), which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent. LabelPropagation uses the raw similarity matrix constructed from the data with no modifications. In contrast, LabelSpreading minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering. Label propagation models have two built-in kernel methods. Choice of kernel affects both scalability and performance of the algorithms. The following are available: - rbf (\\(\\exp(-\\gamma |x-y|^2), \\gamma > 0\\)).\\(\\gamma\\)is specified by keyword gamma. - knn (\\(1[x' \\in kNN(x)]\\)).\\(k\\)is specified by keyword n_neighbors. The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.", "prev_chunk_id": "chunk_223", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_225", "url": "https://scikit-learn.org/stable/modules/semi_supervised.html", "title": "1.14.2. Label Propagation#", "page_title": "1.14. Semi-supervised learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.14.2. Label Propagation#", "content": "Examples - Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset - Label Propagation circles: Learning a complex structure - Label Propagation digits: Demonstrating performance - Label Propagation digits: Active learning References [2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216 [3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 https://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf", "prev_chunk_id": "chunk_224", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_226", "url": "https://scikit-learn.org/stable/modules/isotonic.html", "title": "1.15. Isotonic regression#", "page_title": "1.15. Isotonic regression — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.15. Isotonic regression#", "content": "1.15. Isotonic regression# The class IsotonicRegression fits a non-decreasing real function to 1-dimensional data. It solves the following problem: subject to \\(\\hat{y}_i \\le \\hat{y}_j\\) whenever \\(X_i \\le X_j\\), where the weights \\(w_i\\) are strictly positive, and both X and y are arbitrary real quantities. The increasing parameter changes the constraint to \\(\\hat{y}_i \\ge \\hat{y}_j\\) whenever \\(X_i \\le X_j\\). Setting it to ‘auto’ will automatically choose the constraint based on Spearman’s rank correlation coefficient. IsotonicRegression produces a series of predictions \\(\\hat{y}_i\\) for the training data which are the closest to the targets \\(y\\) in terms of mean squared error. These predictions are interpolated for predicting to unseen data. The predictions of IsotonicRegression thus form a function that is piecewise linear: Examples - Isotonic Regression", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_227", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16. Probability calibration#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16. Probability calibration#", "content": "1.16. Probability calibration# When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction (e.g., some instances of SGDClassifier). The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to, say, 0.8, approximately 80% actually belong to the positive class. Before we show how to re-calibrate a classifier, we first need a way to detect how good a classifier is calibrated.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_228", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.1. Calibration curves#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.1. Calibration curves#", "content": "1.16.1. Calibration curves# Calibration curves, also referred to as reliability diagrams (Wilks 1995 [2]), compare how well the probabilistic predictions of a binary classifier are calibrated. It plots the frequency of the positive label (to be more precise, an estimation of the conditional event probability \\(P(Y=1|\\text{predict_proba})\\)) on the y-axis against the predicted probability predict_proba of a model on the x-axis. The tricky part is to get values for the y-axis. In scikit-learn, this is accomplished by binning the predictions such that the x-axis represents the average predicted probability in each bin. The y-axis is then the fraction of positives given the predictions of that bin, i.e. the proportion of samples whose class is the positive class (in each bin). The top calibration curve plot is created with CalibrationDisplay.from_estimator, which uses calibration_curve to calculate the per bin average predicted probabilities and fraction of positives. CalibrationDisplay.from_estimator takes as input a fitted classifier, which is used to calculate the predicted probabilities. The classifier thus must have predict_proba method. For the few classifiers that do not have a predict_proba method, it is possible to use CalibratedClassifierCV to calibrate the classifier outputs to probabilities. The bottom histogram gives some insight into the behavior of each classifier by showing the number of samples in each predicted probability bin. LogisticRegression is more likely to return well calibrated predictions by itself as it has a canonical link function for its loss, i.e. the logit-link for the Log loss. In the unpenalized case, this leads to the so-called balance property, see [8] and Logistic regression. In the plot above, data is generated according to a linear mechanism, which is consistent with the LogisticRegression model (the model is ‘well specified’), and the value of the regularization parameter C is tuned to be appropriate (neither too strong nor too low). As", "prev_chunk_id": "chunk_227", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_229", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.1. Calibration curves#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.1. Calibration curves#", "content": "a consequence, this model returns accurate predictions from its predict_proba method. In contrast to that, the other shown models return biased probabilities; with different biases per model. GaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features. RandomForestClassifier shows the opposite behavior: the histograms show peaks at probabilities approximately 0.2 and 0.9, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana [3]: “Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict \\(p = 0\\) for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting.” As a result, the calibration curve shows a characteristic sigmoid shape, indicating that the classifier could trust its “intuition” more and return probabilities closer to 0 or 1 typically. LinearSVC (SVC) shows an even", "prev_chunk_id": "chunk_228", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_230", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.1. Calibration curves#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.1. Calibration curves#", "content": "more sigmoid curve than the random forest, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which focus on difficult to classify samples that are close to the decision boundary (the support vectors).", "prev_chunk_id": "chunk_229", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_231", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.2. Calibrating a classifier#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.2. Calibrating a classifier#", "content": "1.16.2. Calibrating a classifier# Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by decision_function or predict_proba) to a calibrated probability in [0, 1]. Denoting the output of the classifier for a given sample by \\(f_i\\), the calibrator tries to predict the conditional event probability \\(P(y_i = 1 | f_i)\\). Ideally, the calibrator is fit on a dataset independent of the training data used to fit the classifier in the first place. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.", "prev_chunk_id": "chunk_230", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_232", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.3. Usage#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.3. Usage#", "content": "1.16.3. Usage# The CalibratedClassifierCV class is used to calibrate a classifier. CalibratedClassifierCV uses a cross-validation approach to ensure unbiased data is always used to fit the calibrator. The data is split into \\(k\\) (train_set, test_set) couples (as determined by cv). When ensemble=True (default), the following procedure is repeated independently for each cross-validation split: - a clone ofbase_estimatoris trained on the train subset - the trainedbase_estimatormakes predictions on the test subset - the predictions are used to fit a calibrator (either a sigmoid or isotonic regressor) (when the data is multiclass, a calibrator is fit for every class) This results in an ensemble of \\(k\\) (classifier, calibrator) couples where each calibrator maps the output of its corresponding classifier into [0, 1]. Each couple is exposed in the calibrated_classifiers_ attribute, where each entry is a calibrated classifier with a predict_proba method that outputs calibrated probabilities. The output of predict_proba for the main CalibratedClassifierCV instance corresponds to the average of the predicted probabilities of the \\(k\\) estimators in the calibrated_classifiers_ list. The output of predict is the class that has the highest probability. It is important to choose cv carefully when using ensemble=True. All classes should be present in both train and test subsets for every split. When a class is absent in the train subset, the predicted probability for that class will default to 0 for the (classifier, calibrator) couple of that split. This skews the predict_proba as it averages across all couples. When a class is absent in the test subset, the calibrator for that class (within the (classifier, calibrator) couple of that split) is fit on data with no positive class. This results in ineffective calibration. When ensemble=False, cross-validation is used to obtain ‘unbiased’ predictions for all the data, via cross_val_predict. These unbiased predictions are then used to train the", "prev_chunk_id": "chunk_231", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_233", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.3. Usage#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.3. Usage#", "content": "calibrator. The attribute calibrated_classifiers_ consists of only one (classifier, calibrator) couple where the classifier is the base_estimator trained on all the data. In this case the output of predict_proba for CalibratedClassifierCV is the predicted probabilities obtained from the single (classifier, calibrator) couple. The main advantage of ensemble=True is to benefit from the traditional ensembling effect (similar to Bagging meta-estimator). The resulting ensemble should both be well calibrated and slightly more accurate than with ensemble=False. The main advantage of using ensemble=False is computational: it reduces the overall fit time by training only a single base classifier and calibrator pair, decreases the final model size and increases prediction speed. Alternatively an already fitted classifier can be calibrated by using a FrozenEstimator as CalibratedClassifierCV(estimator=FrozenEstimator(estimator)). It is up to the user to make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor. CalibratedClassifierCV supports the use of two regression techniques for calibration via the method parameter: \"sigmoid\" and \"isotonic\".", "prev_chunk_id": "chunk_232", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_234", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.3.1. Sigmoid#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.3.1. Sigmoid#", "content": "1.16.3.1. Sigmoid# The sigmoid regressor, method=\"sigmoid\" is based on Platt’s logistic model [4]: where \\(y_i\\) is the true label of sample \\(i\\) and \\(f_i\\) is the output of the un-calibrated classifier for sample \\(i\\). \\(A\\) and \\(B\\) are real numbers to be determined when fitting the regressor via maximum likelihood. The sigmoid method assumes the calibration curve can be corrected by applying a sigmoid function to the raw predictions. This assumption has been empirically justified in the case of Support Vector Machines with common kernel functions on various benchmark datasets in section 2.1 of Platt 1999 [4] but does not necessarily hold in general. Additionally, the logistic model works best if the calibration error is symmetrical, meaning the classifier output for each binary class is normally distributed with the same variance [7]. This can be a problem for highly imbalanced classification problems, where outputs do not have equal variance. In general this method is most effective for small sample sizes or when the un-calibrated model is under-confident and has similar calibration errors for both high and low outputs.", "prev_chunk_id": "chunk_233", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_235", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.3.2. Isotonic#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.3.2. Isotonic#", "content": "1.16.3.2. Isotonic# The method=\"isotonic\" fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function, see sklearn.isotonic. It minimizes: subject to \\(\\hat{f}_i \\geq \\hat{f}_j\\) whenever \\(f_i \\geq f_j\\). \\(y_i\\) is the true label of sample \\(i\\) and \\(\\hat{f}_i\\) is the output of the calibrated classifier for sample \\(i\\) (i.e., the calibrated probability). This method is more general when compared to 'sigmoid' as the only restriction is that the mapping function is monotonically increasing. It is thus more powerful as it can correct any monotonic distortion of the un-calibrated model. However, it is more prone to overfitting, especially on small datasets [6]. Overall, 'isotonic' will perform as well as or better than 'sigmoid' when there is enough data (greater than ~ 1000 samples) to avoid overfitting [3].", "prev_chunk_id": "chunk_234", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_236", "url": "https://scikit-learn.org/stable/modules/calibration.html", "title": "1.16.3.3. Multiclass support#", "page_title": "1.16. Probability calibration — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.16.3.3. Multiclass support#", "content": "1.16.3.3. Multiclass support# Both isotonic and sigmoid regressors only support 1-dimensional data (e.g., binary classification output) but are extended for multiclass classification if the base_estimator supports multiclass predictions. For multiclass predictions, CalibratedClassifierCV calibrates for each class separately in a OneVsRestClassifier fashion [5]. When predicting probabilities, the calibrated probabilities for each class are predicted separately. As those probabilities do not necessarily sum to one, a postprocessing is performed to normalize them. Examples - Probability Calibration curves - Probability Calibration for 3-class classification - Probability calibration of classifiers - Comparison of Calibration of Classifiers References", "prev_chunk_id": "chunk_235", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_237", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1. Array API support (experimental)#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1. Array API support (experimental)#", "content": "12.1. Array API support (experimental)# The Array API specification defines a standard API for all array manipulation libraries with a NumPy-like API. Scikit-learn vendors pinned copies of array-api-compat and array-api-extra. Scikit-learn’s support for the array API standard requires the environment variable SCIPY_ARRAY_API to be set to 1 before importing scipy and scikit-learn: export SCIPY_ARRAY_API=1 Please note that this environment variable is intended for temporary use. For more details, refer to SciPy’s Array API documentation. Some scikit-learn estimators that primarily rely on NumPy (as opposed to using Cython) to implement the algorithmic logic of their fit, predict or transform methods can be configured to accept any Array API compatible input data structures and automatically dispatch operations to the underlying namespace instead of relying on NumPy. At this stage, this support is considered experimental and must be enabled explicitly by the array_api_dispatch configuration. See below for details. The following video provides an overview of the standard’s design principles and how it facilitates interoperability between array libraries: - Scikit-learn on GPUs with Array APIbyThomas Fanat PyData NYC 2023.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_238", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.1. Example usage#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.1. Example usage#", "content": "12.1.1. Example usage# The configuration array_api_dispatch=True needs to be set to True to enable array API support. We recommend setting this configuration globally to ensure consistent behaviour and prevent accidental mixing of array namespaces. Note that we set it with config_context below to avoid having to call set_config(array_api_dispatch=False) at the end of every code snippet that uses the array API. The example code snippet below demonstrates how to use CuPy to run LinearDiscriminantAnalysis on a GPU: >>> from sklearn.datasets import make_classification >>> from sklearn import config_context >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis >>> import cupy >>> X_np, y_np = make_classification(random_state=0) >>> X_cu = cupy.asarray(X_np) >>> y_cu = cupy.asarray(y_np) >>> X_cu.device <CUDA Device 0> >>> with config_context(array_api_dispatch=True): ... lda = LinearDiscriminantAnalysis() ... X_trans = lda.fit_transform(X_cu, y_cu) >>> X_trans.device <CUDA Device 0> After the model is trained, fitted attributes that are arrays will also be from the same Array API namespace as the training data. For example, if CuPy’s Array API namespace was used for training, then fitted attributes will be on the GPU. We provide an experimental _estimator_with_converted_arrays utility that transfers an estimator attributes from Array API to a ndarray: >>> from sklearn.utils._array_api import _estimator_with_converted_arrays >>> cupy_to_ndarray = lambda array : array.get() >>> lda_np = _estimator_with_converted_arrays(lda, cupy_to_ndarray) >>> X_trans = lda_np.transform(X_np) >>> type(X_trans) <class 'numpy.ndarray'>", "prev_chunk_id": "chunk_237", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_239", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.1.1. PyTorch Support#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.1.1. PyTorch Support#", "content": "12.1.1.1. PyTorch Support# PyTorch Tensors can also be passed directly: >>> import torch >>> X_torch = torch.asarray(X_np, device=\"cuda\", dtype=torch.float32) >>> y_torch = torch.asarray(y_np, device=\"cuda\", dtype=torch.float32) >>> with config_context(array_api_dispatch=True): ... lda = LinearDiscriminantAnalysis() ... X_trans = lda.fit_transform(X_torch, y_torch) >>> type(X_trans) <class 'torch.Tensor'> >>> X_trans.device.type 'cuda'", "prev_chunk_id": "chunk_238", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_240", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.2. Support for Array API-compatible inputs#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.2. Support for Array API-compatible inputs#", "content": "12.1.2. Support for Array API-compatible inputs# Estimators and other tools in scikit-learn that support Array API compatible inputs.", "prev_chunk_id": "chunk_239", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_241", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.2.1. Estimators#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.2.1. Estimators#", "content": "12.1.2.1. Estimators# - decomposition.PCA(withsvd_solver=\"full\",svd_solver=\"randomized\"andpower_iteration_normalizer=\"QR\") - linear_model.Ridge(withsolver=\"svd\") - discriminant_analysis.LinearDiscriminantAnalysis(withsolver=\"svd\") - preprocessing.Binarizer - preprocessing.KernelCenterer - preprocessing.LabelEncoder - preprocessing.MaxAbsScaler - preprocessing.MinMaxScaler - preprocessing.Normalizer", "prev_chunk_id": "chunk_240", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_242", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.2.2. Meta-estimators#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.2.2. Meta-estimators#", "content": "12.1.2.2. Meta-estimators# Meta-estimators that accept Array API inputs conditioned on the fact that the base estimator also does: - model_selection.GridSearchCV - model_selection.RandomizedSearchCV - model_selection.HalvingGridSearchCV - model_selection.HalvingRandomSearchCV", "prev_chunk_id": "chunk_241", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_243", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.2.3. Metrics#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.2.3. Metrics#", "content": "12.1.2.3. Metrics# - sklearn.metrics.cluster.entropy - sklearn.metrics.accuracy_score - sklearn.metrics.d2_tweedie_score - sklearn.metrics.explained_variance_score - sklearn.metrics.f1_score - sklearn.metrics.fbeta_score - sklearn.metrics.hamming_loss - sklearn.metrics.jaccard_score - sklearn.metrics.max_error - sklearn.metrics.mean_absolute_error - sklearn.metrics.mean_absolute_percentage_error - sklearn.metrics.mean_gamma_deviance - sklearn.metrics.mean_pinball_loss - sklearn.metrics.mean_poisson_deviance(requiresenabling array API support for SciPy) - sklearn.metrics.mean_squared_error - sklearn.metrics.mean_squared_log_error - sklearn.metrics.mean_tweedie_deviance - sklearn.metrics.multilabel_confusion_matrix - sklearn.metrics.pairwise.additive_chi2_kernel - sklearn.metrics.pairwise.chi2_kernel - sklearn.metrics.pairwise.cosine_similarity - sklearn.metrics.pairwise.cosine_distances - sklearn.metrics.pairwise.euclidean_distances(seeNote on device support for float64) - sklearn.metrics.pairwise.linear_kernel - sklearn.metrics.pairwise.paired_cosine_distances - sklearn.metrics.pairwise.paired_euclidean_distances - sklearn.metrics.pairwise.polynomial_kernel - sklearn.metrics.pairwise.rbf_kernel(seeNote on device support for float64) - sklearn.metrics.pairwise.sigmoid_kernel - sklearn.metrics.precision_score - sklearn.metrics.precision_recall_fscore_support - sklearn.metrics.r2_score - sklearn.metrics.recall_score - sklearn.metrics.root_mean_squared_error - sklearn.metrics.root_mean_squared_log_error - sklearn.metrics.zero_one_loss", "prev_chunk_id": "chunk_242", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_244", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.2.4. Tools#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.2.4. Tools#", "content": "12.1.2.4. Tools# - model_selection.train_test_split - utils.check_consistent_length Coverage is expected to grow over time. Please follow the dedicated meta-issue on GitHub to track progress.", "prev_chunk_id": "chunk_243", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_245", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.3. Input and output array type handling#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.3. Input and output array type handling#", "content": "12.1.3. Input and output array type handling# Estimators and scoring functions are able to accept input arrays from different array libraries and/or devices. When a mixed set of input arrays is passed, scikit-learn converts arrays as needed to make them all consistent. For estimators, the rule is “everything follows `X`” - mixed array inputs are converted so that they all match the array library and device of X. For scoring functions the rule is “everything follows `y_pred`” - mixed array inputs are converted so that they all match the array library and device of y_pred. When a function or method has been called with array API compatible inputs, the convention is to return arrays from the same array library and on the same device as the input data.", "prev_chunk_id": "chunk_244", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_246", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.3.1. Estimators#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.3.1. Estimators#", "content": "12.1.3.1. Estimators# When an estimator is fitted with an array API compatible X, all other array inputs, including constructor arguments, (e.g., y, sample_weight) will be converted to match the array library and device of X, if they do not already. This behaviour enables switching from processing on the CPU to processing on the GPU at any point within a pipeline. This allows estimators to accept mixed input types, enabling X to be moved to a different device within a pipeline, without explicitly moving y. Note that scikit-learn pipelines do not allow transformation of y (to avoid leakage). Take for example a pipeline where X and y both start on CPU, and go through the following three steps: - TargetEncoder, which will transform categorialXbut also requiresy, meaning bothXandyneed to be on CPU. - FunctionTransformer(func=partial(torch.asarray,device=\"cuda\")), which movesXto GPU, to improve performance in the next step. - Ridge, whose performance can be improved when passed arrays on a GPU, as they can handle large matrix operations very efficiently. X initially contains categorical string data (thus needs to be on CPU), which is target encoded to numerical values in TargetEncoder. X is then explicitly moved to GPU to improve the performance of Ridge. y cannot be transformed by the pipeline (recall scikit-learn pipelines do not allow transformation of y) but as Ridge is able to accept mixed input types, this is not a problem and the pipeline is able to be run. The fitted attributes of an estimator fitted with an array API compatible X, will be arrays from the same library as the input and stored on the same device. The predict and transform method subsequently expect inputs from the same array library and device as the data passed to the fit method.", "prev_chunk_id": "chunk_245", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_247", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.3.2. Scoring functions#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.3.2. Scoring functions#", "content": "12.1.3.2. Scoring functions# When an array API compatible y_pred is passed to a scoring function, all other array inputs (e.g., y_true, sample_weight) will be converted to match the array library and device of y_pred, if they do not already. This allows scoring functions to accept mixed input types, enabling them to be used within a meta-estimator (or function that accepts estimators), with a pipeline that moves input arrays between devices (e.g., CPU to GPU). For example, to be able to use the pipeline described above within e.g., cross_validate or GridSearchCV, the scoring function internally called needs to be able to accept mixed input types. The output type of scoring functions depends on the number of output values. When a scoring function returns a scalar value, it will return a Python scalar (typically a float instance) instead of an array scalar value. For scoring functions that support multiclass or multioutput, an array from the same array library and device as y_pred will be returned when multiple values need to be output.", "prev_chunk_id": "chunk_246", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_248", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.4. Common estimator checks#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.4. Common estimator checks#", "content": "12.1.4. Common estimator checks# Add the array_api_support tag to an estimator’s set of tags to indicate that it supports the array API. This will enable dedicated checks as part of the common tests to verify that the estimators’ results are the same when using vanilla NumPy and array API inputs. To run these checks you need to install array-api-strict in your test environment. This allows you to run checks without having a GPU. To run the full set of checks you also need to install PyTorch, CuPy and have a GPU. Checks that can not be executed or have missing dependencies will be automatically skipped. Therefore it’s important to run the tests with the -v flag to see which checks are skipped: pip install array-api-strict # and other libraries as needed pytest -k \"array_api\" -v Running the scikit-learn tests against array-api-strict should help reveal most code problems related to handling multiple device inputs via the use of simulated non-CPU devices. This allows for fast iterative development and debugging of array API related code. However, to ensure full handling of PyTorch or CuPy inputs allocated on actual GPU devices, it is necessary to run the tests against those libraries and hardware. This can either be achieved by using Google Colab or leveraging our CI infrastructure on pull requests (manually triggered by maintainers for cost reasons).", "prev_chunk_id": "chunk_247", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_249", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.4.1. Note on MPS device support#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.4.1. Note on MPS device support#", "content": "12.1.4.1. Note on MPS device support# On macOS, PyTorch can use the Metal Performance Shaders (MPS) to access hardware accelerators (e.g. the internal GPU component of the M1 or M2 chips). However, the MPS device support for PyTorch is incomplete at the time of writing. See the following github issue for more details: - pytorch/pytorch#77764 To enable the MPS support in PyTorch, set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1 before running the tests: PYTORCH_ENABLE_MPS_FALLBACK=1 pytest -k \"array_api\" -v At the time of writing all scikit-learn tests should pass, however, the computational speed is not necessarily better than with the CPU device.", "prev_chunk_id": "chunk_248", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_250", "url": "https://scikit-learn.org/stable/modules/array_api.html", "title": "12.1.4.2. Note on device support for float64#", "page_title": "12.1. Array API support (experimental) — scikit-learn 1.7.1 documentation", "breadcrumbs": "12.1.4.2. Note on device support for float64#", "content": "12.1.4.2. Note on device support for float64# Certain operations within scikit-learn will automatically perform operations on floating-point values with float64 precision to prevent overflows and ensure correctness (e.g., metrics.pairwise.euclidean_distances). However, certain combinations of array namespaces and devices, such as PyTorch on MPS (see Note on MPS device support) do not support the float64 data type. In these cases, scikit-learn will revert to using the float32 data type instead. This can result in different behavior (typically numerically unstable results) compared to not using array API dispatching or using a device with float64 support.", "prev_chunk_id": "chunk_249", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_251", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.experimental.enable_iterative_imputer.html", "title": "enable_iterative_imputer#", "page_title": "enable_iterative_imputer — scikit-learn 1.7.1 documentation", "breadcrumbs": "enable_iterative_imputer#", "content": "enable_iterative_imputer# Enables IterativeImputer The API and results of this estimator might change without any deprecation cycle. Importing this file dynamically sets IterativeImputer as an attribute of the impute module: >>> # explicitly require this experimental feature >>> from sklearn.experimental import enable_iterative_imputer # noqa >>> # now you can import normally from impute >>> from sklearn.impute import IterativeImputer", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_252", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.experimental.enable_halving_search_cv.html", "title": "enable_halving_search_cv#", "page_title": "enable_halving_search_cv — scikit-learn 1.7.1 documentation", "breadcrumbs": "enable_halving_search_cv#", "content": "enable_halving_search_cv# Enables Successive Halving search-estimators The API and results of these estimators might change without any deprecation cycle. Importing this file dynamically sets the HalvingRandomSearchCV and HalvingGridSearchCV as attributes of the model_selection module: >>> # explicitly require this experimental feature >>> from sklearn.experimental import enable_halving_search_cv # noqa >>> # now you can import normally from model_selection >>> from sklearn.model_selection import HalvingRandomSearchCV >>> from sklearn.model_selection import HalvingGridSearchCV The # noqa comment comment can be removed: it just tells linters like flake8 to ignore the import, which appears as unused.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_253", "url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "title": "7.9. Transforming the prediction target (y)#", "page_title": "7.9. Transforming the prediction target (y) — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.9. Transforming the prediction target (y)#", "content": "7.9. Transforming the prediction target (y)# These are transformers that are not intended to be used on features, only on supervised learning targets. See also Transforming target in regression if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_254", "url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "title": "7.9.1.1. LabelBinarizer#", "page_title": "7.9. Transforming the prediction target (y) — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.9.1.1. LabelBinarizer#", "content": "7.9.1.1. LabelBinarizer# LabelBinarizer is a utility class to help create a label indicator matrix from a list of multiclass labels: >>> from sklearn import preprocessing >>> lb = preprocessing.LabelBinarizer() >>> lb.fit([1, 2, 6, 4, 2]) LabelBinarizer() >>> lb.classes_ array([1, 2, 4, 6]) >>> lb.transform([1, 6]) array([[1, 0, 0, 0], [0, 0, 0, 1]]) Using this format can enable multiclass classification in estimators that support the label indicator matrix format. For more information about multiclass classification, refer to Multiclass classification.", "prev_chunk_id": "chunk_253", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_255", "url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "title": "7.9.1.2. MultiLabelBinarizer#", "page_title": "7.9. Transforming the prediction target (y) — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.9.1.2. MultiLabelBinarizer#", "content": "7.9.1.2. MultiLabelBinarizer# In multilabel learning, the joint set of binary classification tasks is expressed with a label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values where the one, i.e. the non zero elements, corresponds to the subset of labels for that sample. An array such as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample. Producing multilabel data as a list of sets of labels may be more intuitive. The MultiLabelBinarizer transformer can be used to convert between a collection of collections of labels and the indicator format: >>> from sklearn.preprocessing import MultiLabelBinarizer >>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] >>> MultiLabelBinarizer().fit_transform(y) array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]) For more information about multilabel classification, refer to Multilabel classification.", "prev_chunk_id": "chunk_254", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_256", "url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "title": "7.9.2. Label encoding#", "page_title": "7.9. Transforming the prediction target (y) — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.9.2. Label encoding#", "content": "7.9.2. Label encoding# LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. This is sometimes useful for writing efficient Cython routines. LabelEncoder can be used as follows: >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels: >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) [np.str_('amsterdam'), np.str_('paris'), np.str_('tokyo')] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]) >>> list(le.inverse_transform([2, 2, 1])) [np.str_('tokyo'), np.str_('tokyo'), np.str_('paris')]", "prev_chunk_id": "chunk_255", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_257", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8. Pairwise metrics, Affinities and Kernels#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8. Pairwise metrics, Affinities and Kernels#", "content": "7.8. Pairwise metrics, Affinities and Kernels# The sklearn.metrics.pairwise submodule implements utilities to evaluate pairwise distances or affinity of sets of samples. This module contains both distance metrics and kernels. A brief summary is given on the two here. Distance metrics are functions d(a, b) such that d(a, b) < d(a, c) if objects a and b are considered “more similar” than objects a and c. Two objects exactly alike would have a distance of zero. One of the most popular examples is Euclidean distance. To be a ‘true’ metric, it must obey the following four conditions: 1. d(a, b) >= 0, for all a and b 2. d(a, b) == 0, if and only if a = b, positive definiteness 3. d(a, b) == d(b, a), symmetry 4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality Kernels are measures of similarity, i.e. s(a, b) > s(a, c) if objects a and b are considered “more similar” than objects a and c. A kernel must also be positive semi-definite. There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let D be the distance, and S be the kernel: - S=np.exp(-D*gamma), where one heuristic for choosinggammais1/num_features - S=1./(D/np.max(D)) The distances between the row vectors of X and the row vectors of Y can be evaluated using pairwise_distances. If Y is omitted the pairwise distances of the row vectors of X are calculated. Similarly, pairwise.pairwise_kernels can be used to calculate the kernel between X and Y using different kernel functions. See the API reference for more details. >>> import numpy as np >>> from sklearn.metrics import pairwise_distances >>> from sklearn.metrics.pairwise import pairwise_kernels >>> X = np.array([[2, 3], [3, 5], [5, 8]]) >>> Y = np.array([[1, 0], [2, 1]]) >>>", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_258", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8. Pairwise metrics, Affinities and Kernels#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8. Pairwise metrics, Affinities and Kernels#", "content": "pairwise_distances(X, Y, metric='manhattan') array([[ 4., 2.], [ 7., 5.], [12., 10.]]) >>> pairwise_distances(X, metric='manhattan') array([[0., 3., 8.], [3., 0., 5.], [8., 5., 0.]]) >>> pairwise_kernels(X, Y, metric='linear') array([[ 2., 7.], [ 3., 11.], [ 5., 18.]])", "prev_chunk_id": "chunk_257", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_259", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.1. Cosine similarity#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.1. Cosine similarity#", "content": "7.8.1. Cosine similarity# cosine_similarity computes the L2-normalized dot product of vectors. That is, if \\(x\\) and \\(y\\) are row vectors, their cosine similarity \\(k\\) is defined as: This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors. This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. cosine_similarity accepts scipy.sparse matrices. (Note that the tf-idf functionality in sklearn.feature_extraction.text can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel, only slower.) References - C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press.https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html", "prev_chunk_id": "chunk_258", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_260", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.2. Linear kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.2. Linear kernel#", "content": "7.8.2. Linear kernel# The function linear_kernel computes the linear kernel, that is, a special case of polynomial_kernel with degree=1 and coef0=0 (homogeneous). If x and y are column vectors, their linear kernel is:", "prev_chunk_id": "chunk_259", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_261", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.3. Polynomial kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.3. Polynomial kernel#", "content": "7.8.3. Polynomial kernel# The function polynomial_kernel computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernel considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction. The polynomial kernel is defined as: where: - x,yare the input vectors - dis the kernel degree If \\(c_0 = 0\\) the kernel is said to be homogeneous.", "prev_chunk_id": "chunk_260", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_262", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.4. Sigmoid kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.4. Sigmoid kernel#", "content": "7.8.4. Sigmoid kernel# The function sigmoid_kernel computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as: where: - x,yare the input vectors - \\(\\gamma\\)is known as slope - \\(c_0\\)is known as intercept", "prev_chunk_id": "chunk_261", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_263", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.5. RBF kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.5. RBF kernel#", "content": "7.8.5. RBF kernel# The function rbf_kernel computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as: where x and y are the input vectors. If \\(\\gamma = \\sigma^{-2}\\) the kernel is known as the Gaussian kernel of variance \\(\\sigma^2\\).", "prev_chunk_id": "chunk_262", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_264", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.6. Laplacian kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.6. Laplacian kernel#", "content": "7.8.6. Laplacian kernel# The function laplacian_kernel is a variant on the radial basis function kernel defined as: where x and y are the input vectors and \\(\\|x-y\\|_1\\) is the Manhattan distance between the input vectors. It has proven useful in ML applied to noiseless data. See e.g. Machine learning for quantum mechanics in a nutshell.", "prev_chunk_id": "chunk_263", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_265", "url": "https://scikit-learn.org/stable/modules/metrics.html", "title": "7.8.7. Chi-squared kernel#", "page_title": "7.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.8.7. Chi-squared kernel#", "content": "7.8.7. Chi-squared kernel# The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using chi2_kernel and then passed to an SVC with kernel=\"precomputed\": >>> from sklearn.svm import SVC >>> from sklearn.metrics.pairwise import chi2_kernel >>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]] >>> y = [0, 1, 0, 1] >>> K = chi2_kernel(X, gamma=.5) >>> K array([[1. , 0.36787944, 0.89483932, 0.58364548], [0.36787944, 1. , 0.51341712, 0.83822343], [0.89483932, 0.51341712, 1. , 0.7768366 ], [0.58364548, 0.83822343, 0.7768366 , 1. ]]) >>> svm = SVC(kernel='precomputed').fit(K, y) >>> svm.predict(K) array([0, 1, 0, 1]) It can also be directly used as the kernel argument: >>> svm = SVC(kernel=chi2_kernel).fit(X, y) >>> svm.predict(X) array([0, 1, 0, 1]) The chi squared kernel is given by The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions. The chi squared kernel is most commonly used on histograms (bags) of visual words. References - Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007https://hal.archives-ouvertes.fr/hal-00171412/document", "prev_chunk_id": "chunk_264", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_266", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7. Kernel Approximation#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7. Kernel Approximation#", "content": "7.7. Kernel Approximation# This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see Support Vector Machines). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms. The advantage of using approximate explicit feature maps compared to the kernel trick, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with SGDClassifier can make non-linear learning on large datasets possible. Since there has not been much empirical work using approximate embeddings, it is advisable to compare results against exact kernel methods when possible.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_267", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.1. Nystroem Method for Kernel Approximation#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.1. Nystroem Method for Kernel Approximation#", "content": "7.7.1. Nystroem Method for Kernel Approximation# The Nystroem method, as implemented in Nystroem is a general method for reduced rank approximations of kernels. It achieves this by subsampling without replacement rows/columns of the data on which the kernel is evaluated. While the computational complexity of the exact method is \\(\\mathcal{O}(n^3_{\\text{samples}})\\), the complexity of the approximation is \\(\\mathcal{O}(n^2_{\\text{components}} \\cdot n_{\\text{samples}})\\), where one can set \\(n_{\\text{components}} \\ll n_{\\text{samples}}\\) without a significant decrease in performance [WS2001]. We can construct the eigendecomposition of the kernel matrix \\(K\\), based on the features of the data, and then split it into sampled and unsampled data points. where: - \\(U\\)is orthonormal - \\(\\Lambda\\)is diagonal matrix of eigenvalues - \\(U_1\\)is orthonormal matrix of samples that were chosen - \\(U_2\\)is orthonormal matrix of samples that were not chosen Given that \\(U_1 \\Lambda U_1^T\\) can be obtained by orthonormalization of the matrix \\(K_{11}\\), and \\(U_2 \\Lambda U_1^T\\) can be evaluated (as well as its transpose), the only remaining term to elucidate is \\(U_2 \\Lambda U_2^T\\). To do this we can express it in terms of the already evaluated matrices: During fit, the class Nystroem evaluates the basis \\(U_1\\), and computes the normalization constant, \\(K_{11}^{-\\frac12}\\). Later, during transform, the kernel matrix is determined between the basis (given by the components_ attribute) and the new data points, X. This matrix is then multiplied by the normalization_ matrix for the final result. By default Nystroem uses the rbf kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter n_components. Examples - See the example entitledTime-related feature engineering, that shows an efficient machine learning pipeline that uses aNystroemkernel. - SeeExplicit feature map approximation for RBF kernelsfor a comparison ofNystroemkernel withRBFSampler.", "prev_chunk_id": "chunk_266", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_268", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.2. Radial Basis Function Kernel#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.2. Radial Basis Function Kernel#", "content": "7.7.2. Radial Basis Function Kernel# The RBFSampler constructs an approximate mapping for the radial basis function kernel, also known as Random Kitchen Sinks [RR2007]. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM: >>> from sklearn.kernel_approximation import RBFSampler >>> from sklearn.linear_model import SGDClassifier >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]] >>> y = [0, 0, 1, 1] >>> rbf_feature = RBFSampler(gamma=1, random_state=1) >>> X_features = rbf_feature.fit_transform(X) >>> clf = SGDClassifier(max_iter=5) >>> clf.fit(X_features, y) SGDClassifier(max_iter=5) >>> clf.score(X_features, y) 1.0 The mapping relies on a Monte Carlo approximation to the kernel values. The fit function performs the Monte Carlo sampling, whereas the transform method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the fit function. The fit function takes two arguments: n_components, which is the target dimensionality of the feature transform, and gamma, the parameter of the RBF-kernel. A higher n_components will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that “fitting” the feature function does not actually depend on the data given to the fit function. Only the dimensionality of the data is used. Details on the method can be found in [RR2007]. For a given value of n_components RBFSampler is often less accurate as Nystroem. RBFSampler is cheaper to compute, though, making use of larger feature spaces more efficient. Examples - SeeExplicit feature map approximation for RBF kernelsfor a comparison ofNystroemkernel withRBFSampler.", "prev_chunk_id": "chunk_267", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_269", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.3. Additive Chi Squared Kernel#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.3. Additive Chi Squared Kernel#", "content": "7.7.3. Additive Chi Squared Kernel# The additive chi squared kernel is a kernel on histograms, often used in computer vision. The additive chi squared kernel as used here is given by This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel. The authors of [VZ2010] prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \\(x_i\\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling. The class AdditiveChi2Sampler implements this component wise deterministic sampling. Each component is sampled \\(n\\) times, yielding \\(2n+1\\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \\(n\\) is usually chosen to be 1 or 2, transforming the dataset to size n_samples * 5 * n_features (in the case of \\(n=2\\)). The approximate feature map provided by AdditiveChi2Sampler can be combined with the approximate feature map provided by RBFSampler to yield an approximate feature map for the exponentiated chi squared kernel. See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.", "prev_chunk_id": "chunk_268", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_270", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.4. Skewed Chi Squared Kernel#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.4. Skewed Chi Squared Kernel#", "content": "7.7.4. Skewed Chi Squared Kernel# The skewed chi squared kernel is given by: It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map. The usage of the SkewedChi2Sampler is the same as the usage described above for the RBFSampler. The only difference is in the free parameter, that is called \\(c\\). For a motivation for this mapping and the mathematical details see [LS2010].", "prev_chunk_id": "chunk_269", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_271", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.5. Polynomial Kernel Approximation via Tensor Sketch#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.5. Polynomial Kernel Approximation via Tensor Sketch#", "content": "7.7.5. Polynomial Kernel Approximation via Tensor Sketch# The polynomial kernel is a popular type of kernel function given by: where: - x,yare the input vectors - dis the kernel degree Intuitively, the feature space of the polynomial kernel of degree d consists of all possible degree-d products among input features, which enables learning algorithms using this kernel to account for interactions between features. The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a scalable, input data independent method for polynomial kernel approximation. It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality reduction technique similar to feature hashing, which instead uses several independent hash functions. TensorSketch obtains a Count Sketch of the outer product of two vectors (or a vector with itself), which can be used as an approximation of the polynomial kernel feature space. In particular, instead of explicitly computing the outer product, TensorSketch computes the Count Sketch of the vectors and then uses polynomial multiplication via the Fast Fourier Transform to compute the Count Sketch of their outer product. Conveniently, the training phase of TensorSketch simply consists of initializing some random variables. It is thus independent of the input data, i.e. it only depends on the number of input features, but not the data values. In addition, this method can transform samples in \\(\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}} \\log(n_{\\text{components}})))\\) time, where \\(n_{\\text{components}}\\) is the desired output dimension, determined by n_components. Examples - Scalable learning with polynomial kernel approximation", "prev_chunk_id": "chunk_270", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_272", "url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "title": "7.7.6. Mathematical Details#", "page_title": "7.7. Kernel Approximation — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.7.6. Mathematical Details#", "content": "7.7.6. Mathematical Details# Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \\(k\\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \\(\\phi\\) into a Hilbert space \\(\\mathcal{H}\\), such that Where \\(\\langle \\cdot, \\cdot \\rangle\\) denotes the inner product in the Hilbert space. If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \\(x_i\\), one may use the value of \\(k(x_i, x_j)\\), which corresponds to applying the algorithm to the mapped data points \\(\\phi(x_i)\\). The advantage of using \\(k\\) is that the mapping \\(\\phi\\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite). One drawback of kernel methods is, that it might be necessary to store many kernel values \\(k(x_i, x_j)\\) during optimization. If a kernelized classifier is applied to new data \\(y_j\\), \\(k(x_i, y_j)\\) needs to be computed to make predictions, possibly for many different \\(x_i\\) in the training set. The classes in this submodule allow to approximate the embedding \\(\\phi\\), thereby working explicitly with the representations \\(\\phi(x_i)\\), which obviates the need to apply the kernel or store training examples. References", "prev_chunk_id": "chunk_271", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_273", "url": "https://scikit-learn.org/stable/modules/random_projection.html", "title": "7.6. Random Projection#", "page_title": "7.6. Random Projection — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.6. Random Projection#", "content": "7.6. Random Projection# The sklearn.random_projection module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: Gaussian random matrix and sparse random matrix. The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method. References - Sanjoy Dasgupta. 2000.Experiments with random projection.In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151. - Ella Bingham and Heikki Mannila. 2001.Random projection in dimensionality reduction: applications to image and text data.In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA, 245-250.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_274", "url": "https://scikit-learn.org/stable/modules/random_projection.html", "title": "7.6.1. The Johnson-Lindenstrauss lemma#", "page_title": "7.6. Random Projection — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.6.1. The Johnson-Lindenstrauss lemma#", "content": "7.6.1. The Johnson-Lindenstrauss lemma# The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia): Knowing only the number of samples, the johnson_lindenstrauss_min_dim estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection: >>> from sklearn.random_projection import johnson_lindenstrauss_min_dim >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5) np.int64(663) >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01]) array([ 663, 11841, 1112658]) >>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1) array([ 7894, 9868, 11841]) Examples - SeeThe Johnson-Lindenstrauss bound for embedding with random projectionsfor a theoretical explication on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices. References - Sanjoy Dasgupta and Anupam Gupta, 1999.An elementary proof of the Johnson-Lindenstrauss Lemma.", "prev_chunk_id": "chunk_273", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_275", "url": "https://scikit-learn.org/stable/modules/random_projection.html", "title": "7.6.2. Gaussian random projection#", "page_title": "7.6. Random Projection — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.6.2. Gaussian random projection#", "content": "7.6.2. Gaussian random projection# The GaussianRandomProjection reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \\(N(0, \\frac{1}{n_{components}})\\). Here is a small excerpt which illustrates how to use the Gaussian random projection transformer: >>> import numpy as np >>> from sklearn import random_projection >>> X = np.random.rand(100, 10000) >>> transformer = random_projection.GaussianRandomProjection() >>> X_new = transformer.fit_transform(X) >>> X_new.shape (100, 3947)", "prev_chunk_id": "chunk_274", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_276", "url": "https://scikit-learn.org/stable/modules/random_projection.html", "title": "7.6.3. Sparse random projection#", "page_title": "7.6. Random Projection — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.6.3. Sparse random projection#", "content": "7.6.3. Sparse random projection# The SparseRandomProjection reduces the dimensionality by projecting the original input space using a sparse random matrix. Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data. If we define s = 1 / density, the elements of the random matrix are drawn from where \\(n_{\\text{components}}\\) is the size of the projected subspace. By default the density of non zero elements is set to the minimum density as recommended by Ping Li et al.: \\(1 / \\sqrt{n_{\\text{features}}}\\). Here is a small excerpt which illustrates how to use the sparse random projection transformer: >>> import numpy as np >>> from sklearn import random_projection >>> X = np.random.rand(100, 10000) >>> transformer = random_projection.SparseRandomProjection() >>> X_new = transformer.fit_transform(X) >>> X_new.shape (100, 3947) References - D. Achlioptas. 2003.Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences 66 (2003) 671-687. - Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.Very sparse random projections.In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA, 287-296.", "prev_chunk_id": "chunk_275", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_277", "url": "https://scikit-learn.org/stable/modules/random_projection.html", "title": "7.6.4. Inverse Transform#", "page_title": "7.6. Random Projection — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.6.4. Inverse Transform#", "content": "7.6.4. Inverse Transform# The random projection transformers have compute_inverse_components parameter. When set to True, after creating the random components_ matrix during fitting, the transformer computes the pseudo-inverse of this matrix and stores it as inverse_components_. The inverse_components_ matrix has shape \\(n_{features} \\times n_{components}\\), and it is always a dense matrix, regardless of whether the components matrix is sparse or dense. So depending on the number of features and components, it may use a lot of memory. When the inverse_transform method is called, it computes the product of the input X and the transpose of the inverse components. If the inverse components have been computed during fit, they are reused at each call to inverse_transform. Otherwise they are recomputed each time, which can be costly. The result is always dense, even if X is sparse. Here is a small code example which illustrates how to use the inverse transform feature: >>> import numpy as np >>> from sklearn.random_projection import SparseRandomProjection >>> X = np.random.rand(100, 10000) >>> transformer = SparseRandomProjection( ... compute_inverse_components=True ... ) ... >>> X_new = transformer.fit_transform(X) >>> X_new.shape (100, 3947) >>> X_new_inversed = transformer.inverse_transform(X_new) >>> X_new_inversed.shape (100, 10000) >>> X_new_again = transformer.transform(X_new_inversed) >>> np.allclose(X_new, X_new_again) True", "prev_chunk_id": "chunk_276", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_278", "url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "title": "7.5. Unsupervised dimensionality reduction#", "page_title": "7.5. Unsupervised dimensionality reduction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.5. Unsupervised dimensionality reduction#", "content": "7.5. Unsupervised dimensionality reduction# If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the Unsupervised learning methods implement a transform method that can be used to reduce the dimensionality. Below we discuss two specific examples of this pattern that are heavily used.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_279", "url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "title": "7.5.1. PCA: principal component analysis#", "page_title": "7.5. Unsupervised dimensionality reduction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.5.1. PCA: principal component analysis#", "content": "7.5.1. PCA: principal component analysis# decomposition.PCA looks for a combination of features that capture well the variance of the original features. See Decomposing signals in components (matrix factorization problems). Examples - Faces recognition example using eigenfaces and SVMs", "prev_chunk_id": "chunk_278", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_280", "url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "title": "7.5.2. Random projections#", "page_title": "7.5. Unsupervised dimensionality reduction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.5.2. Random projections#", "content": "7.5.2. Random projections# The module: random_projection provides several tools for data reduction by random projections. See the relevant section of the documentation: Random Projection. Examples - The Johnson-Lindenstrauss bound for embedding with random projections", "prev_chunk_id": "chunk_279", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_281", "url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "title": "7.5.3. Feature agglomeration#", "page_title": "7.5. Unsupervised dimensionality reduction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.5.3. Feature agglomeration#", "content": "7.5.3. Feature agglomeration# cluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly. Examples - Feature agglomeration vs. univariate selection - Feature agglomeration", "prev_chunk_id": "chunk_280", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_282", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4. Imputation of missing values#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4. Imputation of missing values#", "content": "7.4. Imputation of missing values# For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the glossary entry on imputation.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_283", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.1. Univariate vs. Multivariate Imputation#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.1. Univariate vs. Multivariate Imputation#", "content": "7.4.1. Univariate vs. Multivariate Imputation# One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. SimpleImputer). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. IterativeImputer).", "prev_chunk_id": "chunk_282", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_284", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.2. Univariate feature imputation#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.2. Univariate feature imputation#", "content": "7.4.2. Univariate feature imputation# The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings. The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values: >>> import numpy as np >>> from sklearn.impute import SimpleImputer >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean') >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]]) SimpleImputer() >>> X = [[np.nan, 2], [6, np.nan], [7, 6]] >>> print(imp.transform(X)) [[4. 2. ] [6. 3.666] [7. 6. ]] The SimpleImputer class also supports sparse matrices: >>> import scipy.sparse as sp >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]]) >>> imp = SimpleImputer(missing_values=-1, strategy='mean') >>> imp.fit(X) SimpleImputer(missing_values=-1) >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]]) >>> print(imp.transform(X_test).toarray()) [[3. 2.] [6. 3.] [7. 6.]] Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input. The SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy: >>> import pandas as pd >>> df = pd.DataFrame([[\"a\", \"x\"], ... [np.nan, \"y\"], ... [\"a\", np.nan], ... [\"b\", \"y\"]], dtype=\"category\") ... >>> imp = SimpleImputer(strategy=\"most_frequent\") >>> print(imp.fit_transform(df)) [['a' 'x'] ['a' 'y'] ['a' 'y'] ['b' 'y']] For another example on usage, see Imputing missing values before building an estimator.", "prev_chunk_id": "chunk_283", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_285", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.3. Multivariate feature imputation#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.3. Multivariate feature imputation#", "content": "7.4.3. Multivariate feature imputation# A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned. >>> import numpy as np >>> from sklearn.experimental import enable_iterative_imputer >>> from sklearn.impute import IterativeImputer >>> imp = IterativeImputer(max_iter=10, random_state=0) >>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]]) IterativeImputer(random_state=0) >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]] >>> # the model learns that the second feature is double the first >>> print(np.round(imp.transform(X_test))) [[ 1. 2.] [ 6. 12.] [ 3. 6.]] Both SimpleImputer and IterativeImputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. See Imputing missing values before building an estimator.", "prev_chunk_id": "chunk_284", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_286", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.3.1. Flexibility of IterativeImputer#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.3.1. Flexibility of IterativeImputer#", "content": "7.4.3.1. Flexibility of IterativeImputer# There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with IterativeImputer by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See Imputing missing values with variants of IterativeImputer.", "prev_chunk_id": "chunk_285", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_287", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.3.2. Multiple vs. Single Imputation#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.3.2. Multiple vs. Single Imputation#", "content": "7.4.3.2. Multiple vs. Single Imputation# In the statistics community, it is common practice to perform multiple imputations, generating, for example, m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The m final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation. Our implementation of IterativeImputer was inspired by the R MICE package (Multivariate Imputation by Chained Equations) [1], but differs from it by returning a single imputation instead of multiple imputations. However, IterativeImputer can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when sample_posterior=True. See [2], chapter 4 for more discussion on multiple vs. single imputations. It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values. Note that a call to the transform method of IterativeImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to transform. References", "prev_chunk_id": "chunk_286", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_288", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.4. Nearest neighbors imputation#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.4. Nearest neighbors imputation#", "content": "7.4.4. Nearest neighbors imputation# The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. For more information on the methodology, see ref. [OL2001]. The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean feature value of the two nearest neighbors of samples with missing values: >>> import numpy as np >>> from sklearn.impute import KNNImputer >>> nan = np.nan >>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]] >>> imputer = KNNImputer(n_neighbors=2, weights=\"uniform\") >>> imputer.fit_transform(X) array([[1. , 2. , 4. ], [3. , 4. , 3. ], [5.5, 6. , 5. ], [8. , 8. , 7. ]]) For another example on usage, see Imputing missing values before building an estimator. References", "prev_chunk_id": "chunk_287", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_289", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.5. Keeping the number of features constant#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.5. Keeping the number of features constant#", "content": "7.4.5. Keeping the number of features constant# By default, the scikit-learn imputers will drop fully empty features, i.e. columns containing only missing values. For instance: >>> imputer = SimpleImputer() >>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]]) >>> imputer.fit_transform(X) array([[1.], [2.], [3.]]) The first feature in X containing only np.nan was dropped after the imputation. While this feature will not help in predictive setting, dropping the columns will change the shape of X which could be problematic when using imputers in a more complex machine-learning pipeline. The parameter keep_empty_features offers the option to keep the empty features by imputing with a constant value. In most of the cases, this constant value is zero: >>> imputer.set_params(keep_empty_features=True) SimpleImputer(keep_empty_features=True) >>> imputer.fit_transform(X) array([[0., 1.], [0., 2.], [0., 3.]])", "prev_chunk_id": "chunk_288", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_290", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.6. Marking imputed values#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.6. Marking imputed values#", "content": "7.4.6. Marking imputed values# The MissingIndicator transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. Note that both the SimpleImputer and IterativeImputer have the boolean parameter add_indicator (False by default) which when set to True provides a convenient way of stacking the output of the MissingIndicator transformer with the output of the imputer. NaN is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter missing_values allows to specify other placeholder such as integer. In the following example, we will use -1 as missing values: >>> from sklearn.impute import MissingIndicator >>> X = np.array([[-1, -1, 1, 3], ... [4, -1, 0, -1], ... [8, -1, 1, 0]]) >>> indicator = MissingIndicator(missing_values=-1) >>> mask_missing_values_only = indicator.fit_transform(X) >>> mask_missing_values_only array([[ True, True, False], [False, True, True], [False, True, False]]) The features parameter is used to choose the features for which the mask is constructed. By default, it is 'missing-only' which returns the imputer mask of the features containing missing values at fit time: >>> indicator.features_ array([0, 1, 3]) The features parameter can be set to 'all' to return all features whether or not they contain missing values: >>> indicator = MissingIndicator(missing_values=-1, features=\"all\") >>> mask_all = indicator.fit_transform(X) >>> mask_all array([[ True, True, False, False], [False, True, False, True], [False, True, False, False]]) >>> indicator.features_ array([0, 1, 2, 3]) When using the MissingIndicator in a Pipeline, be sure to use the FeatureUnion or ColumnTransformer to add the indicator features to the regular features. First we obtain the iris dataset, and add some missing values to it. >>> from sklearn.datasets import load_iris", "prev_chunk_id": "chunk_289", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_291", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.6. Marking imputed values#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.6. Marking imputed values#", "content": ">>> from sklearn.impute import SimpleImputer, MissingIndicator >>> from sklearn.model_selection import train_test_split >>> from sklearn.pipeline import FeatureUnion, make_pipeline >>> from sklearn.tree import DecisionTreeClassifier >>> X, y = load_iris(return_X_y=True) >>> mask = np.random.randint(0, 2, size=X.shape).astype(bool) >>> X[mask] = np.nan >>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100, ... random_state=0) Now we create a FeatureUnion. All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the indicator variables from MissingIndicator. >>> transformer = FeatureUnion( ... transformer_list=[ ... ('features', SimpleImputer(strategy='mean')), ... ('indicators', MissingIndicator())]) >>> transformer = transformer.fit(X_train, y_train) >>> results = transformer.transform(X_test) >>> results.shape (100, 8) Of course, we cannot use the transformer to make any predictions. We should wrap this in a Pipeline with a classifier (e.g., a DecisionTreeClassifier) to be able to make predictions. >>> clf = make_pipeline(transformer, DecisionTreeClassifier()) >>> clf = clf.fit(X_train, y_train) >>> results = clf.predict(X_test) >>> results.shape (100,)", "prev_chunk_id": "chunk_290", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_292", "url": "https://scikit-learn.org/stable/modules/impute.html", "title": "7.4.7. Estimators that handle NaN values#", "page_title": "7.4. Imputation of missing values — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.4.7. Estimators that handle NaN values#", "content": "7.4.7. Estimators that handle NaN values# Some estimators are designed to handle NaN values without preprocessing. Below is the list of these estimators, classified by type (cluster, regressor, classifier, transform): - Estimators that allow NaN values for typecluster:HDBSCAN - Estimators that allow NaN values for typeregressor:BaggingRegressorDecisionTreeRegressorExtraTreeRegressorExtraTreesRegressorHistGradientBoostingRegressorRandomForestRegressorStackingRegressorVotingRegressor - Estimators that allow NaN values for typeclassifier:BaggingClassifierDecisionTreeClassifierExtraTreeClassifierExtraTreesClassifierHistGradientBoostingClassifierRandomForestClassifierStackingClassifierVotingClassifier - Estimators that allow NaN values for typetransformer:IterativeImputerKNNImputerMaxAbsScalerMinMaxScalerMissingIndicatorOneHotEncoderOrdinalEncoderPowerTransformerQuantileTransformerRandomTreesEmbeddingRobustScalerSimpleImputerStackingClassifierStackingRegressorStandardScalerTargetEncoderVarianceThresholdVotingClassifierVotingRegressor", "prev_chunk_id": "chunk_291", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_293", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3. Preprocessing data#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3. Preprocessing data#", "content": "7.3. Preprocessing data# The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, many learning algorithms such as linear models benefit from standardization of the data set (see Importance of Feature Scaling). If some outliers are present in the set, robust scalers or other transformers can be more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers are highlighted in Compare the effect of different scalers on data with outliers.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_294", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1. Standardization, or mean removal and variance scaling#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1. Standardization, or mean removal and variance scaling#", "content": "7.3.1. Standardization, or mean removal and variance scaling# Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset: >>> from sklearn import preprocessing >>> import numpy as np >>> X_train = np.array([[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]]) >>> scaler = preprocessing.StandardScaler().fit(X_train) >>> scaler StandardScaler() >>> scaler.mean_ array([1., 0., 0.33]) >>> scaler.scale_ array([0.81, 0.81, 1.24]) >>> X_scaled = scaler.transform(X_train) >>> X_scaled array([[ 0. , -1.22, 1.33 ], [ 1.22, 0. , -0.267], [-1.22, 1.22, -1.06 ]]) Scaled data has zero mean and unit variance: >>> X_scaled.mean(axis=0) array([0., 0., 0.]) >>> X_scaled.std(axis=0) array([1., 1., 1.]) This class implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same", "prev_chunk_id": "chunk_293", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_295", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1. Standardization, or mean removal and variance scaling#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1. Standardization, or mean removal and variance scaling#", "content": "transformation on the testing set. This class is hence suitable for use in the early steps of a Pipeline: >>> from sklearn.datasets import make_classification >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.model_selection import train_test_split >>> from sklearn.pipeline import make_pipeline >>> from sklearn.preprocessing import StandardScaler >>> X, y = make_classification(random_state=42) >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) >>> pipe = make_pipeline(StandardScaler(), LogisticRegression()) >>> pipe.fit(X_train, y_train) # apply scaling on training data Pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())]) >>> pipe.score(X_test, y_test) # apply scaling on testing data, without leaking training data. 0.96 It is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to the constructor of StandardScaler.", "prev_chunk_id": "chunk_294", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_296", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1.1. Scaling features to a range#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1.1. Scaling features to a range#", "content": "7.3.1.1. Scaling features to a range# An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively. The motivation to use this scaling includes robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the [0, 1] range: >>> X_train = np.array([[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]]) ... >>> min_max_scaler = preprocessing.MinMaxScaler() >>> X_train_minmax = min_max_scaler.fit_transform(X_train) >>> X_train_minmax array([[0.5 , 0. , 1. ], [1. , 0.5 , 0.33333333], [0. , 1. , 0. ]]) The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: >>> X_test = np.array([[-3., -1., 4.]]) >>> X_test_minmax = min_max_scaler.transform(X_test) >>> X_test_minmax array([[-1.5 , 0. , 1.66666667]]) It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: >>> min_max_scaler.scale_ array([0.5 , 0.5 , 0.33]) >>> min_max_scaler.min_ array([0. , 0.5 , 0.33]) If MinMaxScaler is given an explicit feature_range=(min, max) the full formula is: X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_scaled = X_std * (max - min) + min MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse", "prev_chunk_id": "chunk_295", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_297", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1.1. Scaling features to a range#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1.1. Scaling features to a range#", "content": "data. Here is how to use the toy data from the previous example with this scaler: >>> X_train = np.array([[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]]) ... >>> max_abs_scaler = preprocessing.MaxAbsScaler() >>> X_train_maxabs = max_abs_scaler.fit_transform(X_train) >>> X_train_maxabs array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]]) >>> X_test = np.array([[ -3., -1., 4.]]) >>> X_test_maxabs = max_abs_scaler.transform(X_test) >>> X_test_maxabs array([[-1.5, -1. , 2. ]]) >>> max_abs_scaler.scale_ array([2., 1., 2.])", "prev_chunk_id": "chunk_296", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_298", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1.2. Scaling sparse data#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1.2. Scaling sparse data#", "content": "7.3.1.2. Scaling sparse data# Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. MaxAbsScaler was specifically designed for scaling sparse data, and is the recommended way to go about this. However, StandardScaler can accept scipy.sparse matrices as input, as long as with_mean=False is explicitly passed to the constructor. Otherwise a ValueError will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. RobustScaler cannot be fitted to sparse inputs, but you can use the transform method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see scipy.sparse.csr_matrix and scipy.sparse.csc_matrix). Any other sparse input will be converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the toarray method of sparse matrices is another option.", "prev_chunk_id": "chunk_297", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_299", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1.3. Scaling data with outliers#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1.3. Scaling data with outliers#", "content": "7.3.1.3. Scaling data with outliers# If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use RobustScaler as a drop-in replacement instead. It uses more robust estimates for the center and range of your data.", "prev_chunk_id": "chunk_298", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_300", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.1.4. Centering kernel matrices#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.1.4. Centering kernel matrices#", "content": "7.3.1.4. Centering kernel matrices# If you have a kernel matrix of a kernel \\(K\\) that computes a dot product in a feature space (possibly implicitly) defined by a function \\(\\phi(\\cdot)\\), a KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space defined by \\(\\phi\\) followed by the removal of the mean in that space. In other words, KernelCenterer computes the centered Gram matrix associated to a positive semidefinite kernel \\(K\\).", "prev_chunk_id": "chunk_299", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_301", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.2. Non-linear transformation#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.2. Non-linear transformation#", "content": "7.3.2. Non-linear transformation# Two types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature. Quantile transforms put all features into the same desired distribution based on the formula \\(G^{-1}(F(X))\\) where \\(F\\) is the cumulative distribution function of the feature and \\(G^{-1}\\) the quantile function of the desired output distribution \\(G\\). This formula is using the two following facts: (i) if \\(X\\) is a random variable with a continuous cumulative distribution function \\(F\\) then \\(F(X)\\) is uniformly distributed on \\([0,1]\\); (ii) if \\(U\\) is a random variable with uniform distribution on \\([0,1]\\) then \\(G^{-1}(U)\\) has distribution \\(G\\). By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features. Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution.", "prev_chunk_id": "chunk_300", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_302", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.2.1. Mapping to a Uniform distribution#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.2.1. Mapping to a Uniform distribution#", "content": "7.3.2.1. Mapping to a Uniform distribution# QuantileTransformer provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1: >>> from sklearn.datasets import load_iris >>> from sklearn.model_selection import train_test_split >>> X, y = load_iris(return_X_y=True) >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) >>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0) >>> X_train_trans = quantile_transformer.fit_transform(X_train) >>> X_test_trans = quantile_transformer.transform(X_test) >>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) array([ 4.3, 5.1, 5.8, 6.5, 7.9]) This feature corresponds to the sepal length in cm. Once the quantile transformation is applied, those landmarks approach closely the percentiles previously defined: >>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100]) ... array([ 0.00 , 0.24, 0.49, 0.73, 0.99 ]) This can be confirmed on an independent testing set with similar remarks: >>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100]) ... array([ 4.4 , 5.125, 5.75 , 6.175, 7.3 ]) >>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100]) ... array([ 0.01, 0.25, 0.46, 0.60 , 0.94])", "prev_chunk_id": "chunk_301", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_303", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.2.2. Mapping to a Gaussian distribution#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.2.2. Mapping to a Gaussian distribution#", "content": "7.3.2.2. Mapping to a Gaussian distribution# In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. PowerTransformer currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform. Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation. It is also possible to map data to a normal distribution using QuantileTransformer by setting output_distribution='normal'. Using the earlier example with the iris dataset: >>> quantile_transformer = preprocessing.QuantileTransformer( ... output_distribution='normal', random_state=0) >>> X_trans = quantile_transformer.fit_transform(X) >>> quantile_transformer.quantiles_ array([[4.3, 2. , 1. , 0.1], [4.4, 2.2, 1.1, 0.1], [4.4, 2.2, 1.2, 0.1], ..., [7.7, 4.1, 6.7, 2.5], [7.7, 4.2, 6.7, 2.5], [7.9, 4.4, 6.9, 2.5]]) Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not become infinite under the transformation.", "prev_chunk_id": "chunk_302", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_304", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.3. Normalization#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.3. Normalization#", "content": "7.3.3. Normalization# Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1, l2, or max norms: >>> X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] >>> X_normalized = preprocessing.normalize(X, norm='l2') >>> X_normalized array([[ 0.408, -0.408, 0.812], [ 1. , 0. , 0. ], [ 0. , 0.707, -0.707]]) The preprocessing module further provides a utility class Normalizer that implements the same operation using the Transformer API (even though the fit method is useless in this case: the class is stateless as this operation treats samples independently). This class is hence suitable for use in the early steps of a Pipeline: >>> normalizer = preprocessing.Normalizer().fit(X) # fit does nothing >>> normalizer Normalizer() The normalizer instance can then be used on sample vectors as any transformer: >>> normalizer.transform(X) array([[ 0.408, -0.408, 0.812], [ 1. , 0. , 0. ], [ 0. , 0.707, -0.707]]) >>> normalizer.transform([[-1., 1., 0.]]) array([[-0.707, 0.707, 0.]]) Note: L2 normalization is also known as spatial sign preprocessing.", "prev_chunk_id": "chunk_303", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_305", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4. Encoding categorical features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4. Encoding categorical features#", "content": "7.3.4. Encoding categorical features# Often features are not given as continuous values but categorical. For example a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1]. To convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1): >>> enc = preprocessing.OrdinalEncoder() >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']] >>> enc.fit(X) OrdinalEncoder() >>> enc.transform([['female', 'from US', 'uses Safari']]) array([[0., 1., 1.]]) Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily). By default, OrdinalEncoder will also passthrough missing values that are indicated by np.nan. >>> enc = preprocessing.OrdinalEncoder() >>> X = [['male'], ['female'], [np.nan], ['female']] >>> enc.fit_transform(X) array([[ 1.], [ 0.], [nan], [ 0.]]) OrdinalEncoder provides a parameter encoded_missing_value to encode the missing values without the need to create a pipeline and using SimpleImputer. >>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1) >>> X = [['male'], ['female'], [np.nan], ['female']] >>> enc.fit_transform(X) array([[ 1.], [ 0.], [-1.], [ 0.]]) The above processing is equivalent to the following pipeline: >>> from sklearn.pipeline import Pipeline >>> from sklearn.impute import SimpleImputer >>> enc = Pipeline(steps=[ ... (\"encoder\", preprocessing.OrdinalEncoder()), ... (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)), ... ]) >>> enc.fit_transform(X) array([[ 1.], [ 0.], [-1.], [ 0.]]) Another possibility to convert categorical features to features that can be used with scikit-learn estimators is", "prev_chunk_id": "chunk_304", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_306", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4. Encoding categorical features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4. Encoding categorical features#", "content": "to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0. Continuing the example above: >>> enc = preprocessing.OneHotEncoder() >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']] >>> enc.fit(X) OneHotEncoder() >>> enc.transform([['female', 'from US', 'uses Safari'], ... ['male', 'from Europe', 'uses Safari']]).toarray() array([[1., 0., 0., 1., 0., 1.], [0., 1., 1., 0., 0., 1.]]) By default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute: >>> enc.categories_ [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)] It is possible to specify this explicitly using the parameter categories. There are two genders, four possible continents and four web browsers in our dataset: >>> genders = ['female', 'male'] >>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US'] >>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari'] >>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers]) >>> # Note that for there are missing categorical values for the 2nd and 3rd >>> # feature >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']] >>> enc.fit(X) OneHotEncoder(categories=[['female', 'male'], ['from Africa', 'from Asia', 'from Europe', 'from US'], ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']]) >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray() array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]]) If there is a possibility that the training data might have missing categorical features, it can often be better to specify handle_unknown='infrequent_if_exist' instead of setting the categories manually as above. When handle_unknown='infrequent_if_exist' is specified and unknown categories are encountered during transform, no error will be raised but", "prev_chunk_id": "chunk_305", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_307", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4. Encoding categorical features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4. Encoding categorical features#", "content": "the resulting one-hot encoded columns for this feature will be all zeros or considered as an infrequent category if enabled. (handle_unknown='infrequent_if_exist' is only supported for one-hot encoding): >>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist') >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']] >>> enc.fit(X) OneHotEncoder(handle_unknown='infrequent_if_exist') >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray() array([[1., 0., 0., 0., 0., 0.]]) It is also possible to encode each column into n_categories - 1 columns instead of n_categories columns by using the drop parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (LinearRegression), since co-linearity would cause the covariance matrix to be non-invertible: >>> X = [['male', 'from US', 'uses Safari'], ... ['female', 'from Europe', 'uses Firefox']] >>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X) >>> drop_enc.categories_ [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)] >>> drop_enc.transform(X).toarray() array([[1., 1., 1.], [0., 0., 0.]]) One might want to drop one of the two columns only for features with 2 categories. In this case, you can set the parameter drop='if_binary'. >>> X = [['male', 'US', 'Safari'], ... ['female', 'Europe', 'Firefox'], ... ['female', 'Asia', 'Chrome']] >>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X) >>> drop_enc.categories_ [array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object), array(['Chrome', 'Firefox', 'Safari'], dtype=object)] >>> drop_enc.transform(X).toarray() array([[1., 0., 0., 1., 0., 0., 1.], [0., 0., 1., 0., 0., 1., 0.], [0., 1., 0., 0., 1., 0., 0.]]) In the transformed X, the first column is the encoding of the feature with categories “male”/”female”, while the remaining 6 columns are the encoding of the 2 features with respectively 3 categories each. When handle_unknown='ignore' and drop is not None, unknown categories will be encoded as all zeros: >>> drop_enc", "prev_chunk_id": "chunk_306", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_308", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4. Encoding categorical features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4. Encoding categorical features#", "content": "= preprocessing.OneHotEncoder(drop='first', ... handle_unknown='ignore').fit(X) >>> X_test = [['unknown', 'America', 'IE']] >>> drop_enc.transform(X_test).toarray() array([[0., 0., 0., 0., 0.]]) All the categories in X_test are unknown during transform and will be mapped to all zeros. This means that unknown categories will have the same mapping as the dropped category. OneHotEncoder.inverse_transform will map all zeros to the dropped category if a category is dropped and None if a category is not dropped: >>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False, ... handle_unknown='ignore').fit(X) >>> X_test = [['unknown', 'America', 'IE']] >>> X_trans = drop_enc.transform(X_test) >>> X_trans array([[0., 0., 0., 0., 0., 0., 0.]]) >>> drop_enc.inverse_transform(X_trans) array([['female', None, None]], dtype=object)", "prev_chunk_id": "chunk_307", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_309", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4.1. Infrequent categories#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4.1. Infrequent categories#", "content": "7.3.4.1. Infrequent categories# OneHotEncoder and OrdinalEncoder support aggregating infrequent categories into a single output for each feature. The parameters to enable the gathering of infrequent categories are min_frequency and max_categories. - min_frequencyis either an integer greater or equal to 1, or a float in the interval(0.0,1.0). Ifmin_frequencyis an integer, categories with a cardinality smaller thanmin_frequencywill be considered infrequent. Ifmin_frequencyis a float, categories with a cardinality smaller than this fraction of the total number of samples will be considered infrequent. The default value is 1, which means every category is encoded separately. - max_categoriesis eitherNoneor any integer greater than 1. This parameter sets an upper limit to the number of output features for each input feature.max_categoriesincludes the feature that combines infrequent categories. In the following example with OrdinalEncoder, the categories 'dog' and 'snake' are considered infrequent: >>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 + ... ['snake'] * 3], dtype=object).T >>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X) >>> enc.infrequent_categories_ [array(['dog', 'snake'], dtype=object)] >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']])) array([[2.], [0.], [1.], [2.]]) OrdinalEncoder’s max_categories do not take into account missing or unknown categories. Setting unknown_value or encoded_missing_value to an integer will increase the number of unique integer codes by one each. This can result in up to max_categories + 2 integer codes. In the following example, “a” and “d” are considered infrequent and grouped together into a single category, “b” and “c” are their own categories, unknown values are encoded as 3 and missing values are encoded as 4. >>> X_train = np.array( ... [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]], ... dtype=object).T >>> enc = preprocessing.OrdinalEncoder( ... handle_unknown=\"use_encoded_value\", unknown_value=3, ... max_categories=3, encoded_missing_value=4) >>> _ = enc.fit(X_train) >>> X_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object) >>> enc.transform(X_test)", "prev_chunk_id": "chunk_308", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_310", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4.1. Infrequent categories#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4.1. Infrequent categories#", "content": "array([[2.], [0.], [1.], [2.], [3.], [4.]]) Similarly, OneHotEncoder can be configured to group together infrequent categories: >>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X) >>> enc.infrequent_categories_ [array(['dog', 'snake'], dtype=object)] >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']])) array([[0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) By setting handle_unknown to 'infrequent_if_exist', unknown categories will be considered infrequent: >>> enc = preprocessing.OneHotEncoder( ... handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6) >>> enc = enc.fit(X) >>> enc.transform(np.array([['dragon']])) array([[0., 0., 1.]]) OneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent feature name: >>> enc.get_feature_names_out() array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object) When 'handle_unknown' is set to 'infrequent_if_exist' and an unknown category is encountered in transform: - If infrequent category support was not configured or there was no infrequent category during training, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted asNone. - If there is an infrequent category during training, the unknown category will be considered infrequent. In the inverse transform, ‘infrequent_sklearn’ will be used to represent the infrequent category. Infrequent categories can also be configured using max_categories. In the following example, we set max_categories=2 to limit the number of features in the output. This will result in all but the 'cat' category to be considered infrequent, leading to two features, one for 'cat' and one for infrequent categories - which are all the others: >>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False) >>> enc = enc.fit(X) >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']]) array([[0., 1.], [1., 0.], [0., 1.], [0., 1.]]) If both max_categories and min_frequency are non-default values, then categories are selected based on min_frequency first and max_categories categories are kept. In the following example, min_frequency=4 considers only snake to be infrequent, but max_categories=3, forces dog to also be infrequent: >>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False) >>> enc = enc.fit(X) >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']]) array([[0.,", "prev_chunk_id": "chunk_309", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_311", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4.1. Infrequent categories#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4.1. Infrequent categories#", "content": "0., 1.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) If there are infrequent categories with the same cardinality at the cutoff of max_categories, then the first max_categories are taken based on lexicon ordering. In the following example, “b”, “c”, and “d”, have the same cardinality and with max_categories=2, “b” and “c” are infrequent because they have a higher lexicon order. >>> X = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T >>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X) >>> enc.infrequent_categories_ [array(['b', 'c'], dtype=object)]", "prev_chunk_id": "chunk_310", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_312", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.4.2. Target Encoder#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.4.2. Target Encoder#", "content": "7.3.4.2. Target Encoder# The TargetEncoder uses the target mean conditioned on the categorical feature for encoding unordered categories, i.e. nominal categories [PAR] [MIC]. This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories are location based such as zip code or region. fit_transform internally relies on a cross fitting scheme to prevent target information from leaking into the train-time representation, especially for non-informative high-cardinality categorical variables, and help prevent the downstream model from overfitting spurious correlations. Note that as a result, fit(X, y).transform(X) does not equal fit_transform(X, y). In fit_transform, the training data is split into k folds (determined by the cv parameter) and each fold is encoded using the encodings learnt using the other k-1 folds. The following diagram shows the cross fitting scheme in fit_transform with the default cv=5: fit_transform also learns a ‘full data’ encoding using the whole training set. This is never used in fit_transform but is saved to the attribute encodings_, for use when transform is called. Note that the encodings learned for each fold during the cross fitting scheme are not saved to an attribute. The fit method does not use any cross fitting schemes and learns one encoding on the entire training set, which is used to encode categories in transform. This encoding is the same as the ‘full data’ encoding learned in fit_transform. Examples - Comparing Target Encoder with Other Encoders - Target Encoder’s Internal Cross fitting References", "prev_chunk_id": "chunk_311", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_313", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.5. Discretization#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.5. Discretization#", "content": "7.3.5. Discretization# Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. For more advanced possibilities, in particular smooth ones, see Generating polynomial features further below.", "prev_chunk_id": "chunk_312", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_314", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.5.1. K-bins discretization#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.5.1. K-bins discretization#", "content": "7.3.5.1. K-bins discretization# KBinsDiscretizer discretizes features into k bins: >>> X = np.array([[ -3., 5., 15 ], ... [ 0., 6., 14 ], ... [ 6., 3., 11 ]]) >>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X) By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the encode parameter. For each feature, the bin edges are computed during fit and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: - feature 1:\\({[-\\infty, -1), [-1, 2), [2, \\infty)}\\) - feature 2:\\({[-\\infty, 5), [5, \\infty)}\\) - feature 3:\\({[-\\infty, 14), [14, \\infty)}\\) Based on these bin intervals, X is transformed as follows: >>> est.transform(X) array([[ 0., 1., 1.], [ 1., 1., 1.], [ 2., 0., 0.]]) The resulting dataset contains ordinal attributes which can be further used in a Pipeline. Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. KBinsDiscretizer implements different binning strategies, which can be selected with the strategy parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently. Be aware that one can specify custom bins by passing a callable defining the discretization strategy to FunctionTransformer. For instance, we can use the Pandas function pandas.cut: >>> import pandas as pd >>> import numpy as np >>> from sklearn import preprocessing >>> >>> bins = [0, 1, 13, 20, 60, np.inf] >>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen'] >>> transformer = preprocessing.FunctionTransformer( ... pd.cut, kw_args={'bins': bins, 'labels':", "prev_chunk_id": "chunk_313", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_315", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.5.1. K-bins discretization#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.5.1. K-bins discretization#", "content": "labels, 'retbins': False} ... ) >>> X = np.array([0.2, 2, 15, 25, 97]) >>> transformer.fit_transform(X) ['infant', 'kid', 'teen', 'adult', 'senior citizen'] Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen'] Examples - Using KBinsDiscretizer to discretize continuous features - Feature discretization - Demonstrating the different strategies of KBinsDiscretizer", "prev_chunk_id": "chunk_314", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_316", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.5.2. Feature binarization#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.5.2. Feature binarization#", "content": "7.3.5.2. Feature binarization# Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the BernoulliRBM. It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the Normalizer, the utility class Binarizer is meant to be used in the early stages of Pipeline. The fit method does nothing as each sample is treated independently of others: >>> X = [[ 1., -1., 2.], ... [ 2., 0., 0.], ... [ 0., 1., -1.]] >>> binarizer = preprocessing.Binarizer().fit(X) # fit does nothing >>> binarizer Binarizer() >>> binarizer.transform(X) array([[1., 0., 1.], [1., 0., 0.], [0., 1., 0.]]) It is possible to adjust the threshold of the binarizer: >>> binarizer = preprocessing.Binarizer(threshold=1.1) >>> binarizer.transform(X) array([[0., 0., 1.], [1., 0., 0.], [0., 0., 0.]]) As for the Normalizer class, the preprocessing module provides a companion function binarize to be used when the transformer API is not necessary. Note that the Binarizer is similar to the KBinsDiscretizer when k = 2, and when the bin edge is at the value threshold.", "prev_chunk_id": "chunk_315", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_317", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.6. Imputation of missing values#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.6. Imputation of missing values#", "content": "7.3.6. Imputation of missing values# Tools for imputing missing values are discussed at Imputation of missing values.", "prev_chunk_id": "chunk_316", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_318", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.7. Generating polynomial features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.7. Generating polynomial features#", "content": "7.3.7. Generating polynomial features# Often it’s useful to add complexity to a model by considering nonlinear features of the input data. We show two possibilities that are both based on polynomials: The first one uses pure polynomials, the second one uses splines, i.e. piecewise polynomials.", "prev_chunk_id": "chunk_317", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_319", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.7.1. Polynomial features#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.7.1. Polynomial features#", "content": "7.3.7.1. Polynomial features# A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in PolynomialFeatures: >>> import numpy as np >>> from sklearn.preprocessing import PolynomialFeatures >>> X = np.arange(6).reshape(3, 2) >>> X array([[0, 1], [2, 3], [4, 5]]) >>> poly = PolynomialFeatures(2) >>> poly.fit_transform(X) array([[ 1., 0., 1., 0., 0., 1.], [ 1., 2., 3., 4., 6., 9.], [ 1., 4., 5., 16., 20., 25.]]) The features of X have been transformed from \\((X_1, X_2)\\) to \\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\). In some cases, only interaction terms among features are required, and it can be gotten with the setting interaction_only=True: >>> X = np.arange(9).reshape(3, 3) >>> X array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) >>> poly = PolynomialFeatures(degree=3, interaction_only=True) >>> poly.fit_transform(X) array([[ 1., 0., 1., 2., 0., 0., 2., 0.], [ 1., 3., 4., 5., 12., 15., 20., 60.], [ 1., 6., 7., 8., 42., 48., 56., 336.]]) The features of X have been transformed from \\((X_1, X_2, X_3)\\) to \\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\). Note that polynomial features are used implicitly in kernel methods (e.g., SVC, KernelPCA) when using polynomial Kernel functions. See Polynomial and Spline interpolation for Ridge regression using created polynomial features.", "prev_chunk_id": "chunk_318", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_320", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.7.2. Spline transformer#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.7.2. Spline transformer#", "content": "7.3.7.2. Spline transformer# Another way to add nonlinear terms instead of pure polynomials of features is to generate spline basis functions for each feature with the SplineTransformer. Splines are piecewise polynomials, parametrized by their polynomial degree and the positions of the knots. The SplineTransformer implements a B-spline basis, cf. the references below. Some of the advantages of splines over polynomials are: - B-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point. - B-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known asRunge’s phenomenon. - B-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the optionextrapolation. - B-splines generate a feature matrix with a banded structure. For a single feature, every row contains onlydegree+1non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the nameVandermonde matrix. A low condition number is important for stable algorithms of linear models. The following code snippet shows splines in action: >>> import numpy as np >>> from sklearn.preprocessing import SplineTransformer >>> X = np.arange(5).reshape(5, 1) >>> X array([[0], [1], [2], [3], [4]]) >>> spline = SplineTransformer(degree=2, n_knots=3) >>> spline.fit_transform(X) array([[0.5 , 0.5 , 0. , 0. ], [0.125, 0.75 , 0.125, 0. ], [0. , 0.5 , 0.5 , 0. ], [0. , 0.125, 0.75 , 0.125], [0. , 0. , 0.5 , 0.5 ]]) As the X is sorted, one can easily see the banded matrix output. Only the three middle diagonals", "prev_chunk_id": "chunk_319", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_321", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.7.2. Spline transformer#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.7.2. Spline transformer#", "content": "are non-zero for degree=2. The higher the degree, the more overlapping of the splines. Interestingly, a SplineTransformer of degree=0 is the same as KBinsDiscretizer with encode='onehot-dense' and n_bins = n_knots - 1 if knots = strategy. Examples - Polynomial and Spline interpolation - Time-related feature engineering", "prev_chunk_id": "chunk_320", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_322", "url": "https://scikit-learn.org/stable/modules/preprocessing.html", "title": "7.3.8. Custom transformers#", "page_title": "7.3. Preprocessing data — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.3.8. Custom transformers#", "content": "7.3.8. Custom transformers# Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer. For example, to build a transformer that applies a log transformation in a pipeline, do: >>> import numpy as np >>> from sklearn.preprocessing import FunctionTransformer >>> transformer = FunctionTransformer(np.log1p, validate=True) >>> X = np.array([[0, 1], [2, 3]]) >>> # Since FunctionTransformer is no-op during fit, we can call transform directly >>> transformer.transform(X) array([[0. , 0.69314718], [1.09861229, 1.38629436]]) You can ensure that func and inverse_func are the inverse of each other by setting check_inverse=True and calling fit before transform. Please note that a warning is raised and can be turned into an error with a filterwarnings: >>> import warnings >>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\", ... category=UserWarning, append=False) For a full code example that demonstrates using a FunctionTransformer to extract features from text data see Column Transformer with Heterogeneous Data Sources and Time-related feature engineering.", "prev_chunk_id": "chunk_321", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_323", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2. Feature extraction#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2. Feature extraction#", "content": "7.2. Feature extraction# The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_324", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.1. Loading features from dicts#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.1. Loading features from dicts#", "content": "7.2.1. Loading features from dicts# The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators. While not particularly fast to process, Python’s dict has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values. DictVectorizer implements what is called one-of-K or “one-hot” coding for categorical (aka nominal, discrete) features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete possibilities without ordering (e.g. topic identifiers, types of objects, tags, names…). In the following, “city” is a categorical attribute while “temperature” is a traditional numerical feature: >>> measurements = [ ... {'city': 'Dubai', 'temperature': 33.}, ... {'city': 'London', 'temperature': 12.}, ... {'city': 'San Francisco', 'temperature': 18.}, ... ] >>> from sklearn.feature_extraction import DictVectorizer >>> vec = DictVectorizer() >>> vec.fit_transform(measurements).toarray() array([[ 1., 0., 0., 33.], [ 0., 1., 0., 12.], [ 0., 0., 1., 18.]]) >>> vec.get_feature_names_out() array(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...) DictVectorizer accepts multiple string values for one feature, like, e.g., multiple categories for a movie. Assume a database classifies each movie using some categories (not mandatory) and its year of release. >>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003}, ... {'category': ['animation', 'family'], 'year': 2011}, ... {'year': 1974}] >>> vec.fit_transform(movie_entry).toarray() array([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03], [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03], [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]]) >>> vec.get_feature_names_out() array(['category=animation', 'category=drama', 'category=family', 'category=thriller', 'year'], ...) >>> vec.transform({'category': ['thriller'], ... 'unseen_feature': '3'}).toarray() array([[0., 0., 0., 1., 0.]]) DictVectorizer is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest. For example, suppose that we have a first algorithm that extracts Part", "prev_chunk_id": "chunk_323", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_325", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.1. Loading features from dicts#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.1. Loading features from dicts#", "content": "of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word ‘sat’ in the sentence ‘The cat sat on the mat.’: >>> pos_window = [ ... { ... 'word-2': 'the', ... 'pos-2': 'DT', ... 'word-1': 'cat', ... 'pos-1': 'NN', ... 'word+1': 'on', ... 'pos+1': 'PP', ... }, ... # in a real application one would extract many such dictionaries ... ] This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a TfidfTransformer for normalization): >>> vec = DictVectorizer() >>> pos_vectorized = vec.fit_transform(pos_window) >>> pos_vectorized <Compressed Sparse...dtype 'float64' with 6 stored elements and shape (1, 6)> >>> pos_vectorized.toarray() array([[1., 1., 1., 1., 1., 1.]]) >>> vec.get_feature_names_out() array(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the'], ...) As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the DictVectorizer class uses a scipy.sparse matrix by default instead of a numpy.ndarray.", "prev_chunk_id": "chunk_324", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_326", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.2. Feature hashing#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.2. Feature hashing#", "content": "7.2.2. Feature hashing# The class FeatureHasher is a high-speed, low-memory vectorizer that uses a technique known as feature hashing, or the “hashing trick”. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of FeatureHasher apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no inverse_transform method. Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature’s value is zero. This mechanism is enabled by default with alternate_sign=True and is particularly useful for small hash table sizes (n_features < 10000). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like MultinomialNB or chi2 feature selectors that expect non-negative inputs. FeatureHasher accepts either mappings (like Python’s dict and its variants in the collections module), (feature, value) pairs, or strings, depending on the constructor parameter input_type. Mappings are treated as lists of (feature, value) pairs, while single strings have an implicit value of 1, so ['feat1', 'feat2', 'feat3'] is interpreted as [('feat1', 1), ('feat2', 1), ('feat3', 1)]. If a single feature occurs multiple times in a sample, the associated values will be summed (so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)). The output from FeatureHasher is always a scipy.sparse matrix in the CSR format. Feature hashing can be employed in document classification, but unlike", "prev_chunk_id": "chunk_325", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_327", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.2. Feature hashing#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.2. Feature hashing#", "content": "CountVectorizer, FeatureHasher does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher. As an example, consider a word-level natural language processing task that needs features extracted from (token, part_of_speech) pairs. One could use a Python generator function to extract features: def token_features(token, part_of_speech): if token.isdigit(): yield \"numeric\" else: yield \"token={}\".format(token.lower()) yield \"token,pos={},{}\".format(token, part_of_speech) if token[0].isupper(): yield \"uppercase_initial\" if token.isupper(): yield \"all_uppercase\" yield \"pos={}\".format(part_of_speech) Then, the raw_X to be fed to FeatureHasher.transform can be constructed using: raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus) and fed to a hasher with: hasher = FeatureHasher(input_type='string') X = hasher.transform(raw_X) to get a scipy.sparse matrix X. Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher. References - Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009).Feature hashing for large scale multitask learning. Proc. ICML.", "prev_chunk_id": "chunk_326", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_328", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.1. The Bag of Words representation#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.1. The Bag of Words representation#", "content": "7.2.3.1. The Bag of Words representation# Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols, cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely: - tokenizingstrings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. - countingthe occurrences of tokens in each document. - normalizingand weighting with diminishing importance tokens that occur in the majority of samples / documents. In this scheme, features and samples are defined as follows: - eachindividual token occurrence frequency(normalized or not) is treated as afeature. - the vector of all the token frequencies for a givendocumentis considered a multivariatesample. A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus. We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.", "prev_chunk_id": "chunk_327", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_329", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.2. Sparsity#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.2. Sparsity#", "content": "7.2.3.2. Sparsity# As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them). For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually. In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package.", "prev_chunk_id": "chunk_328", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_330", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.3. Common Vectorizer usage#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.3. Common Vectorizer usage#", "content": "7.2.3.3. Common Vectorizer usage# CountVectorizer implements both tokenization and occurrence counting in a single class: >>> from sklearn.feature_extraction.text import CountVectorizer This model has many parameters, however the default values are quite reasonable (please see the reference documentation for the details): >>> vectorizer = CountVectorizer() >>> vectorizer CountVectorizer() Let’s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents: >>> corpus = [ ... 'This is the first document.', ... 'This is the second second document.', ... 'And the third one.', ... 'Is this the first document?', ... ] >>> X = vectorizer.fit_transform(corpus) >>> X <Compressed Sparse...dtype 'int64' with 19 stored elements and shape (4, 9)> The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly: >>> analyze = vectorizer.build_analyzer() >>> analyze(\"This is a text document to analyze.\") == ( ... ['this', 'is', 'text', 'document', 'to', 'analyze']) True Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows: >>> vectorizer.get_feature_names_out() array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'], ...) >>> X.toarray() array([[0, 1, 1, 1, 0, 0, 1, 0, 1], [0, 1, 0, 1, 0, 2, 1, 0, 1], [1, 0, 0, 0, 1, 0, 1, 1, 0], [0, 1, 1, 1, 0, 0, 1, 0, 1]]...) The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer: >>> vectorizer.vocabulary_.get('document') 1 Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method: >>> vectorizer.transform(['Something completely new.']).toarray() array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...) Note that in the", "prev_chunk_id": "chunk_329", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_331", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.3. Common Vectorizer usage#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.3. Common Vectorizer usage#", "content": "previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words): >>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), ... token_pattern=r'\\b\\w+\\b', min_df=1) >>> analyze = bigram_vectorizer.build_analyzer() >>> analyze('Bi-grams are cool!') == ( ... ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']) True The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns: >>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray() >>> X_2 array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0], [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...) In particular the interrogative form “Is this” is only present in the last document: >>> feature_index = bigram_vectorizer.vocabulary_.get('is this') >>> X_2[:, feature_index] array([0, 0, 0, 1]...)", "prev_chunk_id": "chunk_330", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_332", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.4. Using stop words#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.4. Using stop words#", "content": "7.2.3.4. Using stop words# Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as informative for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality. There are several known issues in our provided ‘english’ stop word list. It does not aim to be a general, ‘one-size-fits-all’ solution as some tasks may require a more custom solution. See [NQY18] for more details. Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer. You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we’ve is split into we and ve by CountVectorizer’s default tokenizer, so if we’ve is in stop_words, but ve is not, ve will be retained from we’ve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies. References", "prev_chunk_id": "chunk_331", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_333", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.5. Tf–idf term weighting#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.5. Tf–idf term weighting#", "content": "7.2.3.5. Tf–idf term weighting# In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms. In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform. Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \\(\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}\\). Using the TfidfTransformer’s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as \\(\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1\\), where \\(n\\) is the total number of documents in the document set, and \\(\\text{df}(t)\\) is the number of documents in the document set that contain term \\(t\\). The resulting tf-idf vectors are then normalized by the Euclidean norm: \\(v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}\\). This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering. The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn’s TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation that defines the idf as \\(\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.\\) In the TfidfTransformer and TfidfVectorizer with smooth_idf=False, the “1” count is added to the idf instead of the idf’s denominator: \\(\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1\\) This normalization is implemented by the TfidfTransformer class: >>> from sklearn.feature_extraction.text import TfidfTransformer >>> transformer =", "prev_chunk_id": "chunk_332", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_334", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.5. Tf–idf term weighting#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.5. Tf–idf term weighting#", "content": "TfidfTransformer(smooth_idf=False) >>> transformer TfidfTransformer(smooth_idf=False) Again please see the reference documentation for the details on all the parameters.", "prev_chunk_id": "chunk_333", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_335", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.6. Decoding text files#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.6. Decoding text files#", "content": "7.2.3.6. Decoding text files# Text is made of characters, but files are made of bytes. These bytes represent characters according to some encoding. To work with text files in Python, their bytes must be decoded to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist. The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The CountVectorizer takes an encoding parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (encoding=\"utf-8\"). If the text you are loading is not actually encoded with UTF-8, however, you will get a UnicodeDecodeError. The vectorizers can be told to be silent about decoding errors by setting the decode_error parameter to either \"ignore\" or \"replace\". See the documentation for the Python function bytes.decode for more details (type help(bytes.decode) at the Python prompt).", "prev_chunk_id": "chunk_334", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_336", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.7. Applications and examples#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.7. Applications and examples#", "content": "7.2.3.7. Applications and examples# The bag of words representation is quite simplistic but surprisingly useful in practice. In particular in a supervised setting it can be successfully combined with fast and scalable linear models to train document classifiers, for instance: - Classification of text documents using sparse features In an unsupervised setting it can be used to group similar documents together by applying clustering algorithms such as K-means: - Clustering text documents using k-means Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using Non-negative matrix factorization (NMF or NNMF): - Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation", "prev_chunk_id": "chunk_335", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_337", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.8. Limitations of the Bag of Words representation#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.8. Limitations of the Bag of Words representation#", "content": "7.2.3.8. Limitations of the Bag of Words representation# A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings or word derivations. N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted. One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations. For example, let’s say we’re dealing with a corpus of two documents: ['words', 'wprds']. The second document contains a misspelling of the word ‘words’. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better: >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2)) >>> counts = ngram_vectorizer.fit_transform(['words', 'wprds']) >>> ngram_vectorizer.get_feature_names_out() array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...) >>> counts.toarray().astype(int) array([[1, 1, 1, 0, 1, 1, 1, 0], [1, 1, 0, 1, 1, 1, 0, 1]]) In the above example, char_wb analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The char analyzer, alternatively, creates n-grams that span across words: >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5)) >>> ngram_vectorizer.fit_transform(['jumpy fox']) <Compressed Sparse...dtype 'int64' with 4 stored elements and shape (1, 4)> >>> ngram_vectorizer.get_feature_names_out() array([' fox ', ' jump', 'jumpy', 'umpy '], ...) >>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5)) >>> ngram_vectorizer.fit_transform(['jumpy fox']) <Compressed Sparse...dtype 'int64' with 5 stored elements and shape (1, 5)> >>> ngram_vectorizer.get_feature_names_out() array(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...) The word boundaries-aware", "prev_chunk_id": "chunk_336", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_338", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.8. Limitations of the Bag of Words representation#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.8. Limitations of the Bag of Words representation#", "content": "variant char_wb is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw char variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations. While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure. In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as “Structured output” problems which are currently outside of the scope of scikit-learn.", "prev_chunk_id": "chunk_337", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_339", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.9. Vectorizing a large text corpus with the hashing trick#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.9. Vectorizing a large text corpus with the hashing trick#", "content": "7.2.3.9. Vectorizing a large text corpus with the hashing trick# The above vectorization scheme is simple but the fact that it holds an in-memory mapping from the string tokens to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large datasets: - the larger the corpus, the larger the vocabulary will grow and hence the memory use too, - fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset. - building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner. - pickling and un-pickling vectorizers with a largevocabulary_can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size), - it is not easily possible to split the vectorization work into concurrent sub tasks as thevocabulary_attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on the ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers’ performance to the point of making them slower than the sequential variant. It is possible to overcome those limitations by combining the “hashing trick” (Feature hashing) implemented by the FeatureHasher class and the text preprocessing and tokenization features of the CountVectorizer. This combination is implemented in HashingVectorizer, a transformer class that is mostly API compatible with CountVectorizer. HashingVectorizer is stateless, meaning that you don’t have to call fit on it: >>> from sklearn.feature_extraction.text import HashingVectorizer >>> hv = HashingVectorizer(n_features=10) >>> hv.transform(corpus) <Compressed Sparse...dtype 'float64' with 16 stored elements and shape (4, 10)> You can see that 16 non-zero feature tokens were extracted in the", "prev_chunk_id": "chunk_338", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_340", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.9. Vectorizing a large text corpus with the hashing trick#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.9. Vectorizing a large text corpus with the hashing trick#", "content": "vector output: this is less than the 19 non-zeros extracted previously by the CountVectorizer on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the n_features parameter. In a real world setting, the n_features parameter can be left to its default value of 2 ** 20 (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as 2 ** 18 might help without introducing too many additional collisions on typical text classification tasks. Note that the dimensionality does not affect the CPU training time of algorithms which operate on CSR matrices (LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive) but it does for algorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(), etc.). Let’s try again with the default setting: >>> hv = HashingVectorizer() >>> hv.transform(corpus) <Compressed Sparse...dtype 'float64' with 19 stored elements and shape (4, 1048576)> We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other. The HashingVectorizer also comes with the following limitations: - it is not possible to invert the model (noinverse_transformmethod), nor to access the original string representation of the features, because of the one-way nature of the hash function that performs the mapping. - it does not provide IDF weighting as that would introduce statefulness in the model. ATfidfTransformercan be appended to it in a pipeline if required.", "prev_chunk_id": "chunk_339", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_341", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.3.10. Customizing the vectorizer classes#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.3.10. Customizing the vectorizer classes#", "content": "7.2.3.10. Customizing the vectorizer classes# It is possible to customize the behavior by passing a callable to the vectorizer constructor: >>> def my_tokenizer(s): ... return s.split() ... >>> vectorizer = CountVectorizer(tokenizer=my_tokenizer) >>> vectorizer.build_analyzer()(u\"Some... punctuation!\") == ( ... ['some...', 'punctuation!']) True In particular we name: - preprocessor: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc. - tokenizer: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these. - analyzer: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps. (Lucene users might recognize these names, but be aware that scikit-learn concepts may not map one-to-one onto Lucene concepts.) To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the build_preprocessor, build_tokenizer and build_analyzer factory methods instead of passing custom functions.", "prev_chunk_id": "chunk_340", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_342", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.4.1. Patch extraction#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.4.1. Patch extraction#", "content": "7.2.4.1. Patch extraction# The extract_patches_2d function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use reconstruct_from_patches_2d. For example let us generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format): >>> import numpy as np >>> from sklearn.feature_extraction import image >>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3)) >>> one_image[:, :, 0] # R channel of a fake RGB picture array([[ 0, 3, 6, 9], [12, 15, 18, 21], [24, 27, 30, 33], [36, 39, 42, 45]]) >>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2, ... random_state=0) >>> patches.shape (2, 2, 2, 3) >>> patches[:, :, :, 0] array([[[ 0, 3], [12, 15]], [[15, 18], [27, 30]]]) >>> patches = image.extract_patches_2d(one_image, (2, 2)) >>> patches.shape (9, 2, 2, 3) >>> patches[4, :, :, 0] array([[15, 18], [27, 30]]) Let us now try to reconstruct the original image from the patches by averaging on overlapping areas: >>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3)) >>> np.testing.assert_array_equal(one_image, reconstructed) The PatchExtractor class works in the same way as extract_patches_2d, only it supports multiple images as input. It is implemented as a scikit-learn transformer, so it can be used in pipelines. See: >>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3) >>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images) >>> patches.shape (45, 2, 2, 3)", "prev_chunk_id": "chunk_341", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_343", "url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "title": "7.2.4.2. Connectivity graph of an image#", "page_title": "7.2. Feature extraction — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.2.4.2. Connectivity graph of an image#", "content": "7.2.4.2. Connectivity graph of an image# Several estimators in scikit-learn can use connectivity information between features or samples. For instance Ward clustering (Hierarchical clustering) can cluster together only neighboring pixels of an image, thus forming contiguous patches: For this purpose, the estimators use a ‘connectivity’ matrix, giving which samples are connected. The function img_to_graph returns such a matrix from a 2D or 3D image. Similarly, grid_to_graph builds a connectivity matrix for images given the shape of these images. These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (Hierarchical clustering), but also to build precomputed kernels, or similarity matrices.", "prev_chunk_id": "chunk_342", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_344", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1. Pipelines and composite estimators#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1. Pipelines and composite estimators#", "content": "7.1. Pipelines and composite estimators# To build a composite estimator, transformers are usually combined with other transformers or with predictors (such as classifiers or regressors). The most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a transformer. The last step can be anything, a transformer, a predictor, or a clustering estimator which might have or not have a .predict(...) method. A pipeline exposes all methods provided by the last estimator: if the last step provides a transform method, then the pipeline would have a transform method and behave like a transformer. If the last step provides a predict method, then the pipeline would expose that method, and given a data X, use all steps except the last to transform the data, and then give that transformed data to the predict method of the last step of the pipeline. The class Pipeline is often used in combination with ColumnTransformer or FeatureUnion which concatenate the output of transformers into a composite feature space. TransformedTargetRegressor deals with transforming the target (i.e. log-transform y).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_345", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1. Pipeline: chaining estimators#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1. Pipeline: chaining estimators#", "content": "7.1.1. Pipeline: chaining estimators# Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here: All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).", "prev_chunk_id": "chunk_344", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_346", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1.1.1. Build a pipeline#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1.1.1. Build a pipeline#", "content": "7.1.1.1.1. Build a pipeline# The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object: >>> from sklearn.pipeline import Pipeline >>> from sklearn.svm import SVC >>> from sklearn.decomposition import PCA >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())] >>> pipe = Pipeline(estimators) >>> pipe Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])", "prev_chunk_id": "chunk_345", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_347", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1.1.2. Access pipeline steps#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1.1.2. Access pipeline steps#", "content": "7.1.1.1.2. Access pipeline steps# The estimators of a pipeline are stored as a list in the steps attribute. A sub-pipeline can be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse): >>> pipe[:1] Pipeline(steps=[('reduce_dim', PCA())]) >>> pipe[-1:] Pipeline(steps=[('clf', SVC())])", "prev_chunk_id": "chunk_346", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_348", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1.1.3. Tracking feature names in a pipeline#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1.1.3. Tracking feature names in a pipeline#", "content": "7.1.1.1.3. Tracking feature names in a pipeline# To enable model inspection, Pipeline has a get_feature_names_out() method, just like all transformers. You can use pipeline slicing to get the feature names going into each step: >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.feature_selection import SelectKBest >>> iris = load_iris() >>> pipe = Pipeline(steps=[ ... ('select', SelectKBest(k=2)), ... ('clf', LogisticRegression())]) >>> pipe.fit(iris.data, iris.target) Pipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))]) >>> pipe[:-1].get_feature_names_out() array(['x2', 'x3'], ...)", "prev_chunk_id": "chunk_347", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_349", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1.1.4. Access to nested parameters#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1.1.4. Access to nested parameters#", "content": "7.1.1.1.4. Access to nested parameters# It is common to adjust the parameters of an estimator within a pipeline. This parameter is therefore nested because it belongs to a particular sub-step. Parameters of the estimators in the pipeline are accessible using the <estimator>__<parameter> syntax: >>> pipe = Pipeline(steps=[(\"reduce_dim\", PCA()), (\"clf\", SVC())]) >>> pipe.set_params(clf__C=10) Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))]) Examples - Pipeline ANOVA SVM - Sample pipeline for text feature extraction and evaluation - Pipelining: chaining a PCA and a logistic regression - Explicit feature map approximation for RBF kernels - SVM-Anova: SVM with univariate feature selection - Selecting dimensionality reduction with Pipeline and GridSearchCV - Displaying Pipelines", "prev_chunk_id": "chunk_348", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_350", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.1.2. Caching transformers: avoid repeated computation#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.1.2. Caching transformers: avoid repeated computation#", "content": "7.1.1.2. Caching transformers: avoid repeated computation# Fitting transformers may be computationally expensive. With its memory parameter set, Pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration. The last step will never be cached, even if it is a transformer. The parameter memory is needed in order to cache the transformers. memory can be either a string containing the directory where to cache the transformers or a joblib.Memory object: >>> from tempfile import mkdtemp >>> from shutil import rmtree >>> from sklearn.decomposition import PCA >>> from sklearn.svm import SVC >>> from sklearn.pipeline import Pipeline >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())] >>> cachedir = mkdtemp() >>> pipe = Pipeline(estimators, memory=cachedir) >>> pipe Pipeline(memory=..., steps=[('reduce_dim', PCA()), ('clf', SVC())]) >>> # Clear the cache directory when you don't need it anymore >>> rmtree(cachedir) Examples - Selecting dimensionality reduction with Pipeline and GridSearchCV", "prev_chunk_id": "chunk_349", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_351", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.2. Transforming target in regression#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.2. Transforming target in regression#", "content": "7.1.2. Transforming target in regression# TransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable: >>> import numpy as np >>> from sklearn.datasets import make_regression >>> from sklearn.compose import TransformedTargetRegressor >>> from sklearn.preprocessing import QuantileTransformer >>> from sklearn.linear_model import LinearRegression >>> from sklearn.model_selection import train_test_split >>> # create a synthetic dataset >>> X, y = make_regression(n_samples=20640, ... n_features=8, ... noise=100.0, ... random_state=0) >>> y = np.exp( 1 + (y - y.min()) * (4 / (y.max() - y.min()))) >>> X, y = X[:2000, :], y[:2000] # select a subset of data >>> transformer = QuantileTransformer(output_distribution='normal') >>> regressor = LinearRegression() >>> regr = TransformedTargetRegressor(regressor=regressor, ... transformer=transformer) >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) >>> regr.fit(X_train, y_train) TransformedTargetRegressor(...) >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\") R2 score: 0.67 >>> raw_target_regr = LinearRegression().fit(X_train, y_train) >>> print(f\"R2 score: {raw_target_regr.score(X_test, y_test):.2f}\") R2 score: 0.64 For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping: >>> def func(x): ... return np.log(x) >>> def inverse_func(x): ... return np.exp(x) Subsequently, the object is created as: >>> regr = TransformedTargetRegressor(regressor=regressor, ... func=func, ... inverse_func=inverse_func) >>> regr.fit(X_train, y_train) TransformedTargetRegressor(...) >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\") R2 score: 0.67 By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting check_inverse to False: >>> def inverse_func(x): ... return x >>> regr = TransformedTargetRegressor(regressor=regressor, ... func=func, ... inverse_func=inverse_func, ... check_inverse=False) >>> regr.fit(X_train, y_train) TransformedTargetRegressor(...) >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\") R2 score: -3.02 Examples - Effect of transforming the targets", "prev_chunk_id": "chunk_350", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_352", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.2. Transforming target in regression#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.2. Transforming target in regression#", "content": "in regression model", "prev_chunk_id": "chunk_351", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_353", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.3. FeatureUnion: composite feature spaces#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.3. FeatureUnion: composite feature spaces#", "content": "7.1.3. FeatureUnion: composite feature spaces# FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix. When you want to apply different transformations to each field of the data, see the related class ColumnTransformer (see user guide). FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation. FeatureUnion and Pipeline can be combined to create complex models. (A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are is the caller’s responsibility.)", "prev_chunk_id": "chunk_352", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_354", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.3.1. Usage#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.3.1. Usage#", "content": "7.1.3.1. Usage# A FeatureUnion is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object: >>> from sklearn.pipeline import FeatureUnion >>> from sklearn.decomposition import PCA >>> from sklearn.decomposition import KernelPCA >>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())] >>> combined = FeatureUnion(estimators) >>> combined FeatureUnion(transformer_list=[('linear_pca', PCA()), ('kernel_pca', KernelPCA())]) Like pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming of the components. Like Pipeline, individual steps may be replaced using set_params, and ignored by setting to 'drop': >>> combined.set_params(kernel_pca='drop') FeatureUnion(transformer_list=[('linear_pca', PCA()), ('kernel_pca', 'drop')]) Examples - Concatenating multiple feature extraction methods", "prev_chunk_id": "chunk_353", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_355", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.4. ColumnTransformer for heterogeneous data#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.4. ColumnTransformer for heterogeneous data#", "content": "7.1.4. ColumnTransformer for heterogeneous data# Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons: - Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known asdata leakage), for example in the case of scalers or imputing missing values. - You may want to include the parameters of the preprocessors in aparameter search. The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames. To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method: >>> import pandas as pd >>> X = pd.DataFrame( ... {'city': ['London', 'London', 'Paris', 'Sallisaw'], ... 'title': [\"His Last Bow\", \"How Watson Learned the Trick\", ... \"A Moveable Feast\", \"The Grapes of Wrath\"], ... 'expert_rating': [5, 3, 4, 5], ... 'user_rating': [4, 5, 4, 3]}) For this data, we might want to encode the 'city' column as a categorical variable using OneHotEncoder but apply a CountVectorizer to the 'title' column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say 'city_category' and 'title_bow'. By default, the remaining rating columns are ignored (remainder='drop'): >>> from sklearn.compose import ColumnTransformer >>> from sklearn.feature_extraction.text import CountVectorizer >>> from sklearn.preprocessing import OneHotEncoder >>> column_trans = ColumnTransformer( ... [('categories', OneHotEncoder(dtype='int'), ['city']), ... ('title_bow', CountVectorizer(), 'title')], ... remainder='drop', verbose_feature_names_out=False) >>> column_trans.fit(X) ColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'), ['city']), ('title_bow', CountVectorizer(), 'title')], verbose_feature_names_out=False) >>> column_trans.get_feature_names_out() array(['city_London', 'city_Paris',", "prev_chunk_id": "chunk_354", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_356", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.4. ColumnTransformer for heterogeneous data#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.4. ColumnTransformer for heterogeneous data#", "content": "'city_Sallisaw', 'bow', 'feast', 'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the', 'trick', 'watson', 'wrath'], ...) >>> column_trans.transform(X).toarray() array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...) In the above example, the CountVectorizer expects a 1D array as input and therefore the columns were specified as a string ('title'). However, OneHotEncoder as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (['city']). Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a make_column_selector. The make_column_selector is used to select columns based on data type or column name: >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.compose import make_column_selector >>> ct = ColumnTransformer([ ... ('scale', StandardScaler(), ... make_column_selector(dtype_include=np.number)), ... ('onehot', ... OneHotEncoder(), ... make_column_selector(pattern='city', dtype_include=object))]) >>> ct.fit_transform(X) array([[ 0.904, 0. , 1. , 0. , 0. ], [-1.507, 1.414, 1. , 0. , 0. ], [-0.301, 0. , 0. , 1. , 0. ], [ 0.904, -1.414, 0. , 0. , 1. ]]) Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns. We can keep the remaining rating columns by setting remainder='passthrough'. The values are appended to the end of the transformation: >>> column_trans = ColumnTransformer( ... [('city_category', OneHotEncoder(dtype='int'),['city']), ... ('title_bow', CountVectorizer(), 'title')], ... remainder='passthrough') >>> column_trans.fit_transform(X) array([[1, 0, 0, 1, 0, 0, 1,", "prev_chunk_id": "chunk_355", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_357", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.4. ColumnTransformer for heterogeneous data#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.4. ColumnTransformer for heterogeneous data#", "content": "0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4], [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...) The remainder parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation: >>> from sklearn.preprocessing import MinMaxScaler >>> column_trans = ColumnTransformer( ... [('city_category', OneHotEncoder(), ['city']), ... ('title_bow', CountVectorizer(), 'title')], ... remainder=MinMaxScaler()) >>> column_trans.fit_transform(X)[:, -2:] array([[1. , 0.5], [0. , 1. ], [0.5, 0.5], [1. , 0. ]]) The make_column_transformer function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically. The equivalent for the above example would be: >>> from sklearn.compose import make_column_transformer >>> column_trans = make_column_transformer( ... (OneHotEncoder(), ['city']), ... (CountVectorizer(), 'title'), ... remainder=MinMaxScaler()) >>> column_trans ColumnTransformer(remainder=MinMaxScaler(), transformers=[('onehotencoder', OneHotEncoder(), ['city']), ('countvectorizer', CountVectorizer(), 'title')]) If ColumnTransformer is fitted with a dataframe and the dataframe only has string column names, then transforming a dataframe will use the column names to select the columns: >>> ct = ColumnTransformer( ... [(\"scale\", StandardScaler(), [\"expert_rating\"])]).fit(X) >>> X_new = pd.DataFrame({\"expert_rating\": [5, 6, 1], ... \"ignored_new_col\": [1.2, 0.3, -0.1]}) >>> ct.transform(X_new) array([[ 0.9], [ 2.1], [-3.9]])", "prev_chunk_id": "chunk_356", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_358", "url": "https://scikit-learn.org/stable/modules/compose.html", "title": "7.1.5. Visualizing Composite Estimators#", "page_title": "7.1. Pipelines and composite estimators — scikit-learn 1.7.1 documentation", "breadcrumbs": "7.1.5. Visualizing Composite Estimators#", "content": "7.1.5. Visualizing Composite Estimators# Estimators are displayed with an HTML representation when shown in a jupyter notebook. This is useful to diagnose or visualize a Pipeline with many estimators. This visualization is activated by default: >>> column_trans It can be deactivated by setting the display option in set_config to ‘text’: >>> from sklearn import set_config >>> set_config(display='text') >>> # displays text representation in a jupyter context >>> column_trans An example of the HTML output can be seen in the HTML representation of Pipeline section of Column Transformer with Mixed Types. As an alternative, the HTML can be written to a file using estimator_html_repr: >>> from sklearn.utils import estimator_html_repr >>> with open('my_estimator.html', 'w') as f: ... f.write(estimator_html_repr(clf)) Examples - Column Transformer with Heterogeneous Data Sources - Column Transformer with Mixed Types", "prev_chunk_id": "chunk_357", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_359", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html", "title": "5.2. Permutation feature importance#", "page_title": "5.2. Permutation feature importance — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.2. Permutation feature importance#", "content": "5.2. Permutation feature importance# Permutation feature importance is a model inspection technique that measures the contribution of each feature to a fitted model’s statistical performance on a given tabular dataset. This technique is particularly useful for non-linear or opaque estimators, and involves randomly shuffling the values of a single feature and observing the resulting degradation of the model’s score [1]. By breaking the relationship between the feature and the target, we determine how much the model relies on such particular feature. In the following figures, we observe the effect of permuting features on the correlation between the feature and the target and consequently on the model’s statistical performance. On the top figure, we observe that permuting a predictive feature breaks the correlation between the feature and the target, and consequently the model’s statistical performance decreases. On the bottom figure, we observe that permuting a non-predictive feature does not significantly degrade the model’s statistical performance. One key advantage of permutation feature importance is that it is model-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can be calculated multiple times with different permutations of the feature, further providing a measure of the variance in the estimated feature importances for the specific trained model. The figure below shows the permutation feature importance of a RandomForestClassifier trained on an augmented version of the titanic dataset that contains a random_cat and a random_num features, i.e. a categorical and a numerical feature that are not correlated in any way with the target variable: The permutation_importance function calculates the feature importance of estimators for a given dataset. The n_repeats parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances. Let’s consider the following trained regression model: >>> from sklearn.datasets import load_diabetes >>> from sklearn.model_selection import", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_360", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html", "title": "5.2. Permutation feature importance#", "page_title": "5.2. Permutation feature importance — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.2. Permutation feature importance#", "content": "train_test_split >>> from sklearn.linear_model import Ridge >>> diabetes = load_diabetes() >>> X_train, X_val, y_train, y_val = train_test_split( ... diabetes.data, diabetes.target, random_state=0) ... >>> model = Ridge(alpha=1e-2).fit(X_train, y_train) >>> model.score(X_val, y_val) 0.356... Its validation performance, measured via the \\(R^2\\) score, is significantly larger than the chance level. This makes it possible to use the permutation_importance function to probe which features are most predictive: >>> from sklearn.inspection import permutation_importance >>> r = permutation_importance(model, X_val, y_val, ... n_repeats=30, ... random_state=0) ... >>> for i in r.importances_mean.argsort()[::-1]: ... if r.importances_mean[i] - 2 * r.importances_std[i] > 0: ... print(f\"{diabetes.feature_names[i]:<8}\" ... f\"{r.importances_mean[i]:.3f}\" ... f\" +/- {r.importances_std[i]:.3f}\") ... s5 0.204 +/- 0.050 bmi 0.176 +/- 0.048 bp 0.088 +/- 0.033 sex 0.056 +/- 0.023 Note that the importance values for the top features represent a large fraction of the reference score of 0.356. Permutation importances can be computed either on the training set or on a held-out testing or validation set. Using a held-out set makes it possible to highlight which features contribute the most to the generalization power of the inspected model. Features that are important on the training set but not on the held-out set might cause the model to overfit. The permutation feature importance depends on the score function that is specified with the scoring argument. This argument accepts multiple scorers, which is more computationally efficient than sequentially calling permutation_importance several times with a different scorer, as it reuses model predictions.", "prev_chunk_id": "chunk_359", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_361", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html", "title": "5.2.1. Outline of the permutation importance algorithm#", "page_title": "5.2. Permutation feature importance — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.2.1. Outline of the permutation importance algorithm#", "content": "5.2.1. Outline of the permutation importance algorithm# - Inputs: fitted predictive model\\(m\\), tabular dataset (training or validation)\\(D\\). - Compute the reference score\\(s\\)of the model\\(m\\)on data\\(D\\)(for instance the accuracy for a classifier or the\\(R^2\\)for a regressor). - For each feature\\(j\\)(column of\\(D\\)):For each repetition\\(k\\)in\\({1, ..., K}\\):Randomly shuffle column\\(j\\)of dataset\\(D\\)to generate a corrupted version of the data named\\(\\tilde{D}_{k,j}\\).Compute the score\\(s_{k,j}\\)of model\\(m\\)on corrupted data\\(\\tilde{D}_{k,j}\\).Compute importance\\(i_j\\)for feature\\(f_j\\)defined as:\\[i_j = s - \\frac{1}{K} \\sum_{k=1}^{K} s_{k,j}\\]", "prev_chunk_id": "chunk_360", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_362", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html", "title": "5.2.2. Relation to impurity-based importance in trees#", "page_title": "5.2. Permutation feature importance — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.2.2. Relation to impurity-based importance in trees#", "content": "5.2.2. Relation to impurity-based importance in trees# Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Log Loss or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data. Furthermore, impurity-based feature importance for trees is strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories. Permutation-based feature importances do not exhibit such a bias. Additionally, the permutation feature importance may be computed with any performance metric on the model predictions and can be used to analyze any model class (not just tree-based models). The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: Permutation Importance vs Random Forest Feature Importance (MDI).", "prev_chunk_id": "chunk_361", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_363", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html", "title": "5.2.3. Misleading values on strongly correlated features#", "page_title": "5.2. Permutation feature importance — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.2.3. Misleading values on strongly correlated features#", "content": "5.2.3. Misleading values on strongly correlated features# When two features are correlated and one of the features is permuted, the model still has access to the latter through its correlated feature. This results in a lower reported importance value for both features, though they might actually be important. The figure below shows the permutation feature importance of a RandomForestClassifier trained using the Breast cancer Wisconsin (diagnostic) dataset, which contains strongly correlated features. A naive interpretation would suggest that all features are unimportant: One way to handle the issue is to cluster features that are correlated and only keep one feature from each cluster. For more details on such strategy, see the example Permutation Importance with Multicollinear or Correlated Features. Examples - Permutation Importance vs Random Forest Feature Importance (MDI) - Permutation Importance with Multicollinear or Correlated Features References", "prev_chunk_id": "chunk_362", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_364", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1. Partial Dependence and Individual Conditional Expectation plots#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1. Partial Dependence and Individual Conditional Expectation plots#", "content": "5.1. Partial Dependence and Individual Conditional Expectation plots# Partial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response [1] and a set of input features of interest. Both PDPs [H2009] and ICEs [G2015] assume that the input features of interest are independent from the complement features, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE [M2019].", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_365", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.1. Partial dependence plots#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.1. Partial dependence plots#", "content": "5.1.1. Partial dependence plots# Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest. Due to the limits of human perception, the size of the set of input features of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features. The figure below shows two one-way and one two-way partial dependence plots for the bike sharing dataset, with a HistGradientBoostingRegressor: One-way PDPs tell us about the interaction between the target response and an input feature of interest (e.g. linear, non-linear). The left plot in the above figure shows the effect of the temperature on the number of bike rentals; we can clearly see that a higher temperature is related with a higher number of bike rentals. Similarly, we could analyze the effect of the humidity on the number of bike rentals (middle plot). Thus, these interpretations are marginal, considering a feature at a time. PDPs with two input features of interest show the interactions among the two features. For example, the two-variable PDP in the above figure shows the dependence of the number of bike rentals on joint values of temperature and humidity. We can clearly see an interaction between the two features: with a temperature higher than 20 degrees Celsius, mainly the humidity has a strong impact on the number of bike rentals. For lower temperatures, both the temperature and the humidity have an impact on the number of bike rentals. The sklearn.inspection module provides a convenience function from_estimator to create one-way and two-way partial", "prev_chunk_id": "chunk_364", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_366", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.1. Partial dependence plots#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.1. Partial dependence plots#", "content": "dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features 0 and 1 and a two-way PDP between the two features: >>> from sklearn.datasets import make_hastie_10_2 >>> from sklearn.ensemble import GradientBoostingClassifier >>> from sklearn.inspection import PartialDependenceDisplay >>> X, y = make_hastie_10_2(random_state=0) >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, ... max_depth=1, random_state=0).fit(X, y) >>> features = [0, 1, (0, 1)] >>> PartialDependenceDisplay.from_estimator(clf, X, features) <...> You can access the newly created figure and Axes objects using plt.gcf() and plt.gca(). To make a partial dependence plot with categorical features, you need to specify which features are categorical using the parameter categorical_features. This parameter takes a list of indices, names of the categorical features or a boolean mask. The graphical representation of partial dependence for categorical features is a bar plot or a 2D heatmap. If you need the raw values of the partial dependence function rather than the plots, you can use the sklearn.inspection.partial_dependence function: >>> from sklearn.inspection import partial_dependence >>> results = partial_dependence(clf, X, [0]) >>> results[\"average\"] array([[ 2.466..., 2.466..., ... >>> results[\"grid_values\"] [array([-1.624..., -1.592..., ... The values at which the partial dependence should be evaluated are directly generated from X. For 2-way partial dependence, a 2D-grid of values is generated. The values field returned by sklearn.inspection.partial_dependence gives the actual values used in the grid for each input feature of interest. They also correspond to the axis of the plots.", "prev_chunk_id": "chunk_365", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_367", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.2. Individual conditional expectation (ICE) plot#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.2. Individual conditional expectation (ICE) plot#", "content": "5.1.2. Individual conditional expectation (ICE) plot# Similar to a PDP, an individual conditional expectation (ICE) plot shows the dependence between the target function and an input feature of interest. However, unlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample. Due to the limits of human perception, only one input feature of interest is supported for ICE plots. The figures below show two ICE plots for the bike sharing dataset, with a HistGradientBoostingRegressor. The figures plot the corresponding PD line overlaid on ICE lines. While the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we see that the ICE for the temperature feature gives us some additional information: some of the ICE lines are flat while some others show a decrease of the dependence for temperature above 35 degrees Celsius. We observe a similar pattern for the humidity feature: some of the ICE lines show a sharp decrease when the humidity is above 80%. The sklearn.inspection module’s PartialDependenceDisplay.from_estimator convenience function can be used to create ICE plots by setting kind='individual'. In the example below, we show how to create a grid of ICE plots: >>> from sklearn.datasets import make_hastie_10_2 >>> from sklearn.ensemble import GradientBoostingClassifier >>> from sklearn.inspection import PartialDependenceDisplay >>> X, y = make_hastie_10_2(random_state=0) >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, ... max_depth=1, random_state=0).fit(X, y) >>> features = [0, 1] >>> PartialDependenceDisplay.from_estimator(clf, X, features, ... kind='individual') <...> In ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use", "prev_chunk_id": "chunk_366", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_368", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.2. Individual conditional expectation (ICE) plot#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.2. Individual conditional expectation (ICE) plot#", "content": "ICE plots alongside PDPs. They can be plotted together with kind='both'. >>> PartialDependenceDisplay.from_estimator(clf, X, features, ... kind='both') <...> If there are too many lines in an ICE plot, it can be difficult to see differences between individual samples and interpret the model. Centering the ICE at the first value on the x-axis, produces centered Individual Conditional Expectation (cICE) plots [G2015]. This puts emphasis on the divergence of individual conditional expectations from the mean line, thus making it easier to explore heterogeneous relationships. cICE plots can be plotted by setting centered=True: >>> PartialDependenceDisplay.from_estimator(clf, X, features, ... kind='both', centered=True) <...>", "prev_chunk_id": "chunk_367", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_369", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.3. Mathematical Definition#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.3. Mathematical Definition#", "content": "5.1.3. Mathematical Definition# Let \\(X_S\\) be the set of input features of interest (i.e. the features parameter) and let \\(X_C\\) be its complement. The partial dependence of the response \\(f\\) at a point \\(x_S\\) is defined as: where \\(f(x_S, x_C)\\) is the response function (predict, predict_proba or decision_function) for a given sample whose values are defined by \\(x_S\\) for the features in \\(X_S\\), and by \\(x_C\\) for the features in \\(X_C\\). Note that \\(x_S\\) and \\(x_C\\) may be tuples. Computing this integral for various values of \\(x_S\\) produces a PDP plot as above. An ICE line is defined as a single \\(f(x_{S}, x_{C}^{(i)})\\) evaluated at \\(x_{S}\\).", "prev_chunk_id": "chunk_368", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_370", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.4. Computation methods#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.4. Computation methods#", "content": "5.1.4. Computation methods# There are two main methods to approximate the integral above, namely the 'brute' and 'recursion' methods. The method parameter controls which method to use. The 'brute' method is a generic method that works with any estimator. Note that computing ICE plots is only supported with the 'brute' method. It approximates the above integral by computing an average over the data X: where \\(x_C^{(i)}\\) is the value of the i-th sample for the features in \\(X_C\\). For each value of \\(x_S\\), this method requires a full pass over the dataset X which is computationally intensive. Each of the \\(f(x_{S}, x_{C}^{(i)})\\) corresponds to one ICE line evaluated at \\(x_{S}\\). Computing this for multiple values of \\(x_{S}\\), one obtains a full ICE line. As one can see, the average of the ICE lines corresponds to the partial dependence line. The 'recursion' method is faster than the 'brute' method, but it is only supported for PDP plots by some tree-based estimators. It is computed as follows. For a given point \\(x_S\\), a weighted tree traversal is performed: if a split node involves an input feature of interest, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves’ values. With the 'brute' method, the parameter X is used both for generating the grid of values \\(x_S\\) and the complement feature values \\(x_C\\). However with the ‘recursion’ method, X is only used for the grid values: implicitly, the \\(x_C\\) values are those of the training data. By default, the 'recursion' method is used for plotting PDPs on tree-based estimators that support it, and ‘brute’ is used for the rest. Examples -", "prev_chunk_id": "chunk_369", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_371", "url": "https://scikit-learn.org/stable/modules/partial_dependence.html", "title": "5.1.4. Computation methods#", "page_title": "5.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.7.1 documentation", "breadcrumbs": "5.1.4. Computation methods#", "content": "Partial Dependence and Individual Conditional Expectation Plots Footnotes References", "prev_chunk_id": "chunk_370", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_372", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5. Validation curves: plotting scores to evaluate models#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5. Validation curves: plotting scores to evaluate models#", "content": "3.5. Validation curves: plotting scores to evaluate models# Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data. In the following plot, we see a function \\(f(x) = \\cos (\\frac{3}{2} \\pi x)\\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance). Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see Bias-variance dilemma). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance. In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below. Examples - Underfitting vs. Overfitting - Effect of model regularization on training", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_373", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5. Validation curves: plotting scores to evaluate models#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5. Validation curves: plotting scores to evaluate models#", "content": "and test error - Plotting Learning Curves and Checking Models’ Scalability", "prev_chunk_id": "chunk_372", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_374", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5.1. Validation curve#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5.1. Validation curve#", "content": "3.5.1. Validation curve# To validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator is of course grid search or similar methods (see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimize the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set. However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values. The function validation_curve can help in this case: >>> import numpy as np >>> from sklearn.model_selection import validation_curve >>> from sklearn.datasets import load_iris >>> from sklearn.svm import SVC >>> np.random.seed(0) >>> X, y = load_iris(return_X_y=True) >>> indices = np.arange(y.shape[0]) >>> np.random.shuffle(indices) >>> X, y = X[indices], y[indices] >>> train_scores, valid_scores = validation_curve( ... SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 3), ... ) >>> train_scores array([[0.90, 0.94, 0.91, 0.89, 0.92], [0.9 , 0.92, 0.93, 0.92, 0.93], [0.97, 1 , 0.98, 0.97, 0.99]]) >>> valid_scores array([[0.9, 0.9 , 0.9 , 0.96, 0.9 ], [0.9, 0.83, 0.96, 0.96, 0.93], [1. , 0.93, 1 , 1 , 0.9 ]]) If you intend to plot the validation curves only, the class ValidationCurveDisplay is more direct than using matplotlib manually on the results of a call to validation_curve. You can use the method from_estimator similarly to validation_curve to generate and plot the validation curve: from sklearn.datasets import load_iris from", "prev_chunk_id": "chunk_373", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_375", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5.1. Validation curve#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5.1. Validation curve#", "content": "sklearn.model_selection import ValidationCurveDisplay from sklearn.svm import SVC from sklearn.utils import shuffle X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) ValidationCurveDisplay.from_estimator( SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 10) ) If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible.", "prev_chunk_id": "chunk_374", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_376", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5.2. Learning curve#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5.2. Learning curve#", "content": "3.5.2. Learning curve# A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM. For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data. In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization. We can use the function learning_curve to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets): >>> from sklearn.model_selection import learning_curve >>> from sklearn.svm import SVC >>> train_sizes, train_scores, valid_scores = learning_curve( ... SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5) >>> train_sizes array([ 50, 80, 110]) >>> train_scores array([[0.98, 0.98 , 0.98, 0.98, 0.98], [0.98, 1. , 0.98, 0.98, 0.98], [0.98, 1. , 0.98, 0.98, 0.99]]) >>> valid_scores array([[1. , 0.93, 1. , 1. , 0.96], [1. , 0.96, 1. , 1. , 0.96], [1. , 0.96, 1. , 1. , 0.96]]) If you intend to plot the learning curves only, the class LearningCurveDisplay will be easier to use. You can use the method from_estimator similarly to learning_curve to generate and plot the learning curve: from sklearn.datasets import load_iris from sklearn.model_selection import LearningCurveDisplay from sklearn.svm import SVC from sklearn.utils", "prev_chunk_id": "chunk_375", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_377", "url": "https://scikit-learn.org/stable/modules/learning_curve.html", "title": "3.5.2. Learning curve#", "page_title": "3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.5.2. Learning curve#", "content": "import shuffle X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) LearningCurveDisplay.from_estimator( SVC(kernel=\"linear\"), X, y, train_sizes=[50, 80, 110], cv=5) Examples - SeePlotting Learning Curves and Checking Models’ Scalabilityfor an example of using learning curves to check the scalability of a predictive model.", "prev_chunk_id": "chunk_376", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_378", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.1. Which scoring function should I use?#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.1. Which scoring function should I use?#", "content": "3.4.1. Which scoring function should I use?# Before we take a closer look into the details of the many scores and evaluation metrics, we want to give some guidance, inspired by statistical decision theory, on the choice of scoring functions for supervised learning, see [Gneiting2009]: - Which scoring function should I use? - Which scoring function is a good one for my task? In a nutshell, if the scoring function is given, e.g. in a kaggle competition or in a business context, use that one. If you are free to choose, it starts by considering the ultimate goal and application of the prediction. It is useful to distinguish two steps: - Predicting - Decision making Predicting: Usually, the response variable \\(Y\\) is a random variable, in the sense that there is no deterministic function \\(Y = g(X)\\) of the features \\(X\\). Instead, there is a probability distribution \\(F\\) of \\(Y\\). One can aim to predict the whole distribution, known as probabilistic prediction, or—more the focus of scikit-learn—issue a point prediction (or point forecast) by choosing a property or functional of that distribution \\(F\\). Typical examples are the mean (expected value), the median or a quantile of the response variable \\(Y\\) (conditionally on \\(X\\)). Once that is settled, use a strictly consistent scoring function for that (target) functional, see [Gneiting2009]. This means using a scoring function that is aligned with measuring the distance between predictions y_pred and the true target functional using observations of \\(Y\\), i.e. y_true. For classification strictly proper scoring rules, see Wikipedia entry for Scoring rule and [Gneiting2007], coincide with strictly consistent scoring functions. The table further below provides examples. One could say that consistent scoring functions act as truth serum in that they guarantee “that truth telling […] is an optimal strategy in expectation” [Gneiting2014]. Once a", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_379", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.1. Which scoring function should I use?#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.1. Which scoring function should I use?#", "content": "strictly consistent scoring function is chosen, it is best used for both: as loss function for model training and as metric/score in model evaluation and model comparison. Note that for regressors, the prediction is done with predict while for classifiers it is usually predict_proba. Decision Making: The most common decisions are done on binary classification tasks, where the result of predict_proba is turned into a single outcome, e.g., from the predicted probability of rain a decision is made on how to act (whether to take mitigating measures like an umbrella or not). For classifiers, this is what predict returns. See also Tuning the decision threshold for class prediction. There are many scoring functions which measure different aspects of such a decision, most of them are covered with or derived from the metrics.confusion_matrix. List of strictly consistent scoring functions: Here, we list some of the most relevant statistical functionals and corresponding strictly consistent scoring functions for tasks in practice. Note that the list is not complete and that there are more of them. For further criteria on how to select a specific one, see [Fissler2022]. 1 The Brier score is just a different name for the squared error in case of classification. 2 The zero-one loss is only consistent but not strictly consistent for the mode. The zero-one loss is equivalent to one minus the accuracy score, meaning it gives different score values but the same ranking. 3 R² gives the same ranking as squared error. Fictitious Example: Let’s make the above arguments more tangible. Consider a setting in network reliability engineering, such as maintaining stable internet or Wi-Fi connections. As provider of the network, you have access to the dataset of log entries of network connections containing network load over time and many interesting features. Your goal is to improve", "prev_chunk_id": "chunk_378", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_380", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.1. Which scoring function should I use?#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.1. Which scoring function should I use?#", "content": "the reliability of the connections. In fact, you promise your customers that on at least 99% of all days there are no connection discontinuities larger than 1 minute. Therefore, you are interested in a prediction of the 99% quantile (of longest connection interruption duration per day) in order to know in advance when to add more bandwidth and thereby satisfy your customers. So the target functional is the 99% quantile. From the table above, you choose the pinball loss as scoring function (fair enough, not much choice given), for model training (e.g. HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.99)) as well as model evaluation (mean_pinball_loss(..., alpha=0.99) - we apologize for the different argument names, quantile and alpha) be it in grid search for finding hyperparameters or in comparing to other models like QuantileRegressor(quantile=0.99). References", "prev_chunk_id": "chunk_379", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_381", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.2. Scoring API overview#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.2. Scoring API overview#", "content": "3.4.2. Scoring API overview# There are 3 different APIs for evaluating the quality of a model’s predictions: - Estimator score method: Estimators have ascoremethod providing a default evaluation criterion for the problem they are designed to solve. Most commonly this isaccuracyfor classifiers and thecoefficient of determination(\\(R^2\\)) for regressors. Details for each estimator can be found in its documentation. - Scoring parameter: Model-evaluation tools that usecross-validation(such asmodel_selection.GridSearchCV,model_selection.validation_curveandlinear_model.LogisticRegressionCV) rely on an internalscoringstrategy. This can be specified using thescoringparameter of that tool and is discussed in the sectionThe scoring parameter: defining model evaluation rules. - Metric functions: Thesklearn.metricsmodule implements functions assessing prediction error for specific purposes. These metrics are detailed in sections onClassification metrics,Multilabel ranking metrics,Regression metricsandClustering metrics. Finally, Dummy estimators are useful to get a baseline value of those metrics for random predictions.", "prev_chunk_id": "chunk_380", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_382", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3. The scoring parameter: defining model evaluation rules#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3. The scoring parameter: defining model evaluation rules#", "content": "3.4.3. The scoring parameter: defining model evaluation rules# Model selection and evaluation tools that internally use cross-validation (such as model_selection.GridSearchCV, model_selection.validation_curve and linear_model.LogisticRegressionCV) take a scoring parameter that controls what metric they apply to the estimators evaluated. They can be specified in several ways: - None: the estimator’s default evaluation criterion (i.e., the metric used in the estimator’sscoremethod) is used. - String name: common metrics can be passed via a string name. - Callable: more complex metrics can be passed via a custom metric callable (e.g., function). Some tools do also accept multiple metric evaluation. See Using multiple metric evaluation for details.", "prev_chunk_id": "chunk_381", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_383", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3.1. String name scorers#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3.1. String name scorers#", "content": "3.4.3.1. String name scorers# For the most common use cases, you can designate a scorer object with the scoring parameter via a string name; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as ‘neg_mean_squared_error’ which return the negated value of the metric. Usage examples: >>> from sklearn import svm, datasets >>> from sklearn.model_selection import cross_val_score >>> X, y = datasets.load_iris(return_X_y=True) >>> clf = svm.SVC(random_state=0) >>> cross_val_score(clf, X, y, cv=5, scoring='recall_macro') array([0.96, 0.96, 0.96, 0.93, 1. ])", "prev_chunk_id": "chunk_382", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_384", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3.2. Callable scorers#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3.2. Callable scorers#", "content": "3.4.3.2. Callable scorers# For more complex use cases and more flexibility, you can pass a callable to the scoring parameter. This can be done by: - Adapting predefined metrics via make_scorer - Creating a custom scorer object(most flexible)", "prev_chunk_id": "chunk_383", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_385", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3.2.1. Adapting predefined metrics via make_scorer#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3.2.1. Adapting predefined metrics via make_scorer#", "content": "3.4.3.2.1. Adapting predefined metrics via make_scorer# The following metric functions are not implemented as named scorers, sometimes because they require additional parameters, such as fbeta_score. They cannot be passed to the scoring parameters; instead their callable needs to be passed to make_scorer together with the value of the user-settable parameters. One typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the beta parameter for the fbeta_score function: >>> from sklearn.metrics import fbeta_score, make_scorer >>> ftwo_scorer = make_scorer(fbeta_score, beta=2) >>> from sklearn.model_selection import GridSearchCV >>> from sklearn.svm import LinearSVC >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, ... scoring=ftwo_scorer, cv=5) The module sklearn.metrics also exposes a set of simple functions measuring a prediction error given ground truth and prediction: - functions ending with_scorereturn a value to maximize, the higher the better. - functions ending with_error,_loss, or_deviancereturn a value to minimize, the lower the better. When converting into a scorer object usingmake_scorer, set thegreater_is_betterparameter toFalse(Trueby default; see the parameter description below).", "prev_chunk_id": "chunk_384", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_386", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3.2.2. Creating a custom scorer object#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3.2.2. Creating a custom scorer object#", "content": "3.4.3.2.2. Creating a custom scorer object# You can create your own custom scorer object using make_scorer or for the most flexibility, from scratch. See below for details.", "prev_chunk_id": "chunk_385", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_387", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.3.3. Using multiple metric evaluation#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.3.3. Using multiple metric evaluation#", "content": "3.4.3.3. Using multiple metric evaluation# Scikit-learn also permits evaluation of multiple metrics in GridSearchCV, RandomizedSearchCV and cross_validate. There are three ways to specify multiple scoring metrics for the scoring parameter: - As an iterable of string metrics:>>>scoring=['accuracy','precision'] - As adictmapping the scorer name to the scoring function:>>>fromsklearn.metricsimportaccuracy_score>>>fromsklearn.metricsimportmake_scorer>>>scoring={'accuracy':make_scorer(accuracy_score),...'prec':'precision'}Note that the dict values can either be scorer functions or one of the predefined metric strings. - As a callable that returns a dictionary of scores:>>>fromsklearn.model_selectionimportcross_validate>>>fromsklearn.metricsimportconfusion_matrix>>># A sample toy binary classification dataset>>>X,y=datasets.make_classification(n_classes=2,random_state=0)>>>svm=LinearSVC(random_state=0)>>>defconfusion_matrix_scorer(clf,X,y):...y_pred=clf.predict(X)...cm=confusion_matrix(y,y_pred)...return{'tn':cm[0,0],'fp':cm[0,1],...'fn':cm[1,0],'tp':cm[1,1]}>>>cv_results=cross_validate(svm,X,y,cv=5,...scoring=confusion_matrix_scorer)>>># Getting the test set true positive scores>>>print(cv_results['test_tp'])[10 9 8 7 8]>>># Getting the test set false negative scores>>>print(cv_results['test_fn'])[0 1 2 3 2]", "prev_chunk_id": "chunk_386", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_388", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4. Classification metrics#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4. Classification metrics#", "content": "3.4.4. Classification metrics# The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the sample_weight parameter. Some of these are restricted to the binary classification case: Others also work in the multiclass case: Some also work in the multilabel case: And some work with binary and multilabel (but not multiclass) problems: In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.", "prev_chunk_id": "chunk_387", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_389", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.1. From binary to multiclass and multilabel#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.1. From binary to multiclass and multilabel#", "content": "3.4.4.1. From binary to multiclass and multilabel# Some metrics are essentially defined for binary classification tasks (e.g. f1_score, roc_auc_score). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled 1 (though this may be configurable through the pos_label parameter). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter. - \"macro\"simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. - \"weighted\"accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample. - \"micro\"gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored. - \"samples\"applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their (sample_weight-weighted) average. - Selectingaverage=Nonewill return an array with the score", "prev_chunk_id": "chunk_388", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_390", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.1. From binary to multiclass and multilabel#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.1. From binary to multiclass and multilabel#", "content": "for each class. While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell [i, j] has value 1 if sample i has label j and value 0 otherwise.", "prev_chunk_id": "chunk_389", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_391", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.2. Accuracy score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.2. Accuracy score#", "content": "3.4.4.2. Accuracy score# The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions. In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(y_i\\) is the corresponding true value, then the fraction of correct predictions over \\(n_\\text{samples}\\) is defined as where \\(1(x)\\) is the indicator function. >>> import numpy as np >>> from sklearn.metrics import accuracy_score >>> y_pred = [0, 2, 1, 3] >>> y_true = [0, 1, 2, 3] >>> accuracy_score(y_true, y_pred) 0.5 >>> accuracy_score(y_true, y_pred, normalize=False) 2.0 In the multilabel case with binary label indicators: >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 0.5 Examples - SeeTest with permutations the significance of a classification scorefor an example of accuracy score usage using permutations of the dataset.", "prev_chunk_id": "chunk_390", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_392", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.3. Top-k accuracy score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.3. Top-k accuracy score#", "content": "3.4.4.3. Top-k accuracy score# The top_k_accuracy_score function is a generalization of accuracy_score. The difference is that a prediction is considered correct as long as the true label is associated with one of the k highest predicted scores. accuracy_score is the special case of k = 1. The function covers the binary and multiclass classification cases but not the multilabel case. If \\(\\hat{f}_{i,j}\\) is the predicted class for the \\(i\\)-th sample corresponding to the \\(j\\)-th largest predicted score and \\(y_i\\) is the corresponding true value, then the fraction of correct predictions over \\(n_\\text{samples}\\) is defined as where \\(k\\) is the number of guesses allowed and \\(1(x)\\) is the indicator function. >>> import numpy as np >>> from sklearn.metrics import top_k_accuracy_score >>> y_true = np.array([0, 1, 2, 2]) >>> y_score = np.array([[0.5, 0.2, 0.2], ... [0.3, 0.4, 0.2], ... [0.2, 0.4, 0.3], ... [0.7, 0.2, 0.1]]) >>> top_k_accuracy_score(y_true, y_score, k=2) 0.75 >>> # Not normalizing gives the number of \"correctly\" classified samples >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False) 3.0", "prev_chunk_id": "chunk_391", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_393", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.4. Balanced accuracy score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.4. Balanced accuracy score#", "content": "3.4.4.4. Balanced accuracy score# The balanced_accuracy_score function computes the balanced accuracy, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy. In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores: If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \\(\\frac{1}{n\\_classes}\\). The score ranges from 0 to 1, or when adjusted=True is used, it is rescaled to the range \\(\\frac{1}{1 - n\\_classes}\\) to 1, inclusive, with performance at random scoring 0. If \\(y_i\\) is the true value of the \\(i\\)-th sample, and \\(w_i\\) is the corresponding sample weight, then we adjust the sample weight to: where \\(1(x)\\) is the indicator function. Given predicted \\(\\hat{y}_i\\) for sample \\(i\\), balanced accuracy is defined as: With adjusted=True, balanced accuracy reports the relative increase from \\(\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) = \\frac{1}{n\\_classes}\\). In the binary case, this is also known as *Youden’s J statistic*, or informedness. References", "prev_chunk_id": "chunk_392", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_394", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.5. Cohen’s kappa#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.5. Cohen’s kappa#", "content": "3.4.4.5. Cohen’s kappa# The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth. The kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels). Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators. >>> from sklearn.metrics import cohen_kappa_score >>> labeling1 = [2, 0, 2, 2, 0, 1] >>> labeling2 = [0, 0, 2, 2, 0, 2] >>> cohen_kappa_score(labeling1, labeling2) 0.4285714285714286", "prev_chunk_id": "chunk_393", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_395", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.6. Confusion matrix#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.6. Confusion matrix#", "content": "3.4.4.6. Confusion matrix# The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class (Wikipedia and other references may use different convention for axes). By definition, entry \\(i, j\\) in a confusion matrix is the number of observations actually in group \\(i\\), but predicted to be in group \\(j\\). Here is an example: >>> from sklearn.metrics import confusion_matrix >>> y_true = [2, 0, 2, 2, 0, 1] >>> y_pred = [0, 0, 2, 2, 0, 2] >>> confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]]) ConfusionMatrixDisplay can be used to visually represent a confusion matrix as shown in the Confusion matrix example, which creates the following figure: The parameter normalize allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: 'pred', 'true', and 'all' which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively. >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1] >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1] >>> confusion_matrix(y_true, y_pred, normalize='all') array([[0.25 , 0.125], [0.25 , 0.375]]) For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows: >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1] >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1] >>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel().tolist() >>> tn, fp, fn, tp (2, 1, 2, 3) Examples - SeeConfusion matrixfor an example of using a confusion matrix to evaluate classifier output quality. - SeeRecognizing hand-written digitsfor an example of using a confusion matrix to classify hand-written digits. - SeeClassification of text documents using sparse featuresfor an example of using a confusion matrix to classify text documents.", "prev_chunk_id": "chunk_394", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_396", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.7. Classification report#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.7. Classification report#", "content": "3.4.4.7. Classification report# The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom target_names and inferred labels: >>> from sklearn.metrics import classification_report >>> y_true = [0, 1, 2, 2, 0] >>> y_pred = [0, 0, 2, 1, 0] >>> target_names = ['class 0', 'class 1', 'class 2'] >>> print(classification_report(y_true, y_pred, target_names=target_names)) precision recall f1-score support class 0 0.67 1.00 0.80 2 class 1 0.00 0.00 0.00 1 class 2 1.00 0.50 0.67 2 accuracy 0.60 5 macro avg 0.56 0.50 0.49 5 weighted avg 0.67 0.60 0.59 5 Examples - SeeRecognizing hand-written digitsfor an example of classification report usage for hand-written digits. - SeeCustom refit strategy of a grid search with cross-validationfor an example of classification report usage for grid search with nested cross-validation.", "prev_chunk_id": "chunk_395", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_397", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.8. Hamming loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.8. Hamming loss#", "content": "3.4.4.8. Hamming loss# The hamming_loss computes the average Hamming loss or Hamming distance between two sets of samples. If \\(\\hat{y}_{i,j}\\) is the predicted value for the \\(j\\)-th label of a given sample \\(i\\), \\(y_{i,j}\\) is the corresponding true value, \\(n_\\text{samples}\\) is the number of samples and \\(n_\\text{labels}\\) is the number of labels, then the Hamming loss \\(L_{Hamming}\\) is defined as: where \\(1(x)\\) is the indicator function. The equation above does not hold true in the case of multiclass classification. Please refer to the note below for more information. >>> from sklearn.metrics import hamming_loss >>> y_pred = [1, 2, 3, 4] >>> y_true = [2, 2, 3, 4] >>> hamming_loss(y_true, y_pred) 0.25 In the multilabel case with binary label indicators: >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2))) 0.75", "prev_chunk_id": "chunk_396", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_398", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.9. Precision, recall and F-measures#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.9. Precision, recall and F-measures#", "content": "3.4.4.9. Precision, recall and F-measures# Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples. The F-measure (\\(F_\\beta\\) and \\(F_1\\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \\(F_\\beta\\) measure reaches its best value at 1 and its worst score at 0. With \\(\\beta = 1\\), \\(F_\\beta\\) and \\(F_1\\) are equivalent, and the recall and the precision are equally important. The precision_recall_curve computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold. The average_precision_score function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as where \\(P_n\\) and \\(R_n\\) are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples. References [Manning2008] and [Everingham2010] present alternative variants of AP that interpolate the precision-recall curve. Currently, average_precision_score does not implement any interpolated variant. References [Davis2006] and [Flach2015] describe why a linear interpolation of points on the precision-recall curve provides an overly-optimistic measure of classifier performance. This linear interpolation is used when computing area under the curve with the trapezoidal rule in auc. Several functions allow you to analyze the precision, recall and F-measures score: Note that the precision_recall_curve function is restricted to the binary case. The average_precision_score function supports multiclass and multilabel formats by computing each class score in a One-vs-the-rest (OvR) fashion and averaging them or not depending of its average argument value. The PrecisionRecallDisplay.from_estimator and PrecisionRecallDisplay.from_predictions functions will plot the precision-recall curve as follows. Examples - SeeCustom refit strategy of a grid search with cross-validationfor an example", "prev_chunk_id": "chunk_397", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_399", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.9. Precision, recall and F-measures#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.9. Precision, recall and F-measures#", "content": "ofprecision_scoreandrecall_scoreusage to estimate parameters using grid search with nested cross-validation. - SeePrecision-Recallfor an example ofprecision_recall_curveusage to evaluate classifier output quality. References", "prev_chunk_id": "chunk_398", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_400", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.9.1. Binary classification#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.9.1. Binary classification#", "content": "3.4.4.9.1. Binary classification# In a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer to the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to whether that prediction corresponds to the external judgment (sometimes known as the ‘’observation’’). Given these definitions, we can formulate the following table: In this context, we can define the notions of precision and recall: (Sometimes recall is also called ‘’sensitivity’’) F-measure is the weighted harmonic mean of precision and recall, with precision’s contribution to the mean weighted by some parameter \\(\\beta\\): To avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this otherwise-equivalent formula: Note that this formula is still undefined when there are no true positives, false positives, or false negatives. By default, F-1 for a set of exclusively true negatives is calculated as 0, however this behavior can be changed using the zero_division parameter. Here are some small examples in binary classification: >>> from sklearn import metrics >>> y_pred = [0, 1, 0, 0] >>> y_true = [0, 1, 0, 1] >>> metrics.precision_score(y_true, y_pred) 1.0 >>> metrics.recall_score(y_true, y_pred) 0.5 >>> metrics.f1_score(y_true, y_pred) 0.66 >>> metrics.fbeta_score(y_true, y_pred, beta=0.5) 0.83 >>> metrics.fbeta_score(y_true, y_pred, beta=1) 0.66 >>> metrics.fbeta_score(y_true, y_pred, beta=2) 0.55 >>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5) (array([0.66, 1. ]), array([1. , 0.5]), array([0.71, 0.83]), array([2, 2])) >>> import numpy as np >>> from sklearn.metrics import precision_recall_curve >>> from sklearn.metrics import average_precision_score >>> y_true = np.array([0, 0, 1, 1]) >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> precision, recall, threshold = precision_recall_curve(y_true, y_scores) >>> precision array([0.5 , 0.66, 0.5 , 1. , 1. ]) >>> recall array([1. , 1. , 0.5, 0.5, 0. ]) >>> threshold array([0.1 , 0.35, 0.4 , 0.8 ]) >>> average_precision_score(y_true, y_scores) 0.83", "prev_chunk_id": "chunk_399", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_401", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.9.2. Multiclass and multilabel classification#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.9.2. Multiclass and multilabel classification#", "content": "3.4.4.9.2. Multiclass and multilabel classification# In a multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the average argument to the average_precision_score, f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. Note the following behaviors when averaging: - If all labels are included, “micro”-averaging in a multiclass setting will produce precision, recall and\\(F\\)that are all identical to accuracy. - “weighted” averaging may produce a F-score that is not between precision and recall. - “macro” averaging for F-measures is calculated as the arithmetic mean over per-label/class F-measures, not the harmonic mean over the arithmetic precision and recall means. Both calculations can be seen in the literature but are not equivalent, see[OB2019]for details. To make this more explicit, consider the following notation: - \\(y\\)the set oftrue\\((sample, label)\\)pairs - \\(\\hat{y}\\)the set ofpredicted\\((sample, label)\\)pairs - \\(L\\)the set of labels - \\(S\\)the set of samples - \\(y_s\\)the subset of\\(y\\)with sample\\(s\\), i.e.\\(y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}\\) - \\(y_l\\)the subset of\\(y\\)with label\\(l\\) - similarly,\\(\\hat{y}_s\\)and\\(\\hat{y}_l\\)are subsets of\\(\\hat{y}\\) - \\(P(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|B\\right|}\\)for some sets\\(A\\)and\\(B\\) - \\(R(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|A\\right|}\\)(Conventions vary on handling\\(A = \\emptyset\\); this implementation uses\\(R(A, B):=0\\), and similar for\\(P\\).) - \\(F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}\\) Then the metrics are defined as: >>> from sklearn import metrics >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> metrics.precision_score(y_true, y_pred, average='macro') 0.22 >>> metrics.recall_score(y_true, y_pred, average='micro') 0.33 >>> metrics.f1_score(y_true, y_pred, average='weighted') 0.267 >>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5) 0.238 >>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None) (array([0.667, 0., 0.]), array([1., 0., 0.]), array([0.714, 0., 0.]),", "prev_chunk_id": "chunk_400", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_402", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.9.2. Multiclass and multilabel classification#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.9.2. Multiclass and multilabel classification#", "content": "array([2, 2, 2])) For multiclass classification with a “negative class”, it is possible to exclude some labels: >>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro') ... # excluding 0, no labels were correctly recalled 0.0 Similarly, labels not present in the data sample may be accounted for in macro-averaging. >>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro') 0.166 References", "prev_chunk_id": "chunk_401", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_403", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.10. Jaccard similarity coefficient score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.10. Jaccard similarity coefficient score#", "content": "3.4.4.10. Jaccard similarity coefficient score# The jaccard_score function computes the average of Jaccard similarity coefficients, also called the Jaccard index, between pairs of label sets. The Jaccard similarity coefficient with a ground truth label set \\(y\\) and predicted label set \\(\\hat{y}\\), is defined as The jaccard_score (like precision_recall_fscore_support) applies natively to binary targets. By computing it set-wise it can be extended to apply to multilabel and multiclass through the use of average (see above). In the binary case: >>> import numpy as np >>> from sklearn.metrics import jaccard_score >>> y_true = np.array([[0, 1, 1], ... [1, 1, 0]]) >>> y_pred = np.array([[1, 1, 1], ... [1, 0, 0]]) >>> jaccard_score(y_true[0], y_pred[0]) 0.6666 In the 2D comparison case (e.g. image similarity): >>> jaccard_score(y_true, y_pred, average=\"micro\") 0.6 In the multilabel case with binary label indicators: >>> jaccard_score(y_true, y_pred, average='samples') 0.5833 >>> jaccard_score(y_true, y_pred, average='macro') 0.6666 >>> jaccard_score(y_true, y_pred, average=None) array([0.5, 0.5, 1. ]) Multiclass problems are binarized and treated like the corresponding multilabel problem: >>> y_pred = [0, 2, 1, 2] >>> y_true = [0, 1, 2, 2] >>> jaccard_score(y_true, y_pred, average=None) array([1. , 0. , 0.33]) >>> jaccard_score(y_true, y_pred, average='macro') 0.44 >>> jaccard_score(y_true, y_pred, average='micro') 0.33", "prev_chunk_id": "chunk_402", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_404", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.11. Hinge loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.11. Hinge loss#", "content": "3.4.4.11. Hinge loss# The hinge_loss function computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.) If the true label \\(y_i\\) of a binary classification task is encoded as \\(y_i=\\left\\{-1, +1\\right\\}\\) for every sample \\(i\\); and \\(w_i\\) is the corresponding predicted decision (an array of shape (n_samples,) as output by the decision_function method), then the hinge loss is defined as: If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer & Singer. Here is the paper describing it. In this case the predicted decision is an array of shape (n_samples, n_labels). If \\(w_{i, y_i}\\) is the predicted decision for the true label \\(y_i\\) of the \\(i\\)-th sample; and \\(\\hat{w}_{i, y_i} = \\max\\left\\{w_{i, y_j}~|~y_j \\ne y_i \\right\\}\\) is the maximum of the predicted decisions for all the other labels, then the multi-class hinge loss is defined by: Here is a small example demonstrating the use of the hinge_loss function with a svm classifier in a binary class problem: >>> from sklearn import svm >>> from sklearn.metrics import hinge_loss >>> X = [[0], [1]] >>> y = [-1, 1] >>> est = svm.LinearSVC(random_state=0) >>> est.fit(X, y) LinearSVC(random_state=0) >>> pred_decision = est.decision_function([[-2], [3], [0.5]]) >>> pred_decision array([-2.18, 2.36, 0.09]) >>> hinge_loss([-1, 1, 1], pred_decision) 0.3 Here is an example demonstrating the use of the hinge_loss function with a svm classifier in a multiclass problem: >>> X = np.array([[0], [1], [2], [3]]) >>> Y = np.array([0, 1, 2, 3]) >>> labels = np.array([0, 1, 2, 3]) >>> est = svm.LinearSVC() >>> est.fit(X, Y) LinearSVC() >>> pred_decision = est.decision_function([[-1], [2], [3]]) >>> y_true = [0, 2, 3] >>> hinge_loss(y_true, pred_decision, labels=labels) 0.56", "prev_chunk_id": "chunk_403", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_405", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.12. Log loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.12. Log loss#", "content": "3.4.4.12. Log loss# Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (predict_proba) of a classifier instead of its discrete predictions. For binary classification with a true label \\(y \\in \\{0,1\\}\\) and a probability estimate \\(\\hat{p} \\approx \\operatorname{Pr}(y = 1)\\), the log loss per sample is the negative log-likelihood of the classifier given the true label: This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \\(Y\\), i.e., \\(y_{i,k} = 1\\) if sample \\(i\\) has label \\(k\\) taken from a set of \\(K\\) labels. Let \\(\\hat{P}\\) be a matrix of probability estimates, with elements \\(\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)\\). Then the log loss of the whole set is To see how this generalizes the binary log loss given above, note that in the binary case, \\(\\hat{p}_{i,0} = 1 - \\hat{p}_{i,1}\\) and \\(y_{i,0} = 1 - y_{i,1}\\), so expanding the inner sum over \\(y_{i,k} \\in \\{0,1\\}\\) gives the binary log loss. The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator’s predict_proba method. >>> from sklearn.metrics import log_loss >>> y_true = [0, 0, 1, 1] >>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]] >>> log_loss(y_true, y_pred) 0.1738 The first [.9, .1] in y_pred denotes 90% probability that the first sample has label 0. The log loss is non-negative.", "prev_chunk_id": "chunk_404", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_406", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.13. Matthews correlation coefficient#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.13. Matthews correlation coefficient#", "content": "3.4.4.13. Matthews correlation coefficient# The matthews_corrcoef function computes the Matthew’s correlation coefficient (MCC) for binary classes. Quoting Wikipedia: In the binary (two-class) case, \\(tp\\), \\(tn\\), \\(fp\\) and \\(fn\\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as In the multiclass case, the Matthews correlation coefficient can be defined in terms of a confusion_matrix \\(C\\) for \\(K\\) classes. To simplify the definition consider the following intermediate variables: - \\(t_k=\\sum_{i}^{K} C_{ik}\\)the number of times class\\(k\\)truly occurred, - \\(p_k=\\sum_{i}^{K} C_{ki}\\)the number of times class\\(k\\)was predicted, - \\(c=\\sum_{k}^{K} C_{kk}\\)the total number of samples correctly predicted, - \\(s=\\sum_{i}^{K} \\sum_{j}^{K} C_{ij}\\)the total number of samples. Then the multiclass MCC is defined as: When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground truth labels. The maximum value is always +1. For additional information, see [WikipediaMCC2021]. Here is a small example illustrating the usage of the matthews_corrcoef function: >>> from sklearn.metrics import matthews_corrcoef >>> y_true = [+1, +1, +1, -1] >>> y_pred = [+1, -1, +1, +1] >>> matthews_corrcoef(y_true, y_pred) -0.33 References", "prev_chunk_id": "chunk_405", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_407", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.14. Multi-label confusion matrix#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.14. Multi-label confusion matrix#", "content": "3.4.4.14. Multi-label confusion matrix# The multilabel_confusion_matrix function computes class-wise (default) or sample-wise (samplewise=True) multilabel confusion matrix to evaluate the accuracy of a classification. multilabel_confusion_matrix also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.). When calculating class-wise multilabel confusion matrix \\(C\\), the count of true negatives for class \\(i\\) is \\(C_{i,0,0}\\), false negatives is \\(C_{i,1,0}\\), true positives is \\(C_{i,1,1}\\) and false positives is \\(C_{i,0,1}\\). Here is an example demonstrating the use of the multilabel_confusion_matrix function with multilabel indicator matrix input: >>> import numpy as np >>> from sklearn.metrics import multilabel_confusion_matrix >>> y_true = np.array([[1, 0, 1], ... [0, 1, 0]]) >>> y_pred = np.array([[1, 0, 0], ... [0, 1, 1]]) >>> multilabel_confusion_matrix(y_true, y_pred) array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]]) Or a confusion matrix can be constructed for each sample’s labels: >>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True) array([[[1, 0], [1, 1]], [[1, 1], [0, 1]]]) Here is an example demonstrating the use of the multilabel_confusion_matrix function with multiclass input: >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"] >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"] >>> multilabel_confusion_matrix(y_true, y_pred, ... labels=[\"ant\", \"bird\", \"cat\"]) array([[[3, 1], [0, 2]], [[5, 0], [1, 0]], [[2, 1], [1, 2]]]) Here are some examples demonstrating the use of the multilabel_confusion_matrix function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input. Calculating recall (also called the true positive rate or the sensitivity) for each class: >>> y_true = np.array([[0, 0, 1], ... [0, 1, 0], ... [1, 1, 0]]) >>> y_pred = np.array([[0, 1, 0], ... [0, 0, 1], ... [1, 1, 0]]) >>> mcm = multilabel_confusion_matrix(y_true, y_pred) >>> tn", "prev_chunk_id": "chunk_406", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_408", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.14. Multi-label confusion matrix#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.14. Multi-label confusion matrix#", "content": "= mcm[:, 0, 0] >>> tp = mcm[:, 1, 1] >>> fn = mcm[:, 1, 0] >>> fp = mcm[:, 0, 1] >>> tp / (tp + fn) array([1. , 0.5, 0. ]) Calculating specificity (also called the true negative rate) for each class: >>> tn / (tn + fp) array([1. , 0. , 0.5]) Calculating fall out (also called the false positive rate) for each class: >>> fp / (fp + tn) array([0. , 1. , 0.5]) Calculating miss rate (also called the false negative rate) for each class: >>> fn / (fn + tp) array([0. , 0.5, 1. ])", "prev_chunk_id": "chunk_407", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_409", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.15. Receiver operating characteristic (ROC)#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.15. Receiver operating characteristic (ROC)#", "content": "3.4.4.15. Receiver operating characteristic (ROC)# The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia : This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the roc_curve function: >>> import numpy as np >>> from sklearn.metrics import roc_curve >>> y = np.array([1, 1, 2, 2]) >>> scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2) >>> fpr array([0. , 0. , 0.5, 0.5, 1. ]) >>> tpr array([0. , 0.5, 0.5, 1. , 1. ]) >>> thresholds array([ inf, 0.8 , 0.4 , 0.35, 0.1 ]) Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn’t require optimizing a threshold for each label. The roc_auc_score function, denoted by ROC-AUC or AUROC, computes the area under the ROC curve. By doing so, the curve information is summarized in one number. The following figure shows the ROC curve and ROC-AUC score for a classifier aimed to distinguish the virginica flower from the rest of the species in the Iris plants dataset: For more information see the Wikipedia article on AUC.", "prev_chunk_id": "chunk_408", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_410", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.15.1. Binary case#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.15.1. Binary case#", "content": "3.4.4.15.1. Binary case# In the binary case, you can either provide the probability estimates, using the classifier.predict_proba() method, or the non-thresholded decision values given by the classifier.decision_function() method. In the case of providing the probability estimates, the probability of the class with the “greater label” should be provided. The “greater label” corresponds to classifier.classes_[1] and thus classifier.predict_proba(X)[:, 1]. Therefore, the y_score parameter is of size (n_samples,). >>> from sklearn.datasets import load_breast_cancer >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.metrics import roc_auc_score >>> X, y = load_breast_cancer(return_X_y=True) >>> clf = LogisticRegression().fit(X, y) >>> clf.classes_ array([0, 1]) We can use the probability estimates corresponding to clf.classes_[1]. >>> y_score = clf.predict_proba(X)[:, 1] >>> roc_auc_score(y, y_score) 0.99 Otherwise, we can use the non-thresholded decision values >>> roc_auc_score(y, clf.decision_function(X)) 0.99", "prev_chunk_id": "chunk_409", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_411", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.15.2. Multi-class case#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.15.2. Multi-class case#", "content": "3.4.4.15.2. Multi-class case# The roc_auc_score function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the predicted labels are provided in an array with values from 0 to n_classes, and the scores correspond to the probability estimates that a sample belongs to a particular class. The OvO and OvR algorithms support weighting uniformly (average='macro') and by prevalence (average='weighted').", "prev_chunk_id": "chunk_410", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_412", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.15.3. Multi-label case#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.15.3. Multi-label case#", "content": "3.4.4.15.3. Multi-label case# In multi-label classification, the roc_auc_score function is extended by averaging over the labels as above. In this case, you should provide a y_score of shape (n_samples, n_classes). Thus, when using the probability estimates, one needs to select the probability of the class with the greater label for each output. >>> from sklearn.datasets import make_multilabel_classification >>> from sklearn.multioutput import MultiOutputClassifier >>> X, y = make_multilabel_classification(random_state=0) >>> inner_clf = LogisticRegression(random_state=0) >>> clf = MultiOutputClassifier(inner_clf).fit(X, y) >>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)]) >>> roc_auc_score(y, y_score, average=None) array([0.828, 0.851, 0.94, 0.87, 0.95]) And the decision values do not require such processing. >>> from sklearn.linear_model import RidgeClassifierCV >>> clf = RidgeClassifierCV().fit(X, y) >>> y_score = clf.decision_function(X) >>> roc_auc_score(y, y_score, average=None) array([0.82, 0.85, 0.93, 0.87, 0.94]) Examples - SeeMulticlass Receiver Operating Characteristic (ROC)for an example of using ROC to evaluate the quality of the output of a classifier. - SeeReceiver Operating Characteristic (ROC) with cross validationfor an example of using ROC to evaluate classifier output quality, using cross-validation. - SeeSpecies distribution modelingfor an example of using ROC to model species distribution. References", "prev_chunk_id": "chunk_411", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_413", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.16. Detection error tradeoff (DET)#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.16. Detection error tradeoff (DET)#", "content": "3.4.4.16. Detection error tradeoff (DET)# The function det_curve computes the detection error tradeoff curve (DET) curve [WikipediaDET2017]. Quoting Wikipedia: DET curves are a variation of receiver operating characteristic (ROC) curves where False Negative Rate is plotted on the y-axis instead of True Positive Rate. DET curves are commonly plotted in normal deviate scale by transformation with \\(\\phi^{-1}\\) (with \\(\\phi\\) being the cumulative distribution function). The resulting performance curves explicitly visualize the tradeoff of error types for given classification algorithms. See [Martin1997] for examples and further motivation. This figure compares the ROC and DET curves of two example classifiers on the same classification task: Examples - SeeDetection error tradeoff (DET) curvefor an example comparison between receiver operating characteristic (ROC) curves and Detection error tradeoff (DET) curves. References", "prev_chunk_id": "chunk_412", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_414", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.17. Zero one loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.17. Zero one loss#", "content": "3.4.4.17. Zero one loss# The zero_one_loss function computes the sum or the average of the 0-1 classification loss (\\(L_{0-1}\\)) over \\(n_{\\text{samples}}\\). By default, the function normalizes over the sample. To get the sum of the \\(L_{0-1}\\), set normalize to False. In multilabel classification, the zero_one_loss scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set normalize to False. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(y_i\\) is the corresponding true value, then the 0-1 loss \\(L_{0-1}\\) is defined as: where \\(1(x)\\) is the indicator function. The zero-one loss can also be computed as \\(\\text{zero-one loss} = 1 - \\text{accuracy}\\). >>> from sklearn.metrics import zero_one_loss >>> y_pred = [1, 2, 3, 4] >>> y_true = [2, 2, 3, 4] >>> zero_one_loss(y_true, y_pred) 0.25 >>> zero_one_loss(y_true, y_pred, normalize=False) 1.0 In the multilabel case with binary label indicators, where the first label set [0,1] has an error: >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 0.5 >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)), normalize=False) 1.0 Examples - SeeRecursive feature elimination with cross-validationfor an example of zero one loss usage to perform recursive feature elimination with cross-validation.", "prev_chunk_id": "chunk_413", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_415", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.18. Brier score loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.18. Brier score loss#", "content": "3.4.4.18. Brier score loss# The brier_score_loss function computes the Brier score for binary and multiclass probabilistic predictions and is equivalent to the mean squared error. Quoting Wikipedia: Let the true labels for a set of \\(N\\) data points be encoded as a 1-of-K binary indicator matrix \\(Y\\), i.e., \\(y_{i,k} = 1\\) if sample \\(i\\) has label \\(k\\) taken from a set of \\(K\\) labels. Let \\(\\hat{P}\\) be a matrix of probability estimates with elements \\(\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)\\). Following the original definition by [Brier1950], the Brier score is given by: The Brier score lies in the interval \\([0, 2]\\) and the lower the value the better the probability estimates are (the mean squared difference is smaller). Actually, the Brier score is a strictly proper scoring rule, meaning that it achieves the best score only when the estimated probabilities equal the true ones. Note that in the binary case, the Brier score is usually divided by two and ranges between \\([0,1]\\). For binary targets \\(y_i \\in \\{0, 1\\}\\) and probability estimates \\(\\hat{p}_i \\approx \\operatorname{Pr}(y_i = 1)\\) for the positive class, the Brier score is then equal to: The brier_score_loss function computes the Brier score given the ground-truth labels and predicted probabilities, as returned by an estimator’s predict_proba method. The scale_by_half parameter controls which of the two above definitions to follow. >>> import numpy as np >>> from sklearn.metrics import brier_score_loss >>> y_true = np.array([0, 1, 1, 0]) >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"]) >>> y_prob = np.array([0.1, 0.9, 0.8, 0.4]) >>> brier_score_loss(y_true, y_prob) 0.055 >>> brier_score_loss(y_true, 1 - y_prob, pos_label=0) 0.055 >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\") 0.055 >>> brier_score_loss( ... [\"eggs\", \"ham\", \"spam\"], ... [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]], ... labels=[\"eggs\", \"ham\", \"spam\"], ... ) 0.146 The Brier score can be used to assess how well", "prev_chunk_id": "chunk_414", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_416", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.18. Brier score loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.18. Brier score loss#", "content": "a classifier is calibrated. However, a lower Brier score loss does not always mean a better calibration. This is because, by analogy with the bias-variance decomposition of the mean squared error, the Brier score loss can be decomposed as the sum of calibration loss and refinement loss [Bella2012]. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. Refinement loss can change independently from calibration loss, thus a lower Brier score loss does not necessarily mean a better calibrated model. “Only when refinement loss remains the same does a lower Brier score loss always mean better calibration” [Bella2012], [Flach2008]. Examples - SeeProbability calibration of classifiersfor an example of Brier score loss usage to perform probability calibration of classifiers. References", "prev_chunk_id": "chunk_415", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_417", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.19. Class likelihood ratios#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.19. Class likelihood ratios#", "content": "3.4.4.19. Class likelihood ratios# The class_likelihood_ratios function computes the positive and negative likelihood ratios \\(LR_\\pm\\) for binary classes, which can be interpreted as the ratio of post-test to pre-test odds as explained below. As a consequence, this metric is invariant w.r.t. the class prevalence (the number of samples in the positive class divided by the total number of samples) and can be extrapolated between populations regardless of any possible class imbalance. The \\(LR_\\pm\\) metrics are therefore very useful in settings where the data available to learn and evaluate a classifier is a study population with nearly balanced classes, such as a case-control study, while the target application, i.e. the general population, has very low prevalence. The positive likelihood ratio \\(LR_+\\) is the probability of a classifier to correctly predict that a sample belongs to the positive class divided by the probability of predicting the positive class for a sample belonging to the negative class: The notation here refers to predicted (\\(P\\)) or true (\\(T\\)) label and the sign \\(+\\) and \\(-\\) refer to the positive and negative class, respectively, e.g. \\(P+\\) stands for “predicted positive”. Analogously, the negative likelihood ratio \\(LR_-\\) is the probability of a sample of the positive class being classified as belonging to the negative class divided by the probability of a sample of the negative class being correctly classified: For classifiers above chance \\(LR_+\\) above 1 higher is better, while \\(LR_-\\) ranges from 0 to 1 and lower is better. Values of \\(LR_\\pm\\approx 1\\) correspond to chance level. Notice that probabilities differ from counts, for instance \\(\\operatorname{PR}(P+|T+)\\) is not equal to the number of true positive counts tp (see the wikipedia page for the actual formulas). Examples - Class Likelihood Ratios to measure classification performance", "prev_chunk_id": "chunk_416", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_418", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.4.20. D² score for classification#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.4.20. D² score for classification#", "content": "3.4.4.20. D² score for classification# The D² score computes the fraction of deviance explained. It is a generalization of R², where the squared error is generalized and replaced by a classification deviance of choice \\(\\text{dev}(y, \\hat{y})\\) (e.g., Log loss). D² is a form of a skill score. It is calculated as Where \\(y_{\\text{null}}\\) is the optimal prediction of an intercept-only model (e.g., the per-class proportion of y_true in the case of the Log loss). Like R², the best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts \\(y_{\\text{null}}\\), disregarding the input features, would get a D² score of 0.0.", "prev_chunk_id": "chunk_417", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_419", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5. Multilabel ranking metrics#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5. Multilabel ranking metrics#", "content": "3.4.5. Multilabel ranking metrics# In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.", "prev_chunk_id": "chunk_418", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_420", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5.1. Coverage error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5.1. Coverage error#", "content": "3.4.5.1. Coverage error# The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metric is thus the average number of true labels. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the coverage is defined as with \\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\). Given the rank definition, ties in y_scores are broken by giving the maximal rank that would have been assigned to all tied values. Here is a small example of usage of this function: >>> import numpy as np >>> from sklearn.metrics import coverage_error >>> y_true = np.array([[1, 0, 0], [0, 0, 1]]) >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]]) >>> coverage_error(y_true, y_score) 2.5", "prev_chunk_id": "chunk_419", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_421", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5.2. Label ranking average precision#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5.2. Label ranking average precision#", "content": "3.4.5.2. Label ranking average precision# The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall. Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the average precision is defined as where \\(\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\), \\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\), \\(|\\cdot|\\) computes the cardinality of the set (i.e., the number of elements in the set), and \\(||\\cdot||_0\\) is the \\(\\ell_0\\) “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: >>> import numpy as np >>> from sklearn.metrics import label_ranking_average_precision_score >>> y_true = np.array([[1, 0, 0], [0, 0, 1]]) >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]]) >>> label_ranking_average_precision_score(y_true, y_score) 0.416", "prev_chunk_id": "chunk_420", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_422", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5.3. Ranking loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5.3. Ranking loss#", "content": "3.4.5.3. Ranking loss# The label_ranking_loss function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the ranking loss is defined as where \\(|\\cdot|\\) computes the cardinality of the set (i.e., the number of elements in the set) and \\(||\\cdot||_0\\) is the \\(\\ell_0\\) “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: >>> import numpy as np >>> from sklearn.metrics import label_ranking_loss >>> y_true = np.array([[1, 0, 0], [0, 0, 1]]) >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]]) >>> label_ranking_loss(y_true, y_score) 0.75 >>> # With the following prediction, we have perfect and minimal loss >>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]]) >>> label_ranking_loss(y_true, y_score) 0.0", "prev_chunk_id": "chunk_421", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_423", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5.4. Normalized Discounted Cumulative Gain#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5.4. Normalized Discounted Cumulative Gain#", "content": "3.4.5.4. Normalized Discounted Cumulative Gain# Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain (NDCG) are ranking metrics implemented in dcg_score and ndcg_score ; they compare a predicted order to ground-truth scores, such as the relevance of answers to a query. From the Wikipedia page for Discounted Cumulative Gain: “Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.” DCG orders the true targets (e.g. relevance of query answers) in the predicted order, then multiplies them by a logarithmic decay and sums the result. The sum can be truncated after the first \\(K\\) results, in which case we call it DCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so that it is always between 0 and 1. Usually, NDCG is preferred to DCG. Compared with the ranking loss, NDCG can take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking loss should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG can be used. For one sample, given the vector of continuous ground-truth values for each target \\(y \\in \\mathbb{R}^{M}\\), where \\(M\\) is the number of outputs, and the prediction \\(\\hat{y}\\), which induces the ranking function \\(f\\), the DCG score is and the NDCG score is the DCG", "prev_chunk_id": "chunk_422", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_424", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.5.4. Normalized Discounted Cumulative Gain#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.5.4. Normalized Discounted Cumulative Gain#", "content": "score divided by the DCG score obtained for \\(y\\).", "prev_chunk_id": "chunk_423", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_425", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6. Regression metrics#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6. Regression metrics#", "content": "3.4.6. Regression metrics# The sklearn.metrics module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, mean_pinball_loss, d2_pinball_score and d2_absolute_error_score. These functions have a multioutput keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is 'uniform_average', which specifies a uniformly weighted mean over outputs. If an ndarray of shape (n_outputs,) is passed, then its entries are interpreted as weights and an according weighted average is returned. If multioutput is 'raw_values', then all unaltered individual scores or losses will be returned in an array of shape (n_outputs,). The r2_score and explained_variance_score accept an additional value 'variance_weighted' for the multioutput parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on explaining the higher variance variables.", "prev_chunk_id": "chunk_424", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_426", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.1. R² score, the coefficient of determination#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.1. R² score, the coefficient of determination#", "content": "3.4.6.1. R² score, the coefficient of determination# The r2_score function computes the coefficient of determination, usually denoted as \\(R^2\\). It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. As such variance is dataset dependent, \\(R^2\\) may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected (average) value of y, disregarding the input features, would get an \\(R^2\\) score of 0.0. Note: when the prediction residuals have zero mean, the \\(R^2\\) score and the Explained variance score are identical. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(y_i\\) is the corresponding true value for total \\(n\\) samples, the estimated \\(R^2\\) is defined as: where \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\) and \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2\\). Note that r2_score calculates unadjusted \\(R^2\\) without correcting for bias in sample variance of y. In the particular case where the true target is constant, the \\(R^2\\) score is not finite: it is either NaN (perfect predictions) or -Inf (imperfect predictions). Such non-finite scores may prevent correct model optimization such as grid-search cross-validation to be performed correctly. For this reason the default behaviour of r2_score is to replace them with 1.0 (perfect predictions) or 0.0 (imperfect predictions). If force_finite is set to False, this score falls back on the original \\(R^2\\) definition. Here is a small example of usage of the r2_score function: >>> from sklearn.metrics import r2_score >>> y_true = [3, -0.5, 2, 7] >>> y_pred", "prev_chunk_id": "chunk_425", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_427", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.1. R² score, the coefficient of determination#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.1. R² score, the coefficient of determination#", "content": "= [2.5, 0.0, 2, 8] >>> r2_score(y_true, y_pred) 0.948 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> r2_score(y_true, y_pred, multioutput='variance_weighted') 0.938 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> r2_score(y_true, y_pred, multioutput='uniform_average') 0.936 >>> r2_score(y_true, y_pred, multioutput='raw_values') array([0.965, 0.908]) >>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7]) 0.925 >>> y_true = [-2, -2, -2] >>> y_pred = [-2, -2, -2] >>> r2_score(y_true, y_pred) 1.0 >>> r2_score(y_true, y_pred, force_finite=False) nan >>> y_true = [-2, -2, -2] >>> y_pred = [-2, -2, -2 + 1e-8] >>> r2_score(y_true, y_pred) 0.0 >>> r2_score(y_true, y_pred, force_finite=False) -inf Examples - SeeL1-based models for Sparse Signalsfor an example of R² score usage to evaluate Lasso and Elastic Net on sparse signals.", "prev_chunk_id": "chunk_426", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_428", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.2. Mean absolute error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.2. Mean absolute error#", "content": "3.4.6.2. Mean absolute error# The mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or \\(l1\\)-norm loss. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample, and \\(y_i\\) is the corresponding true value, then the mean absolute error (MAE) estimated over \\(n_{\\text{samples}}\\) is defined as Here is a small example of usage of the mean_absolute_error function: >>> from sklearn.metrics import mean_absolute_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> mean_absolute_error(y_true, y_pred) 0.5 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> mean_absolute_error(y_true, y_pred) 0.75 >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values') array([0.5, 1. ]) >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]) 0.85", "prev_chunk_id": "chunk_427", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_429", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.3. Mean squared error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.3. Mean squared error#", "content": "3.4.6.3. Mean squared error# The mean_squared_error function computes mean squared error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample, and \\(y_i\\) is the corresponding true value, then the mean squared error (MSE) estimated over \\(n_{\\text{samples}}\\) is defined as Here is a small example of usage of the mean_squared_error function: >>> from sklearn.metrics import mean_squared_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> mean_squared_error(y_true, y_pred) 0.375 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> mean_squared_error(y_true, y_pred) 0.7083 Examples - SeeGradient Boosting regressionfor an example of mean squared error usage to evaluate gradient boosting regression. Taking the square root of the MSE, called the root mean squared error (RMSE), is another common metric that provides a measure in the same units as the target variable. RMSE is available through the root_mean_squared_error function.", "prev_chunk_id": "chunk_428", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_430", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.4. Mean squared logarithmic error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.4. Mean squared logarithmic error#", "content": "3.4.6.4. Mean squared logarithmic error# The mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample, and \\(y_i\\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \\(n_{\\text{samples}}\\) is defined as Where \\(\\log_e (x)\\) means the natural logarithm of \\(x\\). This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate. Here is a small example of usage of the mean_squared_log_error function: >>> from sklearn.metrics import mean_squared_log_error >>> y_true = [3, 5, 2.5, 7] >>> y_pred = [2.5, 5, 4, 8] >>> mean_squared_log_error(y_true, y_pred) 0.0397 >>> y_true = [[0.5, 1], [1, 2], [7, 6]] >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]] >>> mean_squared_log_error(y_true, y_pred) 0.044 The root mean squared logarithmic error (RMSLE) is available through the root_mean_squared_log_error function.", "prev_chunk_id": "chunk_429", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_431", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.5. Mean absolute percentage error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.5. Mean absolute percentage error#", "content": "3.4.6.5. Mean absolute percentage error# The mean_absolute_percentage_error (MAPE), also known as mean absolute percentage deviation (MAPD), is an evaluation metric for regression problems. The idea of this metric is to be sensitive to relative errors. It is for example not changed by a global scaling of the target variable. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(y_i\\) is the corresponding true value, then the mean absolute percentage error (MAPE) estimated over \\(n_{\\text{samples}}\\) is defined as where \\(\\epsilon\\) is an arbitrary small yet strictly positive number to avoid undefined results when y is zero. The mean_absolute_percentage_error function supports multioutput. Here is a small example of usage of the mean_absolute_percentage_error function: >>> from sklearn.metrics import mean_absolute_percentage_error >>> y_true = [1, 10, 1e6] >>> y_pred = [0.9, 15, 1.2e6] >>> mean_absolute_percentage_error(y_true, y_pred) 0.2666 In above example, if we had used mean_absolute_error, it would have ignored the small magnitude values and only reflected the error in prediction of highest magnitude value. But that problem is resolved in case of MAPE because it calculates relative percentage error with respect to actual output.", "prev_chunk_id": "chunk_430", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_432", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.6. Median absolute error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.6. Median absolute error#", "content": "3.4.6.6. Median absolute error# The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(y_i\\) is the corresponding true value, then the median absolute error (MedAE) estimated over \\(n_{\\text{samples}}\\) is defined as The median_absolute_error does not support multioutput. Here is a small example of usage of the median_absolute_error function: >>> from sklearn.metrics import median_absolute_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> median_absolute_error(y_true, y_pred) 0.5", "prev_chunk_id": "chunk_431", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_433", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.7. Max error#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.7. Max error#", "content": "3.4.6.7. Max error# The max_error function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, max_error would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample, and \\(y_i\\) is the corresponding true value, then the max error is defined as Here is a small example of usage of the max_error function: >>> from sklearn.metrics import max_error >>> y_true = [3, 2, 7, 1] >>> y_pred = [9, 2, 7, 1] >>> max_error(y_true, y_pred) 6.0 The max_error does not support multioutput.", "prev_chunk_id": "chunk_432", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_434", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.8. Explained variance score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.8. Explained variance score#", "content": "3.4.6.8. Explained variance score# The explained_variance_score computes the explained variance regression score. If \\(\\hat{y}\\) is the estimated target output, \\(y\\) the corresponding (correct) target output, and \\(Var\\) is Variance, the square of the standard deviation, then the explained variance is estimated as follow: The best possible score is 1.0, lower values are worse. In the particular case where the true target is constant, the Explained Variance score is not finite: it is either NaN (perfect predictions) or -Inf (imperfect predictions). Such non-finite scores may prevent correct model optimization such as grid-search cross-validation to be performed correctly. For this reason the default behaviour of explained_variance_score is to replace them with 1.0 (perfect predictions) or 0.0 (imperfect predictions). You can set the force_finite parameter to False to prevent this fix from happening and fallback on the original Explained Variance score. Here is a small example of usage of the explained_variance_score function: >>> from sklearn.metrics import explained_variance_score >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> explained_variance_score(y_true, y_pred) 0.957 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> explained_variance_score(y_true, y_pred, multioutput='raw_values') array([0.967, 1. ]) >>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7]) 0.990 >>> y_true = [-2, -2, -2] >>> y_pred = [-2, -2, -2] >>> explained_variance_score(y_true, y_pred) 1.0 >>> explained_variance_score(y_true, y_pred, force_finite=False) nan >>> y_true = [-2, -2, -2] >>> y_pred = [-2, -2, -2 + 1e-8] >>> explained_variance_score(y_true, y_pred) 0.0 >>> explained_variance_score(y_true, y_pred, force_finite=False) -inf", "prev_chunk_id": "chunk_433", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_435", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances#", "content": "3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances# The mean_tweedie_deviance function computes the mean Tweedie deviance error with a power parameter (\\(p\\)). This is a metric that elicits predicted expectation values of regression targets. Following special cases exist, - whenpower=0it is equivalent tomean_squared_error. - whenpower=1it is equivalent tomean_poisson_deviance. - whenpower=2it is equivalent tomean_gamma_deviance. If \\(\\hat{y}_i\\) is the predicted value of the \\(i\\)-th sample, and \\(y_i\\) is the corresponding true value, then the mean Tweedie deviance error (D) for power \\(p\\), estimated over \\(n_{\\text{samples}}\\) is defined as Tweedie deviance is a homogeneous function of degree 2-power. Thus, Gamma distribution with power=2 means that simultaneously scaling y_true and y_pred has no effect on the deviance. For Poisson distribution power=1 the deviance scales linearly, and for Normal distribution (power=0), quadratically. In general, the higher power the less weight is given to extreme deviations between true and predicted targets. For instance, let’s compare the two predictions 1.5 and 150 that are both 50% larger than their corresponding true value. The mean squared error (power=0) is very sensitive to the prediction difference of the second point,: >>> from sklearn.metrics import mean_tweedie_deviance >>> mean_tweedie_deviance([1.0], [1.5], power=0) 0.25 >>> mean_tweedie_deviance([100.], [150.], power=0) 2500.0 If we increase power to 1,: >>> mean_tweedie_deviance([1.0], [1.5], power=1) 0.189 >>> mean_tweedie_deviance([100.], [150.], power=1) 18.9 the difference in errors decreases. Finally, by setting, power=2: >>> mean_tweedie_deviance([1.0], [1.5], power=2) 0.144 >>> mean_tweedie_deviance([100.], [150.], power=2) 0.144 we would get identical errors. The deviance when power=2 is thus only sensitive to relative errors.", "prev_chunk_id": "chunk_434", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_436", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.10. Pinball loss#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.10. Pinball loss#", "content": "3.4.6.10. Pinball loss# The mean_pinball_loss function is used to evaluate the predictive performance of quantile regression models. The value of pinball loss is equivalent to half of mean_absolute_error when the quantile parameter alpha is set to 0.5. Here is a small example of usage of the mean_pinball_loss function: >>> from sklearn.metrics import mean_pinball_loss >>> y_true = [1, 2, 3] >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1) 0.033 >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1) 0.3 >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9) 0.3 >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9) 0.033 >>> mean_pinball_loss(y_true, y_true, alpha=0.1) 0.0 >>> mean_pinball_loss(y_true, y_true, alpha=0.9) 0.0 It is possible to build a scorer object with a specific choice of alpha: >>> from sklearn.metrics import make_scorer >>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95) Such a scorer can be used to evaluate the generalization performance of a quantile regressor via cross-validation: >>> from sklearn.datasets import make_regression >>> from sklearn.model_selection import cross_val_score >>> from sklearn.ensemble import GradientBoostingRegressor >>> >>> X, y = make_regression(n_samples=100, random_state=0) >>> estimator = GradientBoostingRegressor( ... loss=\"quantile\", ... alpha=0.95, ... random_state=0, ... ) >>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p) array([13.6, 9.7, 23.3, 9.5, 10.4]) It is also possible to build scorer objects for hyper-parameter tuning. The sign of the loss must be switched to ensure that greater means better as explained in the example linked below. Examples - SeePrediction Intervals for Gradient Boosting Regressionfor an example of using the pinball loss to evaluate and tune the hyper-parameters of quantile regression models on data with non-symmetric noise and outliers.", "prev_chunk_id": "chunk_435", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_437", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.11. D² score#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.11. D² score#", "content": "3.4.6.11. D² score# The D² score computes the fraction of deviance explained. It is a generalization of R², where the squared error is generalized and replaced by a deviance of choice \\(\\text{dev}(y, \\hat{y})\\) (e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score. It is calculated as Where \\(y_{\\text{null}}\\) is the optimal prediction of an intercept-only model (e.g., the mean of y_true for the Tweedie case, the median for absolute error and the alpha-quantile for pinball loss). Like R², the best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts \\(y_{\\text{null}}\\), disregarding the input features, would get a D² score of 0.0.", "prev_chunk_id": "chunk_436", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_438", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.12. Visual evaluation of regression models#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.12. Visual evaluation of regression models#", "content": "3.4.6.12. Visual evaluation of regression models# Among methods to assess the quality of regression models, scikit-learn provides the PredictionErrorDisplay class. It allows to visually inspect the prediction errors of a model in two different manners. The plot on the left shows the actual values vs predicted values. For a noise-free regression task aiming to predict the (conditional) expectation of y, a perfect regression model would display data points on the diagonal defined by predicted equal to actual values. The further away from this optimal line, the larger the error of the model. In a more realistic setting with irreducible noise, that is, when not all the variations of y can be explained by features in X, then the best model would lead to a cloud of points densely arranged around the diagonal. Note that the above only holds when the predicted values is the expected value of y given X. This is typically the case for regression models that minimize the mean squared error objective function or more generally the mean Tweedie deviance for any value of its “power” parameter. When plotting the predictions of an estimator that predicts a quantile of y given X, e.g. QuantileRegressor or any other model minimizing the pinball loss, a fraction of the points are either expected to lie above or below the diagonal depending on the estimated quantile level. All in all, while intuitive to read, this plot does not really inform us on what to do to obtain a better model. The right-hand side plot shows the residuals (i.e. the difference between the actual and the predicted values) vs. the predicted values. This plot makes it easier to visualize if the residuals follow and homoscedastic or heteroschedastic distribution. In particular, if the true distribution of y|X is Poisson or Gamma distributed, it", "prev_chunk_id": "chunk_437", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_439", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.6.12. Visual evaluation of regression models#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.6.12. Visual evaluation of regression models#", "content": "is expected that the variance of the residuals of the optimal model would grow with the predicted value of E[y|X] (either linearly for Poisson or quadratically for Gamma). When fitting a linear least squares regression model (see LinearRegression and Ridge), we can use this plot to check if some of the model assumptions are met, in particular that the residuals should be uncorrelated, their expected value should be null and that their variance should be constant (homoschedasticity). If this is not the case, and in particular if the residuals plot show some banana-shaped structure, this is a hint that the model is likely mis-specified and that non-linear feature engineering or switching to a non-linear regression model might be useful. Refer to the example below to see a model evaluation that makes use of this display. Examples - SeeEffect of transforming the targets in regression modelfor an example on how to usePredictionErrorDisplayto visualize the prediction quality improvement of a regression model obtained by transforming the target before learning.", "prev_chunk_id": "chunk_438", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_440", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.7. Clustering metrics#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.7. Clustering metrics#", "content": "3.4.7. Clustering metrics# The sklearn.metrics module implements several loss, score, and utility functions to measure clustering performance. For more information see the Clustering performance evaluation section for instance clustering, and Biclustering evaluation for biclustering.", "prev_chunk_id": "chunk_439", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_441", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.8. Dummy estimators#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.8. Dummy estimators#", "content": "3.4.8. Dummy estimators# When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification: - stratifiedgenerates random predictions by respecting the training set class distribution. - most_frequentalways predicts the most frequent label in the training set. - prioralways predicts the class that maximizes the class prior (likemost_frequent) andpredict_probareturns the class prior. - uniformgenerates predictions uniformly at random. - constantalways predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class is in the minority. Note that with all these strategies, the predict method completely ignores the input data! To illustrate DummyClassifier, first let’s create an imbalanced dataset: >>> from sklearn.datasets import load_iris >>> from sklearn.model_selection import train_test_split >>> X, y = load_iris(return_X_y=True) >>> y[y != 1] = -1 >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Next, let’s compare the accuracy of SVC and most_frequent: >>> from sklearn.dummy import DummyClassifier >>> from sklearn.svm import SVC >>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train) >>> clf.score(X_test, y_test) 0.63 >>> clf = DummyClassifier(strategy='most_frequent', random_state=0) >>> clf.fit(X_train, y_train) DummyClassifier(random_state=0, strategy='most_frequent') >>> clf.score(X_test, y_test) 0.579 We see that SVC doesn’t do much better than a dummy classifier. Now, let’s change the kernel: >>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train) >>> clf.score(X_test, y_test) 0.94 We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details. More generally, when the accuracy of a classifier", "prev_chunk_id": "chunk_440", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_442", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "title": "3.4.8. Dummy estimators#", "page_title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.4.8. Dummy estimators#", "content": "is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc… DummyRegressor also implements four simple rules of thumb for regression: - meanalways predicts the mean of the training targets. - medianalways predicts the median of the training targets. - quantilealways predicts a user provided quantile of the training targets. - constantalways predicts a constant value that is provided by the user. In all these strategies, the predict method completely ignores the input data.", "prev_chunk_id": "chunk_441", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_443", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3. Tuning the decision threshold for class prediction#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3. Tuning the decision threshold for class prediction#", "content": "3.3. Tuning the decision threshold for class prediction# Classification is best divided into two parts: - the statistical problem of learning a model to predict, ideally, class probabilities; - the decision problem to take concrete action based on those probability predictions. Let’s take a straightforward example related to weather forecasting: the first point is related to answering “what is the chance that it will rain tomorrow?” while the second point is related to answering “should I take an umbrella tomorrow?”. When it comes to the scikit-learn API, the first point is addressed by providing scores using predict_proba or decision_function. The former returns conditional probability estimates \\(P(y|X)\\) for each class, while the latter returns a decision score for each class. The decision corresponding to the labels is obtained with predict. In binary classification, a decision rule or action is then defined by thresholding the scores, leading to the prediction of a single class label for each sample. For binary classification in scikit-learn, class labels predictions are obtained by hard-coded cut-off rules: a positive class is predicted when the conditional probability \\(P(y|X)\\) is greater than 0.5 (obtained with predict_proba) or if the decision score is greater than 0 (obtained with decision_function). Here, we show an example that illustrates the relatonship between conditional probability estimates \\(P(y|X)\\) and class labels: >>> from sklearn.datasets import make_classification >>> from sklearn.tree import DecisionTreeClassifier >>> X, y = make_classification(random_state=0) >>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y) >>> classifier.predict_proba(X[:4]) array([[0.94 , 0.06 ], [0.94 , 0.06 ], [0.0416, 0.9583], [0.0416, 0.9583]]) >>> classifier.predict(X[:4]) array([0, 0, 1, 1]) While these hard-coded rules might at first seem reasonable as default behavior, they are most certainly not ideal for most use cases. Let’s illustrate with an example. Consider a scenario where a predictive model is being deployed to assist physicians in detecting tumors.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_444", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3. Tuning the decision threshold for class prediction#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3. Tuning the decision threshold for class prediction#", "content": "In this setting, physicians will most likely be interested in identifying all patients with cancer and not missing anyone with cancer so that they can provide them with the right treatment. In other words, physicians prioritize achieving a high recall rate. This emphasis on recall comes, of course, with the trade-off of potentially more false-positive predictions, reducing the precision of the model. That is a risk physicians are willing to take because the cost of a missed cancer is much higher than the cost of further diagnostic tests. Consequently, when it comes to deciding whether to classify a patient as having cancer or not, it may be more beneficial to classify them as positive for cancer when the conditional probability estimate is much lower than 0.5.", "prev_chunk_id": "chunk_443", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_445", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3.1. Post-tuning the decision threshold#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3.1. Post-tuning the decision threshold#", "content": "3.3.1. Post-tuning the decision threshold# One solution to address the problem stated in the introduction is to tune the decision threshold of the classifier once the model has been trained. The TunedThresholdClassifierCV tunes this threshold using an internal cross-validation. The optimum threshold is chosen to maximize a given metric. The following image illustrates the tuning of the decision threshold for a gradient boosting classifier. While the vanilla and tuned classifiers provide the same predict_proba outputs and thus the same Receiver Operating Characteristic (ROC) and Precision-Recall curves, the class label predictions differ because of the tuned decision threshold. The vanilla classifier predicts the class of interest for a conditional probability greater than 0.5 while the tuned classifier predicts the class of interest for a very low probability (around 0.02). This decision threshold optimizes a utility metric defined by the business (in this case an insurance company).", "prev_chunk_id": "chunk_444", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_446", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3.1.1. Options to tune the decision threshold#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3.1.1. Options to tune the decision threshold#", "content": "3.3.1.1. Options to tune the decision threshold# The decision threshold can be tuned through different strategies controlled by the parameter scoring. One way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These metrics can be found by calling the function get_scorer_names. By default, the balanced accuracy is the metric used but be aware that one should choose a meaningful metric for their use case.", "prev_chunk_id": "chunk_445", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_447", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3.1.2. Important notes regarding the internal cross-validation#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3.1.2. Important notes regarding the internal cross-validation#", "content": "3.3.1.2. Important notes regarding the internal cross-validation# By default TunedThresholdClassifierCV uses a 5-fold stratified cross-validation to tune the decision threshold. The parameter cv allows to control the cross-validation strategy. It is possible to bypass cross-validation by setting cv=\"prefit\" and providing a fitted classifier. In this case, the decision threshold is tuned on the data provided to the fit method. However, you should be extremely careful when using this option. You should never use the same data for training the classifier and tuning the decision threshold due to the risk of overfitting. Refer to the following example section for more details (cf. Consideration regarding model refitting and cross-validation). If you have limited resources, consider using a float number for cv to limit to an internal single train-test split. The option cv=\"prefit\" should only be used when the provided classifier was already trained, and you just want to find the best decision threshold using a new validation set.", "prev_chunk_id": "chunk_446", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_448", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3.1.3. Manually setting the decision threshold#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3.1.3. Manually setting the decision threshold#", "content": "3.3.1.3. Manually setting the decision threshold# The previous sections discussed strategies to find an optimal decision threshold. It is also possible to manually set the decision threshold using the class FixedThresholdClassifier. In case that you don’t want to refit the model when calling fit, wrap your sub-estimator with a FrozenEstimator and do FixedThresholdClassifier(FrozenEstimator(estimator), ...).", "prev_chunk_id": "chunk_447", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_449", "url": "https://scikit-learn.org/stable/modules/classification_threshold.html", "title": "3.3.1.4. Examples#", "page_title": "3.3. Tuning the decision threshold for class prediction — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.3.1.4. Examples#", "content": "3.3.1.4. Examples# - See the example entitledPost-hoc tuning the cut-off point of decision function, to get insights on the post-tuning of the decision threshold. - See the example entitledPost-tuning the decision threshold for cost-sensitive learning, to learn about cost-sensitive learning and decision threshold tuning.", "prev_chunk_id": "chunk_448", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_450", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2. Tuning the hyper-parameters of an estimator#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2. Tuning the hyper-parameters of an estimator#", "content": "3.2. Tuning the hyper-parameters of an estimator# Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc. It is possible and recommended to search the hyper-parameter space for the best cross validation score. Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use: estimator.get_params() A search consists of: - an estimator (regressor or classifier such assklearn.svm.SVC()); - a parameter space; - a method for searching or sampling candidates; - a cross-validation scheme; and - ascore function. Two generic approaches to parameter search are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution. Both these tools have successive halving counterparts HalvingGridSearchCV and HalvingRandomSearchCV, which can be much faster at finding a good parameter combination. After describing these tools we detail best practices applicable to these approaches. Some models allow for specialized, efficient parameter search strategies, outlined in Alternatives to brute force parameter search. Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_451", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.1. Exhaustive Grid Search#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.1. Exhaustive Grid Search#", "content": "3.2.1. Exhaustive Grid Search# The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid: param_grid = [ {'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, ] specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001]. The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained. Examples - SeeNested versus non-nested cross-validationfor an example of Grid Search within a cross validation loop on the iris dataset. This is the best practice for evaluating the performance of a model with grid search. - SeeSample pipeline for text feature extraction and evaluationfor an example of Grid Search coupling parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer) with a classifier (here a linear SVM trained with SGD with either elastic net or L2 penalty) using aPipelineinstance.", "prev_chunk_id": "chunk_450", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_452", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.2. Randomized Parameter Optimization#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.2. Randomized Parameter Optimization#", "content": "3.2.2. Randomized Parameter Optimization# While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favorable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search: - A budget can be chosen independent of the number of parameters and possible values. - Adding parameters that do not influence the performance does not decrease efficiency. Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified: {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]} This example uses the scipy.stats module, which contains many useful distributions for sampling parameters, such as expon, gamma, uniform, loguniform or randint. In principle, any function can be passed that provides a rvs (random variate sample) method to sample a value. A call to the rvs function should provide independent random samples from possible parameter values on consecutive calls. For continuous parameters, such as C above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing n_iter will always lead to a finer search. A continuous log-uniform random variable is the continuous version of a log-spaced parameter. For example to specify the equivalent of C from above, loguniform(1, 100) can be used instead of [1, 10, 100]. Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between 1e0 and 1e3: from", "prev_chunk_id": "chunk_451", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_453", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.2. Randomized Parameter Optimization#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.2. Randomized Parameter Optimization#", "content": "sklearn.utils.fixes import loguniform {'C': loguniform(1e0, 1e3), 'gamma': loguniform(1e-4, 1e-3), 'kernel': ['rbf'], 'class_weight':['balanced', None]} Examples - Comparing randomized search and grid search for hyperparameter estimationcompares the usage and efficiency of randomized search and grid search. References - Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)", "prev_chunk_id": "chunk_452", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_454", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.3. Searching for optimal parameters with successive halving#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.3. Searching for optimal parameters with successive halving#", "content": "3.2.3. Searching for optimal parameters with successive halving# Scikit-learn also provides the HalvingGridSearchCV and HalvingRandomSearchCV estimators that can be used to search a parameter space using successive halving [1] [2]. Successive halving (SH) is like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. For parameter tuning, the resource is typically the number of training samples, but it can also be an arbitrary numeric parameter such as n_estimators in a random forest. As illustrated in the figure below, only a subset of candidates ‘survive’ until the last iteration. These are the candidates that have consistently ranked among the top-scoring candidates across all iterations. Each iteration is allocated an increasing amount of resources per candidate, here the number of samples. We here briefly describe the main parameters, but each parameter and their interactions are described more in detail in the dropdown section below. The factor (> 1) parameter controls the rate at which the resources grow, and the rate at which the number of candidates decreases. In each iteration, the number of resources per candidate is multiplied by factor and the number of candidates is divided by the same factor. Along with resource and min_resources, factor is the most important parameter to control the search in our implementation, though a value of 3 usually works well. factor effectively controls the number of iterations in HalvingGridSearchCV and the number of candidates (by default) and iterations in HalvingRandomSearchCV. aggressive_elimination=True can also be used if the number of available resources is small. More control is available through tuning the min_resources parameter. These estimators are still experimental:", "prev_chunk_id": "chunk_453", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_455", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.3. Searching for optimal parameters with successive halving#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.3. Searching for optimal parameters with successive halving#", "content": "their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import enable_halving_search_cv: >>> from sklearn.experimental import enable_halving_search_cv # noqa >>> from sklearn.model_selection import HalvingGridSearchCV >>> from sklearn.model_selection import HalvingRandomSearchCV Examples - Comparison between grid search and successive halving - Successive Halving Iterations The sections below dive into technical aspects of successive halving.", "prev_chunk_id": "chunk_454", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_456", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.3.1. Aggressive elimination of candidates#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.3.1. Aggressive elimination of candidates#", "content": "3.2.3.1. Aggressive elimination of candidates# Using the aggressive_elimination parameter, you can force the search process to end up with less than factor candidates at the last iteration.", "prev_chunk_id": "chunk_455", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_457", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.3.2. Analyzing results with the cv_results_ attribute#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.3.2. Analyzing results with the cv_results_ attribute#", "content": "3.2.3.2. Analyzing results with the cv_results_ attribute# The cv_results_ attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with df = pd.DataFrame(est.cv_results_). The cv_results_ attribute of HalvingGridSearchCV and HalvingRandomSearchCV is similar to that of GridSearchCV and RandomizedSearchCV, with additional information related to the successive halving process.", "prev_chunk_id": "chunk_456", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_458", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.1. Specifying an objective metric#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.1. Specifying an objective metric#", "content": "3.2.4.1. Specifying an objective metric# By default, parameter search uses the score function of the estimator to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative), see Which scoring function should I use? for some guidance. An alternative scoring function can be specified via the scoring parameter of most parameter search tools, see The scoring parameter: defining model evaluation rules for more details.", "prev_chunk_id": "chunk_457", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_459", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.2. Specifying multiple metrics for evaluation#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.2. Specifying multiple metrics for evaluation#", "content": "3.2.4.2. Specifying multiple metrics for evaluation# GridSearchCV and RandomizedSearchCV allow specifying multiple metrics for the scoring parameter. Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details. When specifying multiple metrics, the refit parameter must be set to the metric (string) for which the best_params_ will be found and used to build the best_estimator_ on the whole dataset. If the search should not be refit, set refit=False. Leaving refit to the default value None will result in an error when using multiple metrics. See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage. HalvingRandomSearchCV and HalvingGridSearchCV do not support multimetric scoring.", "prev_chunk_id": "chunk_458", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_460", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.3. Composite estimators and parameter spaces#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.3. Composite estimators and parameter spaces#", "content": "3.2.4.3. Composite estimators and parameter spaces# GridSearchCV and RandomizedSearchCV allow searching over parameters of composite or nested estimators such as Pipeline, ColumnTransformer, VotingClassifier or CalibratedClassifierCV using a dedicated <estimator>__<parameter> syntax: >>> from sklearn.model_selection import GridSearchCV >>> from sklearn.calibration import CalibratedClassifierCV >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.datasets import make_moons >>> X, y = make_moons() >>> calibrated_forest = CalibratedClassifierCV( ... estimator=RandomForestClassifier(n_estimators=10)) >>> param_grid = { ... 'estimator__max_depth': [2, 4, 6, 8]} >>> search = GridSearchCV(calibrated_forest, param_grid, cv=5) >>> search.fit(X, y) GridSearchCV(cv=5, estimator=CalibratedClassifierCV(estimator=RandomForestClassifier(n_estimators=10)), param_grid={'estimator__max_depth': [2, 4, 6, 8]}) Here, <estimator> is the parameter name of the nested estimator, in this case estimator. If the meta-estimator is constructed as a collection of estimators as in pipeline.Pipeline, then <estimator> refers to the name of the estimator, see Access to nested parameters. In practice, there can be several levels of nesting: >>> from sklearn.pipeline import Pipeline >>> from sklearn.feature_selection import SelectKBest >>> pipe = Pipeline([ ... ('select', SelectKBest()), ... ('model', calibrated_forest)]) >>> param_grid = { ... 'select__k': [1, 2], ... 'model__estimator__max_depth': [2, 4, 6, 8]} >>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y) Please refer to Pipeline: chaining estimators for performing parameter searches over pipelines.", "prev_chunk_id": "chunk_459", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_461", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.4. Model selection: development and evaluation#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.4. Model selection: development and evaluation#", "content": "3.2.4.4. Model selection: development and evaluation# Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics. This can be done by using the train_test_split utility function.", "prev_chunk_id": "chunk_460", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_462", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.5. Parallelism#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.5. Parallelism#", "content": "3.2.4.5. Parallelism# The parameter search tools evaluate each parameter combination on each data fold independently. Computations can be run in parallel by using the keyword n_jobs=-1. See function signature for more details, and also the Glossary entry for n_jobs.", "prev_chunk_id": "chunk_461", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_463", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.4.6. Robustness to failure#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.4.6. Robustness to failure#", "content": "3.2.4.6. Robustness to failure# Some parameter settings may result in a failure to fit one or more folds of the data. By default, the score for those settings will be np.nan. This can be controlled by setting error_score=\"raise\" to raise an exception if one fit fails, or for example error_score=0 to set another value for the score of failing parameter combinations.", "prev_chunk_id": "chunk_462", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_464", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.5.1. Model specific cross-validation#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.5.1. Model specific cross-validation#", "content": "3.2.5.1. Model specific cross-validation# Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter. The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the regularization path of the estimator. Here is the list of such models:", "prev_chunk_id": "chunk_463", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_465", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.5.2. Information Criterion#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.5.2. Information Criterion#", "content": "3.2.5.2. Information Criterion# Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation). Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:", "prev_chunk_id": "chunk_464", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_466", "url": "https://scikit-learn.org/stable/modules/grid_search.html", "title": "3.2.5.3. Out of Bag Estimates#", "page_title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.2.5.3. Out of Bag Estimates#", "content": "3.2.5.3. Out of Bag Estimates# When using ensemble methods based upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out. This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes “for free” as no additional data is needed and can be used for model selection. This is currently implemented in the following classes:", "prev_chunk_id": "chunk_465", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_467", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1. Cross-validation: evaluating estimator performance#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1. Cross-validation: evaluating estimator performance#", "content": "3.1. Cross-validation: evaluating estimator performance# Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques. In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. Let’s load the iris data set to fit a linear support vector machine on it: >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> from sklearn import datasets >>> from sklearn import svm >>> X, y = datasets.load_iris(return_X_y=True) >>> X.shape, y.shape ((150, 4), (150,)) We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier: >>> X_train, X_test, y_train, y_test = train_test_split( ... X, y, test_size=0.4, random_state=0) >>> X_train.shape, y_train.shape ((90, 4), (90,)) >>> X_test.shape, y_test.shape ((60, 4), (60,)) >>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train) >>> clf.score(X_test, y_test) 0.96 When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_468", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1. Cross-validation: evaluating estimator performance#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1. Cross-validation: evaluating estimator performance#", "content": "set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set. However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”: - A model is trained using\\(k-1\\)of the folds as training data; - the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy). The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.", "prev_chunk_id": "chunk_467", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_469", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.1. Computing cross-validated metrics#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.1. Computing cross-validated metrics#", "content": "3.1.1. Computing cross-validated metrics# The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset. The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time): >>> from sklearn.model_selection import cross_val_score >>> clf = svm.SVC(kernel='linear', C=1, random_state=42) >>> scores = cross_val_score(clf, X, y, cv=5) >>> scores array([0.96, 1. , 0.96, 0.96, 1. ]) The mean score and the standard deviation are hence given by: >>> print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std())) 0.98 accuracy with a standard deviation of 0.02 By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter: >>> from sklearn import metrics >>> scores = cross_val_score( ... clf, X, y, cv=5, scoring='f1_macro') >>> scores array([0.96, 1., 0.96, 0.96, 1.]) See The scoring parameter: defining model evaluation rules for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal. When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin. It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance: >>> from sklearn.model_selection import ShuffleSplit >>> n_samples = X.shape[0] >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0) >>> cross_val_score(clf, X, y, cv=cv) array([0.977, 0.977, 1., 0.955, 1.]) Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example: >>> def custom_cv_2folds(X): ... n = X.shape[0] ... i = 1", "prev_chunk_id": "chunk_468", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_470", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.1. Computing cross-validated metrics#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.1. Computing cross-validated metrics#", "content": "... while i <= 2: ... idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int) ... yield idx, idx ... i += 1 ... >>> custom_cv = custom_cv_2folds(X) >>> cross_val_score(clf, X, y, cv=custom_cv) array([1. , 0.973])", "prev_chunk_id": "chunk_469", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_471", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.1.1. The cross_validate function and multiple metric evaluation#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.1.1. The cross_validate function and multiple metric evaluation#", "content": "3.1.1.1. The cross_validate function and multiple metric evaluation# The cross_validate function differs from cross_val_score in two ways: - It allows specifying multiple metrics for evaluation. - It returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score. For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - ['test_score', 'fit_time', 'score_time'] And for multiple metric evaluation, the return value is a dict with the following keys - ['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time'] return_train_score is set to False by default to save computation time. To evaluate the scores on the training set as well you need to set it to True. You may also retain the estimator fitted on each training set by setting return_estimator=True. Similarly, you may set return_indices=True to retain the training and testing indices used to split the dataset into train and test sets for each cv split. The multiple metrics can be specified either as a list, tuple or set of predefined scorer names: >>> from sklearn.model_selection import cross_validate >>> from sklearn.metrics import recall_score >>> scoring = ['precision_macro', 'recall_macro'] >>> clf = svm.SVC(kernel='linear', C=1, random_state=0) >>> scores = cross_validate(clf, X, y, scoring=scoring) >>> sorted(scores.keys()) ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro'] >>> scores['test_recall_macro'] array([0.96, 1., 0.96, 0.96, 1.]) Or as a dict mapping scorer name to a predefined or custom scoring function: >>> from sklearn.metrics import make_scorer >>> scoring = {'prec_macro': 'precision_macro', ... 'rec_macro': make_scorer(recall_score, average='macro')} >>> scores = cross_validate(clf, X, y, scoring=scoring, ... cv=5, return_train_score=True) >>> sorted(scores.keys()) ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro', 'train_prec_macro', 'train_rec_macro'] >>> scores['train_rec_macro'] array([0.97, 0.97, 0.99, 0.98, 0.98]) Here is an example of cross_validate using a single metric: >>> scores = cross_validate(clf, X, y, ... scoring='precision_macro', cv=5, ... return_estimator=True) >>> sorted(scores.keys()) ['estimator', 'fit_time', 'score_time', 'test_score']", "prev_chunk_id": "chunk_470", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_472", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.1.2. Obtaining predictions by cross-validation#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.1.2. Obtaining predictions by cross-validation#", "content": "3.1.1.2. Obtaining predictions by cross-validation# The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised). The available cross validation iterators are introduced in the following section. Examples - Receiver Operating Characteristic (ROC) with cross validation, - Recursive feature elimination with cross-validation, - Custom refit strategy of a grid search with cross-validation, - Sample pipeline for text feature extraction and evaluation, - Plotting Cross-Validated Predictions, - Nested versus non-nested cross-validation.", "prev_chunk_id": "chunk_471", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_473", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2. Cross validation iterators#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2. Cross validation iterators#", "content": "3.1.2. Cross validation iterators# The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.", "prev_chunk_id": "chunk_472", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_474", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1. Cross-validation iterators for i.i.d. data#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1. Cross-validation iterators for i.i.d. data#", "content": "3.1.2.1. Cross-validation iterators for i.i.d. data# Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples. The following cross-validators can be used in such cases.", "prev_chunk_id": "chunk_473", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_475", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1.1. K-fold#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1.1. K-fold#", "content": "3.1.2.1.1. K-fold# KFold divides all the samples in \\(k\\) groups of samples, called folds (if \\(k = n\\), this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using \\(k - 1\\) folds, and the fold left out is used for test. Example of 2-fold cross-validation on a dataset with 4 samples: >>> import numpy as np >>> from sklearn.model_selection import KFold >>> X = [\"a\", \"b\", \"c\", \"d\"] >>> kf = KFold(n_splits=2) >>> for train, test in kf.split(X): ... print(\"%s %s\" % (train, test)) [2 3] [0 1] [0 1] [2 3] Here is a visualization of the cross-validation behavior. Note that KFold is not affected by classes or groups. Each fold is constituted by two arrays: the first one is related to the training set, and the second one to the test set. Thus, one can create the training/test sets using numpy indexing: >>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]]) >>> y = np.array([0, 1, 0, 1]) >>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]", "prev_chunk_id": "chunk_474", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_476", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1.2. Repeated K-Fold#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1.2. Repeated K-Fold#", "content": "3.1.2.1.2. Repeated K-Fold# RepeatedKFold repeats KFold \\(n\\) times, producing different splits in each repetition. Example of 2-fold K-Fold repeated 2 times: >>> import numpy as np >>> from sklearn.model_selection import RepeatedKFold >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> random_state = 12883823 >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state) >>> for train, test in rkf.split(X): ... print(\"%s %s\" % (train, test)) ... [2 3] [0 1] [0 1] [2 3] [0 2] [1 3] [1 3] [0 2] Similarly, RepeatedStratifiedKFold repeats StratifiedKFold \\(n\\) times with different randomization in each repetition.", "prev_chunk_id": "chunk_475", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_477", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1.3. Leave One Out (LOO)#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1.3. Leave One Out (LOO)#", "content": "3.1.2.1.3. Leave One Out (LOO)# LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for \\(n\\) samples, we have \\(n\\) different training sets and \\(n\\) different test sets. This cross-validation procedure does not waste much data as only one sample is removed from the training set: >>> from sklearn.model_selection import LeaveOneOut >>> X = [1, 2, 3, 4] >>> loo = LeaveOneOut() >>> for train, test in loo.split(X): ... print(\"%s %s\" % (train, test)) [1 2 3] [0] [0 2 3] [1] [0 1 3] [2] [0 1 2] [3] Potential users of LOO for model selection should weigh a few known caveats. When compared with \\(k\\)-fold cross validation, one builds \\(n\\) models from \\(n\\) samples instead of \\(k\\) models, where \\(n > k\\). Moreover, each is trained on \\(n - 1\\) samples rather than \\((k-1) n / k\\). In both ways, assuming \\(k\\) is not too large and \\(k < n\\), LOO is more computationally expensive than \\(k\\)-fold cross validation. In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \\(n - 1\\) of the \\(n\\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set. However, if the learning curve is steep for the training size in question, then 5 or 10-fold cross validation can overestimate the generalization error. As a general rule, most authors and empirical evidence suggest that 5 or 10-fold cross validation should be preferred to LOO.", "prev_chunk_id": "chunk_476", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_478", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1.4. Leave P Out (LPO)#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1.4. Leave P Out (LPO)#", "content": "3.1.2.1.4. Leave P Out (LPO)# LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing \\(p\\) samples from the complete set. For \\(n\\) samples, this produces \\({n \\choose p}\\) train-test pairs. Unlike LeaveOneOut and KFold, the test sets will overlap for \\(p > 1\\). Example of Leave-2-Out on a dataset with 4 samples: >>> from sklearn.model_selection import LeavePOut >>> X = np.ones(4) >>> lpo = LeavePOut(p=2) >>> for train, test in lpo.split(X): ... print(\"%s %s\" % (train, test)) [2 3] [0 1] [1 3] [0 2] [1 2] [0 3] [0 3] [1 2] [0 2] [1 3] [0 1] [2 3]", "prev_chunk_id": "chunk_477", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_479", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split#", "content": "3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split# The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets. It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state pseudo random number generator. Here is a usage example: >>> from sklearn.model_selection import ShuffleSplit >>> X = np.arange(10) >>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0) >>> for train_index, test_index in ss.split(X): ... print(\"%s %s\" % (train_index, test_index)) [9 1 6 7 3 0 5] [2 8 4] [2 9 8 0 6 7 4] [3 5 1] [4 5 1 0 6 9 7] [2 3 8] [2 7 5 8 0 3 4] [6 1 9] [4 1 0 6 8 9 3] [5 2 7] Here is a visualization of the cross-validation behavior. Note that ShuffleSplit is not affected by classes or groups. ShuffleSplit is thus a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.", "prev_chunk_id": "chunk_478", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_480", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.2. Cross-validation iterators with stratification based on class labels#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.2. Cross-validation iterators with stratification based on class labels#", "content": "3.1.2.2. Cross-validation iterators with stratification based on class labels# Some classification tasks can naturally exhibit rare classes: for instance, there could be orders of magnitude more negative observations than positive observations (e.g. medical screening, fraud detection, etc). As a result, cross-validation splitting can generate train or validation folds without any occurrence of a particular class. This typically leads to undefined classification metrics (e.g. ROC AUC), exceptions raised when attempting to call fit or missing columns in the output of the predict_proba or decision_function methods of multiclass classifiers trained on different folds. To mitigate such problems, splitters such as StratifiedKFold and StratifiedShuffleSplit implement stratified sampling to ensure that relative class frequencies are approximately preserved in each fold.", "prev_chunk_id": "chunk_479", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_481", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.2.1. Stratified K-fold#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.2.1. Stratified K-fold#", "content": "3.1.2.2.1. Stratified K-fold# StratifiedKFold is a variation of K-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with KFold. >>> from sklearn.model_selection import StratifiedKFold, KFold >>> import numpy as np >>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5)) >>> skf = StratifiedKFold(n_splits=3) >>> for train, test in skf.split(X, y): ... print('train - {} | test - {}'.format( ... np.bincount(y[train]), np.bincount(y[test]))) train - [30 3] | test - [15 2] train - [30 3] | test - [15 2] train - [30 4] | test - [15 1] >>> kf = KFold(n_splits=3) >>> for train, test in kf.split(X, y): ... print('train - {} | test - {}'.format( ... np.bincount(y[train]), np.bincount(y[test]))) train - [28 5] | test - [17] train - [28 5] | test - [17] train - [34] | test - [11 5] We can see that StratifiedKFold preserves the class ratios (approximately 1 / 10) in both train and test datasets. Here is a visualization of the cross-validation behavior. RepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times with different randomization in each repetition.", "prev_chunk_id": "chunk_480", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_482", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.2.2. Stratified Shuffle Split#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.2.2. Stratified Shuffle Split#", "content": "3.1.2.2.2. Stratified Shuffle Split# StratifiedShuffleSplit is a variation of ShuffleSplit, which returns stratified splits, i.e. which creates splits by preserving the same percentage for each target class as in the complete set. Here is a visualization of the cross-validation behavior.", "prev_chunk_id": "chunk_481", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_483", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.3. Predefined fold-splits / Validation-sets#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.3. Predefined fold-splits / Validation-sets#", "content": "3.1.2.3. Predefined fold-splits / Validation-sets# For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using PredefinedSplit it is possible to use these folds e.g. when searching for hyperparameters. For example, when using a validation set, set the test_fold to 0 for all samples that are part of the validation set, and to -1 for all other samples.", "prev_chunk_id": "chunk_482", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_484", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4. Cross-validation iterators for grouped data#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4. Cross-validation iterators for grouped data#", "content": "3.1.2.4. Cross-validation iterators for grouped data# The i.i.d. assumption is broken if the underlying generative process yields groups of dependent samples. Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier. In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold. The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the groups parameter.", "prev_chunk_id": "chunk_483", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_485", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4.1. Group K-fold#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4.1. Group K-fold#", "content": "3.1.2.4.1. Group K-fold# GroupKFold is a variation of K-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations. Imagine you have three subjects, each with an associated number from 1 to 3: >>> from sklearn.model_selection import GroupKFold >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10] >>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"] >>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3] >>> gkf = GroupKFold(n_splits=3) >>> for train, test in gkf.split(X, y, groups=groups): ... print(\"%s %s\" % (train, test)) [0 1 2 3 4 5] [6 7 8 9] [0 1 2 6 7 8 9] [3 4 5] [3 4 5 6 7 8 9] [0 1 2] Each subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the folds do not have exactly the same size due to the imbalance in the data. If class proportions must be balanced across folds, StratifiedGroupKFold is a better option. Here is a visualization of the cross-validation behavior. Similar to KFold, the test sets from GroupKFold will form a complete partition of all the data. While GroupKFold attempts to place the same number of samples in each fold when shuffle=False, when shuffle=True it attempts to place an equal number of distinct groups in each fold (but does not account for group sizes).", "prev_chunk_id": "chunk_484", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_486", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4.2. StratifiedGroupKFold#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4.2. StratifiedGroupKFold#", "content": "3.1.2.4.2. StratifiedGroupKFold# StratifiedGroupKFold is a cross-validation scheme that combines both StratifiedKFold and GroupKFold. The idea is to try to preserve the distribution of classes in each split while keeping each group within a single split. That might be useful when you have an unbalanced dataset so that using just GroupKFold might produce skewed splits. Example: >>> from sklearn.model_selection import StratifiedGroupKFold >>> X = list(range(18)) >>> y = [1] * 6 + [0] * 12 >>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6] >>> sgkf = StratifiedGroupKFold(n_splits=3) >>> for train, test in sgkf.split(X, y, groups=groups): ... print(\"%s %s\" % (train, test)) [ 0 2 3 4 5 6 7 10 11 15 16 17] [ 1 8 9 12 13 14] [ 0 1 4 5 6 7 8 9 11 12 13 14] [ 2 3 10 15 16 17] [ 1 2 3 8 9 10 12 13 14 15 16 17] [ 0 4 5 6 7 11] Here is a visualization of cross-validation behavior for uneven groups:", "prev_chunk_id": "chunk_485", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_487", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4.3. Leave One Group Out#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4.3. Leave One Group Out#", "content": "3.1.2.4.3. Leave One Group Out# LeaveOneGroupOut is a cross-validation scheme where each split holds out samples belonging to one specific group. Group information is provided via an array that encodes the group of each sample. Each training set is thus constituted by all the samples except the ones related to a specific group. This is the same as LeavePGroupsOut with n_groups=1 and the same as GroupKFold with n_splits equal to the number of unique labels passed to the groups parameter. For example, in the cases of multiple experiments, LeaveOneGroupOut can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one: >>> from sklearn.model_selection import LeaveOneGroupOut >>> X = [1, 5, 10, 50, 60, 70, 80] >>> y = [0, 1, 1, 2, 2, 2, 2] >>> groups = [1, 1, 2, 2, 3, 3, 3] >>> logo = LeaveOneGroupOut() >>> for train, test in logo.split(X, y, groups=groups): ... print(\"%s %s\" % (train, test)) [2 3 4 5 6] [0 1] [0 1 4 5 6] [2 3] [0 1 2 3] [4 5 6] Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.", "prev_chunk_id": "chunk_486", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_488", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4.4. Leave P Groups Out#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4.4. Leave P Groups Out#", "content": "3.1.2.4.4. Leave P Groups Out# LeavePGroupsOut is similar to LeaveOneGroupOut, but removes samples related to \\(P\\) groups for each training/test set. All possible combinations of \\(P\\) groups are left out, meaning test sets will overlap for \\(P>1\\). Example of Leave-2-Group Out: >>> from sklearn.model_selection import LeavePGroupsOut >>> X = np.arange(6) >>> y = [1, 1, 1, 2, 2, 2] >>> groups = [1, 1, 2, 2, 3, 3] >>> lpgo = LeavePGroupsOut(n_groups=2) >>> for train, test in lpgo.split(X, y, groups=groups): ... print(\"%s %s\" % (train, test)) [4 5] [0 1 2 3] [2 3] [0 1 4 5] [0 1] [2 3 4 5]", "prev_chunk_id": "chunk_487", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_489", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.4.5. Group Shuffle Split#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.4.5. Group Shuffle Split#", "content": "3.1.2.4.5. Group Shuffle Split# The GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut, and generates a sequence of randomized partitions in which a subset of groups are held out for each split. Each train/test split is performed independently meaning there is no guaranteed relationship between successive test sets. Here is a usage example: >>> from sklearn.model_selection import GroupShuffleSplit >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001] >>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"] >>> groups = [1, 1, 2, 2, 3, 3, 4, 4] >>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0) >>> for train, test in gss.split(X, y, groups=groups): ... print(\"%s %s\" % (train, test)) ... [0 1 2 3] [4 5 6 7] [2 3 6 7] [0 1 4 5] [2 3 4 5] [0 1 6 7] [4 5 6 7] [0 1 2 3] Here is a visualization of the cross-validation behavior. This class is useful when the behavior of LeavePGroupsOut is desired, but the number of groups is large enough that generating all possible partitions with \\(P\\) groups withheld would be prohibitively expensive. In such a scenario, GroupShuffleSplit provides a random sample (with replacement) of the train / test splits generated by LeavePGroupsOut.", "prev_chunk_id": "chunk_488", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_490", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.5. Using cross-validation iterators to split train and test#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.5. Using cross-validation iterators to split train and test#", "content": "3.1.2.5. Using cross-validation iterators to split train and test# The above group cross-validation functions may also be useful for splitting a dataset into training and testing subsets. Note that the convenience function train_test_split is a wrapper around ShuffleSplit and thus only allows for stratified splitting (using the class labels) and cannot account for groups. To perform the train and test split, use the indices for the train and test subsets yielded by the generator output by the split() method of the cross-validation splitter. For example: >>> import numpy as np >>> from sklearn.model_selection import GroupShuffleSplit >>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]) >>> y = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]) >>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4]) >>> train_indx, test_indx = next( ... GroupShuffleSplit(random_state=7).split(X, y, groups) ... ) >>> X_train, X_test, y_train, y_test = \\ ... X[train_indx], X[test_indx], y[train_indx], y[test_indx] >>> X_train.shape, X_test.shape ((6,), (2,)) >>> np.unique(groups[train_indx]), np.unique(groups[test_indx]) (array([1, 2, 4]), array([3]))", "prev_chunk_id": "chunk_489", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_491", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.6. Cross validation of time series data#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.6. Cross validation of time series data#", "content": "3.1.2.6. Cross validation of time series data# Time series data is characterized by the correlation between observations that are near in time (autocorrelation). However, classical cross-validation techniques such as KFold and ShuffleSplit assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalization error) on time series data. Therefore, it is very important to evaluate our model for time series data on the “future” observations least like those that are used to train the model. To achieve this, one solution is provided by TimeSeriesSplit.", "prev_chunk_id": "chunk_490", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_492", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.2.6.1. Time Series Split#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.2.6.1. Time Series Split#", "content": "3.1.2.6.1. Time Series Split# TimeSeriesSplit is a variation of k-fold which returns first \\(k\\) folds as train set and the \\((k+1)\\) th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model. This class can be used to cross-validate time series data samples that are observed at fixed time intervals. Indeed, the folds must represent the same duration, in order to have comparable metrics across folds. Example of 3-split time series cross-validation on a dataset with 6 samples: >>> from sklearn.model_selection import TimeSeriesSplit >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([1, 2, 3, 4, 5, 6]) >>> tscv = TimeSeriesSplit(n_splits=3) >>> print(tscv) TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None) >>> for train, test in tscv.split(X): ... print(\"%s %s\" % (train, test)) [0 1 2] [3] [0 1 2 3] [4] [0 1 2 3 4] [5] Here is a visualization of the cross-validation behavior.", "prev_chunk_id": "chunk_491", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_493", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.3. A note on shuffling#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.3. A note on shuffling#", "content": "3.1.3. A note on shuffling# If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross-validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples. Some cross validation iterators, such as KFold, have an inbuilt option to shuffle the data indices before splitting them. Note that: - This consumes less memory than shuffling the data directly. - By default no shuffling occurs, including for the (stratified) K fold cross-validation performed by specifyingcv=some_integertocross_val_score, grid search, etc. Keep in mind thattrain_test_splitstill returns a random split. - Therandom_stateparameter defaults toNone, meaning that the shuffling will be different every timeKFold(...,shuffle=True)is iterated. However,GridSearchCVwill use the same shuffling for each set of parameters validated by a single call to itsfitmethod. - To get identical results for each split, setrandom_stateto an integer. For more details on how to control the randomness of cv splitters and avoid common pitfalls, see Controlling randomness.", "prev_chunk_id": "chunk_492", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_494", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.4. Cross validation and model selection#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.4. Cross validation and model selection#", "content": "3.1.4. Cross validation and model selection# Cross validation iterators can also be used to directly perform model selection using Grid Search for the optimal hyperparameters of the model. This is the topic of the next section: Tuning the hyper-parameters of an estimator.", "prev_chunk_id": "chunk_493", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_495", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.5. Permutation test score#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.5. Permutation test score#", "content": "3.1.5. Permutation test score# permutation_test_score offers another way to evaluate the performance of a predictor. It provides a permutation-based p-value, which represents how likely an observed performance of the estimator would be obtained by chance. The null hypothesis in this test is that the estimator fails to leverage any statistical dependency between the features and the targets to make correct predictions on left-out data. permutation_test_score generates a null distribution by calculating n_permutations different permutations of the data. In each permutation the target values are randomly shuffled, thereby removing any dependency between the features and the targets. The p-value output is the fraction of permutations whose cross-validation score is better or equal than the true score without permuting targets. For reliable results n_permutations should typically be larger than 100 and cv between 3-10 folds. A low p-value provides evidence that the dataset contains some real dependency between features and targets and that the estimator was able to utilize this dependency to obtain good results. A high p-value, in reverse, could be due to either one of these: - a lack of dependency between features and targets (i.e., there is no systematic relationship and any observed patterns are likely due to random chance) - orbecause the estimator was not able to use the dependency in the data (for instance because it underfit). In the latter case, using a more appropriate estimator that is able to use the structure in the data, would result in a lower p-value. Cross-validation provides information about how well an estimator generalizes by estimating the range of its expected scores. However, an estimator trained on a high dimensional dataset with no structure may still perform better than expected on cross-validation, just by chance. This can typically happen with small datasets with less than a few hundred samples. permutation_test_score", "prev_chunk_id": "chunk_494", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_496", "url": "https://scikit-learn.org/stable/modules/cross_validation.html", "title": "3.1.5. Permutation test score#", "page_title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.1 documentation", "breadcrumbs": "3.1.5. Permutation test score#", "content": "provides information on whether the estimator has found a real dependency between features and targets and can help in evaluating the performance of the estimator. It is important to note that this test has been shown to produce low p-values even if there is only weak structure in the data because in the corresponding permutated datasets there is absolutely no structure. This test is therefore only able to show whether the model reliably outperforms random guessing. Finally, permutation_test_score is computed using brute force and internally fits (n_permutations + 1) * n_cv models. It is therefore only tractable with small datasets for which fitting an individual model is very fast. Using the n_jobs parameter parallelizes the computation and thus speeds it up. Examples - Test with permutations the significance of a classification score", "prev_chunk_id": "chunk_495", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_497", "url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "title": "2.9.1. Restricted Boltzmann machines#", "page_title": "2.9. Neural network models (unsupervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.9.1. Restricted Boltzmann machines#", "content": "2.9.1. Restricted Boltzmann machines# Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron. The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides BernoulliRBM, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on. The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (Stochastic Maximum Likelihood) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation. The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training. Examples - Restricted Boltzmann Machine features for digit classification", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_498", "url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "title": "2.9.1.1. Graphical model and parametrization#", "page_title": "2.9. Neural network models (unsupervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.9.1.1. Graphical model and parametrization#", "content": "2.9.1.1. Graphical model and parametrization# The graphical model of an RBM is a fully-connected bipartite graph. The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity. The energy function measures the quality of a joint assignment: In the formula above, \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy: The word restricted refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed: The bipartite structure allows for the use of efficient block Gibbs sampling for inference.", "prev_chunk_id": "chunk_497", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_499", "url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "title": "2.9.1.2. Bernoulli Restricted Boltzmann machines#", "page_title": "2.9. Neural network models (unsupervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.9.1.2. Bernoulli Restricted Boltzmann machines#", "content": "2.9.1.2. Bernoulli Restricted Boltzmann machines# In the BernoulliRBM, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren’t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values. The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives: where \\(\\sigma\\) is the logistic sigmoid function:", "prev_chunk_id": "chunk_498", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_500", "url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "title": "2.9.1.3. Stochastic Maximum Likelihood learning#", "page_title": "2.9. Neural network models (unsupervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.9.1.3. Stochastic Maximum Likelihood learning#", "content": "2.9.1.3. Stochastic Maximum Likelihood learning# The training algorithm implemented in BernoulliRBM is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood: For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples. In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \\(v\\) and \\(h\\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes. The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \\(k\\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution. Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated \\(k\\) Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly. References -", "prev_chunk_id": "chunk_499", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_501", "url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "title": "2.9.1.3. Stochastic Maximum Likelihood learning#", "page_title": "2.9. Neural network models (unsupervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.9.1.3. Stochastic Maximum Likelihood learning#", "content": "“A fast learning algorithm for deep belief nets”, G. Hinton, S. Osindero, Y.-W. Teh, 2006 - “Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient”, T. Tieleman, 2008", "prev_chunk_id": "chunk_500", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_502", "url": "https://scikit-learn.org/stable/modules/density.html", "title": "2.8. Density Estimation#", "page_title": "2.8. Density Estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.8. Density Estimation#", "content": "2.8. Density Estimation# Density estimation walks the line between unsupervised learning, feature engineering, and data modeling. Some of the most popular and useful density estimation techniques are mixture models such as Gaussian Mixtures (GaussianMixture), and neighbor-based approaches such as the kernel density estimate (KernelDensity). Gaussian Mixtures are discussed more fully in the context of clustering, because the technique is also useful as an unsupervised clustering scheme. Density estimation is a very simple concept, and most people are already familiar with one common density estimation technique: the histogram.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_503", "url": "https://scikit-learn.org/stable/modules/density.html", "title": "2.8.1. Density Estimation: Histograms#", "page_title": "2.8. Density Estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.8.1. Density Estimation: Histograms#", "content": "2.8.1. Density Estimation: Histograms# A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure: A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data. Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data. This visualization is an example of a kernel density estimation, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.", "prev_chunk_id": "chunk_502", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_504", "url": "https://scikit-learn.org/stable/modules/density.html", "title": "2.8.2. Kernel Density Estimation#", "page_title": "2.8. Density Estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.8.2. Kernel Density Estimation#", "content": "2.8.2. Kernel Density Estimation# Kernel density estimation in scikit-learn is implemented in the KernelDensity estimator, which uses the Ball Tree or KD Tree for efficient queries (see Nearest Neighbors for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions. In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels: It’s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows: >>> from sklearn.neighbors import KernelDensity >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X) >>> kde.score_samples(X) array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698, -0.41076071]) Here we have used kernel='gaussian', as seen above. Mathematically, a kernel is a positive function \\(K(x;h)\\) which is controlled by the bandwidth parameter \\(h\\). Given this kernel form, the density estimate at a point \\(y\\) within a group of points \\(x_i; i=1, \\cdots, N\\) is given by: The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution. The parameter bandwidth controls this smoothing. One can either set manually this parameter or use Scott’s and Silverman’s estimation methods. KernelDensity implements several common kernel forms, which are shown in the following figure: The kernel density estimator can be used with any of the valid distance metrics (see DistanceMetric for a list of available metrics), though", "prev_chunk_id": "chunk_503", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_505", "url": "https://scikit-learn.org/stable/modules/density.html", "title": "2.8.2. Kernel Density Estimation#", "page_title": "2.8. Density Estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.8.2. Kernel Density Estimation#", "content": "the results are properly normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent: One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model. Here is an example of using this process to create a new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data: The “new” data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model. Examples - Simple 1D Kernel Density Estimation: computation of simple kernel density estimates in one dimension. - Kernel Density Estimation: an example of using Kernel Density estimation to learn a generative model of the hand-written digits data, and drawing new samples from this model. - Kernel Density Estimate of Species Distributions: an example of Kernel Density estimation using the Haversine distance metric to visualize geospatial data", "prev_chunk_id": "chunk_504", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_506", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7. Novelty and Outlier Detection#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7. Novelty and Outlier Detection#", "content": "2.7. Novelty and Outlier Detection# Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an inlier), or should be considered as different (it is an outlier). Often, this ability is used to clean real data sets. Two important distinctions must be made: Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context. The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data: estimator.fit(X_train) new observations can then be sorted as inliers or outliers with a predict method: estimator.predict(X_test) Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the score_samples method, while the threshold can be controlled by the contamination parameter. The decision_function method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers: estimator.decision_function(X_test) Note that neighbors.LocalOutlierFactor does not support predict, decision_function and score_samples methods by default but only a fit_predict method, as this estimator was originally meant to be applied", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_507", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7. Novelty and Outlier Detection#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7. Novelty and Outlier Detection#", "content": "for outlier detection. The scores of abnormality of the training samples are accessible through the negative_outlier_factor_ attribute. If you really want to use neighbors.LocalOutlierFactor for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the novelty parameter set to True before fitting the estimator. In this case, fit_predict is not available. The behavior of neighbors.LocalOutlierFactor is summarized in the following table.", "prev_chunk_id": "chunk_506", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_508", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.1. Overview of outlier detection methods#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.1. Overview of outlier detection methods#", "content": "2.7.1. Overview of outlier detection methods# A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection. ensemble.IsolationForest and neighbors.LocalOutlierFactor perform reasonably well on the data sets considered here. The svm.OneClassSVM is known to be sensitive to outliers and thus does not perform very well for outlier detection. That being said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging. svm.OneClassSVM may still be used with outlier detection but requires fine-tuning of its hyperparameter nu to handle outliers and prevent overfitting. linear_model.SGDOneClassSVM provides an implementation of a linear One-Class SVM with a linear complexity in the number of samples. This implementation is here used with a kernel approximation technique to obtain results similar to svm.OneClassSVM which uses a Gaussian kernel by default. Finally, covariance.EllipticEnvelope assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example Comparing anomaly detection algorithms for outlier detection on toy datasets and the sections hereunder. Examples - SeeComparing anomaly detection algorithms for outlier detection on toy datasetsfor a comparison of thesvm.OneClassSVM, theensemble.IsolationForest, theneighbors.LocalOutlierFactorandcovariance.EllipticEnvelope. - SeeEvaluation of outlier detection estimatorsfor an example showing how to evaluate outlier detection estimators, theneighbors.LocalOutlierFactorand theensemble.IsolationForest, using ROC curves frommetrics.RocCurveDisplay.", "prev_chunk_id": "chunk_507", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_509", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.2. Novelty Detection#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.2. Novelty Detection#", "content": "2.7.2. Novelty Detection# Consider a data set of \\(n\\) observations from the same distribution described by \\(p\\) features. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods. In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \\(p\\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population as the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment. The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the Support Vector Machines module in the svm.OneClassSVM object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The nu parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier. References - Estimating the support of a high-dimensional distributionSchölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471. Examples - SeeOne-class SVM with non-linear kernel (RBF)for visualizing the frontier learned around some data by asvm.OneClassSVMobject. - Species distribution modeling", "prev_chunk_id": "chunk_508", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_510", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.2.1. Scaling up the One-Class SVM#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.2.1. Scaling up the One-Class SVM#", "content": "2.7.2.1. Scaling up the One-Class SVM# An online linear version of the One-Class SVM is implemented in linear_model.SGDOneClassSVM. This implementation scales linearly with the number of samples and can be used with a kernel approximation to approximate the solution of a kernelized svm.OneClassSVM whose complexity is at best quadratic in the number of samples. See section Online One-Class SVM for more details. Examples - SeeOne-Class SVM versus One-Class SVM using Stochastic Gradient Descentfor an illustration of the approximation of a kernelized One-Class SVM with thelinear_model.SGDOneClassSVMcombined with kernel approximation.", "prev_chunk_id": "chunk_509", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_511", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3. Outlier Detection#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3. Outlier Detection#", "content": "2.7.3. Outlier Detection# Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called outliers. Yet, in the case of outlier detection, we don’t have a clean data set representing the population of regular observations that can be used to train any tool.", "prev_chunk_id": "chunk_510", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_512", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3.1. Fitting an elliptic envelope#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3.1. Fitting an elliptic envelope#", "content": "2.7.3.1. Fitting an elliptic envelope# One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the “shape” of the data, and can define outlying observations as observations which stand far enough from the fit shape. The scikit-learn provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode. For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate are used to derive a measure of outlyingness. This strategy is illustrated below. Examples - SeeRobust covariance estimation and Mahalanobis distances relevancefor an illustration of the difference between using a standard (covariance.EmpiricalCovariance) or a robust estimate (covariance.MinCovDet) of location and covariance to assess the degree of outlyingness of an observation. - SeeOutlier detection on a real data setfor an example of robust covariance estimation on a real data set. References - Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum covariance determinant estimator” Technometrics 41(3), 212 (1999)", "prev_chunk_id": "chunk_511", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_513", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3.2. Isolation Forest#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3.2. Isolation Forest#", "content": "2.7.3.2. Isolation Forest# One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. The implementation of ensemble.IsolationForest is based on an ensemble of tree.ExtraTreeRegressor. Following Isolation Forest original paper, the maximum depth of each tree is set to \\(\\lceil \\log_2(n) \\rceil\\) where \\(n\\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details). This algorithm is illustrated below. The ensemble.IsolationForest supports warm_start=True which allows you to add more trees to an already fitted model: >>> from sklearn.ensemble import IsolationForest >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]]) >>> clf = IsolationForest(n_estimators=10, warm_start=True) >>> clf.fit(X) # fit 10 trees >>> clf.set_params(n_estimators=20) # add 10 more trees >>> clf.fit(X) # fit the added trees Examples - SeeIsolationForest examplefor an illustration of the use of IsolationForest. - SeeComparing anomaly detection algorithms for outlier detection on toy datasetsfor a comparison ofensemble.IsolationForestwithneighbors.LocalOutlierFactor,svm.OneClassSVM(tuned to perform like an outlier detection method),linear_model.SGDOneClassSVM, and a covariance-based outlier detection withcovariance.EllipticEnvelope. References - Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008.", "prev_chunk_id": "chunk_512", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_514", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3.2. Isolation Forest#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3.2. Isolation Forest#", "content": "ICDM’08. Eighth IEEE International Conference on.", "prev_chunk_id": "chunk_513", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_515", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3.3. Local Outlier Factor#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3.3. Local Outlier Factor#", "content": "2.7.3.3. Local Outlier Factor# Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm. The neighbors.LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors. In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of its k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density. The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such information is generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below). The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood. When applying LOF for outlier detection, there are no predict, decision_function", "prev_chunk_id": "chunk_514", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_516", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.3.3. Local Outlier Factor#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.3.3. Local Outlier Factor#", "content": "and score_samples methods but only a fit_predict method. The scores of abnormality of the training samples are accessible through the negative_outlier_factor_ attribute. Note that predict, decision_function and score_samples can be used on new unseen data when LOF is applied for novelty detection, i.e. when the novelty parameter is set to True, but the result of predict may differ from that of fit_predict. See Novelty detection with Local Outlier Factor. This strategy is illustrated below. Examples - SeeOutlier detection with Local Outlier Factor (LOF)for an illustration of the use ofneighbors.LocalOutlierFactor. - SeeComparing anomaly detection algorithms for outlier detection on toy datasetsfor a comparison with other anomaly detection methods. References - Breunig, Kriegel, Ng, and Sander (2000)LOF: identifying density-based local outliers.Proc. ACM SIGMOD", "prev_chunk_id": "chunk_515", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_517", "url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "title": "2.7.4. Novelty detection with Local Outlier Factor#", "page_title": "2.7. Novelty and Outlier Detection — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.7.4. Novelty detection with Local Outlier Factor#", "content": "2.7.4. Novelty detection with Local Outlier Factor# To use neighbors.LocalOutlierFactor for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the novelty parameter set to True before fitting the estimator: lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) Note that fit_predict is not available in this case to avoid inconsistencies. Novelty detection with neighbors.LocalOutlierFactor is illustrated below (see Novelty detection with Local Outlier Factor (LOF)).", "prev_chunk_id": "chunk_516", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_518", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6. Covariance estimation#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6. Covariance estimation#", "content": "2.6. Covariance estimation# Many statistical problems require the estimation of a population’s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation’s quality. The sklearn.covariance package provides tools for accurately estimating a population’s covariance matrix under various settings. We assume that the observations are independent and identically distributed (i.i.d.).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_519", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.1. Empirical covariance#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.1. Empirical covariance#", "content": "2.6.1. Empirical covariance# The covariance matrix of a data set is known to be well approximated by the classical maximum likelihood estimator (or “empirical covariance”), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an asymptotically unbiased estimator of the corresponding population’s covariance matrix. The empirical covariance matrix of a sample can be computed using the empirical_covariance function of the package, or by fitting an EmpiricalCovariance object to the data sample with the EmpiricalCovariance.fit method. Be careful that results depend on whether the data are centered, so one may want to use the assume_centered parameter accurately. More precisely, if assume_centered=False, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and assume_centered=True should be used. Examples - SeeShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihoodfor an example on how to fit anEmpiricalCovarianceobject to data.", "prev_chunk_id": "chunk_518", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_520", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.2.1. Basic shrinkage#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.2.1. Basic shrinkage#", "content": "2.6.2.1. Basic shrinkage# Despite being an asymptotically unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the shrinkage. In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the shrunk_covariance method. Also, a shrunk estimator of the covariance can be fitted to data with a ShrunkCovariance object and its ShrunkCovariance.fit method. Again, results depend on whether the data are centered, so one may want to use the assume_centered parameter accurately. Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple convex transformation : \\(\\Sigma_{\\rm shrunk} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm Tr}\\hat{\\Sigma}}{p}\\rm Id\\). Choosing the amount of shrinkage, \\(\\alpha\\) amounts to setting a bias/variance trade-off, and is discussed below. Examples - SeeShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihoodfor an example on how to fit aShrunkCovarianceobject to data.", "prev_chunk_id": "chunk_519", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_521", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.2.2. Ledoit-Wolf shrinkage#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.2.2. Ledoit-Wolf shrinkage#", "content": "2.6.2.2. Ledoit-Wolf shrinkage# In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \\(\\alpha\\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix. The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the ledoit_wolf function of the sklearn.covariance package, or it can be otherwise obtained by fitting a LedoitWolf object to the same sample. Examples - SeeShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihoodfor an example on how to fit aLedoitWolfobject to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of likelihood. References", "prev_chunk_id": "chunk_520", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_522", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.2.3. Oracle Approximating Shrinkage#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.2.3. Oracle Approximating Shrinkage#", "content": "2.6.2.3. Oracle Approximating Shrinkage# Under the assumption that the data are Gaussian distributed, Chen et al. [2] derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance. The OAS estimator of the covariance matrix can be computed on a sample with the oas function of the sklearn.covariance package, or it can be otherwise obtained by fitting an OAS object to the same sample. References Examples - SeeShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihoodfor an example on how to fit anOASobject to data. - SeeLedoit-Wolf vs OAS estimationto visualize the Mean Squared Error difference between aLedoitWolfand anOASestimator of the covariance.", "prev_chunk_id": "chunk_521", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_523", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.3. Sparse inverse covariance#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.3. Sparse inverse covariance#", "content": "2.6.3. Sparse inverse covariance# The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as covariance selection. In the small-samples situation, in which n_samples is on the order of n_features or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure. The GraphicalLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its alpha parameter, the more sparse the precision matrix. The corresponding GraphicalLassoCV object uses cross-validation to automatically set the alpha parameter. The mathematical formulation is the following: Where \\(K\\) is the precision matrix to be estimated, and \\(S\\) is the sample covariance matrix. \\(\\|K\\|_1\\) is the sum of the absolute values of off-diagonal coefficients of \\(K\\). The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R glasso package. Examples - Sparse inverse covariance estimation: example on synthetic data showing some recovery of a structure, and comparing to other covariance estimators. - Visualizing the stock market structure: example on real stock market data, finding which symbols are most linked. References - Friedman et al,“Sparse inverse covariance estimation with the graphical lasso”, Biostatistics 9, pp 432, 2008", "prev_chunk_id": "chunk_522", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_524", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.4. Robust Covariance Estimation#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.4. Robust Covariance Estimation#", "content": "2.6.4. Robust Covariance Estimation# Real data sets are often subject to measurement or recording errors. Regular but uncommon observations may also appear for a variety of reasons. Observations which are very uncommon are called outliers. The empirical covariance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outliers in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data. The sklearn.covariance package implements a robust estimator of covariance, the Minimum Covariance Determinant [3].", "prev_chunk_id": "chunk_523", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_525", "url": "https://scikit-learn.org/stable/modules/covariance.html", "title": "2.6.4.1. Minimum Covariance Determinant#", "page_title": "2.6. Covariance estimation — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.6.4.1. Minimum Covariance Determinant#", "content": "2.6.4.1. Minimum Covariance Determinant# The Minimum Covariance Determinant estimator is a robust estimator of a data set’s covariance introduced by P.J. Rousseeuw in [3]. The idea is to find a given proportion (h) of “good” observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (“consistency step”). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (“reweighting step”). Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order to compute the Minimum Covariance Determinant. This algorithm is used in scikit-learn when fitting an MCD object to data. The FastMCD algorithm also computes a robust estimate of the data set location at the same time. Raw estimates can be accessed as raw_location_ and raw_covariance_ attributes of a MinCovDet robust covariance estimator object. References Examples - SeeRobust vs Empirical covariance estimatefor an example on how to fit aMinCovDetobject to data and see how the estimate remains accurate despite the presence of outliers. - SeeRobust covariance estimation and Mahalanobis distances relevanceto visualize the difference betweenEmpiricalCovarianceandMinCovDetcovariance estimators in terms of Mahalanobis distance (so we get a better estimate of the precision matrix too).", "prev_chunk_id": "chunk_524", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_526", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.1. Exact PCA and probabilistic interpretation#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.1. Exact PCA and probabilistic interpretation#", "content": "2.5.1.1. Exact PCA and probabilistic interpretation# PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns \\(n\\) components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm. Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance: The PCA object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a score method that can be used in cross-validation: Examples - Principal Component Analysis (PCA) on Iris Dataset - Comparison of LDA and PCA 2D projection of Iris dataset - Model selection with Probabilistic PCA and Factor Analysis (FA)", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_527", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.2. Incremental PCA#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.2. Incremental PCA#", "content": "2.5.1.2. Incremental PCA# The PCA object is very useful, but has certain limitations for large datasets. The biggest limitation is that PCA only supports batch processing, which means all of the data to be processed must fit in main memory. The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion. IncrementalPCA makes it possible to implement out-of-core Principal Component Analysis either by: - Using itspartial_fitmethod on chunks of data fetched sequentially from the local hard drive or a network database. - Calling its fit method on a memory mapped file usingnumpy.memmap. IncrementalPCA only stores estimates of component and noise variances, in order to update explained_variance_ratio_ incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset. As in PCA, IncrementalPCA centers but does not scale the input data for each feature before applying the SVD. Examples - Incremental PCA", "prev_chunk_id": "chunk_526", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_528", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.3. PCA using randomized SVD#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.3. PCA using randomized SVD#", "content": "2.5.1.3. PCA using randomized SVD# It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values. For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserving most of the explained variance at the same time. The class PCA used with the optional parameter svd_solver='randomized' is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform. For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \\(n_{samples} = 400\\) and \\(n_{features} = 64 \\times 64 = 4096\\), the computation time is less than 1s: If we note \\(n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})\\) and \\(n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})\\), the time complexity of the randomized PCA is \\(O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})\\) instead of \\(O(n_{\\max}^2 \\cdot n_{\\min})\\) for the exact method implemented in PCA. The memory footprint of randomized PCA is also proportional to \\(2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}\\) instead of \\(n_{\\max}", "prev_chunk_id": "chunk_527", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_529", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.3. PCA using randomized SVD#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.3. PCA using randomized SVD#", "content": "\\cdot n_{\\min}\\) for the exact method. Note: the implementation of inverse_transform in PCA with svd_solver='randomized' is not the exact inverse transform of transform even when whiten=False (default). Examples - Faces recognition example using eigenfaces and SVMs - Faces dataset decompositions References - Algorithm 4.3 in“Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions”Halko, et al., 2009 - “An implementation of a randomized algorithm for principal component analysis”A. Szlam et al. 2014", "prev_chunk_id": "chunk_528", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_530", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)#", "content": "2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)# SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data. Mini-batch sparse PCA (MiniBatchSparsePCA) is a variant of SparsePCA that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations. Principal component analysis (PCA) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces. Sparse principal components yield a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples. The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \\(h \\in \\mathbf{R}^{4096}\\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see [Jen09] for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below. Note that there are many different formulations for the Sparse PCA problem.", "prev_chunk_id": "chunk_529", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_531", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)#", "content": "The one implemented here is based on [Mrl09] . The optimization problem solved is a PCA problem (dictionary learning) with an \\(\\ell_1\\) penalty on the components: \\(||.||_{\\text{Fro}}\\) stands for the Frobenius norm and \\(||.||_{1,1}\\) stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix. The sparsity-inducing \\(||.||_{1,1}\\) matrix norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter alpha. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero. Examples - Faces dataset decompositions References", "prev_chunk_id": "chunk_530", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_532", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.2.1. Exact Kernel PCA#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.2.1. Exact Kernel PCA#", "content": "2.5.2.1. Exact Kernel PCA# KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels) [Scholkopf1997]. It has many applications including denoising, compression and structured prediction (kernel dependency estimation). KernelPCA supports both transform and inverse_transform. Examples - Kernel PCA - Image denoising using kernel PCA References", "prev_chunk_id": "chunk_531", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_533", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.2.2. Choice of solver for Kernel PCA#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.2.2. Choice of solver for Kernel PCA#", "content": "2.5.2.2. Choice of solver for Kernel PCA# While in PCA the number of components is bounded by the number of features, in KernelPCA the number of components is bounded by the number of samples. Many real-world datasets have large number of samples! In these cases finding all the components with a full kPCA is a waste of computation time, as data is mostly described by the first few components (e.g. n_components<=100). In other words, the centered Gram matrix that is eigendecomposed in the Kernel PCA fitting process has an effective rank that is much smaller than its size. This is a situation where approximate eigensolvers can provide speedup with very low precision loss.", "prev_chunk_id": "chunk_532", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_534", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.3. Truncated singular value decomposition and latent semantic analysis#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.3. Truncated singular value decomposition and latent semantic analysis#", "content": "2.5.3. Truncated singular value decomposition and latent semantic analysis# TruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the \\(k\\) largest singular values, where \\(k\\) is a user-specified parameter. TruncatedSVD is very similar to PCA, but differs in that the matrix \\(X\\) does not need to be centered. When the columnwise (per-feature) means of \\(X\\) are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. Examples - Clustering text documents using k-means References - Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),Introduction to Information Retrieval, Cambridge University Press, chapter 18:Matrix decompositions & latent semantic indexing", "prev_chunk_id": "chunk_533", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_535", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.4.1. Sparse coding with a precomputed dictionary#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.4.1. Sparse coding with a precomputed dictionary#", "content": "2.5.4.1. Sparse coding with a precomputed dictionary# The SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a fit method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the transform_method initialization parameter: - Orthogonal matching pursuit (Orthogonal Matching Pursuit (OMP)) - Least-angle regression (Least Angle Regression) - Lasso computed by least-angle regression - Lasso using coordinate descent (Lasso) - Thresholding Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction. The dictionary learning objects offer, via the split_code parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading. The split code for a single sample has length 2 * n_components and is constructed using the following rule: First, the regular code of length n_components is computed. Then, the first n_components entries of the split_code are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative. Examples - Sparse coding with a precomputed dictionary", "prev_chunk_id": "chunk_534", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_536", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.4.2. Generic dictionary learning#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.4.2. Generic dictionary learning#", "content": "2.5.4.2. Generic dictionary learning# Dictionary learning (DictionaryLearning) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data. Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks. Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code. \\(||.||_{\\text{Fro}}\\) stands for the Frobenius norm and \\(||.||_{1,1}\\) stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix. After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see Sparse coding with a precomputed dictionary). It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros. The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like. Examples - Image denoising using dictionary learning References - “Online dictionary learning for sparse coding”J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009", "prev_chunk_id": "chunk_535", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_537", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.4.3. Mini-batch dictionary learning#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.4.3. Mini-batch dictionary learning#", "content": "2.5.4.3. Mini-batch dictionary learning# MiniBatchDictionaryLearning implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets. By default, MiniBatchDictionaryLearning divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition. The estimator also implements partial_fit, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into memory.", "prev_chunk_id": "chunk_536", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_538", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.5. Factor Analysis#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.5. Factor Analysis#", "content": "2.5.5. Factor Analysis# In unsupervised learning we only have a dataset \\(X = \\{x_1, x_2, \\dots, x_n \\}\\). How can this dataset be described mathematically? A very simple continuous latent variable model for \\(X\\) is The vector \\(h_i\\) is called “latent” because it is unobserved. \\(\\epsilon\\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \\(\\Psi\\) (i.e. \\(\\epsilon \\sim \\mathcal{N}(0, \\Psi)\\)), \\(\\mu\\) is some arbitrary offset vector. Such a model is called “generative” as it describes how \\(x_i\\) is generated from \\(h_i\\). If we use all the \\(x_i\\)’s as columns to form a matrix \\(\\mathbf{X}\\) and all the \\(h_i\\)’s as columns of a matrix \\(\\mathbf{H}\\) then we can write (with suitably defined \\(\\mathbf{M}\\) and \\(\\mathbf{E}\\)): In other words, we decomposed matrix \\(\\mathbf{X}\\). If \\(h_i\\) is given, the above equation automatically implies the following probabilistic interpretation: For a complete probabilistic model we also need a prior distribution for the latent variable \\(h\\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \\(h \\sim \\mathcal{N}(0, \\mathbf{I})\\). This yields a Gaussian as the marginal distribution of \\(x\\): Now, without any further assumptions the idea of having a latent variable \\(h\\) would be superfluous – \\(x\\) can be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance \\(\\Psi\\): - \\(\\Psi = \\sigma^2 \\mathbf{I}\\): This assumption leads to the probabilistic model ofPCA. - \\(\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)\\): This model is calledFactorAnalysis, a classical statistical model. The matrix W is sometimes called the “factor loading matrix”. Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex", "prev_chunk_id": "chunk_537", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_539", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.5. Factor Analysis#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.5. Factor Analysis#", "content": "models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. FastICA) if non-Gaussian priors on the latent variables are assumed. Factor analysis can produce similar components (the columns of its loading matrix) to PCA. However, one can not make any general statements about these components (e.g. whether they are orthogonal): The main advantage for Factor Analysis over PCA is that it can model the variance in every direction of the input space independently (heteroscedastic noise): This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise: Factor Analysis is often followed by a rotation of the factors (with the parameter rotation), usually to improve interpretability. For example, Varimax rotation maximizes the sum of the variances of the squared loadings, i.e., it tends to produce sparser factors, which are influenced by only a few features each (the “simple structure”). See e.g., the first example below. Examples - Factor Analysis (with rotation) to visualize patterns - Model selection with Probabilistic PCA and Factor Analysis (FA)", "prev_chunk_id": "chunk_538", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_540", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.6. Independent component analysis (ICA)#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.6. Independent component analysis (ICA)#", "content": "2.5.6. Independent component analysis (ICA)# Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants. It is classically used to separate mixed signals (a problem known as blind source separation), as in the example below: ICA can also be used as yet another non linear decomposition that finds components with some sparsity: Examples - Blind source separation using FastICA - FastICA on 2D point clouds - Faces dataset decompositions", "prev_chunk_id": "chunk_539", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_541", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.7.1. NMF with the Frobenius norm#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.7.1. NMF with the Frobenius norm#", "content": "2.5.7.1. NMF with the Frobenius norm# NMF [1] is an alternative approach to decomposition that assumes that the data and the components are non-negative. NMF can be plugged in instead of PCA or its variants, in the cases where the data matrix does not contain negative values. It finds a decomposition of samples \\(X\\) into two matrices \\(W\\) and \\(H\\) of non-negative elements, by optimizing the distance \\(d\\) between \\(X\\) and the matrix product \\(WH\\). The most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices: Unlike PCA, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text. It has been observed in [Hoyer, 2004] [2] that, when carefully constrained, NMF can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by NMF from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces. The init attribute determines the initialization method applied, which has a great impact on the performance of the method. NMF implements the method Nonnegative Double Singular Value Decomposition. NNDSVD [4] is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case. Note that the Multiplicative Update (‘mu’) solver cannot update", "prev_chunk_id": "chunk_540", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_542", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.7.1. NMF with the Frobenius norm#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.7.1. NMF with the Frobenius norm#", "content": "zeros present in the initialization, so it leads to poorer results when used jointly with the basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or NNDSVDar should be preferred. NMF can also be initialized with correctly scaled random non-negative matrices by setting init=\"random\". An integer seed or a RandomState can also be passed to random_state to control reproducibility. In NMF, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in ElasticNet, we control the combination of L1 and L2 with the l1_ratio (\\(\\rho\\)) parameter, and the intensity of the regularization with the alpha_W and alpha_H (\\(\\alpha_W\\) and \\(\\alpha_H\\)) parameters. The priors are scaled by the number of samples (\\(n\\_samples\\)) for H and the number of features (\\(n\\_features\\)) for W to keep their impact balanced with respect to one another and to the data fit term as independent as possible of the size of the training set. Then the priors terms are: and the regularized objective function is:", "prev_chunk_id": "chunk_541", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_543", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.7.2. NMF with a beta-divergence#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.7.2. NMF with a beta-divergence#", "content": "2.5.7.2. NMF with a beta-divergence# As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices: Other distance functions can be used in NMF as, for example, the (generalized) Kullback-Leibler (KL) divergence, also referred as I-divergence: Or, the Itakura-Saito (IS) divergence: These three distances are special cases of the beta-divergence family, with \\(\\beta = 2, 1, 0\\) respectively [6]. The beta-divergence is defined by : Note that this definition is not valid if \\(\\beta \\in (0; 1)\\), yet it can be continuously extended to the definitions of \\(d_{KL}\\) and \\(d_{IS}\\) respectively. NMF is best used with the fit_transform method, which returns the matrix W. The matrix H is stored into the fitted model in the components_ attribute; the method transform will decompose a new matrix X_new based on these stored components: >>> import numpy as np >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]) >>> from sklearn.decomposition import NMF >>> model = NMF(n_components=2, init='random', random_state=0) >>> W = model.fit_transform(X) >>> H = model.components_ >>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]]) >>> W_new = model.transform(X_new) Examples - Faces dataset decompositions - Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation", "prev_chunk_id": "chunk_542", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_544", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.7.3. Mini-batch Non Negative Matrix Factorization#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.7.3. Mini-batch Non Negative Matrix Factorization#", "content": "2.5.7.3. Mini-batch Non Negative Matrix Factorization# MiniBatchNMF [7] implements a faster, but less accurate version of the non negative matrix factorization (i.e. NMF), better suited for large datasets. By default, MiniBatchNMF divides the data into mini-batches and optimizes the NMF model in an online manner by cycling over the mini-batches for the specified number of iterations. The batch_size parameter controls the size of the batches. In order to speed up the mini-batch algorithm it is also possible to scale past batches, giving them less importance than newer batches. This is done by introducing a so-called forgetting factor controlled by the forget_factor parameter. The estimator also implements partial_fit, which updates H by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or when the data does not fit into memory. References", "prev_chunk_id": "chunk_543", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_545", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.8. Latent Dirichlet Allocation (LDA)#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.8. Latent Dirichlet Allocation (LDA)#", "content": "2.5.8. Latent Dirichlet Allocation (LDA)# Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete datasets such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents. The graphical model of LDA is a three-level generative model: Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013): - The corpus is a collection of\\(D\\)documents. - A document is a sequence of\\(N\\)words. - There are\\(K\\)topics in the corpus. - The boxes represent repeated sampling. In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure. LatentDirichletAllocation implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points. When LatentDirichletAllocation is applied on a “document-term” matrix, the matrix will be decomposed into a “topic-term” matrix and a “document-topic” matrix. While “topic-term” matrix is stored as components_ in the model, “document-topic” matrix can be calculated from transform method. LatentDirichletAllocation also implements partial_fit method. This is used when data can be fetched sequentially. Examples - Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation References - “Latent Dirichlet Allocation”D. Blei, A. Ng, M. Jordan, 2003 - “Online Learning for Latent Dirichlet Allocation”M.", "prev_chunk_id": "chunk_544", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_546", "url": "https://scikit-learn.org/stable/modules/decomposition.html", "title": "2.5.8. Latent Dirichlet Allocation (LDA)#", "page_title": "2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.5.8. Latent Dirichlet Allocation (LDA)#", "content": "Hoffman, D. Blei, F. Bach, 2010 - “Stochastic Variational Inference”M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013 - “The varimax criterion for analytic rotation in factor analysis”H. F. Kaiser, 1958 See also Dimensionality reduction for dimensionality reduction with Neighborhood Components Analysis.", "prev_chunk_id": "chunk_545", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_547", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4. Biclustering#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4. Biclustering#", "content": "2.4. Biclustering# Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties. For instance, given a matrix of shape (10, 10), one possible bicluster with three rows and two columns induces a submatrix of shape (3, 2): >>> import numpy as np >>> data = np.arange(100).reshape(10, 10) >>> rows = np.array([0, 2, 3])[:, np.newaxis] >>> columns = np.array([1, 2]) >>> data[rows, columns] array([[ 1, 2], [21, 22], [31, 32]]) For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous. Algorithms differ in how they define biclusters. Some of the common types include: - constant values, constant rows, or constant columns - unusually high or low values - submatrices with low variance - correlated rows or columns Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions. If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns: In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small: After fitting a model, row and column cluster membership can be found in the rows_ and columns_ attributes. rows_[i] is a binary vector with nonzero entries corresponding to rows that belong to", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_548", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4. Biclustering#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4. Biclustering#", "content": "bicluster i. Similarly, columns_[i] indicates which columns belong to bicluster i. Some models also have row_labels_ and column_labels_ attributes. These models partition the rows and columns, such as in the block diagonal and checkerboard bicluster structures.", "prev_chunk_id": "chunk_547", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_549", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.1. Spectral Co-Clustering#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.1. Spectral Co-Clustering#", "content": "2.4.1. Spectral Co-Clustering# The SpectralCoclustering algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:", "prev_chunk_id": "chunk_548", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_550", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.1.1. Mathematical formulation#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.1.1. Mathematical formulation#", "content": "2.4.1.1. Mathematical formulation# An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix \\(A\\) has shape \\(m \\times n\\), the Laplacian matrix for the corresponding bipartite graph has shape \\((m + n) \\times (m + n)\\). However, in this case it is possible to work directly with \\(A\\), which is smaller and more efficient. The input matrix \\(A\\) is preprocessed as follows: Where \\(R\\) is the diagonal matrix with entry \\(i\\) equal to \\(\\sum_{j} A_{ij}\\) and \\(C\\) is the diagonal matrix with entry \\(j\\) equal to \\(\\sum_{i} A_{ij}\\). The singular value decomposition, \\(A_n = U \\Sigma V^\\top\\), provides the partitions of the rows and columns of \\(A\\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions. The \\(\\ell = \\lceil \\log_2 k \\rceil\\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \\(Z\\): where the columns of \\(U\\) are \\(u_2, \\dots, u_{\\ell + 1}\\), and similarly for \\(V\\). Then the rows of \\(Z\\) are clustered using k-means. The first n_rows labels provide the row partitioning, and the remaining n_columns labels provide the column partitioning. Examples - A demo of the Spectral Co-Clustering algorithm: A simple example showing how to generate a data matrix with biclusters and apply this method to it. - Biclustering documents with the Spectral Co-clustering algorithm: An example of finding biclusters in the twenty newsgroup dataset. References - Dhillon, Inderjit S, 2001.Co-clustering documents and words using bipartite spectral graph partitioning", "prev_chunk_id": "chunk_549", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_551", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.2. Spectral Biclustering#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.2. Spectral Biclustering#", "content": "2.4.2. Spectral Biclustering# The SpectralBiclustering algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.", "prev_chunk_id": "chunk_550", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_552", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.2.1. Mathematical formulation#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.2.1. Mathematical formulation#", "content": "2.4.2.1. Mathematical formulation# The input matrix \\(A\\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods: - Independent row and column normalization, as in Spectral Co-Clustering. This method makes the rows sum to a constant and the columns sum to a different constant. - Bistochastization: repeated row and column normalization until convergence. This method makes both rows and columns sum to the same constant. - Log normalization: the log of the data matrix is computed:\\(L = \\log A\\). Then the column mean\\(\\overline{L_{i \\cdot}}\\), row mean\\(\\overline{L_{\\cdot j}}\\), and overall mean\\(\\overline{L_{\\cdot \\cdot}}\\)of\\(L\\)are computed. The final matrix is computed according to the formula After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm. If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \\(u_1\\) and \\(v_1\\). are discarded. From now on, the “first” singular vectors refers to \\(u_2 \\dots u_{p+1}\\) and \\(v_2 \\dots v_{p+1}\\) except in the case of log normalization. Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vectors are selected. Next, the data is projected to this best subset of singular vectors and clustered. For instance, if \\(p\\) singular vectors were calculated, the \\(q\\) best are found as described, where \\(q<p\\). Let \\(U\\) be the matrix with columns the \\(q\\) best left singular vectors, and similarly \\(V\\) for the right. To partition the rows, the rows of \\(A\\) are projected to a \\(q\\) dimensional space: \\(A * V\\). Treating the \\(m\\) rows of this \\(m \\times q\\) matrix as samples and", "prev_chunk_id": "chunk_551", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_553", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.2.1. Mathematical formulation#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.2.1. Mathematical formulation#", "content": "clustering using k-means yields the row labels. Similarly, projecting the columns to \\(A^{\\top} * U\\) and clustering this \\(n \\times q\\) matrix yields the column labels. Examples - A demo of the Spectral Biclustering algorithm: a simple example showing how to generate a checkerboard matrix and bicluster it. References - Kluger, Yuval, et. al., 2003.Spectral biclustering of microarray data: coclustering genes and conditions", "prev_chunk_id": "chunk_552", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_554", "url": "https://scikit-learn.org/stable/modules/biclustering.html", "title": "2.4.3. Biclustering evaluation#", "page_title": "2.4. Biclustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.4.3. Biclustering evaluation#", "content": "2.4.3. Biclustering evaluation# There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known. To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score. To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented: where \\(A\\) and \\(B\\) are biclusters, \\(|A \\cap B|\\) is the number of elements in their intersection. The Jaccard index achieves its minimum of 0 when the biclusters do not overlap at all and its maximum of 1 when they are identical. Several methods have been developed to compare two sets of biclusters. For now, only consensus_score (Hochreiter et. al., 2010) is available: - Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar measure. - Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed usingscipy.optimize.linear_sum_assignment, which uses a modified Jonker-Volgenant algorithm. - The final sum of similarities is divided by the size of the larger set. The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical. References - Hochreiter, Bodenhofer, et. al., 2010.FABIA: factor analysis for bicluster acquisition.", "prev_chunk_id": "chunk_553", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_555", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3. Clustering#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3. Clustering#", "content": "2.3. Clustering# Clustering of unlabeled data can be performed with the module sklearn.cluster. Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the labels_ attribute.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_556", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.1. Overview of clustering methods#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.1. Overview of clustering methods#", "content": "2.3.1. Overview of clustering methods# Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above. Gaussian mixture models, useful for clustering, are described in another chapter of the documentation dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component. Transductive clustering methods (in contrast to inductive clustering methods) are not designed to be applied to new, unseen data. Examples - Inductive Clustering: An example of an inductive clustering model for handling new data.", "prev_chunk_id": "chunk_555", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_557", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.2. K-means#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.2. K-means#", "content": "2.3.2. K-means# The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields. The k-means algorithm divides a set of \\(N\\) samples \\(X\\) into \\(K\\) disjoint clusters \\(C\\), each described by the mean \\(\\mu_j\\) of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from \\(X\\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion: Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks: - Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes. - Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such asPrincipal component analysis (PCA)prior to k-means clustering can alleviate this problem and speed up the computations. For more detailed descriptions of the issues shown above and how to address them, refer to the examples Demonstration of k-means assumptions and Selecting the number of clusters with silhouette analysis on KMeans clustering. K-means is often referred to as Lloyd’s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method", "prev_chunk_id": "chunk_556", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_558", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.2. K-means#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.2. K-means#", "content": "being to choose \\(k\\) samples from the dataset \\(X\\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly. K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix. The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance. Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='k-means++' parameter). This initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization, as shown in the reference. For detailed examples of comparing different initialization schemes, refer to A demo of K-Means", "prev_chunk_id": "chunk_557", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_559", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.2. K-means#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.2. K-means#", "content": "clustering on the handwritten digits data and Empirical evaluation of the impact of k-means initialization. K-means++ can also be called independently to select seeds for other clustering algorithms, see sklearn.cluster.kmeans_plusplus for details and example usage. The algorithm supports sample weights, which can be given by a parameter sample_weight. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \\(X\\). Examples - Clustering text documents using k-means: Document clustering usingKMeansandMiniBatchKMeansbased on sparse data - An example of K-Means++ initialization: Using K-means++ to select seeds for other clustering algorithms.", "prev_chunk_id": "chunk_558", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_560", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.2.1. Low-level parallelism#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.2.1. Low-level parallelism#", "content": "2.3.2.1. Low-level parallelism# KMeans benefits from OpenMP based parallelism through Cython. Small chunks of data (256 samples) are processed in parallel, which in addition yields a low memory footprint. For more details on how to control the number of threads, please refer to our Parallelism notes. Examples - Demonstration of k-means assumptions: Demonstrating when k-means performs intuitively and when it does not - A demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits", "prev_chunk_id": "chunk_559", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_561", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.2.2. Mini Batch K-Means#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.2.2. Mini Batch K-Means#", "content": "2.3.2.2. Mini Batch K-Means# The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm. The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \\(b\\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached. MiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced. In practice this difference in quality can be quite small, as shown in the example and cited reference. Examples - Comparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison ofKMeansandMiniBatchKMeans - Clustering text documents using k-means: Document clustering usingKMeansandMiniBatchKMeansbased on sparse data - Online learning of a dictionary of parts of faces", "prev_chunk_id": "chunk_560", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_562", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.3. Affinity Propagation#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.3. Affinity Propagation#", "content": "2.3.3. Affinity Propagation# AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \\(O(N^2 T)\\), where \\(N\\) is the number of samples and \\(T\\) is the number of iterations until convergence. Further, the memory complexity is of the order \\(O(N^2)\\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. Examples - Demo of affinity propagation clustering algorithm: Affinity Propagation on a synthetic 2D datasets with 3 classes - Visualizing the stock market structureAffinity Propagation on financial time series to find groups of companies", "prev_chunk_id": "chunk_561", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_563", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.4. Mean Shift#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.4. Mean Shift#", "content": "2.3.4. Mean Shift# MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. The algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set. The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small. Labelling a new sample is performed by finding the nearest centroid for a given sample. Examples - A demo of the mean-shift clustering algorithm: Mean Shift clustering on a synthetic 2D datasets with 3 classes.", "prev_chunk_id": "chunk_562", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_564", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.5. Spectral clustering#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.5. Spectral clustering#", "content": "2.3.5. Spectral clustering# SpectralClustering performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space. It is especially computationally efficient if the affinity matrix is sparse and the amg solver is used for the eigenvalue problem (Note, the amg solver requires that the pyamg module is installed.) The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters. For two clusters, SpectralClustering solves a convex relaxation of the normalized cuts problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image. Examples - Spectral clustering for image segmentation: Segmenting objects from a noisy background using spectral clustering. - Segmenting the picture of greek coins in regions: Spectral clustering to split the image of coins in regions.", "prev_chunk_id": "chunk_563", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_565", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.5.1. Different label assignment strategies#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.5.1. Different label assignment strategies#", "content": "2.3.5.1. Different label assignment strategies# Different label assignment strategies can be used, corresponding to the assign_labels parameter of SpectralClustering. \"kmeans\" strategy can match finer details, but can be unstable. In particular, unless you control the random_state, it may not be reproducible from run-to-run, as it depends on random initialization. The alternative \"discretize\" strategy is 100% reproducible, but tends to create parcels of fairly even and geometrical shape. The recently added \"cluster_qr\" option is a deterministic alternative that tends to create the visually best partitioning on the example application below.", "prev_chunk_id": "chunk_564", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_566", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.5.2. Spectral Clustering Graphs#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.5.2. Spectral Clustering Graphs#", "content": "2.3.5.2. Spectral Clustering Graphs# Spectral Clustering can also be used to partition graphs via their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with affinity='precomputed': >>> from sklearn.cluster import SpectralClustering >>> sc = SpectralClustering(3, affinity='precomputed', n_init=100, ... assign_labels='discretize') >>> sc.fit_predict(adjacency_matrix)", "prev_chunk_id": "chunk_565", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_567", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6. Hierarchical clustering#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6. Hierarchical clustering#", "content": "2.3.6. Hierarchical clustering# Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for more details. The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy: - Wardminimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach. - Maximumorcomplete linkageminimizes the maximum distance between observations of pairs of clusters. - Average linkageminimizes the average of the distances between all observations of pairs of clusters. - Single linkageminimizes the distance between the closest observations of pairs of clusters. AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.", "prev_chunk_id": "chunk_566", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_568", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6.1. Different linkage type: Ward, complete, average, and single linkage#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6.1. Different linkage type: Ward, complete, average, and single linkage#", "content": "2.3.6.1. Different linkage type: Ward, complete, average, and single linkage# AgglomerativeClustering supports Ward, single, average, and complete linkage strategies. Agglomerative cluster has a “rich get richer” behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data. Examples - Various Agglomerative Clustering on a 2D embedding of digits: exploration of the different linkage strategies in a real dataset.Comparing different hierarchical linkage methods on toy datasets: exploration of the different linkage strategies in toy datasets.", "prev_chunk_id": "chunk_567", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_569", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6.2. Visualization of cluster hierarchy#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6.2. Visualization of cluster hierarchy#", "content": "2.3.6.2. Visualization of cluster hierarchy# It’s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes. Examples - Plot Hierarchical Clustering Dendrogram", "prev_chunk_id": "chunk_568", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_570", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6.3. Adding connectivity constraints#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6.3. Adding connectivity constraints#", "content": "2.3.6.3. Adding connectivity constraints# An interesting aspect of AgglomerativeClustering is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll. These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high. The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using sklearn.neighbors.kneighbors_graph to restrict merging to nearest neighbors as in this example, or using sklearn.feature_extraction.image.grid_to_graph to enable only merging of neighboring pixels on an image, as in the coin example. Examples - A demo of structured Ward hierarchical clustering on an image of coins: Ward clustering to split the image of coins in regions. - Hierarchical clustering: structured vs unstructured ward: Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches. - Feature agglomeration vs. univariate selection: Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering. - Agglomerative clustering with and without structure", "prev_chunk_id": "chunk_569", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_571", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6.4. Varying the metric#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6.4. Varying the metric#", "content": "2.3.6.4. Varying the metric# Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance (l2), Manhattan distance (or Cityblock, or l1), cosine distance, or any precomputed affinity matrix. - l1distance is often good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using occurrences of rare words. - cosinedistance is interesting because it is invariant to global scalings of the signal. The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class. Examples - Agglomerative clustering with different metrics", "prev_chunk_id": "chunk_570", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_572", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.6.5. Bisecting K-Means#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.6.5. Bisecting K-Means#", "content": "2.3.6.5. Bisecting K-Means# The BisectingKMeans is an iterative variant of KMeans, using divisive hierarchical clustering. Instead of creating all centroids at once, centroids are picked progressively based on a previous clustering: a cluster is split into two new clusters repeatedly until the target number of clusters is reached. BisectingKMeans is more efficient than KMeans when the number of clusters is large since it only works on a subset of the data at each bisection while KMeans always works on the entire dataset. Although BisectingKMeans can’t benefit from the advantages of the \"k-means++\" initialization by design, it will still produce comparable results than KMeans(init=\"k-means++\") in terms of inertia at cheaper computational costs, and will likely produce better results than KMeans with a random initialization. This variant is more efficient to agglomerative clustering if the number of clusters is small compared to the number of data points. This variant also does not produce empty clusters. Picking by largest amount of data points in most cases produces result as accurate as picking by inertia and is faster (especially for larger amount of data points, where calculating error may be costly). Picking by largest amount of data points will also likely produce clusters of similar sizes while KMeans is known to produce clusters of different sizes. Difference between Bisecting K-Means and regular K-Means can be seen on example Bisecting K-Means and Regular K-Means Performance Comparison. While the regular K-Means algorithm tends to create non-related clusters, clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.", "prev_chunk_id": "chunk_571", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_573", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.7. DBSCAN#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.7. DBSCAN#", "content": "2.3.7. DBSCAN# The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm.", "prev_chunk_id": "chunk_572", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_574", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.7. DBSCAN#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.7. DBSCAN#", "content": "While the parameter min_samples primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter eps is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. Examples - Demo of DBSCAN clustering algorithm - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with NoiseEster, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996 - DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.", "prev_chunk_id": "chunk_573", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_575", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.8. HDBSCAN#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.8. HDBSCAN#", "content": "2.3.8. HDBSCAN# The HDBSCAN algorithm can be seen as an extension of DBSCAN and OPTICS. Specifically, DBSCAN assumes that the clustering criterion (i.e. density requirement) is globally homogeneous. In other words, DBSCAN may struggle to successfully capture clusters with different densities. HDBSCAN alleviates this assumption and explores all possible density scales by building an alternative representation of the clustering problem. Examples - Demo of HDBSCAN clustering algorithm", "prev_chunk_id": "chunk_574", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_576", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.8.1. Mutual Reachability Graph#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.8.1. Mutual Reachability Graph#", "content": "2.3.8.1. Mutual Reachability Graph# HDBSCAN first defines \\(d_c(x_p)\\), the core distance of a sample \\(x_p\\), as the distance to its min_samples th-nearest neighbor, counting itself. For example, if min_samples=5 and \\(x_*\\) is the 5th-nearest neighbor of \\(x_p\\) then the core distance is: Next it defines \\(d_m(x_p, x_q)\\), the mutual reachability distance of two points \\(x_p, x_q\\), as: These two notions allow us to construct the mutual reachability graph \\(G_{ms}\\) defined for a fixed choice of min_samples by associating each sample \\(x_p\\) with a vertex of the graph, and thus edges between points \\(x_p, x_q\\) are the mutual reachability distance \\(d_m(x_p, x_q)\\) between them. We may build subsets of this graph, denoted as \\(G_{ms,\\varepsilon}\\), by removing any edges with value greater than \\(\\varepsilon\\): from the original graph. Any points whose core distance is less than \\(\\varepsilon\\): are at this staged marked as noise. The remaining points are then clustered by finding the connected components of this trimmed graph.", "prev_chunk_id": "chunk_575", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_577", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.8.2. Hierarchical Clustering#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.8.2. Hierarchical Clustering#", "content": "2.3.8.2. Hierarchical Clustering# HDBSCAN can be seen as an algorithm which performs DBSCAN* clustering across all values of \\(\\varepsilon\\). As mentioned prior, this is equivalent to finding the connected components of the mutual reachability graphs for all values of \\(\\varepsilon\\). To do this efficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully -connected mutual reachability graph, then greedily cuts the edges with highest weight. An outline of the HDBSCAN algorithm is as follows: - Extract the MST of\\(G_{ms}\\). - Extend the MST by adding a “self edge” for each vertex, with weight equal to the core distance of the underlying sample. - Initialize a single cluster and label for the MST. - Remove the edge with the greatest weight from the MST (ties are removed simultaneously). - Assign cluster labels to the connected components which contain the end points of the now-removed edge. If the component does not have at least one edge it is instead assigned a “null” label marking it as noise. - Repeat 4-5 until there are no more connected components. HDBSCAN is therefore able to obtain all possible partitions achievable by DBSCAN* for a fixed choice of min_samples in a hierarchical fashion. Indeed, this allows HDBSCAN to perform clustering across multiple densities and as such it no longer needs \\(\\varepsilon\\) to be given as a hyperparameter. Instead it relies solely on the choice of min_samples, which tends to be a more robust hyperparameter. HDBSCAN can be smoothed with an additional hyperparameter min_cluster_size which specifies that during the hierarchical clustering, components with fewer than minimum_cluster_size many samples are considered noise. In practice, one can set minimum_cluster_size = min_samples to couple the parameters and simplify the hyperparameter space. References", "prev_chunk_id": "chunk_576", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_578", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.9. OPTICS#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.9. OPTICS#", "content": "2.3.9. OPTICS# The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a reachability_ distance, and a spot within the cluster ordering_ attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for max_eps, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given eps value using the cluster_optics_dbscan method. Setting max_eps to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set ordering_ produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. ‘Cutting’ the reachability plot at a single value produces DBSCAN like results; all points above the ‘cut’ are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter xi. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by", "prev_chunk_id": "chunk_577", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_579", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.9. OPTICS#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.9. OPTICS#", "content": "the algorithm can be accessed through the cluster_hierarchy_ parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. Examples - Demo of OPTICS clustering algorithm", "prev_chunk_id": "chunk_578", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_580", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.10. BIRCH#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.10. BIRCH#", "content": "2.3.10. BIRCH# The Birch builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children. The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes: - Number of samples in a subcluster. - Linear Sum - An n-dimensional vector holding the sum of all samples - Squared Sum - Sum of the squared L2 norm of all samples. - Centroids - To avoid recalculation linear sum / n_samples. - Squared norm of the centroids. The BIRCH algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters. This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.", "prev_chunk_id": "chunk_579", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_581", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11. Clustering performance evaluation#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11. Clustering performance evaluation#", "content": "2.3.11. Clustering performance evaluation# Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric.", "prev_chunk_id": "chunk_580", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_582", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.1. Rand index#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.1. Rand index#", "content": "2.3.11.1. Rand index# Given the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments of the same samples labels_pred, the (adjusted or unadjusted) Rand index is a function that measures the similarity of the two assignments, ignoring permutations: >>> from sklearn import metrics >>> labels_true = [0, 0, 0, 1, 1, 1] >>> labels_pred = [0, 0, 1, 1, 2, 2] >>> metrics.rand_score(labels_true, labels_pred) 0.66 The Rand index does not ensure to obtain a value close to 0.0 for a random labelling. The adjusted Rand index corrects for chance and will give such a baseline. >>> metrics.adjusted_rand_score(labels_true, labels_pred) 0.24 As with all clustering metrics, one can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score: >>> labels_pred = [1, 1, 0, 0, 3, 3] >>> metrics.rand_score(labels_true, labels_pred) 0.66 >>> metrics.adjusted_rand_score(labels_true, labels_pred) 0.24 Furthermore, both rand_score and adjusted_rand_score are symmetric: swapping the argument does not change the scores. They can thus be used as consensus measures: >>> metrics.rand_score(labels_pred, labels_true) 0.66 >>> metrics.adjusted_rand_score(labels_pred, labels_true) 0.24 Perfect labeling is scored 1.0: >>> labels_pred = labels_true[:] >>> metrics.rand_score(labels_true, labels_pred) 1.0 >>> metrics.adjusted_rand_score(labels_true, labels_pred) 1.0 Poorly agreeing labels (e.g. independent labelings) have lower scores, and for the adjusted Rand index the score will be negative or close to zero. However, for the unadjusted Rand index the score, while lower, will not necessarily be close to zero: >>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1] >>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6] >>> metrics.rand_score(labels_true, labels_pred) 0.39 >>> metrics.adjusted_rand_score(labels_true, labels_pred) -0.072 Examples - Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the value of clustering measures for random assignments.", "prev_chunk_id": "chunk_581", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_583", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.2. Mutual Information based scores#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.2. Mutual Information based scores#", "content": "2.3.11.2. Mutual Information based scores# Given the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments of the same samples labels_pred, the Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, Normalized Mutual Information (NMI) and Adjusted Mutual Information (AMI). NMI is often used in the literature, while AMI was proposed more recently and is normalized against chance: >>> from sklearn import metrics >>> labels_true = [0, 0, 0, 1, 1, 1] >>> labels_pred = [0, 0, 1, 1, 2, 2] >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred) 0.22504 One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score: >>> labels_pred = [1, 1, 0, 0, 3, 3] >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred) 0.22504 All, mutual_info_score, adjusted_mutual_info_score and normalized_mutual_info_score are symmetric: swapping the argument does not change the score. Thus they can be used as a consensus measure: >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true) 0.22504 Perfect labeling is scored 1.0: >>> labels_pred = labels_true[:] >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred) 1.0 >>> metrics.normalized_mutual_info_score(labels_true, labels_pred) 1.0 This is not true for mutual_info_score, which is therefore harder to judge: >>> metrics.mutual_info_score(labels_true, labels_pred) 0.69 Bad (e.g. independent labelings) have non-positive scores: >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1] >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2] >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred) -0.10526 Examples - Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the value of clustering measures for random assignments. This example also includes the Adjusted Rand Index.", "prev_chunk_id": "chunk_582", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_584", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.3. Homogeneity, completeness and V-measure#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.3. Homogeneity, completeness and V-measure#", "content": "2.3.11.3. Homogeneity, completeness and V-measure# Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis. In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment: - homogeneity: each cluster contains only members of a single class. - completeness: all members of a given class are assigned to the same cluster. We can turn those concept as scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better): >>> from sklearn import metrics >>> labels_true = [0, 0, 0, 1, 1, 1] >>> labels_pred = [0, 0, 1, 1, 2, 2] >>> metrics.homogeneity_score(labels_true, labels_pred) 0.66 >>> metrics.completeness_score(labels_true, labels_pred) 0.42 Their harmonic mean called V-measure is computed by v_measure_score: >>> metrics.v_measure_score(labels_true, labels_pred) 0.516 This function’s formula is as follows: beta defaults to a value of 1.0, but for using a value less than 1 for beta: >>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6) 0.547 more weight will be attributed to homogeneity, and using a value greater than 1: >>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8) 0.48 more weight will be attributed to completeness. The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean [B2011]. Homogeneity, completeness and V-measure can be computed at once using homogeneity_completeness_v_measure as follows: >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred) (0.67, 0.42, 0.52) The following clustering assignment is slightly better, since it is homogeneous but not complete: >>> labels_pred = [0, 0, 0, 1, 2, 2] >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred) (1.0, 0.68, 0.81) Examples - Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the value of clustering measures for random assignments. References - V-Measure: A conditional entropy-based external cluster evaluation measureAndrew Rosenberg and Julia Hirschberg,", "prev_chunk_id": "chunk_583", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_585", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.3. Homogeneity, completeness and V-measure#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.3. Homogeneity, completeness and V-measure#", "content": "2007", "prev_chunk_id": "chunk_584", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_586", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.4. Fowlkes-Mallows scores#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.4. Fowlkes-Mallows scores#", "content": "2.3.11.4. Fowlkes-Mallows scores# The original Fowlkes-Mallows index (FMI) was intended to measure the similarity between two clustering results, which is inherently an unsupervised comparison. The supervised adaptation of the Fowlkes-Mallows index (as implemented in sklearn.metrics.fowlkes_mallows_score) can be used when the ground truth class assignments of the samples are known. The FMI is defined as the geometric mean of the pairwise precision and recall: In the above formula: - TP(True Positive): The number of pairs of points that are clustered together both in the true labels and in the predicted labels. - FP(False Positive): The number of pairs of points that are clustered together in the predicted labels but not in the true labels. - FN(False Negative): The number of pairs of points that are clustered together in the true labels but not in the predicted labels. The score ranges from 0 to 1. A high value indicates a good similarity between two clusters. >>> from sklearn import metrics >>> labels_true = [0, 0, 0, 1, 1, 1] >>> labels_pred = [0, 0, 1, 1, 2, 2] >>> metrics.fowlkes_mallows_score(labels_true, labels_pred) 0.47140 One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score: >>> labels_pred = [1, 1, 0, 0, 3, 3] >>> metrics.fowlkes_mallows_score(labels_true, labels_pred) 0.47140 Perfect labeling is scored 1.0: >>> labels_pred = labels_true[:] >>> metrics.fowlkes_mallows_score(labels_true, labels_pred) 1.0 Bad (e.g. independent labelings) have zero scores: >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1] >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2] >>> metrics.fowlkes_mallows_score(labels_true, labels_pred) 0.0", "prev_chunk_id": "chunk_585", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_587", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.5. Silhouette Coefficient#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.5. Silhouette Coefficient#", "content": "2.3.11.5. Silhouette Coefficient# If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores: - a: The mean distance between a sample and all other points in the same class. - b: The mean distance between a sample and all other points in thenext nearest cluster. The Silhouette Coefficient s for a single sample is then given as: The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample. >>> from sklearn import metrics >>> from sklearn.metrics import pairwise_distances >>> from sklearn import datasets >>> X, y = datasets.load_iris(return_X_y=True) In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis. >>> import numpy as np >>> from sklearn.cluster import KMeans >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X) >>> labels = kmeans_model.labels_ >>> metrics.silhouette_score(X, labels, metric='euclidean') 0.55 Examples - Selecting the number of clusters with silhouette analysis on KMeans clustering: In this example the silhouette analysis is used to choose an optimal value for n_clusters.", "prev_chunk_id": "chunk_586", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_588", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.6. Calinski-Harabasz Index#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.6. Calinski-Harabasz Index#", "content": "2.3.11.6. Calinski-Harabasz Index# If the ground truth labels are not known, the Calinski-Harabasz index (sklearn.metrics.calinski_harabasz_score) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters. The index is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared): >>> from sklearn import metrics >>> from sklearn.metrics import pairwise_distances >>> from sklearn import datasets >>> X, y = datasets.load_iris(return_X_y=True) In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis: >>> import numpy as np >>> from sklearn.cluster import KMeans >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X) >>> labels = kmeans_model.labels_ >>> metrics.calinski_harabasz_score(X, labels) 561.59", "prev_chunk_id": "chunk_587", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_589", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.7. Davies-Bouldin Index#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.7. Davies-Bouldin Index#", "content": "2.3.11.7. Davies-Bouldin Index# If the ground truth labels are not known, the Davies-Bouldin index (sklearn.metrics.davies_bouldin_score) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters. This index signifies the average ‘similarity’ between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score. Values closer to zero indicate a better partition. In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows: >>> from sklearn import datasets >>> iris = datasets.load_iris() >>> X = iris.data >>> from sklearn.cluster import KMeans >>> from sklearn.metrics import davies_bouldin_score >>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X) >>> labels = kmeans.labels_ >>> davies_bouldin_score(X, labels) 0.666", "prev_chunk_id": "chunk_588", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_590", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.8. Contingency Matrix#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.8. Contingency Matrix#", "content": "2.3.11.8. Contingency Matrix# Contingency matrix (sklearn.metrics.cluster.contingency_matrix) reports the intersection cardinality for every true/predicted cluster pair. The contingency matrix provides sufficient statistics for all clustering metrics where the samples are independent and identically distributed and one doesn’t need to account for some instances not being clustered. Here is an example: >>> from sklearn.metrics.cluster import contingency_matrix >>> x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"] >>> y = [0, 0, 1, 1, 2, 2] >>> contingency_matrix(x, y) array([[2, 1, 0], [0, 1, 2]]) The first row of the output array indicates that there are three samples whose true cluster is “a”. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is “b”. Of them, none is in predicted cluster 0, one is in 1 and two are in 2. A confusion matrix for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes.", "prev_chunk_id": "chunk_589", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_591", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.9. Pair Confusion Matrix#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.9. Pair Confusion Matrix#", "content": "2.3.11.9. Pair Confusion Matrix# The pair confusion matrix (sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2 similarity matrix between two clusterings computed by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings. It has the following entries: \\(C_{00}\\) : number of pairs with both clusterings having the samples not clustered together \\(C_{10}\\) : number of pairs with the true label clustering having the samples clustered together but the other clustering not having the samples clustered together \\(C_{01}\\) : number of pairs with the true label clustering not having the samples clustered together but the other clustering having the samples clustered together \\(C_{11}\\) : number of pairs with both clusterings having the samples clustered together Considering a pair of samples that is clustered together a positive pair, then as in binary classification the count of true negatives is \\(C_{00}\\), false negatives is \\(C_{10}\\), true positives is \\(C_{11}\\) and false positives is \\(C_{01}\\). Perfectly matching labelings have all non-zero entries on the diagonal regardless of actual label values: >>> from sklearn.metrics.cluster import pair_confusion_matrix >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1]) array([[8, 0], [0, 4]]) >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0]) array([[8, 0], [0, 4]]) Labelings that assign all classes members to the same clusters are complete but may not always be pure, hence penalized, and have some off-diagonal non-zero entries: >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1]) array([[8, 2], [0, 2]]) The matrix is not symmetric: >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2]) array([[8, 0], [2, 2]]) If classes members are completely split across different clusters, the assignment is totally incomplete, hence the matrix has all zero diagonal entries: >>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3]) array([[ 0, 0], [12,", "prev_chunk_id": "chunk_590", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_592", "url": "https://scikit-learn.org/stable/modules/clustering.html", "title": "2.3.11.9. Pair Confusion Matrix#", "page_title": "2.3. Clustering — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.3.11.9. Pair Confusion Matrix#", "content": "0]])", "prev_chunk_id": "chunk_591", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_593", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2. Manifold learning#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2. Manifold learning#", "content": "2.2. Manifold learning# Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_594", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.1. Introduction#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.1. Introduction#", "content": "2.2.1. Introduction# High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way. The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost. To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an “interesting” linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data. Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications. Examples - SeeManifold learning on handwritten digits: Locally Linear Embedding, Isomap…for an example of dimensionality reduction on handwritten digits. - SeeComparison of Manifold Learning methodsfor an example of dimensionality reduction on a toy “S-curve” dataset. - SeeVisualizing the stock market structurefor an example of using manifold learning to map the stock market structure based on historical stock prices. - SeeManifold Learning methods on a severed spherefor an example of manifold learning techniques applied to a spherical data-set. - SeeSwiss Roll And", "prev_chunk_id": "chunk_593", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_595", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.1. Introduction#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.1. Introduction#", "content": "Swiss-Hole Reductionfor an example of using manifold learning techniques on a Swiss Roll dataset. The manifold learning implementations available in scikit-learn are summarized below", "prev_chunk_id": "chunk_594", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_596", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.2. Isomap#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.2. Isomap#", "content": "2.2.2. Isomap# One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap. References - “A global geometric framework for nonlinear dimensionality reduction”Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500)", "prev_chunk_id": "chunk_595", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_597", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.3. Locally Linear Embedding#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.3. Locally Linear Embedding#", "content": "2.2.3. Locally Linear Embedding# Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding. Locally linear embedding can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding. References - “Nonlinear dimensionality reduction by locally linear embedding”Roweis, S. & Saul, L. Science 290:2323 (2000)", "prev_chunk_id": "chunk_596", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_598", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.4. Modified Locally Linear Embedding#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.4. Modified Locally Linear Embedding#", "content": "2.2.4. Modified Locally Linear Embedding# One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter \\(r\\), which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as \\(r \\to 0\\), the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for \\(r > 0\\). This problem manifests itself in embeddings which distort the underlying geometry of the manifold. One method to address the regularization problem is to use multiple weight vectors in each neighborhood. This is the essence of modified locally linear embedding (MLLE). MLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'modified'. It requires n_neighbors > n_components. References - “MLLE: Modified Locally Linear Embedding Using Multiple Weights”Zhang, Z. & Wang, J.", "prev_chunk_id": "chunk_597", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_599", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.5. Hessian Eigenmapping#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.5. Hessian Eigenmapping#", "content": "2.2.5. Hessian Eigenmapping# Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, sklearn implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'hessian'. It requires n_neighbors > n_components * (n_components + 3) / 2. References - “Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data”Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)", "prev_chunk_id": "chunk_598", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_600", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.6. Spectral Embedding#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.6. Spectral Embedding#", "content": "2.2.6. Spectral Embedding# Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding. References - “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation”M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396", "prev_chunk_id": "chunk_599", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_601", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.7. Local Tangent Space Alignment#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.7. Local Tangent Space Alignment#", "content": "2.2.7. Local Tangent Space Alignment# Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'ltsa'. References - “Principal manifolds and nonlinear dimensionality reduction via tangent space alignment”Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)", "prev_chunk_id": "chunk_600", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_602", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.8. Multi-dimensional Scaling (MDS)#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.8. Multi-dimensional Scaling (MDS)#", "content": "2.2.8. Multi-dimensional Scaling (MDS)# Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space. In general, MDS is a technique used for analyzing dissimilarity data. It attempts to model dissimilarities as distances in a Euclidean space. The data can be ratings of dissimilarity between objects, interaction frequencies of molecules, or trade indices between countries. There exist two types of MDS algorithm: metric and non-metric. In scikit-learn, the class MDS implements both. In metric MDS, the distances in the embedding space are set as close as possible to the dissimilarity data. In the non-metric version, the algorithm will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the input dissimilarities. Let \\(\\delta_{ij}\\) be the dissimilarity matrix between the \\(n\\) input points (possibly arising as some pairwise distances \\(d_{ij}(X)\\) between the coordinates \\(X\\) of the input points). Disparities \\(\\hat{d}_{ij} = f(\\delta_{ij})\\) are some transformation of the dissimilarities. The MDS objective, called the raw stress, is then defined by \\(\\sum_{i < j} (\\hat{d}_{ij} - d_{ij}(Z))^2\\), where \\(d_{ij}(Z)\\) are the pairwise distances between the coordinates \\(Z\\) of the embedded points. References - “More on Multidimensional Scaling and Unfolding in R: smacof Version 2”Mair P, Groenen P., de Leeuw J. Journal of Statistical Software (2022) - “Modern Multidimensional Scaling - Theory and Applications”Borg, I.; Groenen P. Springer Series in Statistics (1997) - “Nonmetric multidimensional scaling: a numerical method”Kruskal, J. Psychometrika, 29 (1964) - “Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”Kruskal, J. Psychometrika, 29, (1964)", "prev_chunk_id": "chunk_601", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_603", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)#", "content": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)# t-SNE (TSNE) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques: - Revealing the structure at many scales on a single map - Revealing data that lie in multiple, different, manifolds or clusters - Reducing the tendency to crowd points together at the center While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset. The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence. The disadvantages to using t-SNE are roughly: - t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes - The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings. - The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error. - Global", "prev_chunk_id": "chunk_602", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_604", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)#", "content": "structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (usinginit='pca'). References - “Visualizing High-Dimensional Data Using t-SNE”van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008) - “t-Distributed Stochastic Neighbor Embedding”van der Maaten, L.J.P. - “Accelerating t-SNE using Tree-Based Algorithms”van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014. - “Automated optimized parameters for T-distributed stochastic neighbor embedding improve visualization and analysis of large datasets”Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J., Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019).", "prev_chunk_id": "chunk_603", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_605", "url": "https://scikit-learn.org/stable/modules/manifold.html", "title": "2.2.10. Tips on practical use#", "page_title": "2.2. Manifold learning — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.2.10. Tips on practical use#", "content": "2.2.10. Tips on practical use# - Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. SeeStandardScalerfor convenient ways of scaling heterogeneous data. - The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a\\(d\\)-dimensional manifold embedded in a\\(D\\)-dimensional parameter space, the reconstruction error will decrease asn_componentsis increased untiln_components==d. - Note that noisy data can “short-circuit” the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated. Manifold learning on noisy and/or incomplete data is an active area of research. - Certain input configurations can lead to singular weight matrices, for example when more than two points in the dataset are identical, or when the data is split into disjointed groups. In this case,solver='arpack'will fail to find the null space. The easiest way to address this is to usesolver='dense'which will work on a singular matrix, though it may be very slow depending on the number of input points. Alternatively, one can attempt to understand the source of the singularity: if it is due to disjoint sets, increasingn_neighborsmay help. If it is due to identical points in the dataset, removing these points may help.", "prev_chunk_id": "chunk_604", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_606", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1. Gaussian mixture models#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1. Gaussian mixture models#", "content": "2.1. Gaussian mixture models# sklearn.mixture is a package which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and full covariance matrices supported), sample them, and estimate them from data. Facilities to help determine the appropriate number of components are also provided. A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Scikit-learn implements different classes to estimate Gaussian mixture models, that correspond to different estimation strategies, detailed below.", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_607", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1.1. Gaussian Mixture#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1.1. Gaussian Mixture#", "content": "2.1.1. Gaussian Mixture# The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from training data. Given test data, it can assign to each sample the Gaussian it most probably belongs to using the GaussianMixture.predict method. The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance. Examples - SeeGMM covariancesfor an example of using the Gaussian mixture as clustering on the iris dataset. - SeeDensity Estimation for a Gaussian mixturefor an example on plotting the density estimation.", "prev_chunk_id": "chunk_606", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_608", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1.2. Variational Bayesian Gaussian Mixture#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1.2. Variational Bayesian Gaussian Mixture#", "content": "2.1.2. Variational Bayesian Gaussian Mixture# The BayesianGaussianMixture object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar to the one defined by GaussianMixture. Estimation algorithm: variational inference Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical. Due to its Bayesian nature, the variational algorithm needs more hyperparameters than expectation-maximization, the most important of these being the concentration parameter weight_concentration_prior. Specifying a low value for the concentration prior will make the model put most of the weight on a few components and set the remaining components’ weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture. The parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data. The next figure compares the results obtained for the different types of the weight concentration prior (parameter weight_concentration_prior_type) for different values", "prev_chunk_id": "chunk_607", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_609", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1.2. Variational Bayesian Gaussian Mixture#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1.2. Variational Bayesian Gaussian Mixture#", "content": "of weight_concentration_prior. Here, we can see the value of the weight_concentration_prior parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is ‘dirichlet_distribution’ while this is not necessarily the case for the ‘dirichlet_process’ type (used by default). The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected n_components=5 which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component. On the following figure we are fitting a dataset not well-depicted by a Gaussian mixture. Adjusting the weight_concentration_prior, parameter of the BayesianGaussianMixture controls the number of components used to fit this data. We also present on the last two plots a random sampling generated from the two resulting mixtures. Examples - SeeGaussian Mixture Model Ellipsoidsfor an example on plotting the confidence ellipsoids for bothGaussianMixtureandBayesianGaussianMixture. - Gaussian Mixture Model Sine Curveshows usingGaussianMixtureandBayesianGaussianMixtureto fit a sine wave. - SeeConcentration Prior Type Analysis of Variation Bayesian Gaussian Mixturefor an example plotting the confidence ellipsoids for theBayesianGaussianMixturewith differentweight_concentration_prior_typefor different values of the parameterweight_concentration_prior.", "prev_chunk_id": "chunk_608", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_610", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1.2.1. The Dirichlet Process#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1.2.1. The Dirichlet Process#", "content": "2.1.2.1. The Dirichlet Process# Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on clusterings with an infinite, unbounded, number of partitions. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model. An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn’t fit this manual, one can think of its stick breaking process analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don’t fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller values of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights). Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the “true” number of components,", "prev_chunk_id": "chunk_609", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_611", "url": "https://scikit-learn.org/stable/modules/mixture.html", "title": "2.1.2.1. The Dirichlet Process#", "page_title": "2.1. Gaussian mixture models — scikit-learn 1.7.1 documentation", "breadcrumbs": "2.1.2.1. The Dirichlet Process#", "content": "affects only algorithmic complexity, not the actual number of components used).", "prev_chunk_id": "chunk_610", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_612", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.1. Multi-layer Perceptron#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.1. Multi-layer Perceptron#", "content": "1.17.1. Multi-layer Perceptron# Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function \\(f: R^m \\rightarrow R^o\\) by training on a dataset, where \\(m\\) is the number of dimensions for input and \\(o\\) is the number of dimensions for output. Given a set of features \\(X = \\{x_1, x_2, ..., x_m\\}\\) and a target \\(y\\), it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output. The leftmost layer, known as the input layer, consists of a set of neurons \\(\\{x_i | x_1, x_2, ..., x_m\\}\\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \\(w_1x_1 + w_2x_2 + ... + w_mx_m\\), followed by a non-linear activation function \\(g(\\cdot):R \\rightarrow R\\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values. The module contains the public attributes coefs_ and intercepts_. coefs_ is a list of weight matrices, where weight matrix at index \\(i\\) represents the weights between layer \\(i\\) and layer \\(i+1\\). intercepts_ is a list of bias vectors, where the vector at index \\(i\\) represents the bias values added to layer \\(i+1\\).", "prev_chunk_id": null, "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_613", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.2. Classification#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.2. Classification#", "content": "1.17.2. Classification# Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation. MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples: >>> from sklearn.neural_network import MLPClassifier >>> X = [[0., 0.], [1., 1.]] >>> y = [0, 1] >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5, ... hidden_layer_sizes=(5, 2), random_state=1) ... >>> clf.fit(X, y) MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1, solver='lbfgs') After fitting (training), the model can predict labels for new samples: >>> clf.predict([[2., 2.], [-1., -2.]]) array([1, 0]) MLP can fit a non-linear model to the training data. clf.coefs_ contains the weight matrices that constitute the model parameters: >>> [coef.shape for coef in clf.coefs_] [(2, 5), (5, 2), (2, 1)] Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the predict_proba method. MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \\(P(y|x)\\) per sample \\(x\\): >>> clf.predict_proba([[2., 2.], [1., 2.]]) array([[1.967e-04, 9.998e-01], [1.967e-04, 9.998e-01]]) MLPClassifier supports multi-class classification by applying Softmax as the output function. Further, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to 0. For a predicted output of a sample, the indices where the value is 1 represent the assigned classes of that sample: >>> X = [[0., 0.], [1., 1.]] >>> y = [[0, 1], [1, 1]] >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5, ... hidden_layer_sizes=(15,), random_state=1) ... >>>", "prev_chunk_id": "chunk_612", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_614", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.2. Classification#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.2. Classification#", "content": "clf.fit(X, y) MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1, solver='lbfgs') >>> clf.predict([[1., 2.]]) array([[1, 1]]) >>> clf.predict([[0., 0.]]) array([[0, 1]]) See the examples below and the docstring of MLPClassifier.fit for further information. Examples - Compare Stochastic learning strategies for MLPClassifier - SeeVisualization of MLP weights on MNISTfor visualized representation of trained weights.", "prev_chunk_id": "chunk_613", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_615", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.3. Regression#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.3. Regression#", "content": "1.17.3. Regression# Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values. MLPRegressor also supports multi-output regression, in which a sample can have more than one target.", "prev_chunk_id": "chunk_614", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_616", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.4. Regularization#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.4. Regularization#", "content": "1.17.4. Regularization# Both MLPRegressor and MLPClassifier use parameter alpha for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha. See the examples below for further information. Examples - Varying regularization in Multi-layer Perceptron", "prev_chunk_id": "chunk_615", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_617", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.5. Algorithms#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.5. Algorithms#", "content": "1.17.5. Algorithms# MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e. where \\(\\eta\\) is the learning rate which controls the step-size in the parameter space search. \\(Loss\\) is the loss function used for the network. More details can be found in the documentation of SGD Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments. With SGD or Adam, training supports online and mini-batch learning. L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of L-BFGS. If the selected solver is ‘L-BFGS’, training does not support online nor mini-batch learning.", "prev_chunk_id": "chunk_616", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_618", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.6. Complexity#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.6. Complexity#", "content": "1.17.6. Complexity# Suppose there are \\(n\\) training samples, \\(m\\) features, \\(k\\) hidden layers, each containing \\(h\\) neurons - for simplicity, and \\(o\\) output neurons. The time complexity of backpropagation is \\(O(i \\cdot n \\cdot (m \\cdot h + (k - 1) \\cdot h \\cdot h + h \\cdot o))\\), where \\(i\\) is the number of iterations. Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.", "prev_chunk_id": "chunk_617", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_619", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.7. Tips on Practical Use#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.7. Tips on Practical Use#", "content": "1.17.7. Tips on Practical Use# - Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply thesamescaling to the test set for meaningful results. You can useStandardScalerfor standardization.>>>fromsklearn.preprocessingimportStandardScaler>>>scaler=StandardScaler()>>># Don't cheat - fit only on training data>>>scaler.fit(X_train)>>>X_train=scaler.transform(X_train)>>># apply same transformation to test data>>>X_test=scaler.transform(X_test)An alternative and recommended approach is to useStandardScalerin aPipeline - Finding a reasonable regularization parameter\\(\\alpha\\)is best done usingGridSearchCV, usually in the range10.0**-np.arange(1,7). - Empirically, we observed thatL-BFGSconverges faster and with better solutions on small datasets. For relatively large datasets, however,Adamis very robust. It usually converges quickly and gives pretty good performance.SGDwith momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.", "prev_chunk_id": "chunk_618", "next_chunk_id": null, "type": "section"},
{"chunk_id": "chunk_620", "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "title": "1.17.8. More control with warm_start#", "page_title": "1.17. Neural network models (supervised) — scikit-learn 1.7.1 documentation", "breadcrumbs": "1.17.8. More control with warm_start#", "content": "1.17.8. More control with warm_start# If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using warm_start=True and max_iter=1 and iterating yourself can be helpful: >>> X = [[0., 0.], [1., 1.]] >>> y = [0, 1] >>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True) >>> for i in range(10): ... clf.fit(X, y) ... # additional monitoring / inspection MLPClassifier(...", "prev_chunk_id": "chunk_619", "next_chunk_id": null, "type": "section"}
]